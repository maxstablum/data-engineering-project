Paper title;Authors;Year of publication;Where it was published;Full abstract;Amount of citations
A Framework for Automated Testing;Thomas Fehlmann, Eberhard Kranich;2020;European Conference on Software Process Improvement;Abstract not available;6
Towards MLOps: A Case Study of ML Pipeline Platform;Yue Zhou, Yue Yu, Bo Ding;2020;2020 International Conference on Artificial Intelligence and Computer Engineering (ICAICE);The development and deployment of machine learning (ML) applications differ significantly from traditional applications in many ways, which have led to an increasing need for efficient and reliable production of ML applications and supported infrastructures. Though platforms such as TensorFlow Extended (TFX), ModelOps, and Kubeflow have provided end-to-end lifecycle management for ML applications by orchestrating its phases into multistep ML pipelines, their performance is still uncertain. To address this, we built a functional ML platform with DevOps capability from existing continuous integration (CI) or continuous delivery (CD) tools and Kubeflow, constructed and ran ML pipelines to train models with different layers and hyperparameters while time and computing resources consumed were recorded. On this basis, we analyzed the time and resource consumption of each step in the ML pipeline, explored the consumption concerning the ML platform and computational models, and proposed potential performance bottlenecks such as GPU utilization. Our work provides a valuable reference for ML pipeline platform construction in practice.;58
Euclid. I. Overview of the Euclid mission;Euclid Collaboration Y. Mellier, Abdurro’uf, J. A. A. Barroso, A. Ach'ucarro, J. Adamek, R. Adam, G. E. Addison, N. Aghanim, M. Aguena, V. Ajani, Y. Akrami, A. Al-Bahlawan, A. Alavi, I. S. Albuquerque, G. Alestas, G. Alguero, A. Allaoui, S. W. Allen, V. Allevato, A. V. Alonso-Tetilla, B. Altieri, A. Alvarez-Candal, A. Amara, L. Amendola, J. Amiaux, I. Andika, S. Andreon, A. Andrews, G. Angora, R. E. Angulo, F. Annibali, A. Anselmi, S. Anselmi, S. Arcari, M. Archidiacono, G. Aricò, M. Arnaud, S. Arnouts, M. Asgari, J. Asorey, L. Atayde, H. Atek, F. Atrio-Barandela, M. Aubert, É. Aubourg, T. Auphan, N. Auricchio, B. Aussel, H. Aussel, P. Avelino, A. Avgoustidis, S. Ávila, S. Awan, R. Azzollini, C. Baccigalupi, E. Bachelet, D. Bacon, M. Baes, M. Bagley, B. Bahr-Kalus, A. Balaguera-Antolínez, E. Balbinot, M. Balcells, M. Baldi, I. Baldry, A. Balestra, M. Ballardini, O. Ballester, M. Balogh, E. Bañados, R. Barbier, S. Bardelli, T. Barreiro, J. Barrière, B. J. Barros, A. Barthelemy, N. Bartolo, A. Basset, P. Battaglia, A. J. Battisti, C. M. Baugh, L. Baumont, L. Bazzanini, J. Beaulieu, V. Beckmann, A. N. Belikov, J. Bel, F. Bellagamba, M. Bella, E. Bellini, K. Benabed, R. Bender, G. Benevento, C. L. Bennett, K. Benson, P. Bergamini, J. Bermejo-Climent, F. Bernardeau, D. Bertacca, M. Berthé, J. Berthier, M. Béthermin, F. Beutler, C. Bevillon, S. Bhargava, R. Bhatawdekar, L. Bisigello, A. Biviano, R. Blake, A. Blanchard, J. Blazek, L. Blot, A. Bosco, C. Bodendorf, T. Boenke, H. Bohringer, M. Bolzonella, A. Bonchi, M. Bonici, D. Bonino, L. Bonino, C. Bonvin, W. Bon, J. T. Booth, S. Borgani, A. Borlaff, E. Borsato, B. Bose, M. Botticella, A. Boucaud, F. Bouche, J. Boucher, D. Boutigny, T. Bouvard, H. Bouy, R. Bowler, V. Bozza, E. Bozzo, E. Branchini, S. Brau-Nogué, P. Brekke, M. Bremer, M. Brescia, M.-A. Breton, J. Brinchmann, T. Brinckmann, C. Brockley-Blatt, M. Brodwin, L. Brouard, M. L. Brown, S. Bruton, J. Bucko, H. Buddelmeijer, G. Buenadicha, F. Buitrago, P. Burger, C. Burigana, V. Busillo, D. Busonero, R. Cabanac, L. Cabayol-Garcia, M. S. Cagliari, A. Caillat, L. Caillat, M. Calabrese, A. Calabrò, G. Calderone, F. Calura, B. C. Quevedo, S. Camera, L. Campos, G. Cañas-Herrera, G. Candini, M. Cantiello, V. Capobianco, E. Cappellaro, N. Cappelluti, A. Cappi, K. Caputi, C. Cara, C. Carbone, V. Cardone, E. Carella, R. Carlberg, M. Carle, L. Carminati, F. Caro, J. M. Carrasco, J. Carretero, P. Carrilho, J. C. Duque, B. Carry, A. Carvalho, C. Carvalho, R. Casas, S. Casas, P. Casenove, C. M. Casey, P. Cassata, F. Castander, D. Castelão, M. Castellano, L. Castiblanco, G. Castignani, T. Castro, C. Cavet, S. Cavuoti, P. Chabaud, K. Chambers, Y. Charles, S. Charlot, N. Chartab, R. Chary, F. Chaumeil, H. Cho, G. Chon, E. Ciancetta, P. Ciliegi, A. Cimatti, M. Cimino, M. Cioni, R. Claydon, C. Cleland, B. Cl'ement, D. Clements, N. Clerc, S. Clesse, S. Codis, F. Cogato, J. Colbert, R. Cole, P. Coles, T. Collett, R. S. Collins, C. Colodro-Conde, C. Colombo, F. Combes, V. Conforti, G. Congedo, S. Conseil, C. Conselice, S. Contarini, T. Contini, L. Conversi, A. Cooray, Y. Copin, Pier Stefano Corasaniti, P. Corcho-Caballero, L. Corcione, O. Cordes, O. Corpace, M. Correnti, M. Costanzi, A. Costille, F. Courbin, L. C. Mifsud, H. Courtois, M. Cousinou, G. Covone, T. Cowell, C. Cragg, G. Cresci, S. Cristiani, M. Crocce, M. Cropper, P. Crouzet, B. Csizi, J. Cuby, E. Cucchetti, O. Cucciati, J. Cuillandre, P. Cunha, V. Cuozzo, E. Daddi, M. D'Addona, C. Dafonte, N. Dagoneau, E. Dalessandro, G. Dalton, G. D'Amico, H. Dannerbauer, P. Danto, I. Das, A. D. Silva, R. D. Silva, G. Daste, J. E. Davies, S. Davini, T. D. Boer, R. Decarli, B. D. Caro, H. Degaudenzi, G. Degni, J. D. Jong, L. F. D. L. Bella, S. D. Torre, F. Delhaise, D. Delley, G. Delucchi, G. Lucia, J. Denniston, F. Paolis, M. Petris, A. Derosa, S. Desai, V. Desjacques, G. Despali, G. Desprez, J. D. Vicente-Albendea, Y. Deville, J. Dias, A. D'iaz-S'anchez, J. Diaz, S. Domizio, J. M. Diego, D. Ferdinando, A. D. Giorgio, P. Dimauro, J. Dinis, K. Dolag, C. Dolding, H. Dole, H. D. S'anchez, O. Dor'e, F. Dournac, M. Douspis, H. Dreihahn, B. Droge, B. Dryer, F. Dubath, P. Duc, F. Ducret, C. Duffy, F. Dufresne, C. Duncan, X. Dupac, V. Duret, R. Durrer, F. Durret, S. Dusini, A. Ealet, A. Eggemeier, P. Eisenhardt, D. Elbaz, M. Y. Elkhashab, A. Ellien, J. Endicott, A. Enia, T. Erben, J. Vigo, S. Escoffier, I. E. Sanz, J. Essert, S. Ettori, M. Ezziati, G. Fabbian, M. Fabricius, Y. Fang, A. Farina, M. Farina, R. Farinelli, S. Farrens, F. Faustini, A. Feltre, A. Ferguson, P. Ferrando, A. G. Ferrari, A. Ferr'e-Mateu, P. G. Ferreira, I. Ferreras, I. Ferrero, S. Ferriol, P. Ferruit, D. Filleul, F. Finelli, S. Finkelstein, A. Finoguenov, B. Fiorini, F. Flentge, P. Focardi, J. Fonseca, A. Fontana, F. Fontanot, F. Fornari, P. Fosalba, M. Fossati, S. Fotopoulou, D. Fouchez, N. Fourmanoit, M. Frailis, D. Fraix-Burnet, E. Franceschi, A. Franco, P. Franzetti, J. Freihoefer, G. Frittoli, P. Frugier, N. Frusciante, A. Fumagalli, M. Fumagalli, M. Fumana, Y. Fu, L. Gabarra, S. Galeotta, L. Galluccio, K. Ganga, H. Gao, J. Garc'ia-Bellido, K. Garcia, J. P. Gardner, B. Garilli, L.-M. Gaspar-Venancio, T. Gasparetto, V. Gautard, R. Gavazzi, E. Gaztañaga, L. Genolet, R. G. Santos, F. Gentile, K. George, Z. Ghaffari, F. Giacomini, F. Gianotti, G. P. S. Gibb, W. Gillard, B. Gillis, M. Ginolfi, C. Giocoli, M. Girardi, S. Giri, L. W. K. Goh, P. G'omez-Alvarez, A. H. Gonzalez, E. J. Gonzalez, J. C. Gonzalez, S. G. Beauchamps, G. Gozaliasl, J. Graciá-Carpio, S. Grandis, B. Granett, M. Granvik, A. Grazian, A. Gregorio, C. Grenet, C. Grillo, F. Grupp, C. Gruppioni, A. Gruppuso, C. Guerbuez, S. Guerrini, M. Guidi, P. Guillard, C. M. Gutierrez, P. Guttridge, L. Guzzo, S. Gwyn, J. Haapala, J. Haase, C. Haddow, M. Hailey, A. Hall, D. Hall, N. Hamaus, B. S. Haridasu, J. Harnois-D'eraps, C. Harper, W. Hartley, G. Hasinger, F. Hassani, N. A. Hatch, S. Haugan, B. Haussler, A. Heavens, L. Heisenberg, A. Helmi, G. Helou, S. Hemmati, K. Henares, O. Herent, C. Hern'andez-Monteagudo, T. Heuberger, P. Hewett, S. Heydenreich, H. Hildebrandt, M. Hirschmann, J. Hjorth, J. Hoar, H. Hoekstra, A. Holland, M. Holliman, W. Holmes, I. Hook, B. Horeau, F. Hormuth, A. Hornstrup, S. Hosseini, D. Hu, P. Hudelot, M. Hudson, M. Huertas-Company;2024;"Astronomy &amp; Astrophysics";The current standard model of cosmology successfully describes a variety of measurements, but the nature of its main ingredients, dark matter and dark energy, remains unknown. is a medium-class mission in the Cosmic Vision 2015--2025 programme of the European Space Agency (ESA) that will provide high-resolution optical imaging, as well as near-infrared imaging and spectroscopy, over about 14\,000\,deg$^2$ of extragalactic sky. In addition to accurate weak lensing and clustering measurements that probe structure formation over half of the age of the Universe, its primary probes for cosmology, these exquisite data will enable a wide range of science. This paper provides a high-level overview of the mission, summarising the survey characteristics, the various data-processing steps, and data products. We also highlight the main science objectives and expected performance.;26
The key to leveraging AI at scale;Deborah Leff, Kenneth T. K. Lim;2021;Journal of Revenue and Pricing Management;Abstract not available;6
Reliable Fleet Analytics for Edge IoT Solutions;E. Raj, M. Westerlund, L. E. Leal;2021;arXiv.org;In recent years we have witnessed a boom in Internet of Things (IoT) device deployments, which has resulted in big data and demand for low-latency communication. This shift in the demand for infrastructure is also enabling real-time decision making using artificial intelligence for IoT applications. Artificial Intelligence of Things (AIoT) is the combination of Artificial Intelligence (AI) technologies and the IoT infrastructure to provide robust and efficient operations and decision making. Edge computing is emerging to enable AIoT applications. Edge computing enables generating insights and making decisions at or near the data source, reducing the amount of data sent to the cloud or a central repository. In this paper, we propose a framework for facilitating machine learning at the edge for AIoT applications, to enable continuous delivery, deployment, and monitoring of machine learning models at the edge (Edge MLOps). The contribution is an architecture that includes services, tools, and methods for delivering fleet analytics at scale. We present a preliminary validation of the framework by performing experiments with IoT devices on a university campus's rooms. For the machine learning experiments, we forecast multivariate time series for predicting air quality in the respective rooms by using the models deployed in respective edge devices. By these experiments, we validate the proposed fleet analytics framework for efficiency and robustness.;10
Automating Tiny ML Intelligent Sensors DevOPS Using Microsoft Azure;Chandrasekar Vuppalapati, Anitha Ilapakurti, Karthik Chillara, Sharat Kedari, Vanaja Mamidi;2020;2020 IEEE International Conference on Big Data (Big Data);Microsoft Azure DevOps is a robust ,cross platform and powerful automation engine for script-based automation tools. Azure DevOPS enables to build, test, and deploy Cloud Native and/or Non-Cloud Native applications. The core principle and chief advantage that Azure DevOps provide are the availability of automation techniques such as infrastructure as code and the seamless integration of verifiable frameworks such as Machine Learning Operations (MLOps) with the DevOps automated pipelines to provision and configure the infrastructure that applications need to run.With the increase in application complexity and with the infusion of Machine Learning (ML) and Artificial Intelligence (AI) techniques as part of the software development lifecycle, the Azure DevOps is the most important framework that many organizations are rapidly progressing to incorporate it in their business processes to reduce the cost of building product and improve customer success.As part of the paper, we would like to propose a novel DevOps framework for building intelligent Tiny ML dairy agriculture sensors and the advantages that DevOps provide to develop high quality product in the most cost-efficient manner and serve small scale farmers who are at the bottom economic pyramid.;17
Edge Intelligence: The Convergence of Humans, Things, and AI;T. Rausch, S. Dustdar;2019;2019 IEEE International Conference on Cloud Engineering (IC2E);Edge AI and Human Augmentation are two major technology trends, driven by recent advancements in edge computing, IoT, and AI accelerators. As humans, things, and AI continue to grow closer together, systems engineers and researchers are faced with new and unique challenges. In this paper, we analyze the role of edge computing and AI in the cyber-human evolution, and identify challenges that edge computing systems will consequently be faced with. We take a closer look at how a cyber-physical fabric will be complemented by AI operationalization to enable seamless end-to-end edge intelligence systems.;48
The AIQ Meta-Testbed: Pragmatically Bridging Academic AI Testing and Industrial Q Needs;Markus Borg;2020;International Conference on Software Quality. Process Automation in Software Development;Abstract not available;15
Application of DevOps in the improvement of machine learning processes;Beatriz Mayumi, Andrade Matsui¹, Denise Hideko Goya²;2020;Publication venue not available;: This work deals with the concept of DevOps and its application in the improvement of machine learning processes. DevOps practices have been increasingly used by software development teams to automate and simplify processes ranging from integration, through testing, approval, implementation, to the ﬁnal delivery of the application to users. The present study aims to focus on the possibility of applying this concept also in teams that work with machine learning and could beneﬁt from the improvements brought with the adoption of DevOps.;1
"The Robot Head ""Flobi"": A Research Platform for Cognitive Interaction Technology";Sven Wachsmuth, Simon Schulz, Florian Lier, I. Lütkebohle;2012;Publication venue not available;"Founded on a vision of a human-friendly technology that 
adapts to users’ needs and is easy und intuitive for ordinary people to use, CITEC has established an exciting new field: Cognitive Interaction Technology. It aims to elucidate the principles and mechanisms of cognition in order to find ways of replicating them in technology and 
thus enable a new deep level of service and assistance. In order to proceed in this highly interdisciplinary field, appropriate research platforms and infrastructure are needed. The anthropomorphic robot head “Flobi” combines state-of-the-art sensing functionality with an exterior that elicits a sympathetic emotion response. In order to support several lines of research and at the same time ensure the maintainability of the software and hardware components, a virtual realization of the Flobi head has 
been proposed that allows an efficient prototyping, systematic testing, and software development in a continuous integration framework.";6
Software Architecture Best Practices for Enterprise Artificial Intelligence;Yann Martel, Arne Roßmann, E. Sultanow, O. Weiß, Matthias Wissel, Frank Pelzel, Matthias Seßler;2020;GI-Jahrestagung;Abstract not available;3
Best practices for artificial intelligence in life sciences research.;V. Makarov, T. Stouch, Brandon Allgood, Chris D. Willis, Nick Lynch;2020;Drug Discovery Today;We describe 11 best practices for the successful use of artificial intelligence and machine learning in pharmaceutical and biotechnology research at the data, technology and organizational management levels.;18
Artificial Social Intelligence: Hotel Rate Prediction;James J. Lee, Misuk Lee;2020;Advances in Intelligent Systems and Computing;Abstract not available;1
ModelOps: Cloud-Based Lifecycle Management for Reliable and Trusted AI;W. Hummer, Vinod Muthusamy, T. Rausch, Parijat Dube, K. E. Maghraoui, Anupama Murthi, Punleuk Oum;2019;2019 IEEE International Conference on Cloud Engineering (IC2E);This paper proposes a cloud-based framework and platform for end-to-end development and lifecycle management of artificial intelligence (AI) applications. We build on our previous work on platform-level support for cloud-managed deep learning services, and show how the principles of software lifecycle management can be leveraged and extended to enable automation, trust, reliability, traceability, quality control, and reproducibility of AI pipelines. Based on a discussion of use cases and current challenges, we describe a framework for managingAI application lifecycles and its key components. We also show concrete examples that illustrate how this framework enables managing and executing model training and continuous learning pipelines while infusing trusted AI principles.;79
Automated Trainability Evaluation for Smart Software Functions;I. Gerostathopoulos, Stefan Kugele, Christoph Segler, T. Bures, A. Knoll;2019;International Conference on Automated Software Engineering;More and more software-intensive systems employ machine learning and runtime optimization to improve their functionality by providing advanced features (e. g. personal driving assistants or recommendation engines). Such systems incorporate a number of smart software functions (SSFs) which gradually learn and adapt to the users' preferences. A key property of SSFs is their ability to learn based on data resulting from the interaction with the user (implicit and explicit feedback)-which we call trainability. Newly developed and enhanced features in a SSF must be evaluated based on their effect on the trainability of the system. Despite recent approaches for continuous deployment of machine learning systems, trainability evaluation is not yet part of continuous integration and deployment (CID) pipelines. In this paper, we describe the different facets of trainability for the development of SSFs. We also present our approach for automated trainability evaluation within an automotive CID framework which proposes to use automated quality gates for the continuous evaluation of machine learning models. The results from our indicative evaluation based on real data from eight BMW cars highlight the importance of continuous and rigorous trainability evaluation in the development of SSFs.;6
MLModelCI: An Automatic Cloud Platform for Efficient MLaaS;Huaizheng Zhang, Yuanming Li, Yizheng Huang, Yonggang Wen, Jianxiong Yin, K. Guan;2020;ACM Multimedia;MLModelCI provides multimedia researchers and developers with a one-stop platform for efficient machine learning (ML) services. The system leverages DevOps techniques to optimize, test, and manage models. It also containerizes and deploys these optimized and validated models as cloud services (MLaaS). In its essence, MLModelCI serves as a housekeeper to help users publish models. The models are first automatically converted to optimized formats for production purpose and then profiled under different settings (e.g., batch size and hardware). The profiling information can be used as guidelines for balancing the trade-off between performance and cost of MLaaS. Finally, the system dockerizes the models for ease of deployment to cloud environments. A key feature of MLModelCI is the implementation of a controller, which allows elastic evaluation which only utilizes idle workers while maintaining online service quality. Our system bridges the gap between current ML training and serving systems and thus free developers from manual and tedious work often associated with service deployment. We release the platform as an open-source project on GitHub under Apache 2.0 license, with the aim that it will facilitate and streamline more large-scale ML applications and research projects.;22
Using Continuous Integration to organize and monitor the annotation process of domain specific corpora;Marc Schreiber, Kai Barkschat, B. Kraft;2014;International Conference on Information, Communications and Signal Processing;Applications in the World Wide Web aggregate vast amounts of information from different data sources. The aggregation process is often implemented with Extract, Transform and Load (ETL) processes. Usually ETL processes require information for aggregation available in structured formats, e. g. XML or JSON. In many cases the information is provided in natural language text which makes the application of ETL processes impractical. Due to the fact that information is provided in natural language, Information Extraction (IE) systems have been evolved. They make use of Natural Language Processing (NLP) tools to derive meaning from natural language text. State-of-the-art NLP tools apply Machine Learning methods. These NLP tools perform on newspapers with good quality, but they drop accuracy in other domains. However, to improve the quality for IE systems in specific domains often NLP tools are trained on domain specific text which is a time consuming process. This paper introduces an approach using a Continuous Integration pipeline for organizing and monitoring the annotation process on domain specific corpora.;4
Ease. ML: A Lifecycle Management System for Machine Learning;L. A. Melgar, David Dao, Shaoduo Gan, Nezihe Merve Gürel, Nora Hollenstein, Jiawei Jiang, Bojan Karlas, Thomas Lemmin, Tian Li, Yang Li, S. Rao, Johannes Rausch, Cédric Renggli, Luka Rimanic, Maurice Weber, Shuai Zhang, Zhikuan Zhao, K. Schawinski, Wentao Wu, Ce Zhang;2021;Publication venue not available;Abstract not available;2
From a Data Science Driven Process to a Continuous Delivery Process for Machine Learning Systems;Lucy Ellen Lwakatare, I. Crnkovic, Ellinor Rånge, J. Bosch;2020;International Conference on Product Focused Software Process Improvement;Abstract not available;14
Building Continuous Integration Services for Machine Learning;Bojan Karlas, Matteo Interlandi, Cédric Renggli, Wentao Wu, Ce Zhang, Deepak Mukunthu Iyappan Babu, Jordan Edwards, Chris Lauren, Andy Xu, Markus Weimer;2020;Knowledge Discovery and Data Mining;Continuous integration (CI) has been a de facto standard for building industrial-strength software. Yet, there is little attention towards applying CI to the development of machine learning (ML) applications until the very recent effort on the theoretical side. In this paper, we take a step forward to bring the theory into practice. We develop the first CI system for ML, to the best of our knowledge, that integrates seamlessly with existing ML development tools. We present its design and implementation details.;24
Deploying a Cost-Effective and Production-Ready Deep News Recommender System in the Media Crisis Context;Jean-Philippe Corbeil, Florent Daudens;2020;ORSUM@RecSys;In the actual context of the media crisis, online media companies need cost-effective technological solutions to stay competitive against huge monopolistic software companies massively feeding content to users. News recommender systems are well-suited solutions, even if current commercial solutions are well above most online media’s budget. In this paper, we present a case study of our deployed deep news recommender system at Le Devoir , an independent french Canadian newspaper in the province of Que-bec. We expose the software architecture and the issues we have met with their solutions. Furthermore, we present four qualitative and quantitative analyses done with our custom monitoring dashboard: offline performances of our models, embedding space analysis, fake-user testing and high-traffic simulations. For a tiny fraction of the available commercial solutions’ prices, our current simple software architecture based on the Docker, the Kubernetes and open-source technologies in the cloud has demonstrated to be easily maintainable, scalable, and cost-effective. It also shows excellent offline performance and generates high-quality embeddings as well as relevant recommendations.;2
An IoT Beehive Network for Monitoring Urban Biodiversity: Vision, Method, and Architecture;Mirella Sangiovanni, G.P.J. Schouten, W. Heuvel;2020;Symposium and Summer School on Service-Oriented Computing;Abstract not available;4
A Cloud-Based Framework for Machine Learning Workloads and Applications;Álvaro López García, V. Tran, Andy S. Alic, M. Caballer, I. C. Plasencia, A. Costantini, Stefan Dlugolinsky, D. C. Duma, G. Donvito, J. Gomes, Ignacio Heredia Cachá, J. M. De Lucas, Keiichi Ito, V. Kozlov, Giang T. Nguyen, Pablo Orviz Fernández, Z. Šustr, P. Wolniewicz, M. Antonacci, W. zu Castell, M. David, M. Hardt, L. Lloret Iglesias, Germán Moltó, M. Plóciennik;2020;IEEE Access;In this paper we propose a distributed architecture to provide machine learning practitioners with a set of tools and cloud services that cover the whole machine learning development cycle: ranging from the models creation, training, validation and testing to the models serving as a service, sharing and publication. In such respect, the DEEP-Hybrid-DataCloud framework allows transparent access to existing e-Infrastructures, effectively exploiting distributed resources for the most compute-intensive tasks coming from the machine learning development cycle. Moreover, it provides scientists with a set of Cloud-oriented services to make their models publicly available, by adopting a serverless architecture and a DevOps approach, allowing an easy share, publish and deploy of the developed models.;55
A Development Platform of Intelligent Mobile APP Based on Edge Computing;Wei-chen Liu, Chiang Ting Yu, Tyng-Yeu Liang;2019;2019 Seventh International Symposium on Computing and Networking Workshops (CANDARW);In this paper, we propose a platform based on the architecture of edge computing for the development of intelligent mobile APP. The proposed platform has a number of advantages as follows. First, it supports static and dynamic visualization for reducing the programming and training effort of deep learning models. Second, it offers a runtime library for mobile APPs to exploit external deep learning models and make use of edge servers for the execution of the models. Third, it provides a friendly user interface to partition a deep learning model and execute the model by different work modes for performance optimization. Forth, it can dynamically redirect the requests of mobile APPs for achieving load balance and minimizing the response time of edge servers.;1
Summary: A Domain Specific-Model and DevOps Approach for Big Data Analytics Architectures (short paper);Camilo Castellanos, Carlos A. Varela, Darío Correal;2021;European Conference on Software Architecture;Big data analytics (BDA) applications use machine learning algorithms to extract insights from large, fast, and heterogeneous data sources. Software engineering challenges for BDA applications include ensuring performance levels of data-driven algorithms even in the presence of large data volume, velocity, and variety (3Vs). BDA software complexity frequently leads to delayed deployments and challenging performance assessments. This paper1 proposes a domain-specific modeling (DSM) and DevOps practices to design, deploy, andmonitor performancemetrics for BDA architectures. Our proposal includes a design process and framework to define architectural inputs, functional, and deployment views via integrated high-level abstractions to monitor the achievement of quality scenarios. We evaluate our approach with four use cases from different domains. Our results show shorter deployment and monitoring times and a higher gain factor per iteration than similar approaches.;0
Challenges and Experiences with MLOps for Performance Diagnostics in Hybrid-Cloud Enterprise Software Deployments;Amit Banerjee, Chien-Chia Chen, Chien-Chun Hung, Xiaobo Huang, Yifan Wang, Razvan Chevesaran;2020;USENIX Conference on Operational Machine Learning;This paper presents how VMware addressed the following challenges in operationalizing our ML-based performance di-agnostics solution in enterprise hybrid-cloud environments: data governance, model serving and deployment, dealing with system performance drifts, selecting model features, centralized model training pipeline, setting the appropriate alarm threshold, and explainability. We also share the lessons and experiences we learned over the past four years in deploying ML operations at scale for enterprise customers.;22
Sustainable MLOps: Trends and Challenges;D. Tamburri;2020;Symposium on Symbolic and Numeric Algorithms for Scientific Computing;Even simply through a GoogleTrends search it becomes clear that Machine-Learning Operations-or MLOps, for short-are climbing in interest from both a scientific and practical perspective. On the one hand, software components and middleware are proliferating to support all manners of MLOps, from AutoML (i.e., software which enables developers with limited machine-learning expertise to train high-quality models specific to their domain or data) to feature-specific ML engineering, e.g., Explainability and Interpretability. On the other hand, the more these platforms penetrate the day-to-day activities of software operations, the more the risk for AI Software becoming unsustainable from a social, technical, or organisational perspective. This paper offers a concise definition of MLOps and AI Software Sustainability and outlines key challenges in its pursuit.;74
CodeReef: an open platform for portable MLOps, reusable automation actions and reproducible benchmarking;G. Fursin, H. Guillou, Nicolas Essayan;2020;arXiv.org;We present CodeReef - an open platform to share all the components necessary to enable cross-platform MLOps (MLSysOps), i.e. automating the deployment of ML models across diverse systems in the most efficient way. We also introduce the CodeReef solution - a way to package and share models as non-virtualized, portable, customizable and reproducible archive files. Such ML packages include JSON meta description of models with all dependencies, Python APIs, CLI actions and portable workflows necessary to automatically build, benchmark, test and customize models across diverse platforms, AI frameworks, libraries, compilers and datasets. We demonstrate several CodeReef solutions to automatically build, run and measure object detection based on SSD-Mobilenets, TensorFlow and COCO dataset from the latest MLPerf inference benchmark across a wide range of platforms from Raspberry Pi, Android phones and IoT devices to data centers. Our long-term goal is to help researchers share their new techniques as production-ready packages along with research papers to participate in collaborative and reproducible benchmarking, compare the different ML/software/hardware stacks and select the most efficient ones on a Pareto frontier using online CodeReef dashboards.;16
Who Needs MLOps: What Data Scientists Seek to Accomplish and How Can MLOps Help?;Sasu Mäkinen, Henrik Skogström, Eero Laaksonen, T. Mikkonen;2021;Workshop on AI Engineering - Software Engineering for AI;"Following continuous software engineering practices, there has been an increasing interest in rapid deployment of machine learning (ML) features, called MLOps. In this paper, we study the importance of MLOps in the context of data scientists’ daily activities, based on a survey where we collected responses from 331 professionals from 63 different countries in ML domain, indicating on what they were working on in the last three months. Based on the results, up to 40% respondents say that they work with both models and infrastructure; the majority of the work revolves around relational and time series data; and the largest categories of problems to be solved are predictive analysis, time series data, and computer vision. The biggest perceived problems revolve around data, although there is some awareness of problems related to deploying models to production and related procedures. To hypothesise, we believe that organisations represented in the survey can be divided to three categories – (i) figuring out how to best use data; (ii) focusing on building the first models and getting them to production; and (iii) managing several models, their versions and training datasets, as well as retraining and frequent deployment of retrained models. In the results, the majority of respondents are in category (i) or (ii), focusing on data and models; however the benefits of MLOps only emerge in category (iii) when there is a need for frequent retraining and redeployment. Hence, setting up an MLOps pipeline is a natural step to take, when an organization takes the step from ML as a proof-of-concept to ML as a part of nominal activities.";129
MLOps approach in the cloud-native data pipeline design;István Pölöskei;2021;Acta Technica Jaurinensis;The data modeling process is challenging and involves hypotheses and trials. In the industry, a workflow has been constructed around data modeling. The offered modernized workflow expects to use of the cloud’s full abilities as cloud-native services. For a flourishing big data project, the organization should have analytics and information-technological know-how. MLOps approach concentrates on the modeling, eliminating the personnel and technology gap in the deployment. In this article, the paradigm will be verified with a case-study in the context of composing a data pipeline in the cloud-native ecosystem. Based on the analysis, the considered strategy is the recommended way for data pipeline design.;8
Ethics-by-design: the next frontier of industrialization;Aurélie Bourgais, Issam Ibnouhsein;2021;AI and Ethics;Abstract not available;5
Artificial Intelligence in Organizations: Current State and Future Opportunities;Hind Benbya, T. Davenport, S. Pachidi;2020;Social Science Research Network;Artificial intelligence (AI) is currently viewed as the most important and disruptive new technology for large organizations. However, the technology is still in a relatively early state in large enterprises, and largely absent from smaller ones other than technology startups. Surveys suggest that fewer than half of large organizations have meaningful AI initiatives underway, although the percentage is increasing over time. This essay titled “AI in organizations: current state and future opportunities” details current challenges and implications that might arise from AI applications, and the ways to overcome such challenges to realize the potential of this emerging technology. First, the paper provide a brief history of AI and an overview of AI typologies. We discuss current challenges, implications and future opportunities regarding AI. Finally, we summarize the special issue articles and highlight the contributions each makes.;114
Understanding Machine Learning Model Updates Based on Changes in Feature Attributions;Fan Yun, Toshiki Shibahara, Y. Ohsita, Daiki Chiba, Mitsuaki Akiyama;2020;Publication venue not available;: Machine learning operations (MLOps) are widely adopted in various real-world machine learning (ML) systems. To ensure ML model performance in such systems, it is crucial to minimize the inﬂuence of insu ﬃ cient data and concept drift. One simple and e ﬀ ective solution is to update models with newly added data. In MLOps, after updating a model, accuracy scores are usually used to validate the model. However, it is hard to obtain detailed information regarding the causes of performance changes. We therefore propose a method for understanding ML model updates by using a feature attribution method called Shapley additive explanations (SHAP), which explains the output of a ML model by assigning an importance value called a SHAP value to each feature. We calculate SHAP values using models before and after updates to investigate changes in SHAP values. By analyzing the extent of changes, we can identify the slight changes in models due to updates and the data related to the changes.;2
Adoption and Effects of Software Engineering Best Practices in Machine Learning;A. Serban, K. Blom, H. Hoos, Joost Visser;2020;International Symposium on Empirical Software Engineering and Measurement;Background. The increasing reliance on applications with machine learning (ML) components calls for mature engineering techniques that ensure these are built in a robust and future-proof manner. Aim. We aim to empirically determine the state of the art in how teams develop, deploy and maintain software with ML components. Method. We mined both academic and grey literature and identified 29 engineering best practices for ML applications. We conducted a survey among 313 practitioners to determine the degree of adoption for these practices and to validate their perceived effects. Using the survey responses, we quantified practice adoption, differentiated along demographic characteristics, such as geography or team size. We also tested correlations and investigated linear and non-linear relationships between practices and their perceived effect using various statistical models. Results. Our findings indicate, for example, that larger teams tend to adopt more practices, and that traditional software engineering practices tend to have lower adoption than ML specific practices. Also, the statistical models can accurately predict perceived effects such as agility, software quality and traceability, from the degree of adoption for specific sets of practices. Combining practice adoption rates with practice importance, as revealed by statistical models, we identify practices that are important but have low adoption, as well as practices that are widely adopted but are less important for the effects we studied. Conclusion. Overall, our survey and the analysis of responses received provide a quantitative basis for assessment and step-wise improvement of practice adoption by ML teams.;106
Software Engineering for Machine Learning: A Case Study;Saleema Amershi, Andrew Begel, C. Bird, R. Deline, H. Gall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, Thomas Zimmermann;2019;2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP);"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.";691
TFX: A TensorFlow-Based Production-Scale Machine Learning Platform;Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, M. Ispir, Vihan Jain, L. Koc, C. Koo, Lukasz Lew, Clemens Mewald, A. Modi, N. Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, M. Wicke, Jarek Wilkiewicz, Xin Zhang, Martin A. Zinkevich;2017;Knowledge Discovery and Data Mining;Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful orchestration of many components---a learner for generating models based on training data, modules for analyzing and validating both data as well as models, and finally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such orchestration is often done ad hoc using glue code and custom scripts developed by individual teams for specific use cases, leading to duplicated effort and fragile systems with high technical debt. We present TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at Google. By integrating the aforementioned components into one platform, we were able to standardize the components, simplify the platform configuration, and reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions. We present the case study of one deployment of TFX in the Google Play app store, where the machine learning models are refreshed continuously as new data arrive. Deploying TFX led to reduced custom code, faster experiment cycles, and a 2% increase in app installs resulting from improved data and model analysis.;391
Bighead: A Framework-Agnostic, End-to-End Machine Learning Platform;E. Brumbaugh, Atul S. Kale, Alfredo Luque, Bahador B. Nooraei, John Park, Krishna P. N. Puttaswamy, Kyle Schiller, E. Shapiro, Conglei Shi, Aaron Siegel, N. Simha, Mani Bhushan, Marie Sbrocca, Shi-Jing Yao, P. Yoon, Varant Zanoyan, Xiao-Han T. Zeng, Qiang Zhu, Andrew Cheong, Michelle Du, Jeff Feng, N. Handel, Andrew Hoh, J. Hone, Brad Hunter;2019;International Conference on Data Science and Advanced Analytics;With the increasing need to build systems and products powered by machine learning inside organizations, it is critical to have a platform that provides machine learning practitioners with a unified environment to easily prototype, deploy, and maintain their models at scale. However, due to the diversity of machine learning libraries, the inconsistency between environments, and various scalability requirement, there is no existing work to date that addresses all of these challenges. Here, we introduce Bighead, a framework-agnostic, end-to-end platform for machine learning. It offers a seamless user experience requiring only minimal efforts that span feature set management, prototyping, training, batch (offline) inference, real-time (online) inference, evaluation, and model lifecycle management. In contrast to existing platforms, it is designed to be highly versatile and extensible, and supports all major machine learning frameworks, rather than focusing on one particular framework. It ensures consistency across different environments and stages of the model lifecycle, as well as across data sources and transformations. It scales horizontally and elastically in response to the workload such as dataset size and throughput. Its components include a feature management framework, a model development toolkit, a lifecycle management service with UI, an offline training and inference engine, an online inference service, an interactive prototyping environment, and a Docker image customization tool. It is the first platform to offer a feature management component that is a general-purpose aggregation framework with lambda architecture and temporal joins. Bighead is deployed and widely adopted at Airbnb, and has enabled the data science and engineering teams to develop and deploy machine learning models in a timely and reliable manner. Bighead has shortened the time to deploy a new model from months to days, ensured the stability of the models in production, facilitated adoption of cutting-edge models, and enabled advanced machine learning based product features of the Airbnb platform. We present two use cases of productionizing models of computer vision and natural language processing.;13
TensorFlow-Serving: Flexible, High-Performance ML Serving;Christopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, Jordan Soyke;2017;arXiv.org;We describe TensorFlow-Serving, a system to serve machine learning models inside Google which is also available in the cloud and via open-source. It is extremely flexible in terms of the types of ML platforms it supports, and ways to integrate with systems that convey new models and updated versions from training to serving. At the same time, the core code paths around model lookup and inference have been carefully optimized to avoid performance pitfalls observed in naive implementations. Google uses it in many production deployments, including a multi-tenant model hosting service called TFS^2.;281
Model Governance: Reducing the Anarchy of Production ML;Vinay Sridhar, Sriram Ganapathi Subramanian, D. Arteaga, S. Sundararaman, D. Roselli, Nisha Talagala;2018;USENIX Annual Technical Conference;As the influence of machine learning grows over decisions in businesses and human life, so grows the need for Model Governance. In this paper, we motivate the need for, define the problem of, and propose a solution for Model Governance in production ML. We show that through our approach one can meaningfully track and understand the who, where, what, when, and how an ML prediction came to be. To the best of our knowledge, this is the first work providing a comprehensive framework for production Model Governance, building upon previous work in developer-focused Model Management.;29
ModelHub: Deep Learning Lifecycle Management;Hui Miao, Ang Li, L. Davis, A. Deshpande;2017;IEEE International Conference on Data Engineering;Deep learning has improved the state-of-the-art results in many domains, leading to the development of several systems for facilitating deep learning. Current systems, however, mainly focus on model building and training phases, while the issues of lifecycle management are largely ignored. Deep learning modeling lifecycle contains a rich set of artifacts and frequently conducted tasks, dealing with them is cumbersome and left to the users. To address these issues in a comprehensive manner, we demonstrate ModelHub, which includes a novel model versioning system (dlv), a domain-specific language for searching through model space (DQL), and a hosted service (ModelHub). Video: https://youtu.be/4JVehm5Ohg4.;61
MLOp Lifecycle Scheme for Vision-based Inspection Process in Manufacturing;Ju-Taek Lim, Hoejoo Lee, Youngmin Won, Hunje Yeon;2019;USENIX Conference on Operational Machine Learning;Recent advances in machine learning and the proliferation of edge computing have enabled manufacturing industry to integrate machine learning into its operation to boost productivity. In addition to building high performing machine learning models, stakeholders and infrastructures within the industry should be taken into an account in building an operational lifecycle. In this paper, a practical machine learning operation scheme to build the vision inspection process is proposed, which is mainly motivated from ﬁeld experiences in applying the system in large scale corporate manufacturing plants. We evaluate our scheme in four defect inspection lines in production. The results show that deep neural network models outperform existing algorithms and the scheme is easily extensible to other manufacturing processes.;12
Innovative Devops for Artificial Intelligence;Radu Ciucu, F. Adochiei, I. Adochiei, F. Argatu, G. Seritan, B. Enache, S. Grigorescu, V. Argatu;2019;The Scientific Bulletin of Electrical Engineering Faculty;Abstract Developing Artificial Intelligence is a labor intensive task. It implies both storage and computational resources. In this paper, we present a state-of-the-art service based infrastructure for deploying, managing and serving computational models alongside their respective data-sets and virtual environments. Our architecture uses key-based values to store specific graphs and datasets into memory for fast deployment and model training, furthermore leveraging the need for manual data reduction in the drafting and retraining stages. To develop the platform, we used clustering and orchestration to set up services and containers that allow deployment within seconds. In this article, we cover high performance computing concepts such as swarming, GPU resource management for model implementation in production environments with emphasis on standardized development to reduce integration tasks and performance optimization.;9
Scalable Multi-Framework Multi-Tenant Lifecycle Management of Deep Learning Training Jobs;S. Boag, Parijat Dube, Benjamin Herta, W. Hummer, Vatche Isahagian, K. R. Jayaram, M. Kalantar, Vinod Muthusamy, P. Nagpurkar, Florian Rosenberg;2017;Publication venue not available;With the ongoing rise and phenomenal success of machine learning (ML), particularly deep learning, efficient training of large neural network models in scalable cloud infrastructures becomes a priority. ML workloads have traditionally been run in high-performance computing (HPC) environments, where users log in to dedicated machines and utilize the attached GPUs to run jobs that train models on huge datasets. Providing a similar user experience in a multi-tenant cloud environment comes with its own unique challenges regarding fault tolerance, performance, and security. We tackle these challenges and present a deep learning stack specifically designed for on-demand cloud environments. Based on a detailed discussion of the system architecture, we examine real usage data from internal users, and discuss performance experiments that illustrate the scalability of the system.;15
Towards Unified Data and Lifecycle Management for Deep Learning;Hui Miao, Ang Li, L. Davis, A. Deshpande;2016;IEEE International Conference on Data Engineering;Deep learning has improved state-of-the-art results in many important fields, and has been the subject of much research in recent years, leading to the development of several systems for facilitating deep learning. Current systems, however, mainly focus on model building and training phases, while the issues of data management, model sharing, and lifecycle management are largely ignored. Deep learning modeling lifecycle generates a rich set of data artifacts, e.g., learned parameters and training logs, and it comprises of several frequently conducted tasks, e.g., to understand the model behaviors and to try out new models. Dealing with such artifacts and tasks is cumbersome and largely left to the users. This paper describes our vision and implementation of a data and lifecycle management system for deep learning. First, we generalize model exploration and model enumeration queries from commonly conducted tasks by deep learning modelers, and propose a high-level domain specific language (DSL), inspired by SQL, to raise the abstraction level and thereby accelerate the modeling process. To manage the variety of data artifacts, especially the large amount of checkpointed float parameters, we design a novel model versioning system (dlv), and a read-optimized parameter archival storage system (PAS) that minimizes storage footprint and accelerates query workloads with minimal loss of accuracy. PAS archives versioned models using deltas in a multi-resolution fashion by separately storing the less significant bits, and features a novel progressive query (inference) evaluation algorithm. Third, we develop e cient algorithms for archiving versioned models using deltas under co-retrieval constraints. We conduct extensive experiments over several real datasets from computer vision domain to show the e ciency of the proposed techniques.;123
DLHub: Model and Data Serving for Science;Ryan Chard, Zhuozhao Li, K. Chard, Logan T. Ward, Y. Babuji, A. Woodard, S. Tuecke, B. Blaiszik, M. Franklin, Ian T Foster;2018;IEEE International Parallel and Distributed Processing Symposium;"While the Machine Learning (ML) landscape is evolving rapidly, there has been a relative lag in the development of the ""learning systems"" needed to enable broad adoption. Furthermore, few such systems are designed to support the specialized requirements of scientific ML. Here we present the Data and Learning Hub for science (DLHub), a multi-tenant system that provides both model repository and serving capabilities with a focus on science applications. DLHub addresses two significant shortcomings in current systems. First, its self-service model repository allows users to share, publish, verify, reproduce, and reuse models, and addresses concerns related to model reproducibility by packaging and distributing models and all constituent components. Second, it implements scalable and low-latency serving capabilities that can leverage parallel and distributed computing resources to democratize access to published models through a simple web interface. Unlike other model serving frameworks, DLHub can store and serve any Python 3-compatible model or processing function, plus multiple-function pipelines. We show that relative to other model serving systems including TensorFlow Serving, SageMaker, and Clipper, DLHub provides greater capabilities, comparable performance without memoization and batching, and significantly better performance when the latter two techniques can be employed. We also describe early uses of DLHub for scientific applications.";71
Data Validation for Machine Learning;Eric Breck, N. Polyzotis, Sudip Roy, Steven Euijong Whang, Martin A. Zinkevich;2019;USENIX workshop on Tackling computer systems problems with machine learning techniques;Abstract not available;242
Ease.ml in Action: Towards Multi-tenant Declarative Learning Services;Bojan Karlas, Ji Liu, Wentao Wu, Ce Zhang;2018;Proceedings of the VLDB Endowment;"
 We demonstrate ease.ml, a multi-tenant machine learning service we host at ETH Zurich for various research groups. Unlike existing machine learning services, ease.ml presents a novel architecture that supports multi-tenant, cost-aware model selection that optimizes for minimizing total regrets of all users. Moreover, it provides a novel user interface that enables
 declarative
 machine learning at a higher level: Users only need to specify the input/output schemata of their learning tasks and ease.ml can handle the rest. In this demonstration, we present the design principles of ease.ml, highlight the implementation of its key components, and showcase how ease.ml can help ease machine learning tasks that often perplex even experienced users.
";12
ModelDB: a system for machine learning model management;Manasi Vartak, H. Subramanyam, Wei-En Lee, S. Viswanathan, S. Husnoo, S. Madden, M. Zaharia;2016;HILDA '16;"Building a machine learning model is an iterative process. A data scientist will build many tens to hundreds of models before arriving at one that meets some acceptance criteria (e.g. AUC cutoff, accuracy threshold). However, the current style of model building is ad-hoc and there is no practical way for a data scientist to manage models that are built over time. As a result, the data scientist must attempt to ""remember"" previously constructed models and insights obtained from them. This task is challenging for more than a handful of models and can hamper the process of sensemaking. Without a means to manage models, there is no easy way for a data scientist to answer questions such as ""Which models were built using an incorrect feature?"", ""Which model performed best on American customers?"" or ""How did the two top models compare?"" In this paper, we describe our ongoing work on ModelDB, a novel end-to-end system for the management of machine learning models. ModelDB clients automatically track machine learning models in their native environments (e.g. scikit-learn, spark.ml), the ModelDB backend introduces a common layer of abstractions to represent models and pipelines, and the ModelDB frontend allows visual exploration and analyses of models via a web-based interface.";222
Accelerating the Machine Learning Lifecycle with MLflow;M. Zaharia, A. Chen, A. Davidson, A. Ghodsi, S. Hong, A. Konwinski, Siddharth Murching, Tomas Nykodym, Paul Ogilvie, Mani Parkhe, Fen Xie, Corey Zumar;2018;IEEE Data Engineering Bulletin;Machine learning development creates multiple new challenges that are not present in a traditional software development lifecycle. These include keeping track of the myriad inputs to an ML application (e.g., data versions, code and tuning parameters), reproducing results, and production deployment. In this paper, we summarize these challenges from our experience with Databricks customers, and describe MLﬂow, an open source platform we recently launched to streamline the machine learning lifecycle. MLﬂow covers three key challenges: experimentation, reproducibility, and model deployment, using generic APIs that work with any ML library, algorithm and programming language. The project has a rapidly growing open source community, with over 50 contributors since its launch in June 2018.;309
KETOS: Clinical decision support and machine learning as a service – A training and deployment platform based on Docker, OMOP-CDM, and FHIR Web Services;Julian Gruendner, Thorsten Schwachhofer, Phillip Sippl, Nicolas Wolf, Marcel Erpenbeck, Christian Gulden, L. Kapsner, J. Zierk, Sebastian Mate, M. Stürzl, R. Croner, H. Prokosch, Dennis Toddenroth;2019;PLoS ONE;Background and objective To take full advantage of decision support, machine learning, and patient-level prediction models, it is important that models are not only created, but also deployed in a clinical setting. The KETOS platform demonstrated in this work implements a tool for researchers allowing them to perform statistical analyses and deploy resulting models in a secure environment. Methods The proposed system uses Docker virtualization to provide researchers with reproducible data analysis and development environments, accessible via Jupyter Notebook, to perform statistical analysis and develop, train and deploy models based on standardized input data. The platform is built in a modular fashion and interfaces with web services using the Health Level 7 (HL7) Fast Healthcare Interoperability Resources (FHIR) standard to access patient data. In our prototypical implementation we use an OMOP common data model (OMOP-CDM) database. The architecture supports the entire research lifecycle from creating a data analysis environment, retrieving data, and training to final deployment in a hospital setting. Results We evaluated the platform by establishing and deploying an analysis and end user application for hemoglobin reference intervals within the University Hospital Erlangen. To demonstrate the potential of the system to deploy arbitrary models, we loaded a colorectal cancer dataset into an OMOP database and built machine learning models to predict patient outcomes and made them available via a web service. We demonstrated both the integration with FHIR as well as an example end user application. Finally, we integrated the platform with the open source DataSHIELD architecture to allow for distributed privacy preserving data analysis and training across networks of hospitals. Conclusion The KETOS platform takes a novel approach to data analysis, training and deploying decision support models in a hospital or healthcare setting. It does so in a secure and privacy-preserving manner, combining the flexibility of Docker virtualization with the advantages of standardized vocabularies, a widely applied database schema (OMOP-CDM), and a standardized way to exchange medical data (FHIR).;41
The Agile Deployment of Machine Learning Models in Healthcare;Stuart Jackson, Maha Yaqub, Cheng-Xi Li;2019;Frontiers in Big Data;The continuous delivery of applied machine learning models in healthcare is often hampered by the existence of isolated product deployments with poorly developed architectures and limited or non-existent maintenance plans. For example, actuarial models in healthcare are often trained in total separation from the client-facing software that implements the models in real-world settings. In practice, such systems prove difficult to maintain, to calibrate on new populations, and to re-engineer to include newer design features and capabilities. Here, we briefly describe our product team's ongoing efforts at translating an existing research pipeline into an integrated, production-ready system for healthcare cost estimation, using an agile methodology. In doing so, we illustrate several nearly universal implementation challenges for machine learning models in healthcare, and provide concrete recommendations on how to proactively address these issues.;17
An Architecture for Agile Machine Learning in Real-Time Applications;Johann Schleier-Smith;2015;Knowledge Discovery and Data Mining;Machine learning techniques have proved effective in recommender systems and other applications, yet teams working to deploy them lack many of the advantages that those in more established software disciplines today take for granted. The well-known Agile methodology advances projects in a chain of rapid development cycles, with subsequent steps often informed by production experiments. Support for such workflow in machine learning applications remains primitive. The platform developed at if(we) embodies a specific machine learning approach and a rigorous data architecture constraint, so allowing teams to work in rapid iterative cycles. We require models to consume data from a time-ordered event history, and we focus on facilitating creative feature engineering. We make it practical for data scientists to use the same model code in development and in production deployment, and make it practical for them to collaborate on complex models. We deliver real-time recommendations at scale, returning top results from among 10,000,000 candidates with sub-second response times and incorporating new updates in just a few seconds. Using the approach and architecture described here, our team can routinely go from ideas for new models to production-validated results within two weeks.;22
A Taxonomy of Software Engineering Challenges for Machine Learning Systems: An Empirical Investigation;Lucy Ellen Lwakatare, Aiswarya Raj, J. Bosch, H. H. Olsson, I. Crnkovic;2019;International Conference on Agile Software Development;Abstract not available;160
Continuous Training for Production ML in the TensorFlow Extended (TFX) Platform;Denis Baylor, Kevin Haas, Konstantinos Katsiapis, S. Leong, Rose Liu, Clemens Mewald, Hui Miao, N. Polyzotis, Mitchell Trott, Martin A. Zinkevich;2019;USENIX Conference on Operational Machine Learning;Large organizations rely increasingly on continuous ML pipelines in order to keep machine-learned models continuously up-to-date with respect to data. In this scenario, dis-ruptions in the pipeline can increase model staleness and thus degrade the quality of downstream services supported by these models. In this paper we describe the operation of continuous pipelines in the Tensorﬂow Extended (TFX) platform that we developed and deployed at Google. We present the main mechanisms in TFX to support this type of pipelines in production and the lessons learned from the deployment of the platform internally at Google.;23
DLHub: Simplifying publication, discovery, and use of machine learning models in science;Zhuozhao Li, Zhuozhao Li, Ryan Chard, Logan T. Ward, K. Chard, K. Chard, Tyler J. Skluzacek, Y. Babuji, Y. Babuji, A. Woodard, S. Tuecke, S. Tuecke, B. Blaiszik, B. Blaiszik, M. Franklin, Ian T Foster, Ian T Foster;2021;J. Parallel Distributed Comput.;Abstract not available;23
Flux: Groupon's automated, scalable, extensible machine learning platform;Derrick C. Spell, Xiao-Han T. Zeng, Jae Young Chung, Bahador B. Nooraei, Richard T. Shomer, Ling-Yong Wang, James C. Gibson, D. Kirsche;2017;2017 IEEE International Conference on Big Data (Big Data);As machine learning becomes the driving force of the daily operation of companies within the information technology sector, infrastructure that enables automated, scalable machine learning is a core component of the systems of many large companies. Various systems and products are being built, offered, and open sourced. As an e-commerce company, numerous aspects of Groupon's business is driven by machine learning. To solve the scalability issue and provide a seamless collaboration between data scientists and engineers, we built Flux, a system that expedites the deployment, execution, and monitoring of machine learning models. Flux focuses on enabling data scientists to build model prototypes with languages and tools they are most proficient in, and integrating the models into the enterprise production system. It manages the life cycle of deployed models, and executes them in distributed batch mode, or exposes them as micro-services for real-time use cases. Its design focuses on automation and easy management, scalability, and extensibility. Flux is the central system for supervised machine learning tasks at Groupon and has been supporting multiple teams across the company.;4
DataHub: Collaborative Data Science & Dataset Version Management at Scale;Anant P. Bhardwaj, Souvik Bhattacherjee, Amit Chavan, A. Deshpande, Aaron J. Elmore, S. Madden, Aditya G. Parameswaran;2014;Conference on Innovative Data Systems Research;Relational databases have limited support for data collaboration, where teams collaboratively curate and analyze large datasets. Inspired by software version control systems like git, we propose (a) a dataset version control system, giving users the ability to create, branch, merge, difference and search large, divergent collections of datasets, and (b) a platform, DATAHUB, that gives users the ability to perform collaborative data analysis building on this version control system. We outline the challenges in providing dataset version control at scale.;177
Towards a Serverless Platform for Edge AI;T. Rausch, W. Hummer, Vinod Muthusamy, A. Rashed, S. Dustdar;2019;USENIX Workshop on Hot Topics in Edge Computing;This paper proposes a serverless platform for building and operating edge AI applications. We analyze edge AI use cases to illustrate the challenges in building and operating AI applications in edge cloud scenarios. By elevating concepts from AI lifecycle management into the established serverless model, we enable easy development of edge AI workflow functions. We take a deviceless approach, i.e., we treat edge resources transparently as cluster resources, but give developers fine-grained control over scheduling constraints. Furthermore, we demonstrate the limitations of current serverless function schedulers, and present the current state of our prototype.;76
PADL: A Modeling and Deployment Language for Advanced Analytical Services †;Josu Díaz-de-Arcaya, Raúl Miñón, Ana I. Torre-Bastida, J. Ser, Aitor Almeida;2020;Italian National Conference on Sensors;In the smart city context, Big Data analytics plays an important role in processing the data collected through IoT devices. The analysis of the information gathered by sensors favors the generation of specific services and systems that not only improve the quality of life of the citizens, but also optimize the city resources. However, the difficulties of implementing this entire process in real scenarios are manifold, including the huge amount and heterogeneity of the devices, their geographical distribution, and the complexity of the necessary IT infrastructures. For this reason, the main contribution of this paper is the PADL description language, which has been specifically tailored to assist in the definition and operationalization phases of the machine learning life cycle. It provides annotations that serve as an abstraction layer from the underlying infrastructure and technologies, hence facilitating the work of data scientists and engineers. Due to its proficiency in the operationalization of distributed pipelines over edge, fog, and cloud layers, it is particularly useful in the complex and heterogeneous environments of smart cities. For this purpose, PADL contains functionalities for the specification of monitoring, notifications, and actuation capabilities. In addition, we provide tools that facilitate its adoption in production environments. Finally, we showcase the usefulness of the language by showing the definition of PADL-compliant analytical pipelines over two uses cases in a smart city context (flood control and waste management), demonstrating that its adoption is simple and beneficial for the definition of information and process flows in such environments.;7
Research Directions for Developing and Operating Artificial Intelligence Models in Trustworthy Autonomous Systems;Silverio Mart'inez-Fern'andez, Xavier Franch, Andreas Jedlitschka, Marc Oriol, Adam Trendowicz;2020;Research Challenges in Information Science;Abstract not available;23
Developing ML/DL Models: A Design Framework;Meenu Mary John, H. H. Olsson, J. Bosch;2020;International Conference on Software and Systems Process;Artificial Intelligence is becoming increasingly popular with organizations due to the success of Machine Learning and Deep Learning techniques. Using these techniques, data scientists learn from vast amounts of data to enhance behaviour in software-intensive systems. Despite the attractiveness of these techniques, however, there is a lack of systematic and structured design process for developing ML/DL models. The study uses a multiple-case study approach to explore the different activities and challenges data scientists face when developing ML/DL models in software-intensive embedded systems. In addition, we have identified seven different phases in the proposed design process leading to effective model development based on the case study. Iterations identified between phases and events which trigger these iterations optimize the design process for ML/DL models. Lessons learned from this study allow data scientists and engineers to develop high-performance ML/DL models and also bridge the gap between high demand and low supply of data scientists.;15
Project repositories for machine learning with TensorFlow;P. Janardhanan;2020;Publication venue not available;Abstract not available;26
Deployment of a Machine Learning System for Predicting Lawsuits Against Power Companies: Lessons Learned from an Agile Testing Experience for Improving Software Quality;Luis Rivero, J. O. Diniz, G. L. F. D. Silva, Gabriel Borralho, Geraldo Braz Junior, A. Paiva, Erika W. B. A. L. Alves, M. S. L. Oliveira;2020;Brazilian Symposium on Software Quality;"The advances in Machine Learning (ML) require software organizations to evolve their development processes in order to improve the quality of ML systems. Within the software development process, the testing stage of an ML system is more critical, considering that it is necessary to add data validation, trained model quality evaluation, and model validation to traditional unit, integration tests and system tests. In this paper, we focus on reporting the lessons learned of using model testing and exploratory testing within the context of the agile development process of an ML system that predicts lawsuits proneness in energy supply companies. Through the development of the project, the SCRUM agile methodology was applied and activities related to the development of the ML model and the development of the end-user application were defined. After the testing process of the ML model, we managed to achieve 93.89 accuracy; 95.58 specificity; 88.84 sensitivity; and 87.09 precision. Furthermore, we focused on the quality of use of the application embedding the ML model, by carrying out exploratory testing. As a result, through several iterations, different types of defects were identified and corrected. Our lessons learned support software engineers willing to develop ML systems that consider both the ML model and the end-user application.";4
Continuous Data Quality Management for Machine Learning based Data-as-a-Service Architectures;Shelernaz Azimi, C. Pahl;2021;International Conference on Cloud Computing and Services Science;Data-as-a-Service (DaaS) solutions make raw source data accessible in the form of processable information. Machine learning (ML) allows to produce meaningful information and knowledge based on raw data. Thus, quality is a major concern that applies to raw data as well as to information provided by ML-generated models. At the core of the solution is a conceptual framework that links input data quality and the machine learned data service quality, specifically inferring raw data problems as root causes from observed data service deficiency symptoms. This allows to deduce the hidden origins of quality problems observable by users of DaaS offerings. We analyse the quality framework through an extensive case study from an edge cloud and Internet-of-Thingsbased traffic application. We determine quality assessment mechanisms for symptom and cause analysis in different quality dimensions.;3
Modelling Data Pipelines;Aiswarya Raj, J. Bosch, H. H. Olsson, Tian J. Wang;2020;EUROMICRO Conference on Software Engineering and Advanced Applications;Data is the new currency and key to success. However, collecting high-quality data from multiple distributed sources requires much effort. In addition, there are several other challenges involved while transporting data from its source to the destination. Data pipelines are implemented in order to increase the overall efficiency of data-flow from the source to the destination since it is automated and reduces the human involvement which is required otherwise.Despite existing research on ETL (Extract-Transform-Load) and ELT (Extract-Load-Transform) pipelines, the research on this topic is limited. ETL/ELT pipelines are abstract representations of the end-to-end data pipelines. To utilize the full potential of the data pipeline, we should understand the activities in it and how they are connected in an end-to-end data pipeline. This study gives an overview of how to design a conceptual model of data pipeline which can be further used as a language of communication between different data teams. Furthermore, it can be used for automation of monitoring, fault detection, mitigation and alarming at different steps of data pipeline.;28
Automated end-to-end management of the modeling lifecycle in deep learning;Gharib Gharibi, V. Walunj, Raju Nekadi, Raj Marri, Yugyung Lee;2021;Empirical Software Engineering;Abstract not available;17
TensorFlow Data Validation: Data Analysis and Validation in Continuous ML Pipelines;Emily Caveness, Paul Suganthan G. C., Zhuo Peng, N. Polyzotis, Sudip Roy, Martin A. Zinkevich;2020;SIGMOD Conference;Machine Learning (ML) research has primarily focused on improving the accuracy and efficiency of the training algorithms while paying much less attention to the equally important problem of understanding, validating, and monitoring the data fed to ML. Irrespective of the ML algorithms used, data errors can adversely affect the quality of the generated model. This indicates that we need to adopt a data-centric approach to ML that treats data as a first-class citizen, on par with algorithms and infrastructure which are the typical building blocks of ML pipelines. In this demonstration we showcase TensorFlow Data Validation (TFDV), a scalable data analysis and validation system for ML that we have developed at Google and recently open-sourced. This system is deployed in production as an integral part of TFX - an end-to-end machine learning platform at Google. It is used by hundreds of product teams at Google and has received significant attention from the open-source community as well.;29
Data Lifecycle Challenges in Production Machine Learning;N. Polyzotis, Sudip Roy, Steven Euijong Whang, Martin A. Zinkevich;2018;SIGMOD record;Machine learning has become an essential tool for gleaning knowledge from data and tackling a diverse set of computationally hard tasks. However, the accuracy of a machine learned model is deeply tied to the data that it is trained on. Designing and building robust processes and tools that make it easier to analyze, validate, and transform data that is fed into large-scale machine learning systems poses data management challenges. Drawn from our experience in developing data-centric infrastructure for a production machine learning platform at Google, we summarize some of the interesting research challenges that we encountered, and survey some of the relevant literature from the data management and machine learning communities. Specifically, we explore challenges in three main areas of focus - data understanding, data validation and cleaning, and data preparation. In each of these areas, we try to explore how different constraints are imposed on the solutions depending on where in the lifecycle of a model the problems are encountered and who encounters them.;165
On Challenges in Machine Learning Model Management;Sebastian Schelter, F. Biessmann, Tim Januschowski, David Salinas, Stephan Seufert, Gyuri Szarvas;2018;IEEE Data Engineering Bulletin;The training, maintenance, deployment, monitoring, organization and documentation of machine learning (ML) models – in short model management – is a critical task in virtually all production ML use cases. Wrong model management decisions can lead to poor performance of a ML system and can result in high maintenance cost. As both research on infrastructure as well as on algorithms is quickly evolving, there is a lack of understanding of challenges and best practices for ML model management. Therefore, this ﬁeld is receiving increased attention in recent years, both from the data management as well as from the ML community. In this paper, we discuss a selection of ML use cases, develop an overview over conceptual, engineering, and data-processing related challenges arising in the management of the corresponding ML models, and point out future research directions.;152
MLIoT: An End-to-End Machine Learning System for the Internet-of-Things;Sudershan Boovaraghavan, Anurag Maravi, Prahaladha Mallela, Yuvraj Agarwal;2021;International Conference on Internet-of-Things Design and Implementation;Modern Internet of Things (IoT) applications, from contextual sensing to voice assistants, rely on ML-based training and serving systems using pre-trained models to render predictions. However, real-world IoT environments are diverse, with rich IoT sensors and need ML models to be personalized for each setting using relatively less training data. Most existing general-purpose ML systems are optimized for specific and dedicated hardware resources and do not adapt to changing resources and different IoT application requirements. To address this gap, we propose MLIoT, an end-to-end Machine Learning System tailored towards supporting the entire lifecycle of IoT applications. MLIoT adapts to different IoT data sources, IoT tasks, and compute resources by automatically training, optimizing, and serving models based on expressive application-specific policies. MLIoT also adapts to changes in IoT environments or compute resources by enabling re-training, and updating models served on the fly while maintaining accuracy and performance. Our evaluation across a set of benchmarks show that MLIoT can handle multiple IoT tasks, each with individual requirements, in a scalable manner while maintaining high accuracy and performance. We compare MLIoT with two state-of-the-art hand-tuned systems and a commercial ML system showing that MLIoT improves accuracy from 50% - 75% while reducing or maintaining latency.;10
Machine Learning Lifecycle for Earth Science Application: A Practical Insight into Production Deployment;M. Maskey, A. Molthan, C. Hain, R. Ramachandran, I. Gurung, B. Freitag, J. J. Miller, Muthukumaran Ramasubramanian, Drew Bollinger, Ricardo Mestre, D. Cecil;2019;IEEE International Geoscience and Remote Sensing Symposium;Enterprises are making machine learning for production as an integral part of their future roadmaps and Earth science domain is no exception. However, there is common problem in transitioning machine learning from science to production due to a major difference in constructing a model versus deploying it for people to use to make decisions. Phases of machine learning lifecycle that includes model transition to production using a successful application is discussed.;10
Context : The Missing Piece in the Machine Learning Lifecycle;Rolando Garcia, Vikram Sreekanti, N. Yadwadkar, D. Crankshaw, Joseph E. Gonzalez;2018;Publication venue not available;Machine learning models have become ubiquitous in modern applications. The ML Lifecycle describes a three-phase process used by data scientists and data engineers to develop, train;29
M-Lean: An end-to-end development framework for predictive models in B2B scenarios;Mona Nashaat, Aindrila Ghosh, James Miller, Shaikh Quader, Chad Marston;2019;Information and Software Technology;Abstract not available;9
Deep learning model management for coronary heart disease early warning research;Yang Peili, Yin Xuezhen, Ye Jian, Yang Lingfeng, Zhao Hui, Liang Jimin;2018;2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA);Coronary Heart Disease (CHD) is one of the common diseases that threaten people's health and life. To facilitate the CHD early warning research, the deep learning based methods have drawn much attention. However, the literature mostly focuses on how to establish and optimize the CHD early warning models, while overlooking the training data-model-experimental results modeling lifecycle management. Aiming to promote the early warning research of CHD, we contribute a data management system integrated the CHD patient data with the deep learning model data. In the system, a deep learning model version tree is established to represent the relationship between the models. Tracking-Ancestors algorithm and Find-Specified-Ancestor algorithm are designed to conduct the lineage management of the deep learning model. Considering the big data characteristics of the patient data and deep learning model data, we compare the query response time and select MongoDB as the DBMS for the Pdmdims (Patient Data & Deep Learning Model Data Integrated Management System). The research results show that Pdmdims can provide an effective integrated data management platform for CHD early warning researchers.;6
Smart Manufacturing and Continuous Improvement and Adaptation of Predictive Models;G. Kronberger, F. Bachinger, M. Affenzeller;2020;Publication venue not available;Abstract not available;12
Concept for a Technical Infrastructure for Management of Predictive Models in Industrial Applications;F. Bachinger, G. Kronberger;2019;International Conference/Workshop on Computer Aided Systems Theory;Abstract not available;5
From Ad-Hoc Data Analytics to DataOps;A. Munappy, D. I. Mattos, J. Bosch, H. H. Olsson, Anas Dakkak;2020;International Conference on Software and Systems Process;The collection of high-quality data provides a key competitive advantage to companies in their decision-making process. It helps to understand customer behavior and enables the usage and deployment of new technologies based on machine learning. However, the process from collecting the data, to clean and process it to be used by data scientists and applications is often manual, non-optimized and error-prone. This increases the time that the data takes to deliver value for the business. To reduce this time companies are looking into automation and validation of the data processes. Data processes are the operational side of data analytic workflow.DataOps, a recently coined term by data scientists, data analysts and data engineers refer to a general process aimed to shorten the end-to-end data analytic life-cycle time by introducing automation in the data collection, validation, and verification process. Despite its increasing popularity among practitioners, research on this topic has been limited and does not provide a clear definition for the term or how a data analytic process evolves from ad-hoc data collection to fully automated data analytics as envisioned by DataOps.This research provides three main contributions. First, utilizing multi-vocal literature we provide a definition and a scope for the general process referred to as DataOps. Second, based on a case study with a large mobile telecommunication organization, we analyze how multiple data analytic teams evolve their infrastructure and processes towards DataOps. Also, we provide a stairway showing the different stages of the evolution process. With this evolution model, companies can identify the stage which they belong to and also, can try to move to the next stage by overcoming the challenges they encounter in the current stage.;38
Developments in MLflow: A System to Accelerate the Machine Learning Lifecycle;A. Chen, A. Chow, A. Davidson, Arjun DCunha, A. Ghodsi, S. Hong, A. Konwinski, Clemens Mewald, Siddharth Murching, Tomas Nykodym, Paul Ogilvie, Mani Parkhe, Avesh Singh, Fen Xie, M. Zaharia, Richard Zang, Juntai Zheng, Corey Zumar;2020;DEEM@SIGMOD;MLflow is a popular open source platform for managing ML development, including experiment tracking, reproducibility, and deployment. In this paper, we discuss user feedback collected since MLflow was launched in 2018, as well as three major features we have introduced in response to this feedback: a Model Registry for collaborative model management and review, tools for simplifying ML code instrumentation, and experiment analytics functions for extracting insights from millions of ML experiments.;80
On-Premise AI Platform: From DC to Edge;Bukhary Ikhwan Ismail, Mohammad Fairus Khalid, Rajendar Kandan, Ong Hong Hoe;2019;Proceedings of the 2019 2nd International Conference on Robot Systems and Applications;Artificial Intelligence (AI) is powering everything from devices, applications and services. Machine learning a branch of AI requires powerful infrastructure platform to do training and to serve the AI model. In this paper, we share our blueprint to build and host internal on-premise AI platform. We make use of our existing services such as private cloud, distributed storage, unified authentication platform, and build the AI platform on top of it. We discuss the requirements gathered from user, the technologies to make it possible, implementation and lesson learned from hosting it internally. Based on our evaluation, based on specific need, it is economical and viable option to host on-premise AI Platform.;3
A comprehensive study on challenges in deploying deep learning based software;Zhenpeng Chen, Yanbin Cao, Yuanqiang Liu, Haoyu Wang, Tao Xie, Xuanzhe Liu;2020;ESEC/SIGSOFT FSE;Deep learning (DL) becomes increasingly pervasive, being used in a wide range of software applications. These software applications, named as DL based software (in short as DL software), integrate DL models trained using a large data corpus with DL programs written based on DL frameworks such as TensorFlow and Keras. A DL program encodes the network structure of a desirable DL model and the process by which the model is trained using the training data. To help developers of DL software meet the new challenges posed by DL, enormous research efforts in software engineering have been devoted. Existing studies focus on the development of DL software and extensively analyze faults in DL programs. However, the deployment of DL software has not been comprehensively studied. To fill this knowledge gap, this paper presents a comprehensive study on understanding challenges in deploying DL software. We mine and analyze 3,023 relevant posts from Stack Overflow, a popular Q&A website for developers, and show the increasing popularity and high difficulty of DL software deployment among developers. We build a taxonomy of specific challenges encountered by developers in the process of DL software deployment through manual inspection of 769 sampled posts and report a series of actionable implications for researchers, developers, and DL framework vendors.;104
Who needs to know what, when?: Broadening the Explainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycle;Shipi Dhanorkar, Christine T. Wolf, Kun Qian, Anbang Xu, Lucian Popa, Yunyao Li;2021;Conference on Designing Interactive Systems;The interpretability or explainability of AI systems (XAI) has been a topic gaining renewed attention in recent years across AI and HCI communities. Recent work has drawn attention to the emergent explainability requirements of in situ, applied projects, yet further exploratory work is needed to more fully understand this space. This paper investigates applied AI projects and reports on a qualitative interview study of individuals working on AI projects at a large technology and consulting company. Presenting an empirical understanding of the range of stakeholders in industrial AI projects, this paper also draws out the emergent explainability practices that arise as these projects unfold, highlighting the range of explanation audiences (who), as well as how their explainability needs evolve across the AI project lifecycle (when). We discuss the importance of adopting a sociotechnical lens in designing AI systems, noting how the “AI lifecycle” can serve as a design metaphor to further the XAI design field.;81
Benchmarking Machine Learning Solutions in Production;L. C. Silva, F. Zagatti, B. S. Sette, L. N. S. Silva, D. Lucrédio, Diego Furtado Silva, Helena de Medeiros Caseli;2020;International Conference on Machine Learning and Applications;Machine learning (ML) is becoming critical to many businesses. Keeping an ML solution online and responding is therefore a necessity, and is part of the MLOps (Machine Learning operationalization) movement. One aspect for this process is monitoring not only prediction quality, but also system resources. This is important to correctly provide the necessary infrastructure, either using a fully-managed cloud platform or a local solution. This is not a difficult task, as there are many tools available. However, it requires some planning and knowledge about what to monitor. Also, many ML professionals are not experts in system operations and may not have the skills to easily setup a monitoring and benchmarking environment. In the spirit of MLOps, this paper presents an approach, based on a simple API and set of tools, to monitor ML solutions. The approach was tested with 9 different solutions. The results indicate that the approach can deliver useful information to help in decision making, proper resource provision and operation of ML systems.;16
MLOps Challenges in Multi-Organization Setup: Experiences from Two Real-World Cases;Tuomas Granlund, Aleksi Kopponen, Vlad Stirbu, Lalli Myllyaho, T. Mikkonen;2021;Workshop on AI Engineering - Software Engineering for AI;The emerging age of connected, digital world means that there are tons of data, distributed to various organizations and their databases. Since this data can be confidential in nature, it cannot always be openly shared in seek of artificial intelligence (AI) and machine learning (ML) solutions. Instead, we need integration mechanisms, analogous to integration patterns in information systems, to create multi-organization AI/ML systems. In this paper, we present two real-world cases. First, we study integration between two organizations in detail. Second, we address scaling of AI/ML to multi-organization context. The setup we assume is that of continuous deployment, often referred to DevOps in software development. When also ML components are deployed in a similar fashion, term MLOps is used. Towards the end of the paper, we list the main observations and draw some final conclusions. Finally, we propose some directions for future work.;35
Provenance Data in the Machine Learning Lifecycle in Computational Science and Engineering;Renan Souza, L. Azevedo, Vítor Lourenço, E. Soares, R. Thiago, R. Brandão, D. Civitarese, E. V. Brazil, M. Moreno, P. Valduriez, M. Mattoso, Renato Cerqueira, M. Netto;2019;Works;"Machine Learning (ML) has become essential in several industries. In Computational Science and Engineering (CSE), the complexity of the ML lifecycle comes from the large variety of data, scientists' expertise, tools, and workflows. If data are not tracked properly during the lifecycle, it becomes unfeasible to recreate a ML model from scratch or to explain to stackholders how it was created. The main limitation of provenance tracking solutions is that they cannot cope with provenance capture and integration of domain and ML data processed in the multiple workflows in the lifecycle, while keeping the provenance capture overhead low. To handle this problem, in this paper we contribute with a detailed characterization of provenance data in the ML lifecycle in CSE; a new provenance data representation, called PROV-ML, built on top of W3C PROV and ML Schema; and extensions to a system that tracks provenance from multiple workflows to address the characteristics of ML and CSE, and to allow for provenance queries with a standard vocabulary. We show a practical use in a real case in the O&G industry, along with its evaluation using 239,616 CUDA cores in parallel.";34
AIOps: Real-World Challenges and Research Innovations;Yingnong Dang, Qingwei Lin, Peng Huang;2019;2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion);AIOps is about empowering software and service engineers (e.g., developers, program managers, support engineers, site reliability engineers) to efficiently and effectively build and operate online services and Apps at scale with artificial intelligence (AI) and machine learning (ML) techniques. AIOps can help achieve higher service quality and customer satisfaction, engineering productivity boost, and cost reduction. In this technical briefing, we summarize the real-world challenges on building AIOps solutions based on our practice and experience in Microsoft, propose a roadmap of AIOps related research directions, and share a few successful AIOps solutions we have built for Microsoft service products.;148
Building A Platform for Machine Learning Operations from Open Source Frameworks;Y. Liu, Zhijing Ling, Boyu Huo, Boqian Wang, Tianen Chen, Esma Mouine;2020;Publication venue not available;Abstract not available;20
A Software Engineering Perspective on Engineering Machine Learning Systems: State of the Art and Challenges;G. Giray;2020;Journal of Systems and Software;Abstract not available;103
Kafka-ML: connecting the data stream with ML/AI frameworks;Cristian Mart'in, P. Langendoerfer, M. D'iaz, B. Rubio;2020;Future generations computer systems;Abstract not available;35
Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions;Lucy Ellen Lwakatare, Aiswarya Raj, I. Crnkovic, J. Bosch, H. H. Olsson;2020;Information and Software Technology;Abstract not available;104
Model-Driven ML-Ops for Intelligent Enterprise Applications: Vision, Approaches and Challenges;W. Heuvel, D. Tamburri;2020;International Symposium on Business Modeling and Software Design;Abstract not available;23
An End-to-End Framework for Productive Use of Machine Learning in Software Analytics and Business Intelligence Solutions;Iris Figalist, Christoph Elsner, J. Bosch, H. H. Olsson;2020;International Conference on Product Focused Software Process Improvement;Abstract not available;7
Characterizing machine learning process: A maturity framework;R. Akkiraju, Vibha Sinha, Anbang Xu, J. Mahmud, Pritam Gundecha, Zhe Liu, Xiaotong Liu, John Schumacher;2018;International Conference on Business Process Management;Abstract not available;51
L G ] 1 0 M ay 2 01 9 Assuring the Machine Learning Lifecycle : Desiderata , Methods , and Challenges;Rob Ashmore;2019;Publication venue not available;"Interpretation [57] ✔ ★ Generate tests via simulation ✔ ★ ✩ ✩ Verifier of Random Forests [163] ✔ ★ Verification of ML Libraries [152] ✔ ★ Check for unwanted bias [15] ✔ ★ Use synthetic test data [162] ✔ ✔ ★ ★ ✩ Use GAN to inform test generation [181] ✔ ★ ★ Incorporate system level semantics [45] ✔ ✔ ★ ★ ✩ Counterexample-guided data augmentation [44] ✔ ★ ✩ ★ Probabilistic verification [166] ✔ ★ Use confidence levels [45] ✔ X ✩ ★ Evaluate interpretability [42] ✔ ✔ ★ ★ † ✔ = activity that the method is typically used in;X= activity that may use the method ‡ ★ = desideratum supported by the method; ✩ = desideratum partly supported by the method Table 6. Open challenges for the assurance concerns associated with the Model Verification (MV) stage ID Open Challenge Desideratum (Section) MV01 Understanding how to detect and protect against typical errors Comprehensive MV02 Test coverage measures with theoretical and empirical justification (Section 6.4.1) MV03 Formal verification for ML models other than neural networks MV04 Mapping requirements to model features Contextually Relevant MV05 General framework for synthetic test generation (Section 6.4.2) MV06 Mapping of model-free reinforcement learning states to real-world";0
Recommendations for All: Solving Thousands of Recommendation Problems Daily;Bhargav Kanagal, Sandeep Tata;2018;IEEE International Conference on Data Engineering;Recommender systems are a key technology for many online services including e-commerce, movies, music, and news. Online retailers use product recommender systems to help users discover items that they may like. However, building a large-scale product recommender system is a challenging task. The problems of sparsity and cold-start are much more pronounced in this domain. Large online retailers have used good recommendations to drive user engagement and improve revenue, but the complexity involved is a roadblock to widespread adoption by smaller retailers. In this paper, we tackle the problem of generating product recommendations for tens of thousands of online retailers. Sigmund is an industrial-scale system for providing recommendations as a service. Sigmund was deployed to production in early 2014 and has been serving retailers every day. We describe the design choices that we made in order to train accurate matrix factorization models at minimal cost. We also share the lessons we learned from this experience – both from a machine learning perspective and a systems perspective. We hope that these lessons are useful for building future machine-learning services.;4
Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba;Jizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, Lee;2018;Knowledge Discovery and Data Mining;Recommender systems (RSs) have been the most important technology for increasing the business in Taobao, the largest online consumer-to-consumer (C2C) platform in China. There are three major challenges facing RS in Taobao: scalability, sparsity and cold start. In this paper, we present our technical solutions to address these three challenges. The methods are based on a well-known graph embedding framework. We first construct an item graph from users' behavior history, and learn the embeddings of all items in the graph. The item embeddings are employed to compute pairwise similarities between all items, which are then used in the recommendation process. To alleviate the sparsity and cold start problems, side information is incorporated into the graph embedding framework. We propose two aggregation methods to integrate the embeddings of items and the corresponding side information. Experimental results from offline experiments show that methods incorporating side information are superior to those that do not. Further, we describe the platform upon which the embedding methods are deployed and the workflow to process the billion-scale data in Taobao. Using A/B test, we show that the online Click-Through-Rates (CTRs) are improved comparing to the previous collaborative filtering based methods widely used in Taobao, further demonstrating the effectiveness and feasibility of our proposed methods in Taobao's live production environment.;455
Collaborative Multi-modal deep learning for the personalized product retrieval in Facebook Marketplace;Lu Zheng, Zhao Tan, Kun Han, Ren Mao;2018;arXiv.org;"Facebook Marketplace is quickly gaining momentum among consumers as a favored customer-to-customer (C2C) product trading platform. The recommendation system behind it helps to significantly improve the user experience. Building the recommendation system for Facebook Marketplace is challenging for two reasons: 1) Scalability: the number of products in Facebook Marketplace is huge. Tens of thousands of products need to be scored and recommended within a couple hundred milliseconds for millions of users every day; 2) Cold start: the life span of the C2C products is very short and the user activities on the products are sparse. Thus it is difficult to accumulate enough product level signals for recommendation and we are facing a significant cold start issue. In this paper, we propose to address both the scalability and the cold-start issue by building a collaborative multi-modal deep learning based retrieval system where the compact embeddings for the users and the products are trained with the multi-modal content information. This system shows significant improvement over the benchmark in online and off-line experiments: In the online experiment, it increases the number of messages initiated by the buyer to the seller by +26.95%; in the off-line experiment, it improves the prediction accuracy by +9.58%.";3
Learning in the “Real World”;L. Saitta, F. Neri;1998;Machine-mediated learning;Abstract not available;60
STRING v11: protein–protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets;Damian Szklarczyk, Annika L. Gable, D. Lyon, Alexander Junge, S. Wyder, J. Huerta-Cepas, M. Simonovic, N. Doncheva, J. Morris, P. Bork, L. Jensen, C. V. Mering;2018;Nucleic Acids Res.;Abstract Proteins and their functional interactions form the backbone of the cellular machinery. Their connectivity network needs to be considered for the full understanding of biological phenomena, but the available information on protein–protein associations is incomplete and exhibits varying levels of annotation granularity and reliability. The STRING database aims to collect, score and integrate all publicly available sources of protein–protein interaction information, and to complement these with computational predictions. Its goal is to achieve a comprehensive and objective global network, including direct (physical) as well as indirect (functional) interactions. The latest version of STRING (11.0) more than doubles the number of organisms it covers, to 5090. The most important new feature is an option to upload entire, genome-wide datasets as input, allowing users to visualize subsets as interaction networks and to perform gene-set enrichment analysis on the entire input. For the enrichment analysis, STRING implements well-known classification systems such as Gene Ontology and KEGG, but also offers additional, new classification systems based on high-throughput text-mining as well as on a hierarchical clustering of the association network itself. The STRING resource is available online at https://string-db.org/.;12319
Mobile recommendations based on interest prediction from consumer's installed apps-insights from a large-scale field study;R. M. Frey, Runhua Xu, Christian Ammendola, O. Moling, G. Giglio, A. Ilic;2017;Information Systems;Abstract not available;14
Deep Neural Networks for YouTube Recommendations;Paul Covington, Jay K. Adams, Emre Sargin;2016;ACM Conference on Recommender Systems;YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous user-facing impact.;2917
Quick Access: Building a Smart Experience for Google Drive;Sandeep Tata, Alexandrin Popescul, Marc Najork, Mike Colagrosso, Julian Gibbons, Alan Green, Alexandre Mah, Michael Smith, Divanshu Garg, Cayden Meyer, Reuben Kan;2017;Knowledge Discovery and Data Mining;Google Drive is a cloud storage and collaboration service used by hundreds of millions of users around the world. Quick Access is a new feature in Google Drive that surfaces the most relevant documents when a user visits the home screen. Our metrics show that users locate their documents in half the time with this feature compared to previous approaches. The development of Quick Access illustrates many general challenges and constraints associated with practical machine learning such as protecting user privacy, working with data services that are not designed with machine learning in mind, and evolving product definitions. We believe that the lessons learned from this experience will be useful to practitioners tackling a wide range of applied machine learning problems.;25
Related Pins at Pinterest: The Evolution of a Real-World Recommender System;David C. Liu, Stephanie Rogers, Raymond Shiau, Dmitry Kislyuk, Kevin C. K. Ma, Zhigang Zhong, Jenny Liu, Yushi Jing;2017;The Web Conference;Related Pins is the Web-scale recommender system that powers over 40% of user engagement on Pinterest. This paper is a longitudinal study of three years of its development, exploring the evolution of the system and its components from prototypes to present state. Each component was originally built with many constraints on engineering effort and computational resources, so we prioritized the simplest and highest-leverage solutions. We show how organic growth led to a complex system and how we managed this complexity. Many challenges arose while building this system, such as avoiding feedback loops, evaluating performance, activating content, and eliminating legacy heuristics. Finally, we offer suggestions for tackling these challenges when engineering Web-scale recommender systems.;126
Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time;Chantat Eksombatchai, Pranav Jindal, Zitao Liu, Yuchen Liu, Rahul Sharma, Charles Sugnet, Mark Ulrich, J. Leskovec;2017;The Web Conference;User experience in modern content discovery applications critically depends on high-quality personalized recommendations. However, building systems that provide such recommendations presents a major challenge due to a massive pool of items, a large number of users, and requirements for recommendations to be responsive to user actions and generated on demand in real-time. Here we present Pixie, a scalable graph-based real-time recommender system that we developed and deployed at Pinterest. Given a set of user-specific pins as a query, Pixie selects in real-time from billions of possible pins those that are most related to the query. To generate recommendations, we develop Pixie Random Walk algorithm that utilizes the Pinterest object graph of 3 billion nodes and 17 billion edges. Experiments show that recommendations provided by Pixie lead up to 50% higher user engagement when compared to the previous Hadoop-based production system. Furthermore, we develop a graph pruning strategy at that leads to an additional 58% improvement in recommendations. Last, we discuss system aspects of Pixie, where a single server executes 1,200 recommendation requests per second with 60 millisecond latency. Today, systems backed by Pixie contribute to more than 80% of all user engagement on Pinterest.;190
GraphJet: Real-Time Content Recommendations at Twitter;Aneesh Sharma, Jerry Jiang, Praveen Bommannavar, B. Larson, Jimmy J. Lin;2016;Proceedings of the VLDB Endowment;This paper presents GraphJet, a new graph-based system for generating content recommendations at Twitter. As motivation, we trace the evolution of our formulation and approach to the graph recommendation problem, embodied in successive generations of systems. Two trends can be identified: supplementing batch with real-time processing and a broadening of the scope of recommendations from users to content. Both of these trends come together in Graph-Jet, an in-memory graph processing engine that maintains a real-time bipartite interaction graph between users and tweets. The storage engine implements a simple API, but one that is sufficiently expressive to support a range of recommendation algorithms based on random walks that we have refined over the years. Similar to Cassovary, a previous graph recommendation engine developed at Twitter, GraphJet assumes that the entire graph can be held in memory on a single server. The system organizes the interaction graph into temporally-partitioned index segments that hold adjacency lists. GraphJet is able to support rapid ingestion of edges while concurrently serving lookup queries through a combination of compact edge encoding and a dynamic memory allocation scheme that exploits power-law characteristics of the graph. Each GraphJet server ingests up to one million graph edges per second, and in steady state, computes up to 500 recommendations per second, which translates into several million edge read operations per second.;69
WTF: the who to follow service at Twitter;Pankaj Gupta, Ashish Goel, Jimmy J. Lin, Aneesh Sharma, Dong Wang, R. Zadeh;2013;The Web Conference;"WTF (""Who to Follow"") is Twitter's user recommendation service, which is responsible for creating millions of connections daily between users based on shared interests, common connections, and other related factors. This paper provides an architectural overview and shares lessons we learned in building and running the service over the past few years. Particularly noteworthy was our design decision to process the entire Twitter graph in memory on a single server, which significantly reduced architectural complexity and allowed us to develop and deploy the service in only a few months. At the core of our architecture is Cassovary, an open-source in-memory graph processing engine we built from scratch for WTF. Besides powering Twitter's user recommendations, Cassovary is also used for search, discovery, promoted products, and other services as well. We describe and evaluate a few graph recommendation algorithms implemented in Cassovary, including a novel approach based on a combination of random walks and SALSA. Looking into the future, we revisit the design of our architecture and comment on its limitations, which are presently being addressed in a second-generation system under development.";495
Email Volume Optimization at LinkedIn;Rupesh Gupta, Guanfeng Liang, Hsiao-Ping Tseng, Ravi Kiran Holur Vijay, Xiaoyu Chen, Rómer Rosales;2016;Knowledge Discovery and Data Mining;Online social networking services distribute various types of messages to their members. Common types of messages include news, connection requests, membership notifications, promotions and event notifications. Such communication, if used judiciously, can provide an enormous value to members thereby keeping them engaged. However sending a message for every instance of news, connection request, or the like can result in an overwhelming number of messages in a member's mailbox. This may result in reduced effectiveness of communication if the messages are not sufficiently relevant to the member's interests. It may also result in a poor brand perception of the networking service. In this paper we discuss our strategy and experience with regard to the problem of email volume optimization at LinkedIn. In particular, we present a cost-benefit analysis of sending emails, the key factors to administer an effective volume optimization, our algorithm for volume optimization, the architecture of the supporting system and experimental results from online A/B tests.;32
Notification Volume Control and Optimization System at Pinterest;Bo Zhao, Koichiro Narita, Burkay Orten, J. Egan;2018;Knowledge Discovery and Data Mining;Notifications (including emails, mobile / desktop push notifications, SMS, etc.) are very effective channels for online services to engage with users and drive user engagement metrics and other business metrics. One of the most important and challenging problems in a production notification system is to decide the right frequency for each user. In this paper, we propose a novel machine learning approach to decide notification volume for each user such that long term user engagement is optimized. We will also discuss a few practical issues and design choices we have made. The new system has been deployed to production at Pinterest in mid 2017 and significantly reduced notification volume and improved CTR of notifications and site engagement metrics compared with the previous machine learning approach.;15
Optimizing Email Volume For Sitewide Engagement;Rupesh Gupta, Guanfeng Liang, Rómer Rosales;2017;International Conference on Information and Knowledge Management;"In this paper we focus on the problem of optimizing email volume for maximizing sitewide engagement of an online social networking service. Email volume optimization approaches published in the past have proposed optimization of email volume for maximization of engagement metrics which are impacted exclusively by email; for example, the number of sessions that begin with clicks on links within emails. The impact of email on such downstream engagement metrics can be estimated easily because of the ease of attribution of such an engagement event to an email. However, this framework is limited in its view of the ecosystem of the networking service which comprises of several tools and utilities that contribute towards delivering value to members; with email being just one such utility. Thus, in this paper we depart from previous approaches by exploring and optimizing the contribution of email to this ecosystem. In particular, we present and contrast the differential impact of email on sitewide engagement metrics for various types of users. We propose a new email volume optimization approach which maximizes sitewide engagement metrics, such as the total number of active users. This is in sharp contrast to the previous approaches whose objective has been maximization of downstream engagement metrics. We present details of our prediction function for predicting the impact of emails on a user's activeness on the mobile or web application. We describe how certain approximations to this prediction function can be made for solving the volume optimization problem, and present results from online A/B tests.";18
Cascade Ranking for Operational E-commerce Search;Shichen Liu, Fei Xiao, Wenwu Ou, Luo Si;2017;Knowledge Discovery and Data Mining;"In the 'Big Data' era, many real-world applications like search involve the ranking problem for a large number of items. It is important to obtain effective ranking results and at the same time obtain the results efficiently in a timely manner for providing good user experience and saving computational costs. Valuable prior research has been conducted for learning to efficiently rank like the cascade ranking (learning) model, which uses a sequence of ranking functions to progressively filter some items and rank the remaining items. However, most existing research of learning to efficiently rank in search is studied in a relatively small computing environments with simulated user queries. This paper presents novel research and thorough study of designing and deploying a Cascade model in a Large-scale Operational E-commerce Search application (CLOES), which deals with hundreds of millions of user queries per day with hundreds of servers. The challenge of the real-world application provides new insights for research: 1). Real-world search applications often involve multiple factors of preferences or constraints with respect to user experience and computational costs such as search accuracy, search latency, size of search results and total CPU cost, while most existing search solutions only address one or two factors; 2). Effectiveness of e-commerce search involves multiple types of user behaviors such as click and purchase, while most existing cascade ranking in search only models the click behavior. Based on these observations, a novel cascade ranking model is designed and deployed in an operational e-commerce search application. An extensive set of experiments demonstrate the advantage of the proposed work to address multiple factors of effectiveness, efficiency and user experience in the real-world application.";84
Perceive Your Users in Depth: Learning Universal User Representations from Multiple E-commerce Tasks;Yabo Ni, Dan Ou, Shichen Liu, Xiang Li, Wenwu Ou, Anxiang Zeng, Luo Si;2018;Knowledge Discovery and Data Mining;Tasks such as search and recommendation have become increasingly important for E-commerce to deal with the information overload problem. To meet the diverse needs of different users, personalization plays an important role. In many large portals such as Taobao and Amazon, there are a bunch of different types of search and recommendation tasks operating simultaneously for personalization. However, most of current techniques address each task separately. This is suboptimal as no information about users shared across different tasks. In this work, we propose to learn universal user representations across multiple tasks for more effective personalization. In particular, user behavior sequences (e.g., click, bookmark or purchase of products) are modeled by LSTM and attention mechanism by integrating all the corresponding content, behavior and temporal information. User representations are shared and learned in an end-to-end setting across multiple tasks. Benefiting from better information utilization of multiple tasks, the user representations are more effective to reflect their interests and are more general to be transferred to new tasks. We refer this work as Deep User Perception Network (DUPN) and conduct an extensive set of offline and online experiments. Across all tested five different tasks, our DUPN consistently achieves better results by giving more effective user representations. Moreover, we deploy DUPN in large scale operational tasks in Taobao. Detailed implementations, e.g., incremental model updating, are also provided to address the practical issues for the real world applications.;132
Personalized Federated Search at LinkedIn;Dhruv Arya, Viet Ha-Thuc, Shakti Sinha;2015;International Conference on Information and Knowledge Management;"LinkedIn has grown to become a platform hosting diverse sources of information ranging from member profiles, jobs, professional groups, slideshows etc. Given the existence of multiple sources, when a member issues a query like ""software engineer"", the member could look for software engineer profiles, jobs or professional groups. To tackle this problem, we exploit a data-driven approach that extracts searcher intents from their profile data and recent activities at a large scale. The intents such as job seeking, hiring, content consuming are used to construct features to personalize federated search experience. We tested the approach on the LinkedIn homepage and A/B tests show significant improvements in member engagement. As of writing this paper, the approach powers all of federated search on LinkedIn homepage.";17
Click-through Prediction for Advertising in Twitter Timeline;Cheng Li, Yue Lu, Qiaozhu Mei, Dong Wang, Sandeep Pandey;2015;Knowledge Discovery and Data Mining;We present the problem of click-through prediction for advertising in Twitter timeline, which displays a stream of Tweets from accounts a user choose to follow. Traditional computational advertising usually appears in two forms: sponsored search that places ads onto the search result page when a query is issued to a search engine, and contextual advertising that places ads onto a regular, usually static Web page. Compared with these two paradigms, placing ads into a Tweet stream is particularly challenging given the nature of the data stream: the context into which an ad can be placed updates dynamically and never replicates. Every ad is therefore placed into a unique context. This makes the information available for training a machine learning model extremely sparse. In this study, we propose a learning-to-rank method which not only addresses the sparsity of training signals but also can be trained and updated online. The proposed method is evaluated using both offline experiments and online A/B tests, which involve very large collections of Twitter data and real Twitter users. Results of the experiments prove the effectiveness and efficiency of our solution, and its superiority over the current production model adopted by Twitter.;119
Design principles of massive, robust prediction systems;Troy Raeder, Ori Stitelman, Brian Dalessandro, Claudia Perlich, F. Provost;2012;Knowledge Discovery and Data Mining;Most data mining research is concerned with building high-quality classification models in isolation. In massive production systems, however, the ability to monitor and maintain performance over time while growing in size and scope is equally important. Many external factors may degrade classification performance including changes in data distribution, noise or bias in the source data, and the evolution of the system itself. A well-functioning system must gracefully handle all of these. This paper lays out a set of design principles for large-scale autonomous data mining systems and then demonstrates our application of these principles within the m6d automated ad targeting system. We demonstrate a comprehensive set of quality control processes that allow us monitor and maintain thousands of distinct classification models automatically, and to add new models, take on new data, and correct poorly-performing models without manual intervention or system disruption.;40
Machine learning for targeted display advertising: transfer learning in action;Claudia Perlich, Brian Dalessandro, Troy Raeder, Ori Stitelman, F. Provost;2013;Machine-mediated learning;Abstract not available;173
Robust Large-Scale Machine Learning in the Cloud;Steffen Rendle, D. Fetterly, E. Shekita, Bor-Yiing Su;2016;Knowledge Discovery and Data Mining;The convergence behavior of many distributed machine learning (ML) algorithms can be sensitive to the number of machines being used or to changes in the computing environment. As a result, scaling to a large number of machines can be challenging. In this paper, we describe a new scalable coordinate descent (SCD) algorithm for generalized linear models whose convergence behavior is always the same, regardless of how much SCD is scaled out and regardless of the computing environment. This makes SCD highly robust and enables it to scale to massive datasets on low-cost commodity servers. Experimental results on a real advertising dataset in Google are used to demonstrate SCD's cost effectiveness and scalability. Using Google's internal cloud, we show that SCD can provide near linear scaling using thousands of cores for 1 trillion training examples on a petabyte of compressed data. This represents 10,000x more training examples than the 'large-scale' Netflix prize dataset. We also show that SCD can learn a model for 20 billion training examples in two hours for about $10.;19
COTA: Improving the Speed and Accuracy of Customer Support through Ranking and Deep Networks;Piero Molino, H. Zheng, Yi-Chia Wang;2018;Knowledge Discovery and Data Mining;For a company looking to provide delightful user experiences, it is of paramount importance to take care of any customer issues. This paper proposes COTA, a system to improve speed and reliability of customer support for end users through automated ticket classification and answers selection for support representatives. Two machine learning and natural language processing techniques are demonstrated: one relying on feature engineering (COTA v1) and the other exploiting raw signals through deep learning architectures (COTA v2). COTA v1 employs a new approach that converts the multi-classification task into a ranking problem, demonstrating significantly better performance in the case of thousands of classes. For COTA v2, we propose an Encoder-Combiner-Decoder, a novel deep learning architecture that allows for heterogeneous input and output feature types and injection of prior knowledge through network architecture choices. This paper compares these models and their variants on the task of ticket classification and answer selection, showing model COTA v2 outperforms COTA v1, and analyzes their inner workings and shortcomings. Finally, an A/B test is conducted in a production setting validating the real-world impact of COTA in reducing issue resolution time by 10 percent without reducing customer satisfaction.;21
Rhythmedia: A Study of Facebook Immune System;Elinor Carmi;2020;Theory, Culture and Society. Explorations in Critical Social Science;This paper examines the politics behind algorithmic ordering in social media, focusing on the advertising logic behind them. This is explored through a practice I call rhythmedia – the way media companies render people, objects and their relations as rhythms and (re)order them for economic purposes. As a case study I examine the way the Facebook Immune System algorithm orchestrates people’s mediated experience towards a desired rhythm (sociality) while filtering out problematic rhythms (spam). This anti-spam algorithm shows that it is important for Facebook to understand people as rhythms and assemble a dynamic database from their mediated experiences, to convince advertisers that they know when and where people do things. People’s rhythms become a product that advertisers pay and bid for through Ad Auction to intervene in specific moments and shape people’s experience. Thus, the company can shape, manage, and filter specific rhythms to order sociality that brings more value.;20
Early security classification of skype users via machine learning;A. Leontjeva, M. Goldszmidt, Yinglian Xie, Fang Yu, M. Abadi;2013;Security and Artificial Intelligence;We investigate possible improvements in online fraud detection based on information about users and their interactions. We develop, apply, and evaluate our methods in the context of Skype. Specifically, in Skype, we aim to provide tools that identify fraudsters that have eluded the first line of detection systems and have been active for months. Our approach to automation is based on machine learning methods. We rely on a variety of features present in the data, including static user profiles (e.g., age), dynamic product usage (e.g., time series of calls), local social behavior (addition/deletion of friends), and global social features (e.g., PageRank). We introduce new techniques for pre-processing the dynamic (time series) features and fusing them with social features. We provide a thorough analysis of the usefulness of the different categories of features and of the effectiveness of our new techniques.;18
Session-Based Fraud Detection in Online E-Commerce Transactions Using Recurrent Neural Networks;Shuhao Wang, Cancheng Liu, Xiang Gao, Hongtao Qu, W. Xu;2017;ECML/PKDD;Abstract not available;61
Fraud detection within bankcard enrollment on mobile device based payment using machine learning;Hao Zhou, Hongfeng Chai, Mao-lin Qiu;2018;Frontiers of Information Technology & Electronic Engineering;The rapid growth of mobile Internet technologies has induced a dramatic increase in mobile payments as well as concomitant mobile transaction fraud. As the first step of mobile transactions, bankcard enrollment on mobile devices has become the primary target of fraud attempts. Although no immediate financial loss is incurred after a fraud attempt, subsequent fraudulent transactions can be quickly executed and could easily deceive the fraud detection systems if the fraud attempt succeeds at the bankcard enrollment step. In recent years, financial institutions and service providers have implemented rule-based expert systems and adopted short message service (SMS) user authentication to address this problem. However, the above solution is inadequate to face the challenges of data loss and social engineering. In this study, we introduce several traditional machine learning algorithms and finally choose the improved gradient boosting decision tree (GBDT) algorithm software library for use in a real system, namely, XGBoost. We further expand multiple features based on analysis of the enrollment behavior and plan to add historical transactions in future studies. Subsequently, we use a real card enrollment dataset covering the year 2017, provided by a worldwide payment processor. The results and framework are adopted and absorbed into a new design for a mobile payment fraud detection system within the Chinese payment processor.;22
The ConnectedDrive Context Server – flexible Software Architecture for a Context Aware Vehicle;J. Valldorf, W. Gessner;2007;Publication venue not available;Abstract not available;6
Insert beyond the traffic sign recognition: constructing an auto-pilot map for autonomous vehicles;Zhenhua Zhang, Leon Stenneth, Ram Marappan, Zaba Sebastian, Philip S. Yu;2018;SIGSPATIAL/GIS;"Traffic sign recognition (TSR) systems on the vehicles can collect posted speed limit sign information and have been in commercial usage since 2008. A daily-updated auto-pilot map can be constructed based on the massive amounts of TSR observations from multiple consumer vehicles; the data is then aggregated, filtered and processed, and the learned posted speed limit signs can be finally transferred to vehicles with high-coverage and real-time speed limit information. Compared with the direct sign detection by TSR systems, the auto-pilot map can complement the current detection errors, reduce the camera cost and provide a continuous speed limit information for autonomous vehicle applications. A pipeline of methods are specifically designed to deliver our research purpose by making full utilization of TSR observations and HERE map. Experimental results indicate that our proposed algorithms and methods can construct an auto-pilot map with an overall accuracy of 95.8%. It is also expected to update the speed limit information in a map at a faster pace than the traditional map since we are using sensors of customer vehicles instead of dedicated map construction vehicles. The utility of our proposed auto-pilot map opens a new perspective in autonomous driving.";4
Adasa: A Conversational In-Vehicle Digital Assistant for Advanced Driver Assistance Features;Shi-Chieh Lin, Chang-Hong Hsu, W. Talamonti, Yunqi Zhang, Steve Oney, Jason Mars, Lingjia Tang;2018;ACM Symposium on User Interface Software and Technology;Advanced Driver Assistance Systems (ADAS) come equipped on most modern vehicles and are intended to assist the driver and enhance the driving experience through features such as lane keeping system and adaptive cruise control. However, recent studies show that few people utilize these features for several reasons. First, ADAS features were not common until recently. Second, most users are unfamiliar with these features and do not know what to expect. Finally, the interface for operating these features is not intuitive. To help drivers understand ADAS features, we present a conversational in-vehicle digital assistant that responds to drivers' questions and commands in natural language. With the system prototyped herein, drivers can ask questions or command using unconstrained natural language in the vehicle, and the assistant trained by using advanced machine learning techniques, coupled with access to vehicle signals, responds in real-time based on conversational context. Results of our system prototyped on a production vehicle are presented, demonstrating its effectiveness in improving driver understanding and usability of ADAS.;37
Impact of Advanced Synoptics and Simplified Checklists during Aircraft Systems Failures;T. Etherington, L. Kramer, Lisa R. Le Vie, M. C. Last, Kellie D. Kennedy, R. Bailey, Vincent E. Houston;2018;Symposium on Dependable Autonomic and Secure Computing;Natural human capacities are becoming increasingly mismatched to the enormous data volumes, processing capabilities, and decision speeds demanded in today's aviation environment. Increasingly Autonomous Systems (IAS) are uniquely suited to solve this problem. NASA is conducting research and development of IAS - hardware and software systems, utilizing machine learning algorithms, seamlessly integrated with humans whereby task performance of the combined system is significantly greater than the individual components. IAS offer the potential for significantly improved levels of performance and safety that are superior to either human or automation alone. A human-in-the-loop test was conducted in NASA Langley's Integration Flight Deck B-737-800 simulator to evaluate advanced synoptic pages with simplified interactive electronic checklists as an IAS for routine air carrier flight operations and in response to aircraft system failures. Twelve U.S. airline crews flew various normal and non-normal procedures and their actions and performance were recorded in response to failures. These data are fundamental to and critical for the design and development of future increasingly autonomous systems that can better support the human in the cockpit. Synoptic pages and electronic checklists significantly improved pilot responses to non-normal scenarios, but implementation of these aids and other intelligent assistants have barriers to implementation (e.g., certification cost) that must be overcome.;3
Real-time prediction of flight arrival times using surveillance information;A. M. Hernández, David Scarlatti, P. Costas;2018;European Conference on Software Architecture;Accurate estimations of the time of arrival of a flight prior to its landing are beneficial for most of the stakeholders in the air traffic management and control industry, as they could lead to reductions in potential safety risks and improvements in resources allocation. In this paper, and within the European Commission funded Transforming Transport project, we propose a methodology for real-time prediction of flight arrival times based on the application of machine learning techniques. For this purpose, we employ state-of-the-art data warehousing and broadcasting processes, that allow both the training of a regression machine learning model and the integration of its predictions of current flights on a real-time visualization tool set up for customer usage. The model only makes use of the information included in aircraft surveillance messages. Predictions obtained with such model are compared to those provided by other current services to observe the added value of the application of the proposed system on real-time operations.;1
Intelligent network management mechanisms as a step towards SG;A. Bosneag, Mingxue Wang;2017;International Conference on Network of the Future;The evolution of telecom networks towards 5G comes with challenges that permeate the entire network structure and its management principles. In this paper, we outline how challenges such as the need to support different types of users and new network business models, the movement towards virtualization, as well as the need for more automation drive the requirement for a holistic and flexible way of managing the networks. We detail the architectural principles that underline such a cognitive management framework and we exemplify its use through a scenario based on software defined networking, where we combine machine learning, control and automation in the context of flexible resource provisioning in the Radio Access Network. Our experiments were conducted in collaboration with a major telecom operator and clearly show the advantages of introducing intelligence and automation into the network.;8
DARN: Dynamic Baselines for Real-time Network Monitoring;Rashid Mijumbi, A. Asthana, Markku Koivunen, Haiyong Fu, Zhu Norman;2018;IEEE Conference on Network Softwarization;Network monitoring is necessary so as to ensure high reliability and availability in telecom networks. One of the main challenges posed by state-of-the-art monitoring tools is the creation of network baselines. Such baselines include thresholds that can be used to determine whether monitored values (with a given context, e.g. time) represent normal network operation or not. The size and complexity of current (and future) networks makes it infeasible to manually determine and set baselines for each network operator and metric, let alone adapting the thresholds to changes in network conditions. This leads to the use of default baselines and/or setting baselines only once and never changing them throughout the lifetime of network elements. This does not only cause inefficient operation, but could have implications for network reliability and availability. In this paper, we present the design, implementation, and evaluation of DARN: a collection of analytics and machine learning-based algorithms aimed at ensuring that network baselines are automatically adapted to different metric evolution. DARN has been comprehensively evaluated on a deployment with real traffic to confirm accuracy of generated baselines, a 22% improvement in accuracy due to baseline adaptation, and a 72% reduction in false alarms.;7
Subscriber classification within telecom networks utilizing big data technologies and machine learning;Jonathan Magnusson, T. Kvernvik;2012;BigMine '12;This paper describes a scalable solution for identifying influential subscribers in for example telecom networks. The solution estimates one weighted value of influence out of several Social Network Analysis(SNA) metrics. The novel method for aggregation of several metrics utilizes machine learning to train models. A prototype solution has been implemented on a Hadoop platform to support scalability and to reduce hard ware cost by enabling the usage of commodity computers. The SNA algorithms have been adapted to efficiently execute on the MapReduce distributed platform. The prototype solution has been tested on a Hadoop cluster. The tests have verified that the solution can scale to support networks with millions of subscribers. Both real data from a telecom network operator with 2.4 million subscribers and synthetic data for networks up to 100 million subscribers have been used to verify the scalability and accuracy of the solution. The correlation between metrics have been analyzed to identify the information gain from each metric.;21
Preprocessing Prediction of Advanced Algorithms for Medical Imaging;Bella Fadida-Specktor;2018;Journal of digital imaging;Abstract not available;1
Development of an Infrastructure for the Prediction of Biological Endpoints in Industrial Environments. Lessons Learned at the eTOX Project;M. Pastor, Jordi Quintana, F. Sanz;2018;Frontiers in Pharmacology;In silico methods are increasingly being used for assessing the chemical safety of substances, as a part of integrated approaches involving in vitro and in vivo experiments. A paradigmatic example of these strategies is the eTOX project http://www.etoxproject.eu, funded by the European Innovative Medicines Initiative (IMI), which aimed at producing high quality predictions of in vivo toxicity of drug candidates and resulted in generating about 200 models for diverse endpoints of toxicological interest. In an industry-oriented project like eTOX, apart from the predictive quality, the models need to meet other quality parameters related to the procedures for their generation and their intended use. For example, when the models are used for predicting the properties of drug candidates, the prediction system must guarantee the complete confidentiality of the compound structures. The interface of the system must be designed to provide non-expert users all the information required to choose the models and appropriately interpret the results. Moreover, procedures like installation, maintenance, documentation, validation and versioning, which are common in software development, must be also implemented for the models and for the prediction platform in which they are implemented. In this article we describe our experience in the eTOX project and the lessons learned after 7 years of close collaboration between industrial and academic partners. We believe that some of the solutions found and the tools developed could be useful for supporting similar initiatives in the future.;10
Implementation of Cyber-Physical Production Systems for Quality Prediction and Operation Control in Metal Casting;JuneHyuck Lee, S. Noh, Hyun-Jung Kim, Yong-Shin Kang;2018;Italian National Conference on Sensors;The prediction of internal defects of metal casting immediately after the casting process saves unnecessary time and money by reducing the amount of inputs into the next stage, such as the machining process, and enables flexible scheduling. Cyber-physical production systems (CPPS) perfectly fulfill the aforementioned requirements. This study deals with the implementation of CPPS in a real factory to predict the quality of metal casting and operation control. First, a CPPS architecture framework for quality prediction and operation control in metal-casting production was designed. The framework describes collaboration among internet of things (IoT), artificial intelligence, simulations, manufacturing execution systems, and advanced planning and scheduling systems. Subsequently, the implementation of the CPPS in actual plants is described. Temperature is a major factor that affects casting quality, and thus, temperature sensors and IoT communication devices were attached to casting machines. The well-known NoSQL database, HBase and the high-speed processing/analysis tool, Spark, are used for IoT repository and data pre-processing, respectively. Many machine learning algorithms such as decision tree, random forest, artificial neural network, and support vector machine were used for quality prediction and compared with R software. Finally, the operation of the entire system is demonstrated through a CPPS dashboard. In an era in which most CPPS-related studies are conducted on high-level abstract models, this study describes more specific architectural frameworks, use cases, usable software, and analytical methodologies. In addition, this study verifies the usefulness of CPPS by estimating quantitative effects. This is expected to contribute to the proliferation of CPPS in the industry.;111
Intelligent security on the edge of the cloud;Dimitrios Zissis;2017;International Conference on Engineering, Technology and Innovation;Edge or Fog computing is a relatively new architectural deployment model, ideally fit for the unique requirements of the Internet of Things. This paper presents a novel solution, which leverages the architectural characteristics of edge computing for security reasons. Machine learning models (specifically Support Vector Machines) are employed on the edge of the cloud, to perform low footprint unsupervised learning and analysis of sensor data for anomaly detection purposes. To this end, a proof of concept system is developed, capable of detecting anomalies in real world vessel sensor streams (big data) in a smart port environment. We report on early results, that validate the potential of the solution. The quality and performance of the model is investigated in real world conditions.;20
Practical Machine Learning for Cloud Intrusion Detection: Challenges and the Way Forward;R. Kumar, Andrew W. Wicker, Matt Swann;2017;AISec@CCS;"Operationalizing machine learning based security detections is extremely challenging, especially in a continuously evolving cloud environment. Conventional anomaly detection does not produce satisfactory results for analysts that are investigating security incidents in the cloud. Model evaluation alone presents its own set of problems due to a lack of benchmark datasets. When deploying these detections, we must deal with model compliance, localization, and data silo issues, among many others. We pose the problem of ""attack disruption"" as a way forward in the security data science space. In this paper, we describe the framework, challenges, and open questions surrounding the successful operationalization of machine learning based security detections in a cloud environment and provide some insights on how we have addressed them.";41
The Data Linter: Lightweight Automated Sanity Checking for ML Data Sets;Nicholas Hynes, D. Sculley, Michael Terry;2017;Publication venue not available;"Data cleaning and feature engineering are both common practices when developing machine learning (ML) models. However, developers are not always aware of best practices for preparing or transforming data for a given model type, which can lead to suboptimal representations of input features. To address this issue, we introduce the data linter , a new class of ML tool that automatically inspects ML data sets to 1) identify potential issues in the data and 2) suggest potentially useful feature transforms, for a given model type. As with traditional code linting, data linting automatically identiﬁes potential issues or inefﬁciencies; codiﬁes best practices and educates end-users about these practices through tool use; and can lead to quality improvements. In this paper, we provide a detailed description of data linting, describe our initial implementation of a data linter for deep neural networks";79
"""Deep"" Learning for Missing Value Imputationin Tables with Non-Numerical Data";F. Biessmann, David Salinas, Sebastian Schelter, Philipp Schmidt, Dustin Lange;2018;International Conference on Information and Knowledge Management;The success of applications that process data critically depends on the quality of the ingested data. Completeness of a data source is essential in many cases. Yet, most missing value imputation approaches suffer from severe limitations. They are almost exclusively restricted to numerical data, and they either offer only simple imputation methods or are difficult to scale and maintain in production. Here we present a robust and scalable approach to imputation that extends to tables with non-numerical values, including unstructured text data in diverse languages. Experiments on public data sets as well as data sets sampled from a large product catalog in different languages (English and Japanese) demonstrate that the proposed approach is both scalable and yields more accurate imputations than previous approaches. Training on data sets with several million rows is a matter of minutes on a single machine. With a median imputation F1 score of 0.93 across a broad selection of data sets our approach achieves on average a 23-fold improvement compared to mode imputation. While our system allows users to apply state-of-the-art deep learning models if needed, we find that often simple linear n-gram models perform on par with deep learning methods at a much lower operational cost. The proposed method learns all parameters of the entire imputation pipeline automatically in an end-to-end fashion, rendering it attractive as a generic plugin both for engineers in charge of data pipelines where data completeness is relevant, as well as for practitioners without expertise in machine learning who need to impute missing values in tables with non-numerical data.;82
DeepX: A Software Accelerator for Low-Power Deep Learning Inference on Mobile Devices;N. Lane, S. Bhattacharya, Petko Georgiev, Claudio Forlivesi, Lei Jiao, Lorena Qendro, F. Kawsar;2016;International Symposium on Information Processing in Sensor Networks;"Breakthroughs from the field of deep learning are radically changing how sensor data are interpreted to extract the high-level information needed by mobile apps. It is critical that the gains in inference accuracy that deep models afford become embedded in future generations of mobile apps. In this work, we present the design and implementation of DeepX, a software accelerator for deep learning execution. DeepX signif- icantly lowers the device resources (viz. memory, computation, energy) required by deep learning that currently act as a severe bottleneck to mobile adoption. The foundation of DeepX is a pair of resource control algorithms, designed for the inference stage of deep learning, that: (1) decompose monolithic deep model network architectures into unit- blocks of various types, that are then more efficiently executed by heterogeneous local device processors (e.g., GPUs, CPUs); and (2), perform principled resource scaling that adjusts the architecture of deep models to shape the overhead each unit-blocks introduces. Experiments show, DeepX can allow even large-scale deep learning models to execute efficently on modern mobile processors and significantly outperform existing solutions, such as cloud-based offloading.";479
Diagnostic visualization for non-expert machine learning practitioners: A design study;Dong Chen, R. Bellamy, Peter K. Malkin, Thomas Erickson;2016;IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments;"As machine learning (ML) becomes increasingly popular, developers without deep experience in ML - who we will refer to as ML practitioners - are facing the need to diagnose problems with ML models. Yet successful diagnosis requires high-level expertise that practitioners lack. As in many complex data-oriented domains, visualization could help. This two-phase study explored the design of visualizations to aid ML diagnosis. In phase 1, twelve ML practitioners were asked to diagnose a model using ten state-of-the-art visualizations; seven design themes were identified. In phase 2, several design themes were embodied in an interactive visualization. The visualization was used to engage practitioners in a participatory design exercise that explored how they would carry out multi-step diagnosis using the visualization. Our findings provide design implications for tools that better support ML diagnosis by non-expert practitioners.";21
Dynamic control flow in large-scale machine learning;Yuan Yu, Martín Abadi, P. Barham, E. Brevdo, M. Burrows, Andy Davis, J. Dean, Sanjay Ghemawat, Tim Harley, Peter Hawkins, M. Isard, M. Kudlur, R. Monga, D. Murray, Xiaoqiang Zheng;2018;European Conference on Computer Systems;Many recent machine learning models rely on fine-grained dynamic control flow for training and inference. In particular, models based on recurrent neural networks and on reinforcement learning depend on recurrence relations, data-dependent conditional execution, and other features that call for dynamic control flow. These applications benefit from the ability to make rapid control-flow decisions across a set of computing devices in a distributed system. For performance, scalability, and expressiveness, a machine learning system must support dynamic control flow in distributed and heterogeneous environments. This paper presents a programming model for distributed machine learning that supports dynamic control flow. We describe the design of the programming model, and its implementation in TensorFlow, a distributed machine learning system. Our approach extends the use of dataflow graphs to represent machine learning models, offering several distinctive features. First, the branches of conditionals and bodies of loops can be partitioned across many machines to run on a set of heterogeneous devices, including CPUs, GPUs, and custom ASICs. Second, programs written in our model support automatic differentiation and distributed gradient computations, which are necessary for training machine learning models that use control flow. Third, our choice of non-strict semantics enables multiple loop iterations to execute in parallel across machines, and to overlap compute and I/O operations. We have done our work in the context of TensorFlow, and it has been used extensively in research and production. We evaluate it using several real-world applications, and demonstrate its performance and scalability.;103
Swayam: distributed autoscaling to meet SLAs of machine learning inference services with resource efficiency;A. Gujarati, S. Elnikety, Yuxiong He, K. McKinley, Björn B. Brandenburg;2017;International Middleware Conference;Developers use Machine Learning (ML) platforms to train ML models and then deploy these ML models as web services for inference (prediction). A key challenge for platform providers is to guarantee response-time Service Level Agreements (SLAs) for inference workloads while maximizing resource efficiency. Swayam is a fully distributed autoscaling framework that exploits characteristics of production ML inference workloads to deliver on the dual challenge of resource efficiency and SLA compliance. Our key contributions are (1) model-based autoscaling that takes into account SLAs and ML inference workload characteristics, (2) a distributed protocol that uses partial load information and prediction at frontends to provision new service instances, and (3) a backend self-decommissioning protocol for service instances. We evaluate Swayam on 15 popular services that were hosted on a production ML-as-a-service platform, for the following service-specific SLAs: for each service, at least 99% of requests must complete within the response-time threshold. Compared to a clairvoyant autoscaler that always satisfies the SLAs (i.e., even if there is a burst in the request rates), Swayam decreases resource utilization by up to 27%, while meeting the service-specific SLAs over 96% of the time during a three hour window. Microsoft Azure's Swayam-based framework was deployed in 2016 and has hosted over 100,000 services.;112
TensorFlow Acceleration on ARM Hikey Board;M. Goli, L. Iwanski, John Lawson, Uwe Dolinsky, A. Richards;2018;International Workshop on OpenCL;There is huge demand for targeting complex and large-scale machine learning applications particularly those based on popular actively-maintained frameworks such as TensorFlow and CAFFE to a variety of platforms with accelerators ranging from high-end desktop GPUs to resource-constrained embedded or mobile GPUs, FPGAs, and DSPs. However, to deliver good performance different platforms may require different algorithms or data structures, yet code should be easily portable and reused as much as possible across different devices. The open SYCL standard addresses this by providing parallel processing through a single-source programming model enabling the same standard C++ code to be used on the CPU and accelerator. This allows high-level C++ abstractions and templates to be used to quickly configure device and host code to cover specific features of the platform. By targeting OpenCL, SYCL enables C++ applications such as TensorFlow to run efficiently on OpenCL devices without having to write OpenCL code.;2
TensorFlow Debugger: Debugging Dataflow Graphs for Machine Learning;Shanqing Cai, Eric Breck, Eric Nielsen, M. Salib, D. Sculley;2016;Publication venue not available;"Debuggability is important in the development of machine-learning (ML) systems. Several widely-used ML libraries, such as TensorFlow and Theano, are based on dataﬂow graphs. While offering important beneﬁts such as facilitating distributed training, the dataﬂow graph paradigm makes the debugging of model issues more challenging compared to debugging in the more conventional procedural paradigm. In this paper, we present the design of the TensorFlow Debugger ( tfdbg ), a specialized debugger for ML models written in TensorFlow. tfdbg provides features to inspect runtime dataﬂow graphs and the state of the intermediate graph elements (""tensors""), as well as simulating stepping on the graph. We will discuss the application of this debugger in development and testing use cases.";17
GaDei: On Scale-Up Training as a Service for Deep Learning;Wei Zhang, Minwei Feng, Yunhui Zheng, Yufei Ren, Yandong Wang, Ji Liu, Peng Liu, Bing Xiang, Li Zhang, Bowen Zhou, Fei Wang;2016;Industrial Conference on Data Mining;Deep learning (DL) training-as-a-service (TaaS) is an important emerging industrial workload. TaaS must satisfy a wide range of customers who have no experience and/or resources to tune DL hyper-parameters (e.g., mini-batch size and learning rate), and meticulous tuning for each user's dataset is prohibitively expensive. Therefore, TaaS hyper-parameters must be fixed with values that are applicable to all users. Unfortunately, few research papers have studied how to design a system for TaaS workloads. By evaluating the IBM Watson Natural Language Classfier (NLC) workloads, the most popular IBM cognitive service used by thousands of enterprise-level clients globally, we provide empirical evidence that only the conservative hyper-parameter setup (e.g., small mini-batch size) can guarantee acceptable model accuracy for a wide range of customers. Unfortunately, smaller mini-batch size requires higher communication bandwidth in a parameter-server based DL training system. In this paper, we characterize the exceedingly high communication bandwidth requirement of TaaS using representative industrial deep learning workloads. We then present GaDei, a highly optimized shared-memory based scale-up parameter server design. We evaluate GaDei using both commercial benchmarks and public benchmarks and demonstrate that GaDei significantly outperforms the state-of-the-art parameter-server based implementation while maintaining the required accuracy. GaDei achieves near-best-possible runtime performance, constrained only by the hardware limitation. Furthermore, to the best of our knowledge, GaDei is the only scale-up DL system that provides fault-tolerance.;10
Declarative Metadata Management : A Missing Piece in End-To-End Machine Learning;Sebastian Schelter, Joos-Hendrik Böse, Johannes Kirschnick, Thoralf Klein, Stephan Seufert, Amazon;2018;Publication venue not available;We argue for the necessity of managing the metadata and lineage of common artifacts in machine learning (ML). We discuss a recently presented lightweight system built for this task, which accelerates users in their ML workflows, and provides a basis for comparability and repeatability of ML experiments. This system tracks the lineage of produced artifacts in ML workloads and automatically extracts metadata such as hyperparameters of models, schemas of datasets and layouts of deep neural networks. It provides a general declarative representation of common ML artifacts, is integrated with popular frameworks such as MXNet, SparkML and scikit-learn, and meets the demands of various production use cases at Amazon. ACM Reference Format: Sebastian Schelter, Joos-Hendrik Böse, Johannes Kirschnick, Thoralf Klein, Stephan Seufert. 2018. Declarative Metadata Management: A Missing Piece in End-To-End Machine Learning. In Proceedings of SysML Conference, Stanford, USA, Feb 2018 (SYSML’18), 3 pages. https://doi.org/10.475/123_4;17
IBM Deep Learning Service;Bishwaranjan Bhattacharjee, S. Boag, C. Doshi, Parijat Dube, Benjamin Herta, Vatche Isahagian, K. R. Jayaram, Rania Y. Khalaf, A. Krishna, Yu Bo Li, Vinod Muthusamy, Ruchi Puri, Yufei Ren, Florian Rosenberg, Seetharami R. Seelam, Yandong Wang, J. Zhang, Li Zhang;2017;IBM Journal of Research and Development;"Deep learning driven by large neural network models is overtaking traditional machine learning methods for understanding unstructured and perceptual data domains such as speech, text, and vision. At the same time, the ""as-a-Service""-based business model on the cloud is fundamentally transforming the information technology industry. These two trends: deep learning, and ""as-a-service"" are colliding to give rise to a new business model for cognitive application delivery: deep learning as a service in the cloud. In this paper, we will discuss the details of the software architecture behind IBM's deep learning as a service (DLaaS). DLaaS provides developers the flexibility to use popular deep learning libraries such as Caffe, Torch and TensorFlow, in the cloud in a scalable and resilient manner with minimal effort. The platform uses a distribution and orchestration layer that facilitates learning from a large amount of data in a reasonable amount of time across compute nodes. A resource provisioning layer enables flexible job management on heterogeneous resources, such as graphics processing units (GPUs) and central processing units (CPUs), in an infrastructure as a service (IaaS) cloud.";31
Nexus: Bringing Efficient and Scalable Training to Deep Learning Frameworks;Yandong Wang, Li Zhang, Yufei Ren, Wei Zhang;2017;IEEE/ACM International Symposium on Modeling, Analysis, and Simulation On Computer and Telecommunication Systems;Demand is mounting in the industry for scalable GPU-based deep learning systems. Unfortunately, existing training applications built atop popular deep learning frameworks, including Caffe, Theano, and Torch, etc, are incapable of conducting distributed GPU training over large-scale clusters.To remedy such a situation, this paper presents Nexus, a platform that allows existing deep learning frameworks to easily scale out to multiple machines without sacrificing model accuracy. Nexus leverages recently proposed distributed parameter management architecture to orchestrate distributed training by a large number of learners spread across the cluster. Through characterizing the run-time behavior of existing single-node based applications, Nexus is equipped with a suite of optimization schemes, including hierarchical and hybrid parameter aggregation, enhanced network and computation layer, and quality-guided communication adjustment, etc, to strengthen the communication channels and resource utilization. Empirical evaluations with a diverse set of deep learning applications demonstrate that Nexus is easy to integrate and can deliver efficient distributed training services to major deep learning frameworks. In addition, Nexus's optimization schemes are highly effective to shorten the training time with targeted accuracy bounds.;7
Runway : machine learning model experiment management tool;Jason Tsay, Todd W. Mummert, N. Bobroff, Alan Braz, P. Westerink, Martin Hirzel, Yorktown Heights;2018;Publication venue not available;Runway is a cloud-native tool for managing machine learning experiments and their associated models. The iterative nature of developing models results in a large number of experiments and models that are often managed in an ad hoc manner. Runway is a workflow and framework independent tool that centrally manages and maintains metadata and links to artifacts needed to reproduce models and experiments. Runway provides a web dashboard with multiple levels of visualizations to evaluate performance and enable side-by-side comparisons of models and experiments.;22
The ML test score: A rubric for ML production readiness and technical debt reduction;Eric Breck, Shanqing Cai, Eric Nielsen, M. Salib, D. Sculley;2017;2017 IEEE International Conference on Big Data (Big Data);Creating reliable, production-level machine learning systems brings on a host of concerns not found in small toy examples or even large offline research experiments. Testing and monitoring are key considerations for ensuring the production-readiness of an ML system, and for reducing technical debt of ML systems. But it can be difficult to formulate specific tests, given that the actual prediction behavior of any given model is difficult to specify a priori. In this paper, we present 28 specific tests and monitoring needs, drawn from experience with a wide range of production ML systems to help quantify these issues and present an easy to follow road-map to improve production readiness and pay down ML technical debt.;168
Automating Large-Scale Data Quality Verification;Sebastian Schelter, Dustin Lange, Philipp Schmidt, Meltem Celikel, F. Biessmann, Andreas Grafberger;2018;Proceedings of the VLDB Endowment;Modern companies and institutions rely on data to guide every single business process and decision. Missing or incorrect information seriously compromises any decision process downstream. Therefore, a crucial, but tedious task for everyone involved in data processing is to verify the quality of their data. We present a system for automating the verification of data quality at scale, which meets the requirements of production use cases. Our system provides a declarative API, which combines common quality constraints with user-defined validation code, and thereby enables 'unit tests' for data. We efficiently execute the resulting constraint validation workload by translating it to aggregation queries on Apache Spark. Our platform supports the incremental validation of data quality on growing datasets, and leverages machine learning, e.g., for enhancing constraint suggestions, for estimating the 'predictability' of a column, and for detecting anomalies in historic data quality time series. We discuss our design decisions, describe the resulting system architecture, and present an experimental evaluation on various datasets.;197
Making the Case for Safety of Machine Learning in Highly Automated Driving;S. Burton, Lydia Gauerhof, Christian Heinzemann;2017;SAFECOMP Workshops;Abstract not available;113
Deep learning challenges and solutions with Xilinx FPGAs;Elliott Delaye, Ashish Sirasao, Chaithanya Dudha, Sabya Das;2017;2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD);In this paper, we will describe the architectural, software, performance, and implementation challenges and solutions and current research on the use of programmable logic to enable deep learning applications. First a discussion of characteristics of building a deep learning system will described. Next architectural choices will be explained for how a FPGA fabric can efficiently solve deep learning tasks. Finally specific techniques for how DSPs, memories and are used in high performance applications will be described.;12
A Service Mesh-Based Load Balancing and Task Scheduling System for Deep Learning Applications;Xiao-song Xie, Shyam Govardhan;2020;IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing;In recent years, there are increasing technologies that benefit from cloud computing and edge computing, especially the application in the Deep Learning and Internet of Things topics. Functionalities such as load balancing, traffic routing, and task scheduling, which used to be part of the software, are now enabled and provided as API or microservices on the underlying infrastructure. Therefore, when we want to deliver a container application to the users either on the cloud cluster or edge cluster, developers do not need to worry about these issues and only focus on the development of the functionality. Nevertheless, for the deployment stage, the system architect should design the fundamental architecture to build an ecosystem for the application. In this paper, a practical approach is proposed to develop a flexible and on-demand system for deep learning applications based on container and service mesh technologies. Our experiment chooses Kubernetes as the container orchestration platform and introduces Istio to implement the service mesh. We packaged one Flask based Deep learning model into a Docker container, and successfully deploy and provision this image classifier application to the public users with Functionalities like load balancing and task scheduling. Furthermore, we empower the system with an evaluation of the resource utilization services, and traffic flows inside the Kubernetes cluster in a visualized manner.;11
Deep Learning Cookbook: Recipes for your AI Infrastructure and Applications;S. Serebryakov, D. Milojicic, N. Vassilieva, S. Fleischman, R. Clark;2019;International Conference on Rebooting Computing;Deep Learning (DL) has gained wide adoption and different DL models have been deployed for an expanding number of applications. It is being used both for inference at the edge and for training in datacenters. Applications include image recognition, video analytics, pattern recognition in networking traffic, and many others. Different applications rely on different neural network models, and it has proven difficult to predict resource requirements for different models and applications. This leads to the nonsystematic and suboptimal selection of computational resources for DL applications resulting in overpaying for underutilized infrastructure or, even worse, the deployment of models on underpowered hardware and missed service level objectives. In this paper we present the DL Cookbook, a toolset that a) helps with benchmarking models on different hardware, b) guides the use of DL and c) provides reference designs. Automated benchmarking collects performance data for different DL workloads (training and inference with different models) on various hardware and software configurations. A web-based tool guides a choice of optimal hardware and software configuration via analysis of collected performance data and applying performance models. And finally, it offers reference hardware/software stacks for particular classes of deep learning workloads. This way the DL Cookbook helps both customers and hardware vendors match optimal DL models to the available hardware and vice versa, in case of acquisition, specify required hardware to models in question. Finally, DL Cookbook helps with reproducibility of results.;1
Deep Learning Based Parking Prediction on Cloud Platform;Jia-Chen Li, Jiming Li, Haitao Zhang;2018;International Conference on Big Data Computing and Communications;"With the explosive growth of the urban population, an increasing number of cities face the parking problem. Studies shows that more than 30% of the urban traffic is due to these ""search berth"" traffic, which aggravate the traffic congestion and create more traffic pollutants. The prediction for parking availability plays more and more important role in the urban traffic management system. However, existing parking prediction methods have lower accuracy or require a large number of data and computation cost, which make it difficult for parking prediction to be widely used. In this paper, we propose a deep learning based parking prediction system architecture on cloud platform. The LSTM network is used to predict the availability of parking space, which performs well in time series prediction. In order to improve the prediction accuracy, we comprehensively take more factors into account such as time of day, weather and holiday. In addition, we also propose an economical workflow based on the elastic computing service (ECS) provided by cloud platform. In the optimized workflow, the model training and updating processes need not to be in running state all the time, which can reduce avoidable computation and cost significantly. We implement the proposed system and deploy it on the Ali Could platform. Experimental results show that our prediction model based on LSTM network outperforms BP neural network, and our system has good scalability to apply to the cloud platform.";18
Deploying AI Frameworks on Secure HPC Systems with Containers.;D. Brayford, S. Vallecorsa, Atanas Z. Atanasov, F. Baruffa, Walter Riviera;2019;IEEE Conference on High Performance Extreme Computing;The increasing interest in the usage of Artificial Intelligence (AI) techniques from the research community and industry to tackle “real world” problems, requires High Performance Computing (HPC) resources to efficiently compute and scale complex algorithms across thousands of nodes. Unfortunately, typical data scientists are not familiar with the unique requirements and characteristics of HPC environments. They usually develop their applications with high level scripting languages or frameworks such as TensorFlow and the installation processes often require connection to external systems to download open source software during the build. HPC environments, on the other hand, are often based on closed source applications that incorporate parallel and distributed computing API’s such as MPI and OpenMP, while users have restricted administrator privileges, and face security restrictions such as not allowing access to external systems. In this paper we discuss the issues associated with the deployment of AI frameworks in a secure HPC environment and how we successfully deploy AI frameworks on SuperMUC-NG with Charliecloud.;18
EdgeLens: Deep Learning based Object Detection in Integrated IoT, Fog and Cloud Computing Environments;Shreshth Tuli, Nipam Basumatary, R. Buyya;2019;2019 4th International Conference on Information Systems and Computer Networks (ISCON);Data-intensive applications are growing at an increasing rate and there is a growing need to solve scalability and high-performance issues in them. By the advent of Cloud computing paradigm, it became possible to harness remote resources to build and deploy these applications. In recent years, new set of applications and services based on Internet of Things (IoT) paradigm, require to process large amount of data in very less time. Among them surveillance and object detection have gained prime importance, but cloud is unable to bring down the network latencies to meet the response time requirements. This problem is solved by Fog computing which harnesses resources in the edge of the network along with remote cloud resources as required. However, there is still a lack of frameworks that are successfully able to integrate sophisticated software and applications, especially deep learning, with fog and cloud computing environments. In this work, we propose a framework to deploy deep learning-based applications in fog-cloud environments to harness edge and cloud resources to provide better service quality for such applications. Our proposed framework, called EdgeLens, adapts to the application or user requirements to provide high accuracy or low latency modes of services. We also tested the performance of the software in terms of accuracy, response time, jitter, network bandwidth and power consumption and show how EdgeLens adapts to different service requirements.;58
Architecting AI Deployment: A Systematic Review of State-of-the-Art and State-of-Practice Literature;Meenu Mary John, H. H. Olsson, J. Bosch;2020;International Conference on Software Business;Abstract not available;22
Containerized Architecture for Edge Computing in Smart Home : A consistent architecture for model deployment;Nitu Gupta, Katpagavalli Anantharaj, K. Subramani;2020;International Conference on Computational Collective Intelligence;Network bandwidth and high latency are the main bottlenecks of cloud computing. To combat such scenarios, new paradigm Edge Computing is used. Edge computing shifts the computation of resources from centralized cloud closer to the devices which generates data. Edge Computing reduces the response time, latency and improves the battery life while maintaining data safety and privacy. The distributed architecture of edge computing makes resource management an important aspect of edge computing. In such a resource constrained environment edge devices should be capable of processing all types of request coming from IoT devices. With the advancement in Data science and Machine learning models in different domains, many intelligent services have emerged to provide better user experience. These deep learning models have frequent updates to adapt to new hardware and software requirements. These models also have some installation dependencies and requirement of cross platform compatibility for training and prediction. In home edge environment, these issues are more important due to resource constraint in terms of memory and computing power. Also, due to the availability of extensive list of deep learning models for different service, it is difficult to maintain an environment supporting such models across various devices in the smart home to support all services. To solve this issue in smart home environment, this paper proposes architecture, using containerization techniques to deploy and manage the deep learning models. This paper also explains about the steps to convert the existing model into containers. Minimal space requirement on the edge device, data privacy, low latency along with device independence for deep learning models are prime benefits of the architecture proposed. To test the performance of the architecture, deep learning model was containerized and compared with the actual model deployed in the same edge environment. The experimental results demonstrated the performance is almost similar of the containerized architecture in terms of model execution time and CPU load vs execution time. But it comes with ease of model deployment and cross platform model execution.;17
A Reality Check on Inference at Mobile Networks Edge;Alejandro Cartas, M. Kocour, Aravindh Raman, Ilias Leontiadis, J. Luque, Nishanth R. Sastry, José Núñez-Martínez, Diego Perino, C. Segura;2019;EdgeSys@EuroSys;Edge computing is considered a key enabler to deploy Artificial Intelligence platforms to provide real-time applications such as AR/VR or cognitive assistance. Previous works show computing capabilities deployed very close to the user can actually reduce the end-to-end latency of such interactive applications. Nonetheless, the main performance bottleneck remains in the machine learning inference operation. In this paper, we question some assumptions of these works, as the network location where edge computing is deployed, and considered software architectures within the framework of a couple of popular machine learning tasks. Our experimental evaluation shows that after performance tuning that leverages recent advances in deep learning algorithms and hardware, network latency is now the main bottleneck on end-to-end application performance. We also report that deploying computing capabilities at the first network node still provides latency reduction but, overall, it is not required by all applications. Based on our findings, we overview the requirements and sketch the design of an adaptive architecture for general machine learning inference across edge locations.;30
OpenEI: An Open Framework for Edge Intelligence;Xingzhou Zhang, Yifan Wang, Sidi Lu, Liangkai Liu, Lanyu Xu, Weisong Shi;2019;IEEE International Conference on Distributed Computing Systems;In the last five years, edge computing has attracted tremendous attention from industry and academia due to its promise to reduce latency, save bandwidth, improve availability, and protect data privacy to keep data secure. At the same time, we have witnessed the proliferation of AI algorithms and models which accelerate the successful deployment of intelligence mainly in cloud services. These two trends, combined together, have created a new horizon: Edge Intelligence (EI). The development of EI requires much attention from both the computer systems research community and the AI community to meet these demands. However, existing computing techniques used in the cloud are not applicable to edge computing directly due to the diversity of computing sources and the distribution of data sources. We envision that there missing a framework that can be rapidly deployed on edge and enable edge AI capabilities. To address this challenge, in this paper we first present the definition and a systematic review of EI. Then, we introduce an Open Framework for Edge Intelligence (OpenEI), which is a lightweight software platform to equip edges with intelligent processing and data sharing capability. We analyze four fundamental EI techniques which are used to build OpenEI and identify several open problems based on potential research directions. Finally, four typical application scenarios enabled by OpenEI are presented.;93
Cloud Computing Architecture for High-volume ML-based Solutions;M. M. Rovnyagin, Kirill Timofeev V., Aleksandr A. Elenkin, Vladislav A. Shipugin;2019;IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering;A large number of modern projects use machine learning technology to perform a variety of business calculations. There are two main ways to integrate machine-learning models into the logic of industrial applications. The first way is to rewrite models from the data analysis language (for example R or Python) to the industrial development language (for example Java, Go or Scala). The second way is to equip models with a web-interface and integrate it into the calculation. In this article, we explore the second method. A deployment architecture for machine learning in the clouds is proposed. The possibilities of the proposed scheme for scaling are described. Examples of practical use of the proposed architecture for organizing data storage with compression are also given.;2
Extending reference architecture of big data systems towards machine learning in edge computing environments;Pekka Pääkkönen, Daniel Pakkala;2020;Journal of Big Data;Abstract not available;24
Automating Cloud Deployment for Deep Learning Inference of Real-time Online Services;Y. Li, Zhenhua Han, Quanlu Zhang, Zhenhua Li, Haisheng Tan;2020;IEEE Conference on Computer Communications;Real-time online services using pre-trained deep neural network (DNN) models, e.g., Siri and Instagram, require low-latency and cost-efficiency for quality-of-service and commercial competitiveness. When deployed in a cloud environment, such services call for an appropriate selection of cloud configurations (i.e., specific types of VM instances), as well as a considerate device placement plan that places the operations of a DNN model to multiple computation devices like GPUs and CPUs. Currently, the deployment mainly relies on service providers’ manual efforts, which is not only onerous but also far from satisfactory oftentimes (for a same service, a poor deployment can incur significantly more costs by tens of times). In this paper, we attempt to automate the cloud deployment for real-time online DNN inference with minimum costs under the constraint of acceptably low latency. This attempt is enabled by jointly leveraging the Bayesian Optimization and Deep Reinforcement Learning to adaptively unearth the (nearly) optimal cloud configuration and device placement with limited search time. We implement a prototype system of our solution based on TensorFlow and conduct extensive experiments on top of Microsoft Azure. The results show that our solution essentially outperforms the nontrivial baselines in terms of inference speed and cost-efficiency.;34
Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective;K. Hazelwood, Sarah Bird, D. Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhulgakov, Mohamed Fawzy, Bill Jia, Yangqing Jia, Aditya Kalro, James Law, Kevin Lee, Jason Lu, P. Noordhuis, M. Smelyanskiy, Liang Xiong, Xiaodong Wang;2018;International Symposium on High-Performance Computer Architecture;Machine learning sits at the core of many essential products and services at Facebook. This paper describes the hardware and software infrastructure that supports machine learning at global scale. Facebook's machine learning workloads are extremely diverse: services require many different types of models in practice. This diversity has implications at all layers in the system stack. In addition, a sizable fraction of all data stored at Facebook flows through machine learning pipelines, presenting significant challenges in delivering data to high-performance distributed training flows. Computational requirements are also intense, leveraging both GPU and CPU platforms for training and abundant CPU capacity for real-time inference. Addressing these and other emerging challenges continues to require diverse efforts that span machine learning algorithms, software, and hardware design.;546
Deploying Deep Learning Models via IOT Deployment Tools;R. Krishnamurthi, Raghav Maheshwari, Rishabh Gulati;2019;International Conference on Contemporary Computing;Deep Learning and Internet of Things (IoT) are some top fields of research in computer science now days. Many researches are going on to incorporate the best of both the fields and to collaborate them into one. Inspired from this, this paper works on using and creating an efficient Deep Learning model for colorizing the image and transport them to remote systems through IoT deployment tools. This paper develops two models, namely Alpha and Beta, for the colorization of the greyscale images. Efficient models are developed to lessen the loss rate to around 0.005. Then the paper uses tools like AWS Greengrass and Docker for the deployment of the model, thus combining Deep Learning with IoT deployment tools.;5
Engineering AI Systems: A Research Agenda;J. Bosch, I. Crnkovic, H. H. Olsson;2020;Advances in Systems Analysis, Software Engineering, and High Performance Computing;Artificial intelligence (AI) and machine learning (ML) are increasingly broadly adopted in industry. However, based on well over a dozen case studies, we have learned that deploying industry-strength, production quality ML models in systems proves to be challenging. Companies experience challenges related to data quality, design methods and processes, performance of models as well as deployment and compliance. We learned that a new, structured engineering approach is required to construct and evolve systems that contain ML/DL components. In this chapter, the authors provide a conceptualization of the typical evolution patterns that companies experience when employing ML as well as an overview of the key problems experienced by the companies that they have studied. The main contribution of the chapter is a research agenda for AI engineering that provides an overview of the key engineering challenges surrounding ML solutions and an overview of open items that need to be addressed by the research community at large.;85
Demystifying MLOps and Presenting a Recipe for the Selection of Open-Source Tools;Philipp Ruf, M. Madan, C. Reich, D. Ould-Abdeslam;2021;Applied Sciences;Nowadays, machine learning projects have become more and more relevant to various real-world use cases. The success of complex Neural Network models depends upon many factors, as the requirement for structured and machine learning-centric project development management arises. Due to the multitude of tools available for different operational phases, responsibilities and requirements become more and more unclear. In this work, Machine Learning Operations (MLOps) technologies and tools for every part of the overall project pipeline, as well as involved roles, are examined and clearly defined. With the focus on the inter-connectivity of specific tools and comparison by well-selected requirements of MLOps, model performance, input data, and system quality metrics are briefly discussed. By identifying aspects of machine learning, which can be reused from project to project, open-source tools which help in specific parts of the pipeline, and possible combinations, an overview of support in MLOps is given. Deep learning has revolutionized the field of Image processing, and building an automated machine learning workflow for object detection is of great interest for many organizations. For this, a simple MLOps workflow for object detection with images is portrayed.;74
Towards Regulatory-Compliant MLOps: Oravizio’s Journey from a Machine Learning Experiment to a Deployed Certified Medical Product;Tuomas Granlund, Vlad Stirbu, T. Mikkonen;2021;SN Computer Science;Abstract not available;35
Mímir: Building and Deploying an ML Framework for Industrial IoT;Devon Peticolas, Russell Kirmayer, D. Turaga;2019;2019 International Conference on Data Mining Workshops (ICDMW);In this paper we describe Mímir, a production grade cloud and edge spanning ML framework for Industrial IoT applications. We first describe our infrastructure for optimized capture, streaming and multi-resolution storage of manufacturing data and its context. We then describe our workflow for scalable ML model training, validation, and deployment that leverages a manufacturing taxonomy and parameterized ML pipelines to determine the best metrics, hyper-parameters and models to use for a given task. We also discuss our design decisions on model deployment for real-time and batch data in the cloud and at the edge. Finally, we describe the use of the framework in building and deploying an application for Predictive Quality monitoring during a Plastics Extrusion manufacturing process.;3
Missing the Forest for the Trees: End-to-End AI Application Performance in Edge Data Centers;Daniel Richins, Dharmisha Doshi, Matthew Blackmore, A. Nair, Neha Pathapati, Ankit Patel, Brainard Daguman, Daniel Dobrijalowski, R. Illikkal, Kevin Long, David Zimmerman, V. Reddi;2020;International Symposium on High-Performance Computer Architecture;"Artificial intelligence and machine learning are experiencing widespread adoption in the industry, academia, and even public consciousness. This has been driven by the rapid advances in the applications and accuracy of AI through increasingly complex algorithms and models; this, in turn, has spurred research into developing specialized hardware AI accelerators. The rapid pace of the advances makes it easy to miss the forest for the trees: they are often developed and evaluated in a vacuum without considering the full application environment in which they must eventually operate. In this paper, we deploy and characterize Face Recognition, an AI-centric edge video analytics application built using open source and widely adopted infrastructure and ML tools. We evaluate its holistic, end-to-end behavior in a production-size edge data center and reveal the ""AI tax"" for all the processing that is involved. Even though the application is built around state-of-the-art AI and ML algorithms, it relies heavily on pre-and post-processing code which must be executed on a general-purpose CPU. As AI-centric applications start to reap the acceleration promised by so many accelerators, we find they impose stresses on the underlying software infrastructure and the data center's capabilities: storage and network bandwidth become major bottlenecks with increasing AI acceleration. By not having to serve a wide variety of applications, we show that a purpose-built edge data center can be designed to accommodate the stresses of accelerated AI at 15% lower TCO than one derived from homogeneous servers and infrastructure. We also discuss how our conclusions generalize beyond Face Recognition as many AI-centric applications at the edge rely upon the same underlying software and hardware infrastructure.";23
Scaling Machine Learning as a Service;Erran L. Li, Eric Chen, Jeremy Hermann, Pusheng Zhang, Luming Wang;2017;International Conference on Predictive APIs and Apps;Abstract not available;35
Clipper: A Low-Latency Online Prediction Serving System;D. Crankshaw, Xin Wang, Giulio Zhou, M. Franklin, Joseph E. Gonzalez, Ion Stoica;2016;Symposium on Networked Systems Design and Implementation;"Machine learning is being deployed in a growing number of applications which demand real-time, accurate, and robust predictions under heavy query load. However, most machine learning frameworks and systems only address model training and not deployment. 
In this paper, we introduce Clipper, a general-purpose low-latency prediction serving system. Interposing between end-user applications and a wide range of machine learning frameworks, Clipper introduces a modular architecture to simplify model deployment across frameworks and applications. Furthermore, by introducing caching, batching, and adaptive model selection techniques, Clipper reduces prediction latency and improves prediction throughput, accuracy, and robustness without modifying the underlying machine learning frameworks. We evaluate Clipper on four common machine learning benchmark datasets and demonstrate its ability to meet the latency, accuracy, and throughput demands of online serving applications. Finally, we compare Clipper to the TensorFlow Serving system and demonstrate that we are able to achieve comparable throughput and latency while enabling model composition and online learning to improve accuracy and render more robust predictions.";615
150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com;Lucas Bernardi, Themistoklis Mavridis, PabloA . Estevez;2019;Knowledge Discovery and Data Mining;Booking.com is the world's largest online travel agent where millions of guests find their accommodation and millions of accommodation providers list their properties including hotels, apartments, bed and breakfasts, guest houses, and more. During the last years we have applied Machine Learning to improve the experience of our customers and our business. While most of the Machine Learning literature focuses on the algorithmic or mathematical aspects of the field, not much has been published about how Machine Learning can deliver meaningful impact in an industrial environment where commercial gains are paramount. We conducted an analysis on about 150 successful customer facing applications of Machine Learning, developed by dozens of teams in Booking.com, exposed to hundreds of millions of users worldwide and validated through rigorous Randomized Controlled Trials. Following the phases of a Machine Learning project we describe our approach, the many challenges we found, and the lessons we learned while scaling up such a complex technology across our organization. Our main conclusion is that an iterative, hypothesis driven process, integrated with other disciplines was fundamental to build 150 successful products enabled by Machine Learning.;67
A Case for Managed and Model-less Inference Serving;N. Yadwadkar, Francisco Romero, Qian Li, Christos Kozyrakis;2019;USENIX Workshop on Hot Topics in Operating Systems;The number of applications relying on inference from machine learning models, especially neural networks, is already large and expected to keep growing. For instance, Facebook applications issue tens-of-trillions of inference queries per day with varying performance, accuracy, and cost constraints. Unfortunately, today's inference serving systems are neither easy to use nor cost effective. Developers must manually match the performance, accuracy, and cost constraints of their applications to a large design space that includes decisions such as selecting the right model and model optimizations, selecting the right hardware architecture, selecting the right scale-out factor, and avoiding cold-start effects. These interacting decisions are difficult to make, especially when the application load varies over time, applications evolve over time, and the available resources vary over time. If we want an increasing number of applications to use machine learning, we must automate issues that affect ease-of-use, performance, and cost efficiency for both users and providers. Hence, we define and make the case for managed and model-less inference serving. In this paper, we identify and discuss open research directions to realize this vision.;26
Migrating a Recommendation System to Cloud Using ML Workflow;Dheeraj Chahal, Ravi Ojha, Sharod Roy Choudhury, M. Nambiar;2020;ICPE Companion;Inference is the production stage of machine learning workflow in which a trained model is used to infer or predict with real world data. A recommendation system improves customer experience by displaying most relevant items based on historical behavior of a customer. Machine learning models built for recommendation systems are deployed either on-premise or migrated to a cloud for inference in real time or batch. A recommendation system should be cost effective while honoring service level agreements (SLAs). In this work we discuss on-premise implementation of our recommendation system called iPrescribe. We show a methodology to migrate on-premise implementation of recommendation system to a cloud using ML workflow. We also present our study on performance of recommendation system model when deployed on different types of virtual instances.;9
Lazy Batching: An SLA-aware Batching System for Cloud Machine Learning Inference;Yujeong Choi, Yunseong Kim, Minsoo Rhu;2020;International Symposium on High-Performance Computer Architecture;In cloud ML inference systems, batching is an essential technique to increase throughput which helps optimize total-cost-of-ownership. Prior graph batching combines the individual DNN graphs into a single one, allowing multiple inputs to be concurrently executed in parallel. We observe that the coarse-grained graph batching becomes suboptimal in effectively handling the dynamic inference request traffic, leaving significant performance left on the table. This paper proposes LazyBatching, an SLA-aware batching system that considers both scheduling and batching in the granularity of individual graph nodes, rather than the entire graph for flexible batching. We show that LazyBatching can intelligently determine the set of nodes that can be efficiently batched together, achieving an average $15\times, 1.5\times$, and $5.5\times$ improvement than graph batching in terms of average response time, throughput, and SLA satisfaction, respectively.;56
Model-Switching: Dealing with Fluctuating Workloads in Machine-Learning-as-a-Service Systems;Jeff Zhang, S. Elnikety, Shuayb Zarar, Atul Gupta, S. Garg;2020;USENIX Workshop on Hot Topics in Cloud Computing;Machine learning (ML) based prediction models, and especially deep neural networks (DNNs) are increasingly being served in the cloud in order to provide fast and accurate inferences. However, existing service ML serving systems have trouble dealing with fluctuating workloads and either drop re-quests or significantly expand hardware resources in response to load spikes. In this paper, we introduce Model-Switching, a new approach to dealing with fluctuating workloads when serving DNN models. Motivated by the observation that end-users of ML primarily care about the accuracy of responses that are returned within the deadline (which we refer to as effective accuracy), we propose to switch from complex and highly accurate DNN models to simpler but less accurate models in the presence of load spikes. We show that the flexibility introduced by enabling online model switching provides higher effective accuracy in the presence of fluctuating workloads compared to serving using any single model. We implement Model-Switching within Clipper, a state-of-art DNN model serving system, and demonstrate its advantages over baseline approaches.;42
Architecture for Enabling Edge Inference via Model Transfer from Cloud Domain in a Kubernetes Environment;Pekka Pääkkönen, Daniel Pakkala, Jussi Kiljander, R. Sarala;2020;Future Internet;The current approaches for energy consumption optimisation in buildings are mainly reactive or focus on scheduling of daily/weekly operation modes in heating. Machine Learning (ML)-based advanced control methods have been demonstrated to improve energy efficiency when compared to these traditional methods. However, placing of ML-based models close to the buildings is not straightforward. Firstly, edge-devices typically have lower capabilities in terms of processing power, memory, and storage, which may limit execution of ML-based inference at the edge. Secondly, associated building information should be kept private. Thirdly, network access may be limited for serving a large number of edge devices. The contribution of this paper is an architecture, which enables training of ML-based models for energy consumption prediction in private cloud domain, and transfer of the models to edge nodes for prediction in Kubernetes environment. Additionally, predictors at the edge nodes can be automatically updated without interrupting operation. Performance results with sensor-based devices (Raspberry Pi 4 and Jetson Nano) indicated that a satisfactory prediction latency (~7–9 s) can be achieved within the research context. However, model switching led to an increase in prediction latency (~9–13 s). Partial evaluation of a Reference Architecture for edge computing systems, which was used as a starting point for architecture design, may be considered as an additional contribution of the paper.;14
Software engineering for artificial intelligence and machine learning software: A systematic literature review;E. Nascimento, Anh Nguyen-Duc, Ingrid Sundbø, T. Conte;2020;arXiv.org;Artificial Intelligence (AI) or Machine Learning (ML) systems have been widely adopted as value propositions by companies in all industries in order to create or extend the services and products they offer. However, developing AI/ML systems has presented several engineering problems that are different from those that arise in, non-AI/ML software development. This study aims to investigate how software engineering (SE) has been applied in the development of AI/ML systems and identify challenges and practices that are applicable and determine whether they meet the needs of professionals. Also, we assessed whether these SE practices apply to different contexts, and in which areas they may be applicable. We conducted a systematic review of literature from 1990 to 2019 to (i) understand and summarize the current state of the art in this field and (ii) analyze its limitations and open challenges that will drive future research. Our results show these systems are developed on a lab context or a large company and followed a research-driven development process. The main challenges faced by professionals are in areas of testing, AI software quality, and data management. The contribution types of most of the proposed SE practices are guidelines, lessons learned, and tools.;37
Understanding Development Process of Machine Learning Systems: Challenges and Solutions;E. Nascimento, Iftekhar Ahmed, E. Oliveira, Márcio Piedade Palheta, Igor Steinmacher, T. Conte;2019;International Symposium on Empirical Software Engineering and Measurement;Background: The number of Machine Learning (ML) systems developed in the industry is increasing rapidly. Since ML systems are different from traditional systems, these differences are clearly visible in different activities pertaining to ML systems software development process. These differences make the Software Engineering (SE) activities more challenging for ML systems because not only the behavior of the system is data dependent, but also the requirements are data dependent. In such scenario, how can Software Engineering better support the development of ML systems? Aim: Our objective is twofold. First, better understand the process that developers use to build ML systems. Second, identify the main challenges that developers face, proposing ways to overcome these challenges. Method: We conducted interviews with seven developers from three software small companies that develop ML systems. Based on the challenges uncovered, we proposed a set of checklists to support the developers. We assessed the checklists by using a focus group. Results: We found that the ML systems development follow a 4-stage process in these companies. These stages are: understanding the problem, data handling, model building, and model monitoring. The main challenges faced by the developers are: identifying the clients’ business metrics, lack of a defined development process, and designing the database structure. We have identified in the focus group that our proposed checklists provided support during identification of the client’s business metrics and in increasing visibility of the progress of the project tasks. Conclusions: Our research is an initial step towards supporting the development of ML systems, suggesting checklists that support developers in essential development tasks, and also serve as a basis for future research in the area.;54
How Do Engineers Perceive Difficulties in Engineering of Machine-Learning Systems? - Questionnaire Survey;F. Ishikawa, Nobukazu Yoshioka;2019;Conducting Empirical Studies in Industry;There is increasing interest in machine learning (ML) techniques and their applications in recent years. Although there has been intensive support by frameworks and libraries for the implementation of ML-based systems, investigation into engineering disciplines and methods is still at the early phase. The most pressing issue in this field is identifying the essential challenges for the software engineering research community as engineering of ML-based systems requires novel approaches due to the essentially different nature of ML-based systems. In this paper, we analyze the results of a questionnaire administered to 278 people who have worked on ML-based systems in practice, clarify the essential difficulties and their causes as perceived by practitioners, and suggest potential research directions.;78
Data Management Challenges for Deep Learning;A. Munappy, J. Bosch, H. H. Olsson, Anders Arpteg, B. Brinne;2019;EUROMICRO Conference on Software Engineering and Advanced Applications;Deep learning is one of the most exciting and fast-growing techniques in Artificial Intelligence. The unique capacity of deep learning models to automatically learn patterns from the data differentiates it from other machine learning techniques. Deep learning is responsible for a significant number of recent breakthroughs in AI. However, deep learning models are highly dependent on the underlying data. So, consistency, accuracy, and completeness of data is essential for a deep learning model. Thus, data management principles and practices need to be adopted throughout the development process of deep learning models. The objective of this study is to identify and categorise data management challenges faced by practitioners in different stages of end-to-end development. In this paper, a case study approach is employed to explore the data management issues faced by practitioners across various domains when they use real-world data for training and deploying deep learning models. Our case study is intended to provide valuable insights to the deep learning community as well as for data scientists to guide discussion and future research in applied deep learning with real-world data.;58
Implementing Ethics in AI: Initial Results of an Industrial Multiple Case Study;Ville Vakkuri, Kai-Kristian Kemell, P. Abrahamsson;2019;International Conference on Product Focused Software Process Improvement;Abstract not available;13
Implementing AI Ethics in Practice: An Empirical Evaluation of the RESOLVEDD Strategy;Ville Vakkuri, Kai-Kristian Kemell;2019;International Conference on Software Business;Abstract not available;14
Taming Functional Deficiencies of Automated Driving Systems: a Methodology Framework toward Safety Validation;Meng Chen, Andreas Knapp, M. Pohl, K. Dietmayer;2018;2018 IEEE Intelligent Vehicles Symposium (IV);Safety is one of the key aspects of road vehicles. With applications of machine learning and artificial intelligence (AI) technologies, driver assistance and automated driving systems have been rapidly developed. This paper identifies one of the emerging safety issues of automated driving systems: functional deficiencies resulting from limited sensing abilities and algorithmic performance. Safety validation problem and challenges for some methodologies provided by ISO 26262 are addressed. To this end, we provide a methodology framework for identifying functional deficiencies during system development. A novel methodology based on possibility theory and a fuzzy relation model, Causal Scenario Analysis (CSA), is introduced as one essential part in this framework. A traffic light handling case study is presented.;5
Your System Gets Better Every Day You Use It: Towards Automated Continuous Experimentation;D. I. Mattos, J. Bosch, H. H. Olsson;2017;EUROMICRO Conference on Software Engineering and Advanced Applications;Innovation and optimization in software systems can occur from pre-development to post-deployment stages. Companies are increasingly reporting the use of experiments with customers in their systems in the post-deployment stage. Experiments with customers and users are can lead to a significant learning and return-on-investment. Experiments are used for both validation of manual hypothesis testing and feature optimization, linked to business goals. Automated experimentation refers to having the system controlling and running the experiments, opposed to having the R&D organization in control. Currently, there are no systematic approaches that combine manual hypothesis validation and optimization in automated experiments. This paper presents concepts related to automated experimentation, as controlled experiments, machine learning and software architectures for adaptation. However, this paper focuses on how architectural aspects that can contribute to support automated experimentation. A case study using an autonomous system is used to demonstrate the developed initial architecture framework. The contributions of this paper are threefold. First, it identifies software architecture qualities to support automated experimentation. Second, it develops an initial architecture framework that supports automated experiments and validates the framework with an autonomous mobile robot. Third, it identifies key research challenges that need to be addressed to support further development of automated experimentation.;21
DataLab: Introducing Software Engineering Thinking into Data Science Education at Scale;Yang Zhang, Tingjian Zhang, Yongzheng Jia, Jiao Sun, Fangzhou Xu, W. Xu;2017;2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering Education and Training Track (ICSE-SEET);Data science education is a new area in computer science that has attracted increasing attention in recent years. However, currently, data science educators lack good tools and methodologies. In particular, they lack integrated tools through which their students can acquire hands-on software engineering experience. To address these problems, we designed and implemented DataLab, a web-based tool for data science education that integrates code, data and execution management into one system. The goal of DataLab is to provide a hands-on online lab environment to train students to have basic software engineering thinking and habits while maintaining a focus on the core data science contents. In this paper, we present the user-experience design and system-level implementation of DataLab. Further, we evaluate DataLab's performance through an in-classroom use case. Finally, using objective log-based learning behavior analysis and a subjective survey, we demonstrate DataLab's effectiveness.;4
Trials and tribulations of developers of intelligent systems: A field study;Charles Hill, R. Bellamy, Thomas Erickson, M. Burnett;2016;IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments;Intelligent systems are gaining in popularity and receiving increased media attention, but little is known about how people actually go about developing them. In this paper, we attempt to fill this gap through a set of field interviews that investigate how people develop intelligent systems that incorporate machine learning algorithms. The developers we interviewed were experienced at working with machine learning algorithms and dealing with the large amounts of data needed to develop intelligent systems. Despite their level of experience, we learned that they struggle to establish a repeatable process. They described problems with each step of the processes they perform, as well as cross-cutting issues that pervade multiple steps of their processes. The unique difficulties that developers like these face seem to point to a need for software engineering advances that address such machine learning systems, and we conclude by discussing this need and some of its implications.;92
Lean Data Science Research Life Cycle: A Concept for Data Analysis Software Development;M. Shcherbakov, N. Shcherbakova, A. Brebels, Timur Janovsky, V. Kamaev;2014;Joint Conference on Knowledge-Based Software Engineering;Abstract not available;12
An Empirical Study of Bugs in Machine Learning Systems;Ferdian Thung, Shaowei Wang, D. Lo, Lingxiao Jiang;2012;IEEE International Symposium on Software Reliability Engineering;Many machine learning systems that include various data mining, information retrieval, and natural language processing code and libraries are used in real world applications. Search engines, internet advertising systems, product recommendation systems are sample users of these algorithm-intensive code and libraries. Machine learning code and toolkits have also been used in many recent studies on software mining and analytics that aim to automate various software engineering tasks. With the increasing number of important applications of machine learning systems, the reliability of such systems is also becoming increasingly important. A necessary step for ensuring reliability of such systems is to understand the features and characteristics of bugs occurred in the systems. A number of studies have investigated bugs and fixes in various software systems, but none focuses on machine learning systems. Machine learning systems are unique due to their algorithm-intensive nature and applications to potentially large-scale data, and thus deserve a special consideration. In this study, we fill the research gap by performing an empirical study on the bugs in machine learning systems. We analyze three systems, Apache Mahout, Lucene, and OpenNLP, which are data mining, information retrieval, and natural language processing tools respectively. We look into their bug databases and code repositories, analyze a sample set of bugs and corresponding fixes, and label the bugs into various categories. Our study finds that 22.6% of the bugs belong to the algorithm/method category, 15.6% of the bugs belong to the non-functional category, and 13% of the bugs belong to the assignment/initialization category. We also report the relationship between bug categories and bug severities, the time and effort needed to fix the bugs, and bug impacts. We highlight several bug categories that deserve attention in future research.;129
Beyond the Prototype: The Design Evolution of a Deployed AI System;Cheryl E. Martin, D. Schreckenghost;2005;AAAI Conference on Artificial Intelligence;Our previous experiences with deployed intelligent control agents for NASA advanced life support systems (Schreckenghost et al., 2002) inspired us to develop the Distributed Collaboration and Interaction (DCI) system to help humans and mostly-autonomous software agents work together. We discovered many unaddressed needs for human interaction with control agents that operate continuously over months to years to monitor and perform process control for regenerative life support systems. These systems recover usable water or air from the waste products created by biological systems over time. Through the DCI project, we have addressed the needs for interaction between humans and autonomous control systems. This paper describes the software engineering aspects of our experiences, first, in designing and developing a prototype of the DCI system, and later, in adjusting the implementation to make the leap from initial prototype to a system ready to be applied under varying circumstances to meet different needs. The details in the paper focus primarily on the second stage of development. The DCI system is currently deployed, operating 24/7, to assist humans to interact with an advanced Water Recovery System (WRS). In addition, we have demonstrated an application of DCI to support ground personnel in mission support roles.;2
Software development for cim — a case study;Z. Kouba, J. Lazanský, V. Marík, G. Quirchmayr, W. Retschitzegger, T. Vlček, R. Wagner;1992;Publication venue not available;Abstract not available;1
An Empirical Study Towards Characterizing Deep Learning Development and Deployment Across Different Frameworks and Platforms;Qianyu Guo, Sen Chen, Xiaofei Xie, Lei Ma, Q. Hu, Hongtao Liu, Yang Liu, Jianjun Zhao, Xiaohong Li;2019;International Conference on Automated Software Engineering;Deep Learning (DL) has recently achieved tremendous success. A variety of DL frameworks and platforms play a key role to catalyze such progress. However, the differences in architecture designs and implementations of existing frameworks and platforms bring new challenges for DL software development and deployment. Till now, there is no study on how various mainstream frameworks and platforms influence both DL software development and deployment in practice. To fill this gap, we take the first step towards understanding how the most widely-used DL frameworks and platforms support the DL software development and deployment. We conduct a systematic study on these frameworks and platforms by using two types of DNN architectures and three popular datasets. (1) For development process, we investigate the prediction accuracy under the same runtime training configuration or same model weights/biases. We also study the adversarial robustness of trained models by leveraging the existing adversarial attack techniques. The experimental results show that the computing differences across frameworks could result in an obvious prediction accuracy decline, which should draw the attention of DL developers. (2) For deployment process, we investigate the prediction accuracy and performance (refers to time cost and memory consumption) when the trained models are migrated/quantized from PC to real mobile devices and web browsers. The DL platform study unveils that the migration and quantization still suffer from compatibility and reliability issues. Meanwhile, we find several DL software bugs by using the results as a benchmark. We further validate the results through bug confirmation from stakeholders and industrial positive feedback to highlight the implications of our study. Through our study, we summarize practical guidelines, identify challenges and pinpoint new research directions, such as understanding the characteristics of DL frameworks and platforms, avoiding compatibility and reliability issues, detecting DL software bugs, and reducing time cost and memory consumption towards developing and deploying high quality DL systems effectively.;111
Reusability in Artificial Neural Networks: An Empirical Study;Javad Ghofrani, Ehsan Kozegar, Arezoo Bozorgmehr, Mohammad Divband Soorati;2019;Software Product Lines Conference;Machine learning, especially deep learning has aroused interests of researchers and practitioners for the last few years in development of intelligent systems such as speech, natural language, and image processing. Software solutions based on machine learning techniques attract more attention as alternatives to conventional software systems. In this paper, we investigate how reusability techniques are applied in implementation of artificial neural networks (ANNs). We conducted an empirical study with an online survey among experts with experience in developing solutions with ANNs. We analyze the feedback of more than 100 experts to our survey. The results show existing challenges and some of the applied solutions in an intersection between reusability and ANNs.;7
Metric-Based Evaluation of Multiagent Systems Models;L. Damasceno, Vera Werneck, Marcelo Schots;2018;2018 IEEE/ACM 10th International Workshop on Modelling in Software Engineering (MiSE);The use of Multiagent Systems (MAS) has been increasing over the years due to their capacity of dealing with problems in a variety of domains. The modeling of such systems is not trivial: besides the knowledge and skills on agent-oriented software engineering and a basic understanding of the target domain to be modeled, it demands familiarity with agent-oriented modeling methodologies. This is not always the case, though, especially for newcomers in the field. This work proposes a set of guidelines in the form of a questionnaire for the evaluation of MAS models, aiming at supporting the verification of their quality. The questionnaire is built upon the results of a systematic mapping conducted to identify how MAS models have been evaluated and what metrics have been used. The proposed guidelines were evaluated through (i) a peer review by experts and (ii) its actual application by graduate students applying modeling methodologies in the context of Guardian Angel (GA), a patient-centered health system that automatically supports patients suffering from chronic diseases. Participants provided an overall positive feedback and proposed some improvements on the questionnaire, most of which were promptly incorporated.;1
Guiding Deep Learning System Testing Using Surprise Adequacy;Jinhan Kim, R. Feldt, S. Yoo;2018;International Conference on Software Engineering;Deep Learning (DL) systems are rapidly being adopted in safety and security critical domains, urgently calling for ways to test their correctness and robustness. Testing of DL systems has traditionally relied on manual collection and labelling of data. Recently, a number of coverage criteria based on neuron activation values have been proposed. These criteria essentially count the number of neurons whose activation during the execution of a DL system satisfied certain properties, such as being above predefined thresholds. However, existing coverage criteria are not sufficiently fine grained to capture subtle behaviours exhibited by DL systems. Moreover, evaluations have focused on showing correlation between adversarial examples and proposed criteria rather than evaluating and guiding their use for actual testing of DL systems. We propose a novel test adequacy criterion for testing of DL systems, called Surprise Adequacy for Deep Learning Systems (SADL), which is based on the behaviour of DL systems with respect to their training data. We measure the surprise of an input as the difference in DL system's behaviour between the input and the training data (i.e., what was learnt during training), and subsequently develop this as an adequacy criterion: a good test input should be sufficiently but not overtly surprising compared to training data. Empirical evaluation using a range of DL systems from simple image classifiers to autonomous driving car platforms shows that systematic sampling of inputs based on their surprise can improve classification accuracy of DL systems against adversarial examples by up to 77.5% via retraining.;389
Requirements and initial model for KnowLang: a language for knowledge representation in autonomic service-component ensembles;Emil Vassev, M. Hinchey, B. Gaudin, P. Nixon;2011;Canadian Conference on Computer Science & Software Engineering;Autonomic Service-Component Ensembles (ASCENS) is a class of multi-agent systems formed as mobile, intelligent and open-ended swarms of special autonomic service components capable of local and distributed reasoning. Such components encapsulate rules, constraints and mechanisms for self-adaptation and acquire and process knowledge about themselves, other service components and their environment. ASCENS systems pose distinct challenges for knowledge representation languages. In this paper, we present requirements and an initial model for such a language called KnowLang. KnowLang is intended to provide for formal specification of distinct knowledge models each representing a different knowledge domain of an ASCENS system, such as the internal world of a service component, the world of a service-component ensemble, the surrounding external world and information of special situations related to state changes and operations of service components. KnowLang provides the necessary constructs and mechanisms for specifying such knowledge models at two main levels -- an ontology level and a logic-foundations level, where the latter is formed by special facts, rules, constraints and inter-ontology operators. In this paper, we also survey one of the ASCENS case studies to derive some of the requirements for KnowLang.;8
Black box fairness testing of machine learning models;Aniya Aggarwal, P. Lohia, Seema Nagar, K. Dey, Diptikalyan Saha;2019;ESEC/SIGSOFT FSE;Any given AI system cannot be accepted unless its trustworthiness is proven. An important characteristic of a trustworthy AI system is the absence of algorithmic bias. 'Individual discrimination' exists when a given individual different from another only in 'protected attributes' (e.g., age, gender, race, etc.) receives a different decision outcome from a given machine learning (ML) model as compared to the other individual. The current work addresses the problem of detecting the presence of individual discrimination in given ML models. Detection of individual discrimination is test-intensive in a black-box setting, which is not feasible for non-trivial systems. We propose a methodology for auto-generation of test inputs, for the task of detecting individual discrimination. Our approach combines two well-established techniques - symbolic execution and local explainability for effective test case generation. We empirically show that our approach to generate test cases is very effective as compared to the best-known benchmark systems that we examine.;141
Bridging the gap between ML solutions and their business requirements using feature interactions;Guy Barash, E. Farchi, Ilan Jayaraman, Orna Raz, Rachel Tzoref, Marcel Zalmanovici;2019;ESEC/SIGSOFT FSE;Machine Learning (ML) based solutions are becoming increasingly popular and pervasive. When testing such solutions, there is a tendency to focus on improving the ML metrics such as the F1-score and accuracy at the expense of ensuring business value and correctness by covering business requirements. In this work, we adapt test planning methods of classical software to ML solutions. We use combinatorial modeling methodology to define the space of business requirements and map it to the ML solution data, and use the notion of data slices to identify the weaker areas of the ML solution and strengthen them. We apply our approach to three real-world case studies and demonstrate its value.;28
Boosting operational DNN testing efficiency through conditioning;Zenan Li, Xiaoxing Ma, Chang Xu, Chun Cao, Jingwei Xu, Jian Lu;2019;ESEC/SIGSOFT FSE;With the increasing adoption of Deep Neural Network (DNN) models as integral parts of software systems, efficient operational testing of DNNs is much in demand to ensure these models' actual performance in field conditions. A challenge is that the testing often needs to produce precise results with a very limited budget for labeling data collected in field. Viewing software testing as a practice of reliability estimation through statistical sampling, we re-interpret the idea behind conventional structural coverages as conditioning for variance reduction. With this insight we propose an efficient DNN testing method based on the conditioning on the representation learned by the DNN model under testing. The representation is defined by the probability distribution of the output of neurons in the last hidden layer of the model. To sample from this high dimensional distribution in which the operational data are sparsely distributed, we design an algorithm leveraging cross entropy minimization. Experiments with various DNN models and datasets were conducted to evaluate the general efficiency of the approach. The results show that, compared with simple random sampling, this approach requires only about a half of labeled inputs to achieve the same level of precision.;90
Stress Testing an AI Based Web Service: A Case Study;Anand Chakravarty;2010;2010 Seventh International Conference on Information Technology: New Generations;The stress testing of AI-based systems differs from the approach taken for more traditional Web services, both in terms of the design of test cases and the metrics used to measure quality. The expected variability in responses of an AI-based system to the same request adds a level of complexity to stress testing, when compared to more standard systems where the system response is deterministic and any deviations may easily be characterized as product defects. Generating test cases for AI-based systems requires balancing breadth of test cases with depth of response quality: most AI-systems may not return a perfect answer. An example of a machine learning translation system is considered, and the approach used for stress testing it is presented, alongside comparisons with a more traditional approach. The challenges of shipping such a system to support a growing set of features and language pairs necessitate a mature prioritization of test cases. This approach has been successful in shipping a Web service that currently serves millions of users per day.;8
The Emerging Role of Data Scientists on Software Development Teams;Miryung Kim, Thomas Zimmermann, R. Deline, Andrew Begel;2016;International Conference on Software Engineering;"Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their missions in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists: (1) Insight Providers, who work with engineers to collect the data needed to inform decisions that managers make; (2) Modeling Specialists, who use their machine learning expertise to build predictive models; (3) Platform Builders, who create data platforms, balancing both engineering and data analysis concerns; (4) Polymaths, who do all data science activities themselves; and (5) Team Leaders, who run teams of data scientists and spread best practices. We further describe a set of strategies that they employ to increase the impact and actionability of their work.";207
The current status of expert system development and related technologies in Japan;H. Motoda;1990;IEEE Expert;General trends and statistics obtained from responses to a questionnaire are reviewed, showing that more than 400 expert systems are now under development or in practical use in Japan. Three expert systems selected as representative examples from among planning-and-design-system candidates are described: IBM Japan's Scheplan, Kayaba's OHCS, and Hitachi's automatic pipe-routing system for a power plant. Current Japanese research in related methodologies, including knowledge compilation, knowledge acquisition, generic methods for building expert systems, the automatic generation of task-oriented expert systems, and machine learning, is examined. It is noted that AI is spreading into almost every Japanese industry, and Japanese companies are seeking the most efficient way of introducing AI technologies, particularly expert systems.<<ETX>>;12
On the Engineering of AI-Powered Systems;Evgeny Kusmenko, Svetlana Pavlitskaya, Bernhard Rumpe, Sebastian Stüber;2019;2019 34th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW);More and more tasks become solvable using deep learning technology nowadays. Consequently, the amount of neural network code in software rises continuously. To make the new paradigm more accessible, frameworks, languages, and tools keep emerging. Although, the maturity of these tools is steadily increasing, we still lack appropriate domain specific languages and a high degree of automation when it comes to deep learning for productive systems. In this paper we present a multi-paradigm language family allowing the AI engineer to model and train deep neural networks as well as to integrate them into software architectures containing classical code. Using input and output layers as strictly typed interfaces enables a seamless embedding of neural networks into component-based models. The lifecycle of deep learning components can then be governed by a compiler accordingly, e.g. detecting when (re-)training is necessary or when network weights can be shared between different network instances. We provide a compelling case study, where we train an autonomous vehicle for the TORCS simulator. Furthermore, we discuss how the methodology automates the AI development process if neural networks are changed or added to the system.;14
Why is Developing Machine Learning Applications Challenging? A Study on Stack Overflow Posts;Moayad Alshangiti, Hitesh Sapkota, P. Murukannaiah, Xumin Liu, Qi Yu;2019;International Symposium on Empirical Software Engineering and Measurement;"As smart and automated applications pervade our lives, an increasing number of software developers are required to incorporate machine learning (ML) techniques into application development. However, acquiring the ML skill set can be nontrivial for software developers owing to both the breadth and depth of the ML domain. Aims: We seek to understand the challenges developers face in the process of ML application development and offer insights to simplify the process. Despite its importance, there has been little research on this topic. A few existing studies on development challenges with ML are outdated, small scale, or they do no involve a representative set of developers. Method: We conduct an empirical study of ML-related developer posts on Stack Overflow. We perform in-depth quantitative and qualitative analyses focusing on a series of research questions related to the challenges of developing ML applications and the directions to address them. Results: Our findings include: (1) ML questions suffer from a much higher percentage of unanswered questions on Stack Overflow than other domains; (2) there is a lack of ML experts in the Stack Overflow QA community; (3) the data preprocessing and model deployment phases are where most of the challenges lay; and (4) addressing most of these challenges require more ML implementation knowledge than ML conceptual knowledge. Conclusions: Our findings suggest that most challenges are under the data preparation and model deployment phases, i.e., early and late stages. Also, the implementation aspect of ML shows much higher difficulty level among developers than the conceptual aspect.";55
A Practical Framework for Agent-Based Hybrid Intelligent Systems;Chunsheng Li, Kan Li;2011;2011 Seventh International Conference on Computational Intelligence and Security;The design and development of hybrid intelligent systems are difficult because there are many interactions between various components. Existing software development techniques cannot manage those complex interactions efficiently as those interactions may occur at unpredictable times, for unpredictable reasons, between unpredictable components. In this paper, we contribute a multi-agent framework to organize those components and interactions. The framework consists of user interface, decision making, knowledge discovering, information management facilitators, and distributed heterogeneous data resources. We employ the middle agent concept to match task requesters with specific intelligent agents. We demonstrate the potentials of the framework by case study and present theoretical and empirical evidence that our framework is available and robust.;4
Studying Software Engineering Patterns for Designing Machine Learning Systems;H. Washizaki, Hiromu Uchida, Foutse Khomh, Yann-Gaël Guéhéneuc;2019;International Workshop on Empirical Software Engineering in Practice;Machine-learning (ML) techniques are becoming more prevalent. ML techniques rely on mathematics and software engineering. Researchers and practitioners studying best practices strive to design ML systems and software that address software complexity and quality issues. Such design practices are often formalized as architecture and design patterns by encapsulating reusable solutions to common problems within given contexts. However, a systematic study to collect, classify, and discuss these software-engineering (SE) design patterns for ML techniques have yet to be reported. Our research collects good/bad SE design patterns for ML techniques to provide developers with a comprehensive classification of such patterns. Herein we report the preliminary results of a systematic-literature review (SLR) of good/bad design patterns for ML.;70
AI Deployment Architecture: Multi-Case Study for Key Factor Identification;Meenu Mary John, H. H. Olsson, J. Bosch;2020;Asia-Pacific Software Engineering Conference;Machine learning and deep learning techniques are becoming increasingly popular and critical for companies as part of their systems. However, although the development and prototyping of ML/DL systems are common across companies, the transition from prototype to production-quality deployment models are challenging. One of the key challenges is how to determine the selection of an optimal architecture for AI deployment. Based on our previous research, and to offer support and guidance to practitioners, we developed a framework in which we present five architectural alternatives for AI deployment ranging from centralized to fully decentralized edge architectures. As part of our research, we validated the framework in software-intensive embedded system companies and identified key challenges they face when deploying ML/DL models. In this paper, and to further advance our research on this topic, we identify factors that help practitioners determine what architecture to select for the ML/D L model deployment. For this, we conducted a follow-up study involving interviews and workshops in seven case companies in the embedded systems domain. Based on our findings, we identify three key factors and develop a framework in which we outline how prioritization and trade-offs between these results in certain architecture. The contribution of the paper is threefold. First, we identify key factors critical for AI system deployment. Second, we present the architecture selection framework that explains how prioritization and trade-offs between key factors result in the selection of a certain architecture. Third, we discuss additional factors that mayor may not influence the selection of an optimal architecture.;7
Software Engineering Challenges of Deep Learning;Anders Arpteg, B. Brinne, L. Crnkovic-Friis, J. Bosch;2018;EUROMICRO Conference on Software Engineering and Advanced Applications;Surprisingly promising results have been achieved by deep learning (DL) systems in recent years. Many of these achievements have been reached in academic settings, or by large technology companies with highly skilled research groups and advanced supporting infrastructure. For companies without large research groups or advanced infrastructure, building high-quality production-ready systems with DL components has proven challenging. There is a clear lack of well-functioning tools and best practices for building DL systems. It is the goal of this research to identify what the main challenges are, by applying an interpretive research approach in close collaboration with companies of varying size and type. A set of seven projects have been selected to describe the potential with this new technology and to identify associated main challenges. A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges. Furthermore, a mapping between the challenges and the projects is defined, together with selected motivating descriptions of how and why the challenges apply to specific projects. Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of high-quality systems. The challenges identified in this paper can be used to guide future research by the software engineering and DL communities. Together, we could enable a large number of companies to start taking advantage of the high potential of the DL technology.;158
Hidden Technical Debt in Machine Learning Systems;D. Sculley, Gary Holt, D. Golovin, Eugene Davydov, Todd Phillips, D. Ebner, Vinay Chaudhary, Michael Young, Jean-François Crespo, Dan Dennison;2015;Neural Information Processing Systems;Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.;988
Developing Bug-Free Machine Learning Systems With Formal Mathematics;Daniel Selsam, Percy Liang, D. Dill;2017;International Conference on Machine Learning;Noisy data, non-convex objectives, model misspecification, and numerical instability can all cause undesired behaviors in machine learning systems. As a result, detecting actual implementation errors can be extremely difficult. We demonstrate a methodology in which developers use an interactive proof assistant to both implement their system and to state a formal theorem defining what it means for their system to be correct. The process of proving this theorem interactively in the proof assistant exposes all implementation errors since any error in the program would cause the proof to fail. As a case study, we implement a new system, Certigrad, for optimizing over stochastic computation graphs, and we generate a formal (i.e. machine-checkable) proof that the gradients sampled by the system are unbiased estimates of the true mathematical gradients. We train a variational autoencoder using Certigrad and find the performance comparable to training the same model in TensorFlow.;54
Machine Learning System Architectural Pattern for Improving Operational Stability;Haruki Yokoyama;2019;2019 IEEE International Conference on Software Architecture Companion (ICSA-C);Recently, machine learning systems with inference engines have been widely used for a variety of purposes (such as prediction and classification) in our society. While it is quite important to keep the services provided by these machine learning systems stable, maintaining stability can be difficult given the nature of machine learning systems whose behaviors can be determined by program codes and input data. Therefore, quick troubleshooting (problem localization, rollback, etc.) is necessary. However, common machine learning systems with three-layer architectural patterns complicate the troubleshooting process because of their tightly coupled functions (e.g., business logic coded from design and inference engine derived from data). To solve the problem, we propose a novel architectural pattern for machine learning systems in which components for business logic and components for machine learning are separated. This architectural pattern helps operators break down the failures into a business logic part and a ML-specific part, and they can rollback the inference engine independent of the business logic when the inference engine has some problems. Through a practical case study scenario, we will show how our architectural pattern can make troubleshooting easier than common three-layer architecture.;33
On Testing Machine Learning Programs;Houssem Ben Braiek, Foutse Khomh;2018;Journal of Systems and Software;Abstract not available;154
A Machine Learning-based Intrinsic Method for Cross-topic and Cross-genre Authorship Verification;Yunita Sari, Mark Stevenson;2015;Conference and Labs of the Evaluation Forum;This paper presents our approach for the Author Identification task in the PAN CLEF Challenge 2015. We identified the challenges of this year’s are the limited amount of training data and the problems in the sub-corpora are independent in terms of topic and genre. We adopted a machine learning based intrinsic method to verify whether a pair of documents have been written by same or different authors. Several content-independent features, such as function words and stylometric features, were used to capture the difference between documents. Evaluation results on the test corpora show our approach works best on the Spanish data set with 0.7238 and 0.67 for the AUC and C@1 scores respectively.;5
Standardizing Ethical Design for Artificial Intelligence and Autonomous Systems;J. Bryson, A. Winfield;2017;Computer;AI is here now, available to anyone with access to digital technology and the Internet. But its consequences for our social order aren't well understood. How can we guide the way technology impacts society?;168
How do scientists develop and use scientific software?;J. Hannay, H. Langtangen, Carolyn MacLeod, Dietmar Pfahl, J. Singer, G. Wilson;2009;2009 ICSE Workshop on Software Engineering for Computational Science and Engineering;"New knowledge in science and engineering relies increasingly on results produced by scientific software. Therefore, knowing how scientists develop and use software in their research is critical to assessing the necessity for improving current development practices and to making decisions about the future allocation of resources. To that end, this paper presents the results of a survey conducted online in October-December 2008 which received almost 2000 responses. Our main conclusions are that (1) the knowledge required to develop and use scientific software is primarily acquired from peers and through self-study, rather than from formal education and training; (2) the number of scientists using supercomputers is small compared to the number using desktop or intermediate computers; (3) most scientists rely primarily on software with a large user base; (4) while many scientists believe that software testing is important, a smaller number believe they have sufficient understanding about testing concepts; and (5) that there is a tendency for scientists to rank standard software engineering concepts higher if they work in large software development projects and teams, but that there is no uniform trend of association between rank of importance of software engineering concepts and project/team size.";247
Validating a Deep Learning Framework by Metamorphic Testing;Junhua Ding, Xiaojun Kang, Xin-Hua Hu;2017;International Workshop on Metamorphic Testing;Deep learning has become an important tool for image classification and natural language processing. However, the effectiveness of deep learning is highly dependent on the quality of the training data as well as the net model for the learning. The training data set for deep learning normally is fairly large, and the net model is pretty complex. It is necessary to validate the deep learning framework including the net model, executing environment, and training data set before it is used for any applications. In this paper, we propose an approach for validating the classification accuracy of a deep learning framework that includes a convolutional neural network, a deep learning executing environment, and a massive image data set. The framework is first validated with a classifier built on support vector machine, and then it is tested using a metamorphic validation approach. The effectiveness of the approach is demonstrated by validating a deep learning classifier for automated classification of biology cell images. The proposed approach can be used for validating other deep learning framework for different applications.;59
Data Scientists in Software Teams: State of the Art and Challenges;Miryung Kim, Thomas Zimmermann, R. Deline, Andrew Begel;2018;IEEE Transactions on Software Engineering;The demand for analyzing large scale telemetry, machine, and quality data is rapidly increasing in software industry. Data scientists are becoming popular within software teams, e.g., Facebook, LinkedIn and Microsoft are creating a new career path for data scientists. In this paper, we present a large-scale survey with 793 professional data scientists at Microsoft to understand their educational background, problem topics that they work on, tool usages, and activities. We cluster these data scientists based on the time spent for various activities and identify 9 distinct clusters of data scientists, and their corresponding characteristics. We also discuss the challenges that they face and the best practices they share with other data scientists. Our study finds several trends about data scientists in the software engineering context at Microsoft, and should inform managers on how to leverage data science capability effectively within their teams.;73
Engineering safety in machine learning;Kush R. Varshney;2016;Information Theory and Applications Workshop;Machine learning algorithms are increasingly influencing our decisions and interacting with us in all parts of our daily lives. Therefore, just like for power plants, highways, and myriad other engineered sociotechnical systems, we must consider the safety of systems involving machine learning. In this paper, we first discuss the definition of safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. Then we examine dimensions, such as the choice of cost function and the appropriateness of minimizing the empirical average training cost, along which certain real-world applications may not be completely amenable to the foundational principle of modern statistical machine learning: empirical risk minimization. In particular, we note an emerging dichotomy of applications: ones in which safety is important and risk minimization is not the complete story (we name these Type A applications), and ones in which safety is not so critical and risk minimization is sufficient (we name these Type B applications). Finally, we discuss how four different strategies for achieving safety in engineering (inherently safe design, safety reserves, safe fail, and procedural safeguards) can be mapped to the machine learning context through inter-pretability and causality of predictive models, objectives beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software.;109
DeepXplore: Automated Whitebox Testing of Deep Learning Systems;Kexin Pei, Yinzhi Cao, Junfeng Yang, S. Jana;2017;Symposium on Operating Systems Principles;Deep learning (DL) systems are increasingly deployed in safety- and security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system's behavior for corner case inputs are of great importance. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose erroneous behaviors for rare inputs. We design, implement, and evaluate DeepXplore, the first whitebox framework for systematically testing real-world DL systems. First, we introduce neuron coverage for systematically measuring the parts of a DL system exercised by test inputs. Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles to avoid manual checking. Finally, we demonstrate how finding inputs for DL systems that both trigger many differential behaviors and achieve high neuron coverage can be represented as a joint optimization problem and solved efficiently using gradient-based search techniques. DeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models with thousands of neurons trained on five popular datasets including ImageNet and Udacity self-driving challenge data. For all tested DL models, on average, DeepXplore generated one test input demonstrating incorrect behavior within one second while running only on a commodity laptop. We further show that the test inputs generated by DeepXplore can also be used to retrain the corresponding DL model to improve the model's accuracy by up to 3%.;1257
DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems;L. Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, Yadong Wang;2018;International Conference on Automated Software Engineering;Deep learning (DL) defines a new data-driven programming paradigm that constructs the internal system logic of a crafted neuron network through a set of training data. We have seen wide adoption of DL in many safety-critical scenarios. However, a plethora of studies have shown that the state-of-the-art DL systems suffer from various vulnerabilities which can lead to severe consequences when applied to real-world applications. Currently, the testing adequacy of a DL system is usually measured by the accuracy of test data. Considering the limitation of accessible high quality test data, good accuracy performance on test data can hardly provide confidence to the testing adequacy and generality of DL systems. Unlike traditional software systems that have clear and controllable logic and functionality, the lack of interpretability in a DL system makes system analysis and defect detection difficult, which could potentially hinder its real-world deployment. In this paper, we propose DeepGauge, a set of multi-granularity testing criteria for DL systems, which aims at rendering a multi-faceted portrayal of the testbed. The in-depth evaluation of our proposed testing criteria is demonstrated on two well-known datasets, five DL systems, and with four state-of-the-art adversarial attack techniques against DL. The potential usefulness of DeepGauge sheds light on the construction of more generic and robust DL systems.;580
Machine Learning Software Engineering in Practice: An Industrial Case Study;M. S. Rahman, Emilio Rivera, Foutse Khomh, Yann-Gaël Guéhéneuc, Bernd Lehnert;2019;arXiv.org;SAP is the market leader in enterprise software offering an end-to-end suite of applications and services to enable their customers worldwide to operate their business. Especially, retail customers of SAP deal with millions of sales transactions for their day-to-day business. Transactions are created during retail sales at the point of sale (POS) terminals and then sent to some central servers for validations and other business operations. A considerable proportion of the retail transactions may have inconsistencies due to many technical and human errors. SAP provides an automated process for error detection but still requires a manual process by dedicated employees using workbench software for correction. However, manual corrections of these errors are time-consuming, labor-intensive, and may lead to further errors due to incorrect modifications. This is not only a performance overhead on the customers' business workflow but it also incurs high operational costs. Thus, automated detection and correction of transaction errors are very important regarding their potential business values and the improvement in the business workflow. In this paper, we present an industrial case study where we apply machine learning (ML) to automatically detect transaction errors and propose corrections. We identify and discuss the challenges that we faced during this collaborative research and development project, from three distinct perspectives: Software Engineering, Machine Learning, and industry-academia collaboration. We report on our experience and insights from the project with guidelines for the identified challenges. We believe that our findings and recommendations can help researchers and practitioners embarking into similar endeavors.;38
Feature-Guided Black-Box Safety Testing of Deep Neural Networks;Matthew Wicker, Xiaowei Huang, M. Kwiatkowska;2017;International Conference on Tools and Algorithms for Construction and Analysis of Systems;Abstract not available;225
DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for Autonomous Driving Systems;Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, S. Khurshid;2018;International Conference on Automated Software Engineering;While Deep Neural Networks (DNNs) have established the fundamentals of image-based autonomous driving systems, they may exhibit erroneous behaviors and cause fatal accidents. To address the safety issues in autonomous driving systems, a recent set of testing techniques have been designed to automatically generate artificial driving scenes to enrich test suite, e.g., generating new input images transformed from the original ones. However, these techniques are insufficient due to two limitations: first, many such synthetic images often lack diversity of driving scenes, and hence compromise the resulting efficacy and reliability. Second, for machine-learning-based systems, a mismatch between training and application domain can dramatically degrade system accuracy, such that it is necessary to validate inputs for improving system robustness. In this paper, we propose DeepRoad, an unsupervised DNN-based framework for automatically testing the consistency of DNN-based autonomous driving systems and online validation. First, DeepRoad automatically synthesizes large amounts of diverse driving scenes without using image transformation rules (e.g. scale, shear and rotation). In particular, DeepRoad is able to produce driving scenes with various weather conditions (including those with rather extreme conditions) by applying Generative Adversarial Networks (GANs) along with the corresponding real-world weather scenes. Second, DeepRoad utilizes metamorphic testing techniques to check the consistency of such systems using synthetic images. Third, DeepRoad validates input images for DNN-based systems by measuring the distance of the input and training images using their VGGNet features. We implement DeepRoad to test three well-recognized DNN-based autonomous driving systems in Udacity self-driving car challenge. The experimental results demonstrate that DeepRoad can detect thousands of inconsistent behaviors for these systems, and effectively validate input images to potentially enhance the system robustness as well.;514
An architectural framework for the construction of hybrid intelligent forecasting systems: application for electricity demand prediction;N. Lertpalangsunti, Christine W. Chan;1998;Publication venue not available;Abstract not available;32
Solution Patterns for Machine Learning;S. Nalchigar, E. Yu, Yazan Obeidi, Sebastian Carbajales, John Green, A. Chan;2019;International Conference on Advanced Information Systems Engineering;Abstract not available;12
MODE: automated neural network model debugging via state differential analysis and input selection;Shiqing Ma, Yingqi Liu, Wen-Chuan Lee, X. Zhang, A. Grama;2018;ESEC/SIGSOFT FSE;Artificial intelligence models are becoming an integral part of modern computing systems. Just like software inevitably has bugs, models have bugs too, leading to poor classification/prediction accuracy. Unlike software bugs, model bugs cannot be easily fixed by directly modifying models. Existing solutions work by providing additional training inputs. However, they have limited effectiveness due to the lack of understanding of model misbehaviors and hence the incapability of selecting proper inputs. Inspired by software debugging, we propose a novel model debugging technique that works by first conducting model state differential analysis to identify the internal features of the model that are responsible for model bugs and then performing training input selection that is similar to program input selection in regression testing. Our evaluation results on 29 different models for 6 different applications show that our technique can fix model bugs effectively and efficiently without introducing new bugs. For simple applications (e.g., digit recognition), MODE improves the test accuracy from 75% to 93% on average whereas the state-of-the-art can only improve to 85% with 11 times more training time. For complex applications and models (e.g., object recognition), MODE is able to improve the accuracy from 75% to over 91% in minutes to a few hours, whereas state-of-the-art fails to fix the bug or even degrades the test accuracy.;184
Testing and validating machine learning classifiers by metamorphic testing;Xiaoyuan Xie, J. Ho, Christian Murphy, G. Kaiser, Baowen Xu, T. Chen;2011;Journal of Systems and Software;Abstract not available;345
Concolic Testing for Deep Neural Networks;Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, M. Kwiatkowska, D. Kroening;2018;International Conference on Automated Software Engineering;Concolic testing combines program execution and symbolic analysis to explore the execution paths of a software program. This paper presents the first concolic testing approach for Deep Neural Networks (DNNs). More specifically, we formalise coverage criteria for DNNs that have been studied in the literature, and then develop a coherent method for performing concolic testing to increase test coverage. Our experimental results show the effectiveness of the concolic testing approach in both achieving high coverage and finding adversarial examples.;322
Is neuron coverage a meaningful measure for testing deep neural networks?;Fabrice Harel-Canada, Lingxiao Wang, Muhammad Ali Gulzar, Quanquan Gu, Miryung Kim;2020;ESEC/SIGSOFT FSE;Recent effort to test deep learning systems has produced an intuitive and compelling test criterion called neuron coverage (NC), which resembles the notion of traditional code coverage. NC measures the proportion of neurons activated in a neural network and it is implicitly assumed that increasing NC improves the quality of a test suite. In an attempt to automatically generate a test suite that increases NC, we design a novel diversity promoting regularizer that can be plugged into existing adversarial attack algorithms. We then assess whether such attempts to increase NC could generate a test suite that (1) detects adversarial attacks successfully, (2) produces natural inputs, and (3) is unbiased to particular class predictions. Contrary to expectation, our extensive evaluation finds that increasing NC actually makes it harder to generate an effective test suite: higher neuron coverage leads to fewer defects detected, less natural inputs, and more biased prediction preferences. Our results invoke skepticism that increasing neuron coverage may not be a meaningful objective for generating tests for deep neural networks and call for a new test generation technique that considers defect detection, naturalness, and output impartiality in tandem.;129
DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars;Yuchi Tian, Kexin Pei, S. Jana, Baishakhi Ray;2017;International Conference on Software Engineering;Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads. However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner-case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases. In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.;1252
Best Practices for Machine Learning Systems: An Industrial Framework for Analysis and Optimization;G. Chouliaras, Kornel Kielczewski, Amit Beka, D. Konopnicki, Lucas Bernardi;2023;arXiv.org;In the last few years, the Machine Learning (ML) and Artificial Intelligence community has developed an increasing interest in Software Engineering (SE) for ML Systems leading to a proliferation of best practices, rules, and guidelines aiming at improving the quality of the software of ML Systems. However, understanding their impact on the overall quality has received less attention. Practices are usually presented in a prescriptive manner, without an explicit connection to their overall contribution to software quality. Based on the observation that different practices influence different aspects of software-quality and that one single quality aspect might be addressed by several practices we propose a framework to analyse sets of best practices with focus on quality impact and prioritization of their implementation. We first introduce a hierarchical Software Quality Model (SQM) specifically tailored for ML Systems. Relying on expert knowledge, the connection between individual practices and software quality aspects is explicitly elicited for a large set of well-established practices. Applying set-function optimization techniques we can answer questions such as what is the set of practices that maximizes SQM coverage, what are the most important ones, which practices should be implemented in order to improve specific quality aspects, among others. We illustrate the usage of our framework by analyzing well-known sets of practices.;0
When DevOps Meets Meta-Learning: A Portfolio to Rule them all;Benjamin Benni, M. Blay-Fornarino, Sébastien Mosser, F. Precioso, Günther Jungbluth;2019;2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C);The Machine Learning (ML) world is in constant evolution, as the amount of different algorithms in this context is evolving quickly. Until now, it is the responsibility of data scientists to create ad-hoc ML pipelines for each situation they encounter, gaining knowledge about the adequacy between their context and the chosen pipeline. Considering that it is not possible at a human scale to analyze the exponential number of potential pipelines, picking the right pipeline that combines the proper preprocessing and algorithms is a hard task that requires knowledge and experience. In front of the complexity of building a right ML pipeline, algorithm portfolios aim to drive algorithm selection, learning from the past in a continuous process. However, building a portfolio requires that (i) data scientists develop and test pipelines and (ii) portfolio maintainers ensure the quality of the portfolio and enrich it. The firsts are the developers, while the seconds are the operators. In this paper, we present a set of criteria to be respected, and propose a pipeline-based meta-model, to support a DevOps approach in the context of Machine Learning Pipelines. The exploitation of this meta-model, both as a graph and as a logical expression, serves to ensure continuity between Dev and Ops. We depict our proposition through the simplified study of two primary use cases, one with developer's point-of-view, the other with ops'.;8
Collective knowledge: organizing research projects as a database of reusable components and portable workflows with common interfaces;G. Fursin;2021;Philosophical Transactions of the Royal Society A;This article provides the motivation and overview of the Collective Knowledge Framework (CK or cKnowledge). The CK concept is to decompose research projects into reusable components that encapsulate research artifacts and provide unified application programming interfaces (APIs), command-line interfaces (CLIs), meta descriptions and common automation actions for related artifacts. The CK framework is used to organize and manage research projects as a database of such components. Inspired by the USB ‘plug and play’ approach for hardware, CK also helps to assemble portable workflows that can automatically plug in compatible components from different users and vendors (models, datasets, frameworks, compilers, tools). Such workflows can build and run algorithms on different platforms and environments in a unified way using the customizable CK program pipeline with software detection plugins and the automatic installation of missing packages. This article presents a number of industrial projects in which the modular CK approach was successfully validated in order to automate benchmarking, auto-tuning and co-design of efficient software and hardware for machine learning and artificial intelligence in terms of speed, accuracy, energy, size and various costs. The CK framework also helped to automate the artifact evaluation process at several computer science conferences as well as to make it easier to reproduce, compare and reuse research techniques from published papers, deploy them in production, and automatically adapt them to continuously changing datasets, models and systems. The long-term goal is to accelerate innovation by connecting researchers and practitioners to share and reuse all their knowledge, best practices, artifacts, workflows and experimental results in a common, portable and reproducible format at https://cKnowledge.io/. This article is part of the theme issue ‘Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification in silico’.;19
Machine Learning Operations (MLOps): Overview, Definition, and Architecture;Dominik Kreuzberger, Niklas Kühl, Sebastian Hirschl;2022;IEEE Access;The final goal of all industrial machine learning (ML) projects is to develop ML products and rapidly bring them into production. However, it is highly challenging to automate and operationalize ML products and thus many ML endeavors fail to deliver on their expectations. The paradigm of Machine Learning Operations (MLOps) addresses this issue. MLOps includes several aspects, such as best practices, sets of concepts, and development culture. However, MLOps is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we conduct mixed-method research, including a literature review, a tool review, and expert interviews. As a result of these investigations, we contribute to the body of knowledge by providing an aggregated overview of the necessary principles, components, and roles, as well as the associated architecture and workflows. Furthermore, we provide a comprehensive definition of MLOps and highlight open challenges in the field. Finally, this work provides guidance for ML researchers and practitioners who want to automate and operate their ML products with a designated set of technologies.;237
Applying DevOps Practices of Continuous Automation for Machine Learning;I. Karamitsos, Saeed Albarhami, Charalampos Apostolopoulos;2020;Inf.;This paper proposes DevOps practices for machine learning application, integrating both the development and operation environment seamlessly. The machine learning processes of development and deployment during the experimentation phase may seem easy. However, if not carefully designed, deploying and using such models may lead to a complex, time-consuming approaches which may require significant and costly efforts for maintenance, improvement, and monitoring. This paper presents how to apply continuous integration (CI) and continuous delivery (CD) principles, practices, and tools so as to minimize waste, support rapid feedback loops, explore the hidden technical debt, improve value delivery and maintenance, and improve operational functions for real-world machine learning applications.;76
Dynamic Autoselection and Autotuning of Machine Learning Models for Cloud Network Analytics;R. Karn, P. Kudva, I. Elfadel;2019;IEEE Transactions on Parallel and Distributed Systems;Cloud network monitoring data is dynamic and distributed. Signals to monitor the cloud can appear, disappear or change their importance and clarity over time. Machine learning (ML) models tuned to a given data set can therefore quickly become inadequate. A model might be highly accurate at one point in time but may lose its accuracy at a later time due to changes in input data and their features. Distributed learning with dynamic model selection is therefore often required. Under such selection, poorly performing models (although aggressively tuned for the prior data) are retired or put on standby while new or standby models are brought in. The well-known method of Ensemble ML (EML) may potentially be applied to improve the overall accuracy of a family of ML models. Unfortunately, EML has several disadvantages, including the need for continuous training, excessive computational resources, requirement for large training datasets, high risks of overfitting, and a time-consuming model-building process. In this paper, we propose a novel cloud methodology for automatic ML model selection and tuning that automates model building and selection and is competitive with existing methods. We use unsupervised learning to better explore the data space before the generation of targeted supervised learning models in an automated fashion. In particular, we create a Cloud DevOps architecture for autotuning and selection based on container orchestration and messaging between containers, and take advantage of a new autoscaling method to dynamically create and evaluate instantiations of ML algorithms. The proposed methodology and tool are demonstrated on cloud network security datasets.;23
ml-experiment: A Python framework for reproducible data science;Antonio Molner Domenech, A. Guillén;2020;Journal of Physics: Conference Series;Nowadays, data science projects are usually developed in an unstructured way, which makes it difficult to reproduce. It is also hard to move from an experimental environment to production. Operational workflows such as containerization, continuous deployment, and cloud orchestration allow data science researchers to move a pipeline from a local environment to the cloud. Being aware of the difficulties of setting those workflows up, this paper presents a framework to ease experiment tracking and operationalizing machine learning by combining existent and well-supported technologies. These technologies include Docker, Mlflow, Ray, among others. The framework provides an opinionated workflow to design and execute experiments either on a local environment or the cloud. ml-experiment includes: an automatic tracking system for the most famous machine learning libraries: Tensorflow, Keras, Fastai, Xgboost and Lightgdm, first-class support for distributed training and hyperparameter optimization, and a Command Line Interface (CLI) for packaging and running projects inside containers.;3
Machine Learning Pipelines: From Research to Production;Alexandra Posoldova;2020;IEEE potentials;Machine learning (ML) and artificial intelligence (AI) are getting lot of attention these days, and an increasing number of people are interested in becoming data scientists. Many imagine ML/AI as a black box that does its magic and is somehow able to make predictions. Well, the magic is that an ML algorithm is designed in such a way that it is able to find a pattern in data that have the correct answers and use that to predict the next answer. The learning task may vary from predicting the next word in a sentence, a sentiment, or what else would you like to buy based on what you already have in your basket.;10
A Data Quality-Driven View of MLOps;Cédric Renggli, Luka Rimanic, Nezihe Merve Gurel, Bojan Karlavs, Wentao Wu, Ce Zhang;2021;IEEE Data Engineering Bulletin;Developing machine learning models can be seen as a process similar to the one established for traditional software development. A key difference between the two lies in the strong dependency between the quality of a machine learning model and the quality of the data used to train or perform evaluations. In this work, we demonstrate how different aspects of data quality propagate through various stages of machine learning development. By performing a joint analysis of the impact of well-known data quality dimensions and the downstream machine learning process, we show that different components of a typical MLOps pipeline can be efficiently designed, providing both a technical and theoretical perspective.;59
The machine learning life cycle and the cloud: implications for drug discovery;O. Spjuth, Jens Frid, A. Hellander;2021;Expert Opinion on Drug Discovery;"ABSTRACT Introduction: Artificial intelligence (AI) and machine learning (ML) are increasingly used in many aspects of drug discovery. Larger data sizes and methods such as Deep Neural Networks contribute to challenges in data management, the required software stack, and computational infrastructure. There is an increasing need in drug discovery to continuously re-train models and make them available in production environments. Areas covered: This article describes how cloud computing can aid the ML life cycle in drug discovery. The authors discuss opportunities with containerization and scientific workflows and introduce the concept of MLOps and describe how it can facilitate reproducible and robust ML modeling in drug discovery organizations. They also discuss ML on private, sensitive and regulated data. Expert opinion: Cloud computing offers a compelling suite of building blocks to sustain the ML life cycle integrated in iterative drug discovery. Containerization and platforms such as Kubernetes together with scientific workflows can enable reproducible and resilient analysis pipelines, and the elasticity and flexibility of cloud infrastructures enables scalable and efficient access to compute resources. Drug discovery commonly involves working with sensitive or private data, and cloud computing and federated learning can contribute toward enabling collaborative drug discovery within and between organizations. Abbreviations: AI = Artificial Intelligence; DL = Deep Learning; GPU = Graphics Processing Unit; IaaS = Infrastructure as a Service; K8S = Kubernetes; ML = Machine Learning; MLOps = Machine Learning and Operations; PaaS = Platform as a Service; QC = Quality Control; SaaS = Software as a Service";33
AI Models and Their Worlds: Investigating Data-Driven, AI/ML Ecosystems Through a Work Practices Lens;Christine T. Wolf;2020;iConference;Abstract not available;10
An Automatic Artificial Intelligence Training Platform Based on Kubernetes;Chaoyu Wu, E. Haihong, Meina Song;2020;International Conferences on Big Data Engineering and Technology;For large-scale AI training, the manual allocation of GPU resources is too inefficient, and it faces the problems of task allocation and fault restart. In this paper, a fully automatic machine learning platform is designed, which manages server resources uniformly, and users describe the required resources through configuration files. The platform automatically performs AI task allocation and scheduling based on the cluster load, which solves the problems of low cluster resource utilization and uneven machine load distribution. The platform also provides an automatic release and continuous integration of the model, which greatly simplifies the configuration of the model's operating environment and external release process, enabling researchers to focus more on model adjustments. Finally, it is verified by experiments that the extra time spent on AI task training through this platform is negligible, which confirms the feasibility of the platform.;6
DevOps Portal Design for SmartX AI Cluster Employing Cloud-Native Machine Learning Workflows;GeumSeong Yoon, Jungsu Han, Seunghyun Lee, JongWon Kim;2020;International Conference on Emerging Intelligent Data and Web Technologies;Abstract not available;4
Brazilian Data Scientists: Revealing their Challenges and Practices on Machine Learning Model Development;J. Correia, Juliana Alves Pereira, R. Mello, Alessandro F. Garcia, B. Neto, Márcio Ribeiro, Rohit Gheyi, Marcos Kalinowski, Renato Cerqueira, Willy Tiengo;2020;Brazilian Symposium on Software Quality;"Data scientists often develop machine learning models to solve a variety of problems in the industry and academy. To build these models, these professionals usually perform activities that are also performed in the traditional software development lifecycle, such as eliciting and implementing requirements. One might argue that data scientists could rely on the engineering of traditional software development to build machine learning models. However, machine learning development presents certain characteristics, which may raise challenges that lead to the need for adopting new practices. The literature lacks in characterizing this knowledge from the perspective of the data scientists. In this paper, we characterize challenges and practices addressing the engineering of machine learning models that deserve attention from the research community. To this end, we performed a qualitative study with eight data scientists across five different companies having different levels of experience in developing machine learning models. Our findings suggest that: (i) data processing and feature engineering are the most challenging stages in the development of machine learning models; (ii) it is essential synergy between data scientists and domain experts in most of stages; and (iii) the development of machine learning models lacks the support of a well-engineered process.";6
An Empirical Study of Common Challenges in Developing Deep Learning Applications;Tianyi Zhang, Cuiyun Gao, Lei Ma, Michael R. Lyu, Miryung Kim;2019;IEEE International Symposium on Software Reliability Engineering;Recent advances in deep learning promote the innovation of many intelligent systems and applications such as autonomous driving and image recognition. Despite enormous efforts and investments in this field, a fundamental question remains under-investigated—what challenges do developers commonly face when building deep learning applications? To seek an answer, this paper presents a large-scale empirical study of deep learning questions in a popular Q&A website, Stack Overflow. We manually inspect a sample of 715 questions and identify seven kinds of frequently asked questions. We further build a classification model to quantify the distribution of different kinds of deep learning questions in the entire set of 39,628 deep learning questions. We find that program crashes, model migration, and implementation questions are the top three most frequently asked questions. After carefully examining accepted answers of these questions, we summarize five main root causes that may deserve attention from the research community, including API misuse, incorrect hyperparameter selection, GPU computation, static graph computation, and limited debugging and profiling support. Our results highlight the need for new techniques such as cross-framework differential testing to improve software development productivity and software reliability in deep learning.;129
Towards a Software Engineering Process for Developing Data-Driven Applications;M. Hesenius, Nils Schwenzfeier, Ole Meyer, Wilhelm Koop, V. Gruhn;2019;2019 IEEE/ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE);Machine Learning and Artificial Intelligence allow the development of a new type of applications that automatically identify hidden patterns, process large amounts of data, and classify data according to aforementioned patterns. While they offer interesting solutions for several problems, they also impose challenges on software engineers in charge of structuring the development effort. The new applications require to incorporate additional specialists and their work into an overall development effort. We thus propose a software engineering process for data-driven applications.;34
Achieving guidance in applied machine learning through software engineering techniques;Lars Reimann, Günter Kniesel-Wünsche;2020;International Conference on the Art, Science and Engineering of Programming;Development of machine learning (ML) applications is hard. Producing successful applications requires, among others, being deeply familiar with a variety of complex and quickly evolving application programming interfaces (APIs). It is therefore critical to understand what prevents developers from learning these APIs, using them properly at development time, and understanding what went wrong when it comes to debugging. We look at the (lack of) guidance that currently used development environments and ML APIs provide to developers of ML applications, contrast these with software engineering best practices, and identify gaps in the current state of the art. We show that current ML tools fall short of fulfilling some basic software engineering gold standards and point out ways in which software engineering concepts, tools and techniques need to be extended and adapted to match the special needs of ML application development. Our findings point out ample opportunities for research on ML-specific software engineering.;8
Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure;Ben Hutchinson, A. Smart, A. Hanna, Emily L. Denton, Christina Greer, Oddur Kjartansson, Parker Barnes, Margaret Mitchell;2020;Conference on Fairness, Accountability and Transparency;Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.;248
Towards concept based software engineering for intelligent agents;Ole Meyer, V. Gruhn;2019;RAISE@ICSE;The development of AI and machine learning applications at an industry mature level while maintaining quality and productivity goals is one of today's major challenges. Research in the field of intelligent agents has achieved many successes in recent years, especially due to various reinforcement learning techniques, and promises a high benefit in times of automation and autonomous systems. Bringing them into production, however, requires optimization against many other criteria than just accuracy. This leads to the emerging field of machine teaching. We already know many of the objectives used there from software engineering research, which has led to many well-established principles in recent decades. One of them is the component-based development whose idea finds an interesting counterpart in hierarchical reinforcement learning. We show that both areas can benefit from each other and introduce our approach of concept based software engineering, which is focused on supporting productivity and quality goals during the development of such systems.;8
How does Machine Learning Change Software Development Practices?;Zhiyuan Wan, Xin Xia, David Lo, G. Murphy;2021;IEEE Transactions on Software Engineering;Adding an ability for a system to learn inherently adds uncertainty into the system. Given the rising popularity of incorporating machine learning into systems, we wondered how the addition alters software development practices. We performed a mixture of qualitative and quantitative studies with 14 interviewees and 342 survey respondents from 26 countries across four continents to elicit significant differences between the development of machine learning systems and the development of non-machine-learning systems. Our study uncovers significant differences in various aspects of software engineering (e.g., requirements, design, testing, and process) and work characteristics (e.g., skill variety, problem solving and task identity). Based on our findings, we highlight future research directions and provide recommendations for practitioners.;149
Sensemaking Practices in the Everyday Work of AI/ML Software Engineering;Christine T. Wolf, Drew Paine;2020;International Conference on Software Engineering;"This paper considers sensemaking as it relates to everyday software engineering (SE) work practices and draws on a multi-year ethnographic study of SE projects at a large, global technology company building digital services infused with artificial intelligence (AI) and machine learning (ML) capabilities. Our findings highlight the breadth of sensemaking practices in AI/ML projects, noting developers' efforts to make sense of AI/ML environments (e.g., algorithms/methods and libraries), of AI/ML model ecosystems (e.g., pre-trained models and ""upstream"" models), and of business-AI relations (e.g., how the AI/ML service relates to the domain context and business problem at hand). This paper builds on recent scholarship drawing attention to the integral role of sensemaking in everyday SE practices by empirically investigating how and in what ways AI/ML projects present software teams with emergent sensemaking requirements and opportunities.";11
Risk-based data validation in machine learning-based software systems;Harald Foidl, M. Felderer;2019;MaLTeSQuE@ESEC/SIGSOFT FSE;Data validation is an essential requirement to ensure the reliability and quality of Machine Learning-based Software Systems. However, an exhaustive validation of all data fed to these systems (i.e. up to several thousand features) is practically unfeasible. In addition, there has been little discussion about methods that support software engineers of such systems in determining how thorough to validate each feature (i.e. data validation rigor). Therefore, this paper presents a conceptual data validation approach that prioritizes features based on their estimated risk of poor data quality. The risk of poor data quality is determined by the probability that a feature is of low data quality and the impact of this low (data) quality feature on the result of the machine learning model. Three criteria are presented to estimate the probability of low data quality (Data Source Quality, Data Smells, Data Pipeline Quality). To determine the impact of low (data) quality features, the importance of features according to the performance of the machine learning model (i.e. Feature Importance) is utilized. The presented approach provides decision support (i.e. data validation prioritization and rigor) for software engineers during the implementation of data validation techniques in the course of deploying a trained machine learning model and its software stack.;30
Supporting Software Engineering Practices in the Development of Data-Intensive HPC Applications with the JuML Framework;Markus Götz, Matthias Book, Christian Bodenstein, M. Riedel;2017;Publication venue not available;The development of high performance computing applications is considerably different from traditional software development. This distinction is due to the complex hardware systems, inherent parallelism, different software lifecycle and workflow, as well as (especially for scientific computing applications) partially unknown requirements at design time. This makes the use of software engineering practices challenging, so only a small subset of them are actually applied. In this paper, we discuss the potential for applying software engineering techniques to an emerging field in high performance computing, namely large-scale data analysis and machine learning. We argue for the employment of software engineering techniques in the development of such applications from the start, and the design of generic, reusable components. Using the example of the Juelich Machine Learning Library (JuML), we demonstrate how such a framework can not only simplify the design of new parallel algorithms, but also increase the productivity of the actual data analysis workflow. We place particular focus on the abstraction from heterogeneous hardware, the architectural design as well as aspects of parallel and distributed unit testing.;3
AIMMX: Artificial Intelligence Model Metadata Extractor;Jason Tsay, Alan Braz, Martin Hirzel, Avraham Shinnar, Todd W. Mummert;2020;IEEE Working Conference on Mining Software Repositories;Despite all of the power that machine learning and artificial intelligence (AI) models bring to applications, much of AI development is currently a fairly ad hoc process. Software engineering and AI development share many of the same languages and tools, but AI development as an engineering practice is still in early stages. Mining software repositories of AI models enables insight into the current state of AI development. However, much of the relevant metadata around models are not easily extractable directly from repositories and require deduction or domain knowledge. This paper presents a library called AIMMX that enables simplified AI Model Metadata eXtraction from software repositories. The extractors have five modules for extracting AI model-specific metadata: model name, associated datasets, references, AI frameworks used, and model domain. We evaluated AIMMX against 7,998 open-source models from three sources: model zoos, arXiv AI papers, and state-of-the-art AI papers. Our platform extracted metadata with 87% precision and 83% recall. As preliminary examples of how AI model metadata extraction enables studies and tools to advance engineering support for AI development, this paper presents an exploratory analysis for data and method reproducibility over the models in the evaluation dataset and a catalog tool for discovering and managing models. Our analysis suggests that while data reproducibility may be relatively poor with 42% of models in our sample citing their datasets, method reproducibility is more common at 72% of models in our sample, particularly state-of-the-art models. Our collected models are searchable in a catalog that uses existing metadata to enable advanced discovery features for efficiently finding models.;17
A large-scale comparative analysis of Coding Standard conformance in Open-Source Data Science projects;Andrew J. Simmons, Scott Barnett, Jessica Rivera-Villicana, Akshat Bajaj, Rajesh Vasa;2020;International Symposium on Empirical Software Engineering and Measurement;Background: Meeting the growing industry demand for Data Science requires cross-disciplinary teams that can translate machine learning research into production-ready code. Software engineering teams value adherence to coding standards as an indication of code readability, maintainability, and developer expertise. However, there are no large-scale empirical studies of coding standards focused specifically on Data Science projects. Aims: This study investigates the extent to which Data Science projects follow code standards. In particular, which standards are followed, which are ignored, and how does this differ to traditional software projects? Method: We compare a corpus of 1048 Open-Source Data Science projects to a reference group of 1099 non-Data Science projects with a similar level of quality and maturity. Results: Data Science projects suffer from a significantly higher rate of functions that use an excessive numbers of parameters and local variables. Data Science projects also follow different variable naming conventions to non-Data Science projects. Conclusions: The differences indicate that Data Science codebases are distinct from traditional software codebases and do not follow traditional software engineering conventions. Our conjecture is that this may be because traditional software engineering conventions are inappropriate in the context of Data Science projects.;20
Software Engineering for Data Analytics;Miryung Kim;2020;IEEE Software;We are at an inflection point where software engineering meets the data-centric world of big data, machine learning, and artificial intelligence. In this article, I summarize findings from studies of professional data scientists and discuss my perspectives on open research problems to improve data-centric software development.;7
Systematic literature reviews: A Case Study in FinTech and Automated Tool Support;Wim Spaargaren;2020;Publication venue not available;Systematic literature reviews in software engineering as well as other disciplines, serve as the foundation for sound scientific research. The aim for these literature reviews is to aggregate all existing knowledge on a research problem and produce informed guidelines for practitioners. This enables practitioners to apply appropriate software engineering solutions in a specific contexts. However, one major problem exists regarding systematic literature reviews, the overall execution duration may take up as much as 24 months. The first objective of this study is to provide a solid base for the AI for FinTech Research collaboration by performing a systematic literature review. This literature review is used to identify different machine learning techniques in the context of the FinTech domain. However, during this study, we found that a significant amount of time was spent on repetitive work which potentially could have been automated. Therefore, the second objective of this work is to reduce the overall workload for performing systematic literature reviews. First, a literature review is performed regarding automation solutions for different steps of systematic literature reviews. The identified solutions were used to create a tool to automate steps in both the retrieval and screening phase of systematic literature reviews. First, this work presented the state of the art regarding machine learning applications in the FinTech domain. Afterwards, a complete overview of possible automation solutions for every step of performing literature reviews was detailed. Using this overview, a tool was created which showed that the overall workload of the retrieval and screening phase of systematic literature reviews can be significantly reduced.;1
Toward Human-in-the-Loop Collaboration Between Software Engineers and Machine Learning Algorithms;N. Nascimento, P. Alencar, C. Lucena, D. Cowan;2018;2018 IEEE International Conference on Big Data (Big Data);Several papers have recently contained reports on applying machine learning (ML) to the automation of software engineering (SE) tasks, such as project management, modeling and development. However, there appear to be no approaches comparing how software engineers fare against machine-learning algorithms as applied to specific software development tasks. Such a comparison is essential to gain insight into which tasks are better performed by humans and which by machine learning and how cooperative work or human-in-the-loop processes can be implemented more effectively. In this paper, we present an empirical study that compares how software engineers and machine-learning algorithms perform and reuse tasks. The empirical study involves the synthesis of the control structure of an autonomous streetlight application.;12
A Scientific Knowledge Discovery and Data Mining Process Model for Metabolomics;Ahmed Banimustafa, Nigel W. Hardy;2020;IEEE Access;This work presents a scientific data mining process model for metabolomics that provides a systematic and formalised framework for guiding and performing metabolomics data analysis in a justifiable and traceable manner. The process model is designed to promote the achievement of the analytical objectives of metabolomics investigations and to ensure the validity, interpretability and reproducibility of their results. It satisfies the requirements of metabolomics data mining, focuses on the contextual meaning of metabolomics knowledge, and addresses the shortcomings of existing data mining process models, while paying attention to the practical aspects of metabolomics investigations and other desirable features. The process model development involved investigating the ontologies and standards of science, data mining and metabolomics and its design was based on the principles, best practices and inspirations from Process Engineering, Software Engineering, Scientific Methodology and Machine Learning. A software environment was built to realise and automate the process model execution and was then applied to a number of metabolomics datasets to demonstrate and evaluate its applicability to different metabolomics investigations, approaches and data acquisition instruments on one hand, and to different data mining approaches, goals, tasks and techniques on the other. The process model was successful in satisfying the requirements of metabolomics data mining and can be generalised to perform data mining in other scientific disciplines.;10
Analysis of Software Engineering for Agile Machine Learning Projects;K. Singla, Joy Bose, C. Naik;2018;IEEE India Conference;The number of machine learning, artificial intelligence or data science related software engineering projects using Agile methodology is increasing. However, there are very few studies on how such projects work in practice. In this paper, we analyze project issues tracking data taken from Scrum (a popular tool for Agile) for several machine learning projects. We compare this data with corresponding data from non-machine learning projects, in an attempt to analyze how machine learning projects are executed differently from normal software engineering projects. On analysis, we find that machine learning project issues use different kinds of words to describe issues, have higher number of exploratory or research oriented tasks as compared to implementation tasks, and have a higher number of issues in the product backlog after each sprint, denoting that it is more difficult to estimate the duration of machine learning project related tasks in advance. After analyzing this data, we propose a few ways in which Agile machine learning projects can be better logged and executed, given their differences with normal software engineering projects.;7
Software Engineering Practices for Machine Learning;P. Kriens, Tim Verbelen;2019;arXiv.org;In the last couple of years we have witnessed an enormous increase of machine learning (ML) applications. More and more program functions are no longer written in code, but learnt from a huge amount of data samples using an ML algorithm. However, what is often overlooked is the complexity of managing the resulting ML models as well as bringing these into a real production system. In software engineering, we have spent decades on developing tools and methodologies to create, manage and assemble complex software modules. We present an overview of current techniques to manage complex software, and how this applies to ML models.;7
A Systematic Literature Review on Federated Machine Learning;Sin Kit Lo, Q. Lu, Chen Wang, Hye-young Paik, Liming Zhu;2020;ACM Computing Surveys;Federated learning is an emerging machine learning paradigm where clients train models locally and formulate a global model based on the local model updates. To identify the state-of-the-art in federated learning and explore how to develop federated learning systems, we perform a systematic literature review from a software engineering perspective, based on 231 primary studies. Our data synthesis covers the lifecycle of federated learning system development that includes background understanding, requirement analysis, architecture design, implementation, and evaluation. We highlight and summarise the findings from the results and identify future trends to encourage researchers to advance their current work.;69
Synergy between Machine/Deep Learning and Software Engineering: How Far Are We?;Simin Wang, LiGuo Huang, Jidong Ge, Tengfei Zhang, Haitao Feng, Ming Li, He Zhang, Vincent Ng;2020;arXiv.org;Since 2009, the deep learning revolution, which was triggered by the introduction of ImageNet, has stimulated the synergy between Machine Learning (ML)/Deep Learning (DL) and Software Engineering (SE). Meanwhile, critical reviews have emerged that suggest that ML/DL should be used cautiously. To improve the quality (especially the applicability and generalizability) of ML/DL-related SE studies, and to stimulate and enhance future collaborations between SE/AI researchers and industry practitioners, we conducted a 10-year Systematic Literature Review (SLR) on 906 ML/DL-related SE papers published between 2009 and 2018. Our trend analysis demonstrated the mutual impacts that ML/DL and SE have had on each other. At the same time, however, we also observed a paucity of replicable and reproducible ML/DL-related SE studies and identified five factors that influence their replicability and reproducibility. To improve the applicability and generalizability of research results, we analyzed what ingredients in a study would facilitate an understanding of why a ML/DL technique was selected for a specific SE problem. In addition, we identified the unique trends of impacts of DL models on SE tasks, as well as five unique challenges that needed to be met in order to better leverage DL to improve the productivity of SE tasks. Finally, we outlined a road-map that we believe can facilitate the transfer of ML/DL-based SE research results into real-world industry practices.;7
Software Engineering for Machine-Learning Applications: The Road Ahead;Foutse Khomh, Bram Adams, Jinghui Cheng, Marios Fokaefs, G. Antoniol;2018;IEEE Software;The First Symposium on Software Engineering for Machine Learning Applications (SEMLA) aimed to create a space in which machine learning (ML) and software engineering (SE) experts could come together to discuss challenges, new insights, and practical ideas regarding the engineering of ML and AI-based systems. Key challenges discussed included the accuracy of systems built using ML and AI models, the testing of those systems, industrial applications of AI, and the rift between the ML and SE communities. This article is part of a theme issue on software engineering’s 50th anniversary.;22
Automotive Safety and Machine Learning: Initial Results from a Study on How to Adapt the ISO 26262 Safety Standard;Jens Henriksson, Markus Borg, Cristofer Englund;2018;2018 IEEE/ACM 1st International Workshop on Software Engineering for AI in Autonomous Systems (SEFAIAS);Machine learning (ML) applications generate a continuous stream of success stories from various domains. ML enables many novel applications, also in safety-critical contexts. However, the functional safety standards such as ISO 26262 did not evolve to cover ML. We conduct an exploratory study on which parts of ISO 26262 represent the most critical gaps between safety engineering and ML development. While this paper only reports the first steps toward a larger research endeavor, we report three adaptations that are critically needed to allow ISO 26262 compliant engineering, and related suggestions on how to evolve the standard.;46
Towards Classes of Architectural Dependability Assurance for Machine-Learning-Based Systems;Max Scheerer, Jonas Klamroth, Ralf H. Reussner, Bernhard Beckert;2020;International Symposium on Software Engineering for Adaptive and Self-Managing Systems;Advances in Machine Learning (ML) have brought previously hard to handle problems within arm's reach. However, this power comes at the cost of unassured reliability and lacking transparency. Overcoming this drawback is very hard due to the probabilistic nature of ML. Current approaches mainly tackle this problem by developing more robust learning procedures. Such algorithmic approaches, however, are limited to certain types of uncertainties and cannot deal with all of them, e.g., hardware failure. This paper discusses how this problem can be addressed at architectural rather than algorithmic level to assess systems dependability properties in early development stages. Moreover, we argue that Self-Adaptive Systems (SAS) are more suited to safeguard ML w.r.t. various uncertainties. As a step towards this we propose classes of dependability in which ML-based systems may be categorized and discuss which and how assurances can be made for each class.;9
Interpreting Cloud Computer Vision Pain-Points: A Mining Study of Stack Overflow;Alex Cummaudo, Rajesh Vasa, Scott Barnett, J. Grundy, Mohamed Abdelrazek;2020;International Conference on Software Engineering;"Intelligent services are becoming increasingly more pervasive; application developers want to leverage the latest advances in areas such as computer vision to provide new services and products to users, and large technology firms enable this via RESTful APIs. While such APIs promise an easy-to-integrate on-demand machine intelligence, their current design, documentation and developer interface hides much of the underlying machine learning techniques that power them. Such APIs look and feel like conventional APIs but abstract away data-driven probabilistic behaviour—the implications of a developer treating these APIs in the same way as other, traditional cloud services, such as cloud storage, is of concern. The objective of this study is to determine the various pain-points developers face when implementing systems that rely on the most mature of these intelligent services, specifically those that provide computer vision. We use Stack Overflow to mine indications of the frustrations that developers appear to face when using computer vision services, classifying their questions against two recent classification taxonomies (documentation-related and general questions). We find that, unlike mature fields like mobile development, there is a contrast in the types of questions asked by developers. These indicate a shallow understanding of the underlying technology that empower such systems. We discuss several implications of these findings via the lens of learning taxonomies to suggest how the software engineering community can improve these services and comment on the nature by which developers use them.";29
Importance-Driven Deep Learning System Testing;Simos Gerasimou, Hasan Ferit Eniser, A. Sen, Alper Çakan;2020;International Conference on Software Engineering;Deep Learning (DL) systems are key enablers for engineering intelligent applications due to their ability to solve complex tasks such as image recognition and machine translation. Nevertheless, using DL systems in safety- and security-critical applications requires to provide testing evidence for their dependable operation. Recent research in this direction focuses on adapting testing criteria from traditional software engineering as a means of increasing confidence for their correct behaviour. However, they are inadequate in capturing the intrinsic properties exhibited by these systems. We bridge this gap by introducing DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems, across multiple DL datasets and with state-of-the-art adversarial generation techniques demonstrates the usefulness and effectiveness of DeepImportance and its ability to support the engineering of more robust DL systems.;92
Developing Machine Learning Products Better and Faster at Startups;R. Shams;2018;IEEE Engineering Management Review;There are many uncertainties in developing machine-learning (ML)-based products. Due to the gap between research and development, the overall progress becomes slow, and experiences many failures and learnings only to see an initial idea not working or generating no significant revenue. To minimize these drastic effects, there are continuing studies to make a balanced handshake between research and development to shorten the time span of ML-based products from idea generation to deployment. This paper demonstrates a three-phase ML product development workflow at OneClass. The workflow is a combination of multiple best-practices in the innovation-based startup industry. The first phase of the workflow considers the pivotal idea generation for products that involves data reliability assessment, idea prioritization, expectation setting, and building trust among users. The second phase concentrates on several state-of-the-art strategies for planning and future re-use of several product components. Finally, the actual research and development phase describes the fail-fast method practiced by OneClass to learn quickly from failures and act accordingly. The workflow is followed by the company to develop many sophisticated ML-based products successfully within a very short period of time.;8
Asset Management in Machine Learning: A Survey;S. Idowu, D. Strüber, T. Berger;2021;2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP);Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.;28
Toward Requirements Specification for Machine-Learned Components;Mona Rahimi, Jin L. C. Guo, Sahar Kokaly, M. Chechik;2019;2019 IEEE 27th International Requirements Engineering Conference Workshops (REW);"In current practice, the behavior of Machine-Learned Components (MLCs) is not sufficiently specified by the predefined requirements. Instead, they ""learn"" existing patterns from the available training data, and make predictions for unseen data when deployed. On the surface, their ability to extract patterns and to behave accordingly is specifically useful for hard-to-specify concepts in certain safety critical domains (e.g., the definition of a pedestrian in a pedestrian detection component in a vehicle). However, the lack of requirements specifications on their behaviors makes further software engineering tasks challenging for such components. This is especially concerning for tasks such as safety assessment and assurance. In this position paper, we call for more attention from the requirements engineering community on supporting the specification of requirements for MLCs in safety critical domains. Towards that end, we propose an approach to improve the process of requirements specification in which an MLC is developed and operates by explicitly specifying domain-related concepts. Our approach extracts a universally accepted benchmark for hard-to-specify concepts (e.g., ""pedestrian"") and can be used to identify gaps in the associated dataset and the constructed machine-learned model.";35
Building Artificial Intelligent (AI) Products that Make Sense;Ralph Vincent J. Regalado;2018;Publication venue not available;"Developing AI products for the mass market has been an ongoing challenge considering product-market fit, limited technology know-how, and user adoption. This paper provides a list of challenges and explores how we can launch AI products from idea to market by applying design thinking and lean-startup strategies. Artificial Intelligence;";0
A business model template for AI solutions;Iuliia Metelskaia, Olga Ignatyeva, Sebastian Denef, Tatjana Samsonowa;2018;International Conference on Information and Software Technologies;In this paper, we present a canvas that describes the building blocks of business models for AI solutions. As startups, spin-offs and existing corporations increasingly transfer AI research and technology into commercial products and services, AI engineers can benefit from focusing and positioning their work within the overall strategy of such ventures. We designed the business model canvas for AI solutions based on existing research, nine cases from secondary sources, as well as five case studies that we conducted ourselves. Our research highlights the most frequently used elements in each block of the business model canvas, such as common characteristics in the value propositions, multi-sided platforms in company segments, automated service in customer relationship, social networks in channels, investors in key partners, R&D in key activities, human resources in key resources and Software-as-a-Service in revenue. We designed the canvas as a tool useful for anybody creating or analyzing AI solutions. On a larger perspective, our study contributes to the existing literature on business model and business model innovation by consolidating existing practices of AI sector and emphasizing on important patterns.;16
Data Management in Machine Learning: Challenges, Techniques, and Systems;Arun Kumar, Matthias Boehm, Jun Yang;2017;SIGMOD Conference;Large-scale data analytics using statistical machine learning (ML), popularly called advanced analytics, underpins many modern data-driven applications. The data management community has been working for over a decade on tackling data management-related challenges that arise in ML workloads, and has built several systems for advanced analytics. This tutorial provides a comprehensive review of such systems and analyzes key data management challenges and techniques. We focus on three complementary lines of work: (1) integrating ML algorithms and languages with existing data systems such as RDBMSs, (2) adapting data management-inspired techniques such as query optimization, partitioning, and compression to new systems that target ML workloads, and (3) combining data management and ML ideas to build systems that improve ML lifecycle-related tasks. Finally, we identify key open data management challenges for future research in this important area.;132
Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need?;Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daumé, Miroslav Dudík, Hanna M. Wallach;2018;International Conference on Human Factors in Computing Systems;The potential for machine learning (ML) systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness. If these tools are to have a positive impact on industry practice, however, it is crucial that their design be informed by an understanding of real-world needs. Through 35 semi-structured interviews and an anonymous survey of 267 ML practitioners, we conduct the first systematic investigation of commercial product teams' challenges and needs for support in developing fairer ML systems. We identify areas of alignment and disconnect between the challenges faced by teams in practice and the solutions proposed in the fair ML research literature. Based on these findings, we highlight directions for future ML and HCI research that will better address practitioners' needs.;654
Machine Learning for the Developing World;Maria De-Arteaga, William Herlands, Daniel B. Neill, A. Dubrawski;2018;ACM Transactions on Management Information Systems;Researchers from across the social and computer sciences are increasingly using machine learning to study and address global development challenges. This article examines the burgeoning field of machine learning for the developing world (ML4D). First, we present a review of prominent literature. Next, we suggest best practices drawn from the literature for ensuring that ML4D projects are relevant to the advancement of development objectives. Finally, we discuss how developing world challenges can motivate the design of novel machine learning methodologies. This article provides insights into systematic differences between ML4D and more traditional machine learning applications. It also discusses how technical complications of ML4D can be treated as novel research questions, how ML4D can motivate new research directions, and where machine learning can be most useful.;60
Machine Learning in the Real World;Vineet Chaoji, R. Rastogi, Gourav Roy;2016;Proceedings of the VLDB Endowment;Machine Learning (ML) has become a mature technology that is being applied to a wide range of business problems such as web search, online advertising, product recommendations, object recognition, and so on. As a result, it has become imperative for researchers and practitioners to have a fundamental understanding of ML concepts and practical knowledge of end-to-end modeling. This tutorial takes a hands-on approach to introducing the audience to machine learning. The first part of the tutorial gives a broad overview and discusses some of the key concepts within machine learning. The second part of the tutorial takes the audience through the end-to-end modeling pipeline for a real-world income prediction problem.;15
Vertica-ML: Distributed Machine Learning in Vertica Database;A. Fard, Anh Le, George Larionov, Waqas Dhillon, Chuck Bear;2020;SIGMOD Conference;"A growing number of companies rely on machine learning as a key element for gaining a competitive edge from their collected Big Data. An in-database machine learning system can provide many advantages in this scenario, e.g., eliminating the overhead of data transfer, avoiding the maintenance costs of a separate analytical system, and addressing data security and provenance concerns. In this paper, we present our distributed machine learning subsystem within the Vertica database. This subsystem, Vertica-ML, includes machine learning functionalities with SQL API which cover a complete data science workflow as well as model management. We treat machine learning models in Vertica as first-class database objects like tables and views; therefore, they enjoy a similar mechanism for archiving and managing. We explain the architecture of the subsystem, and present a set of experiments to evaluate the performance of the machine learning algorithms implemented on top of it.";27
Modeling machine learning requirements from three perspectives: a case report from the healthcare domain;S. Nalchigar, E. Yu, K. Keshavjee;2021;Requirements Engineering;Abstract not available;30
Is Machine Learning Software Just Software: A Maintainability View;T. Mikkonen, J. Nurminen, M. Raatikainen, Ilenia Fronza, Niko Mäkitalo, T. Männistö;2021;International Conference on Software Quality. Process Automation in Software Development;Abstract not available;17
Artificial intelligence and machine learning as business tools: A framework for diagnosing value destruction potential;A. Canhoto, F. Clear;2020;Publication venue not available;Abstract not available;188
Machine learning for enterprises: Applications, algorithm selection, and challenges;In Lee, Yongjae Shin;2020;Publication venue not available;Abstract not available;212
Toward data mining engineering: A software engineering approach;Ó. Marbán, J. Segovia, Ernestina Menasalvas Ruiz, Covadonga Fernández-Baizán;2009;Information Systems;Abstract not available;80
How do data scientists and managers influence machine learning value creation?;H. Ferreira, P. Ruivo, Carolina Reis;2020;CENTERIS/ProjMAN/HCist;Abstract not available;6
A Systematic Literature Review on Applying CRISP-DM Process Model;Christoph Schröer, Felix Kruse, J. Gómez;2020;CENTERIS/ProjMAN/HCist;Abstract not available;264
Data Science Methodologies: Current Challenges and Future Approaches;Iñigo Martinez, E. Viles, Igor García Olaizola;2021;Big Data Research;Abstract not available;56
Business Analysis Method for Constructing Business-AI Alignment Model;H. Takeuchi, Shuichiro Yamamoto;2020;International Conference on Knowledge-Based Intelligent Information & Engineering Systems;Abstract not available;15
Big Data analytics in Agile software development: A systematic mapping study;Katarzyna Biesialska, Xavier Franch, V. Muntés-Mulero;2020;Information and Software Technology;Abstract not available;36
Machine learning algorithms in production: A guideline for efficient data source selection;P. Stanula, Amina Ziegenbein, J. Metternich;2018;Publication venue not available;Abstract not available;27
A Case for Serverless Machine Learning;J. Carreira;2018;Publication venue not available;The scale and complexity of ML workflows makes it hard to provision and manage resources—a burden for ML practitioners that hinders both their productivity and effectiveness. Encouragingly, however, serverless computing has recently emerged as a compelling solution to address the general problem of data center resource management. This work analyzes the resource management problem in the specific context of ML workloads and explores a research direction that leverages serverless infrastructures to automate the management of resources for ML workflows. We make a case for a serverless machine learning framework, specializing both for serverless infrastructures and Machine Learning workflows, and argue that either of those in isolation is insufficient.;56
Exploring Serverless Computing for Neural Network Training;Lang Feng, P. Kudva, D. D. Silva, Jiang Hu;2018;IEEE International Conference on Cloud Computing;Serverless or functions as a service runtimes have shown significant benefits to efficiency and cost for event-driven cloud applications. Although serverless runtimes are limited to applications requiring lightweight computation and memory, such as machine learning prediction and inference, they have shown improvements on these applications beyond other cloud runtimes. Training deep learning can be both compute and memory intensive. We investigate the use of serverless runtimes while leveraging data parallelism for large models, show the challenges and limitations due to the tightly coupled nature of such models, and propose modifications to the underlying runtime implementations that would mitigate them. For hyperparameter optimization of smaller deep learning models, we show that serverless runtimes can provide significant benefit.;85
Implementation of Unsupervised k-Means Clustering Algorithm Within Amazon Web Services Lambda;A. Deese;2018;IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing;This work demonstrates how an unsupervised learning algorithm based on k-Means Clustering with Kaufman Initialization may be implemented effectively as an Amazon Web Services Lambda Function, within their serverless cloud computing service. It emphasizes the need to employ a lean and modular design philosophy, transfer data efficiently between Lambda and DynamoDB, as well as employ Lambda Functions within mobile applications seamlessly and with negligible latency. This work presents a novel application of serverless cloud computing and provides specific examples that will allow readers to develop similar algorithms. The author provides compares the computation speed and cost of machine learning implementations on traditional PC and mobile hardware (running locally) as well as implementations that employ Lambda.;5
Pay-Per-Request Deployment of Neural Network Models Using Serverless Architectures;Zhucheng Tu, Mengping Li, Jimmy J. Lin;2018;North American Chapter of the Association for Computational Linguistics;We demonstrate the serverless deployment of neural networks for model inferencing in NLP applications using Amazon’s Lambda service for feedforward evaluation and DynamoDB for storing word embeddings. Our architecture realizes a pay-per-request pricing model, requiring zero ongoing costs for maintaining server instances. All virtual machine management is handled behind the scenes by the cloud provider without any direct developer intervention. We describe a number of techniques that allow efficient use of serverless resources, and evaluations confirm that our design is both scalable and inexpensive.;26
Serving Deep Learning Models in a Serverless Platform;Vatche Isahagian, Vinod Muthusamy, Aleksander Slominski;2017;2018 IEEE International Conference on Cloud Engineering (IC2E);Serverless computing has emerged as a compelling paradigm for the development and deployment of a wide range of event based cloud applications. At the same time, cloud providers and enterprise companies are heavily adopting machine learning and Artificial Intelligence to either differentiate themselves, or provide their customers with value added services. In this work we evaluate the suitability of a serverless computing environment for the inferencing of large neural network models. Our experimental evaluations are executed on the AWS Lambda environment using the MxNet deep learning framework. Our experimental results show that while the inferencing latency can be within an acceptable range, longer delays due to cold starts can skew the latency distribution and hence risk violating more stringent SLAs.;162
BARISTA: Efficient and Scalable Serverless Serving System for Deep Learning Prediction Services;Anirban Bhattacharjee, A. Chhokra, Zhuangwei Kang, Hongyang Sun, A. Gokhale, G. Karsai;2019;2019 IEEE International Conference on Cloud Engineering (IC2E);Pre-trained deep learning models are increasingly being used to offer a variety of compute-intensive predictive analytics services such as fitness tracking, speech, and image recognition. The stateless and highly parallelizable nature of deep learning models makes them well-suited for serverless computing paradigm. However, making effective resource management decisions for these services is a hard problem due to the dynamic workloads and diverse set of available resource configurations that have different deployment and management costs. To address these challenges, we present a distributed and scalable deep-learning prediction serving system called Barista and make the following contributions. First, we present a fast and effective methodology for forecasting workloads by identifying various trends. Second, we formulate an optimization problem to minimize the total cost incurred while ensuring bounded prediction latency with reasonable accuracy. Third, we propose an efficient heuristic to identify suitable compute resource configurations. Fourth, we propose an intelligent agent to allocate and manage the compute resources by horizontal and vertical scaling to maintain the required prediction latency. Finally, using representative real-world workloads for an urban transportation service, we demonstrate and validate the capabilities of Barista.;59
Behavior Analysis using Serverless Machine Learning;Darezik Damkevala, Rohit Lunavara, Mansi Kosamkar, Suja Jayachandran;2019;International Conference on Computing for Sustainable Global Development;This paper supplies a route for using the Watson Machine Learning API on IBM Cloud to carry out serverless data analytics using machine learning as a service. Transforming the large amount of data produced by an organization into intelligence can be done using advanced analytics methods such as using a modified Mahalanobis Distance algorithm for synthesis of correlation data under the purview of machine learning. Further refinement of correlation data is done using a Multivariate Reliability Classifier model. The consumption of this advanced analytics service can be done in a serverless manner where the developer only must be concerned with how the data is analyzed, i.e., scoring, batch or stream models with a continuous learning system without the outlay of hardware upon which to train those models. This paper examines the usage of such serverless AI systems in the scope of user behavior analysis over varied demographics.;4
Cirrus: a Serverless Framework for End-to-end ML Workflows;J. Carreira, Pedro Fonseca, Alexey Tumanov, Andrew Zhang, R. Katz;2019;ACM Symposium on Cloud Computing;Machine learning (ML) workflows are extremely complex. The typical workflow consists of distinct stages of user interaction, such as preprocessing, training, and tuning, that are repeatedly executed by users but have heterogeneous computational requirements. This complexity makes it challenging for ML users to correctly provision and manage resources and, in practice, constitutes a significant burden that frequently causes over-provisioning and impairs user productivity. Serverless computing is a compelling model to address the resource management problem, in general, but there are numerous challenges to adopt it for existing ML frameworks due to significant restrictions on local resources. This work proposes Cirrus---an ML framework that automates the end-to-end management of datacenter resources for ML workflows by efficiently taking advantage of serverless infrastructures. Cirrus combines the simplicity of the serverless interface and the scalability of the serverless infrastructure (AWS Lambdas and S3) to minimize user effort. We show a design specialized for both serverless computation and iterative ML training is needed for robust and efficient ML training on serverless infrastructure. Our evaluation shows that Cirrus outperforms frameworks specialized along a single dimension: Cirrus is 100x faster than a general purpose serverless system [36] and 3.75x faster than specialized ML frameworks for traditional infrastructures [49].;184
