APA;Name;Year;Title;Authors;Proceeding;Abstract;Link;Google Scholar Reference
Fehlmann, T., & Kranich, E. (2020, August). A framework for automated testing. In European Conference on Software Process Improvement (pp. 275-288). Cham: Springer International Publishing.;2_safety_system_autonomous_vehicle;2020;A framework for automated testing;Thomas Fehlmann, Eberhard Kranich;European Conference on Software Process Improvement, 2020;"Autonomous Real-time Testing requires test automation. Test automation is closely related to Continuous Integration/ Continuous Delivery (CI/CD).

However, test automation is a difficult undertaking. While many tools exist that automate the execution of tests, the generation of tests remains manual even though complex systems require a high number of test cases.

This paper explains how to generate new test cases by recombination, distinguish relevant test cases from redundant test cases and proposes a framework for how to automate test generation and execution.";https://link.springer.com/chapter/10.1007/978-3-030-85521-5_25;Todo
Zhou, Y., Yu, Y., & Ding, B. (2020, October). Towards mlops: A case study of ml pipeline platform. In 2020 International conference on artificial intelligence and computer engineering (ICAICE) (pp. 494-500). IEEE.;1_ml_machine_data_learning;2020;Towards mlops: A case study of ml pipeline platform;Yue Zhou, Yue Yu, Bo Ding;2020 International conference on artificial intelligence and computer engineering (ICAICE), 494-500, 2020;The development and deployment of machine learning (ML) applications differ significantly from traditional applications in many ways, which have led to an increasing need for efficient and reliable production of ML applications and supported infrastructures. Though platforms such as TensorFlow Extended (TFX), ModelOps, and Kubeflow have provided end-to-end lifecycle management for ML applications by orchestrating its phases into multistep ML pipelines, their performance is still uncertain. To address this, we built a functional ML platform with DevOps capability from existing continuous integration (CI) or continuous delivery (CD) tools and Kubeflow, constructed and ran ML pipelines to train models with different layers and hyperparameters while time and computing resources consumed were recorded. On this basis, we analyzed the time and resource consumption of each step in the ML pipeline, explored the consumption concerning the ML platform and computational models, and proposed potential performance bottlenecks such as GPU utilization. Our work provides a valuable reference for ML pipeline platform construction in practice.;https://ieeexplore.ieee.org/abstract/document/9361315/;YKSdnxPFzBAJ
Lwakatare, L. E., Crnkovic, I., & Bosch, J. (2020, September). DevOps for AI–Challenges in Development of AI-enabled Applications. In 2020 international conference on software, telecommunications and computer networks (SoftCOM) (pp. 1-6). IEEE.;1_ml_machine_data_learning;2020;DevOps for AIâ€“Challenges in Development of AI-enabled Applications;Lucy Ellen Lwakatare, Ivica Crnkovic, Jan Bosch;2020 international conference on software, telecommunications and computer networks (SoftCOM), 1-6, 2020;When developing software systems that contain Machine Learning (ML) based components, the development process become significantly more complex. The central part of the ML process is training iterations to find the best possible prediction model. Modern software development processes, such as DevOps, have widely been adopted and typically emphasise frequent development iterations and continuous delivery of software changes. Despite the ability of modern approaches in solving some of the problems faced when building ML-based software systems, there are no established procedures on how to combine them with processes in ML workflow in practice today. This paper points out the challenges in development of complex systems that include ML components, and discuss possible solutions driven by the combination of DevOps and ML workflow processes. Industrial cases are presented to illustrate these challenges and the possible solutions.;https://ieeexplore.ieee.org/abstract/document/9238323/;_QFe207KYBsJ
Leff, D., & Lim, K. T. (2023). The key to leveraging AI at scale. In Artificial Intelligence and Machine Learning in the Travel Industry: Simplifying Complex Decision Making (pp. 171-175). Cham: Springer Nature Switzerland.;1_ml_machine_data_learning;2023;The key to leveraging AI at scale;Deborah Leff, Kenneth TK Lim;Artificial Intelligence and Machine Learning in the Travel Industry: Simplifying Complex Decision Making, 171-175, 2023;With the explosive growth of AI and ML-driven processes, companies are under more pressure than ever to drive innovation. For many, adding a Data Science capability into their organization is the easy part. Deploying models into a large, complex enterprise that is operating at scale is new, unchartered territory and quickly becoming the technology challenge of this decade. This article takes an in-depth look at the primary organizational barriers that have not only hindered success but often prevented organizations from moving beyond just experimentation. These obstacles include challenges with fragmented and siloed enterprise data, rigid legacy systems that cannot easily be infused with AI processes, and insufficient skills needed for data science, engineering and the emerging field of AI-ops. Operationalizing AI is hard, especially at the fast pace at which the business operates today. This paper uses real-world examples to guide a discussion around each of these hurdles and will equip industry leaders with a better understanding of how to overcome the challenges they will face as they navigate their data and AI journey.;https://link.springer.com/chapter/10.1007/978-3-031-25456-7_14;NNCvcZHsSzsJ
Raj, E., Westerlund, M., & Espinosa-Leal, L. (2021). Reliable fleet analytics for edge iot solutions. arXiv preprint arXiv:2101.04414.;7_edge_computing_deep_learning;2021;Reliable fleet analytics for edge iot solutions;Emmanuel Raj, Magnus Westerlund, Leonardo Espinosa-Leal;arXiv preprint arXiv:2101.04414, 2021;In recent years we have witnessed a boom in Internet of Things (IoT) device deployments, which has resulted in big data and demand for low-latency communication. This shift in the demand for infrastructure is also enabling real-time decision making using artificial intelligence for IoT applications. Artificial Intelligence of Things (AIoT) is the combination of Artificial Intelligence (AI) technologies and the IoT infrastructure to provide robust and efficient operations and decision making. Edge computing is emerging to enable AIoT applications. Edge computing enables generating insights and making decisions at or near the data source, reducing the amount of data sent to the cloud or a central repository. In this paper, we propose a framework for facilitating machine learning at the edge for AIoT applications, to enable continuous delivery, deployment, and monitoring of machine learning models at the edge (Edge MLOps). The contribution is an architecture that includes services, tools, and methods for delivering fleet analytics at scale. We present a preliminary validation of the framework by performing experiments with IoT devices on a university campus's rooms. For the machine learning experiments, we forecast multivariate time series for predicting air quality in the respective rooms by using the models deployed in respective edge devices. By these experiments, we validate the proposed fleet analytics framework for efficiency and robustness.;https://arxiv.org/abs/2101.04414;3M-5DdLamK4J
Vuppalapati, C., Ilapakurti, A., Chillara, K., Kedari, S., & Mamidi, V. (2020, December). Automating tiny ml intelligent sensors devops using microsoft azure. In 2020 ieee international conference on big data (big data) (pp. 2375-2384). IEEE.;1_ml_machine_data_learning;2020;Automating tiny ml intelligent sensors devops using microsoft azure;Chandrasekar Vuppalapati, Anitha Ilapakurti, Karthik Chillara, Sharat Kedari, Vanaja Mamidi;2020 ieee international conference on big data (big data), 2375-2384, 2020;Microsoft Azure DevOps is a robust ,cross platform and powerful automation engine for script-based automation tools. Azure DevOPS enables to build, test, and deploy Cloud Native and/or Non-Cloud Native applications. The core principle and chief advantage that Azure DevOps provide are the availability of automation techniques such as infrastructure as code and the seamless integration of verifiable frameworks such as Machine Learning Operations (MLOps) with the DevOps automated pipelines to provision and configure the infrastructure that applications need to run.With the increase in application complexity and with the infusion of Machine Learning (ML) and Artificial Intelligence (AI) techniques as part of the software development lifecycle, the Azure DevOps is the most important framework that many organizations are rapidly progressing to incorporate it in their business processes to reduce the cost of building product and improve customer success.As part of the paper, we would like to propose a novel DevOps framework for building intelligent Tiny ML dairy agriculture sensors and the advantages that DevOps provide to develop high quality product in the most cost-efficient manner and serve small scale farmers who are at the bottom economic pyramid.;https://ieeexplore.ieee.org/abstract/document/9377755/;ZbuT7AY9TboJ
Rausch, T., & Dustdar, S. (2019, June). Edge intelligence: The convergence of humans, things, and ai. In 2019 IEEE International Conference on Cloud Engineering (IC2E) (pp. 86-96). IEEE.;7_edge_computing_deep_learning;2019;Edge intelligence: The convergence of humans, things, and ai;Thomas Rausch, Schahram Dustdar;2019 IEEE International Conference on Cloud Engineering (IC2E), 86-96, 2019;Edge AI and Human Augmentation are two major technology trends, driven by recent advancements in edge computing, IoT, and AI accelerators. As humans, things, and AI continue to grow closer together, systems engineers and researchers are faced with new and unique challenges. In this paper, we analyze the role of edge computing and AI in the cyber-human evolution, and identify challenges that edge computing systems will consequently be faced with. We take a closer look at how a cyber-physical fabric will be complemented by AI operationalization to enable seamless end-to-end edge intelligence systems.;https://ieeexplore.ieee.org/abstract/document/8789967/;numtyCPvidUJ
Borg, M. (2021). The aiq meta-testbed: Pragmatically bridging academic ai testing and industrial q needs. In Software Quality: Future Perspectives on Software Engineering Quality: 13th International Conference, SWQD 2021, Vienna, Austria, January 19–21, 2021, Proceedings 13 (pp. 66-77). Springer International Publishing.;1_ml_machine_data_learning;2021;The aiq meta-testbed: Pragmatically bridging academic ai testing and industrial q needs;Markus Borg;Software Quality: Future Perspectives on Software Engineering Quality: 13th International Conference, SWQD 2021, Vienna, Austria, January 19â€“21, 2021, Proceedings 13, 66-77, 2021;AI solutions seem to appear in any and all application domains. As AI becomes more pervasive, the importance of quality assurance increases. Unfortunately, there is no consensus on what artificial intelligence means and interpretations range from simple statistical analysis to sentient humanoid robots. On top of that, quality is a notoriously hard concept to pinpoint. What does this mean for AI quality? In this paper, we share our working definition and a pragmatic approach to address the corresponding quality assurance with a focus on testing. Finally, we present our ongoing work on establishing the AIQ Meta-Testbed.;https://link.springer.com/chapter/10.1007/978-3-030-65854-0_6;w8toS310YEIJ
Matsui¹, B. M. A., & Goya, D. H. (2020). Application of DevOps in the improvement of machine learning processes.;1_ml_machine_data_learning;2020;Application of DevOps in the improvement of machine learning processes;Beatriz Mayumi Andrade MatsuiÂ¹, Denise Hideko Goya;-;This work deals with the concept of DevOps and its application in the improvement of machine learning processes. DevOps practices have been increasingly used by software development teams to automate and simplify processes ranging from integration, through testing, approval, implementation, to the final delivery of the application to users. The present study aims to focus on the possibility of applying this concept also in teams that work with machine learning and could benefit from the improvements brought with the adoption of DevOps.;https://www.researchgate.net/profile/Beatriz-Matsui/publication/346668389_Application_of_DevOps_in_the_improvement_of_machine_learning_processes/links/5fcd976fa6fdcc697be87120/Application-of-DevOps-in-the-improvement-of-machine-learning-processes.pdf;YYqhHYPRejgJ
Wachsmuth, S., Schulz, S., Lier, F., Siepmann, F., & Lütkebohle, I. (2012, September). The robot head ‘Flobi’: a research platform for cognitive interaction technology. In Proc. of the German Conference on Artificial Intelligence (pp. 3-7).;2_safety_system_autonomous_vehicle;2012;The robot head ‘Flobi’: a research platform for cognitive interaction technology;Sven Wachsmuth, Simon Schulz, Florian Lier, Frederic Siepmann, Ingo Lütkebohle;Proc. of the German Conference on Artificial Intelligence, 3-7, 2012;Founded on a vision of a human-friendly technology that adapts to users’ needs and is easy und intuitive for ordinary people to use, CITEC has established an exciting new field: Cognitive Interaction Technology. It aims to elucidate the principles and mechanisms of cognition in order to find ways of replicating them in technology and thus enable a new deep level of service and assistance. In order to proceed in this highly interdisciplinary field, appropriate research platforms and infrastructure are needed. The anthropomorphic robot head “Flobi” combines state-of-the-art sensing functionality with an exterior that elicits a sympathetic emotion response. In order to support several lines of research and at the same time ensure the maintainability of the software and hardware components, a virtual realization of the Flobi head has been proposed that allows an efficient prototyping, systematic testing, and software development in a continuous integration framework.;https://www.academia.edu/download/53698789/ki2012pd01.pdf;vLTMrJSiMvgJ
Martel, Y., Roßmann, A., Sultanow, E., Weiß, O., Wissel, M., Pelzel, F., & Seßler, M. (2021). Software Architecture Best Practices for Enterprise Artificial Intelligence.;1_ml_machine_data_learning;2021;Software Architecture Best Practices for Enterprise Artificial Intelligence;Yannick Martel, Arne Roßmann, Eldar Sultanow, Oliver Weiß, Matthias Wissel, Frank Pelzel, Matthias Seßler;Gesellschaft für Informatik, Bonn, 2021;AI systems are increasingly evolving from laboratory experiments in data analysis to increments of productive software products. A professional AI platform must therefore not only function as a laboratory environment but must be designed and procured as a workbench for the development, productive implementation, operation and maintenance of ML models. Subsequently, it needs to integrate within a global software engineering approach. This way, Enterprise Architecture Management (EAM) must implement efficient governance of the development cycle, to enable organization-wide collaboration, to accelerate the go-live and to standardize operations. In this paper we highlight obstacles and show best practices on how architects can integrate data science and AI in their environment. Additionally, we suggest an integrated approach adapting the best practices from both the data science and DevOps.;https://dl.gi.de/items/a625cd59-8f25-4530-894c-85f2032b833c;-bbBzYtRdlkJ
Makarov, V. A., Stouch, T., Allgood, B., Willis, C. D., & Lynch, N. (2021). Best practices for artificial intelligence in life sciences research. Drug Discovery Today, 26(5), 1107-1110.;1_ml_machine_data_learning;2021;Best practices for artificial intelligence in life sciences research;Vladimir A Makarov, Terry Stouch, Brandon Allgood, Chris D Willis, Nick Lynch;Drug Discovery Today 26 (5), 1107-1110, 2021;We describe 11 best practices for the successful use of artificial intelligence and machine learning in pharmaceutical and biotechnology research at the data, technology and organizational management levels.;https://www.sciencedirect.com/science/article/pii/S1359644621000477;Cm2YWSAUklwJ
Lee, J. J., & Lee, M. (2020). Artificial Social Intelligence: Hotel Rate Prediction. In Advances in Information and Communication: Proceedings of the 2020 Future of Information and Communication Conference (FICC), Volume 2 (pp. 78-82). Springer International Publishing.;1_ml_machine_data_learning;2020;Artificial Social Intelligence: Hotel Rate Prediction;James J Lee, Misuk Lee;Advances in Information and Communication: Proceedings of the 2020 Future of Information and Communication Conference (FICC), Volume 2, 78-82, 2020;Artificial Intelligence has enabled new possibilities in today’s business domain from operational efficiency to smart decision making and even innovative product/service design. Still there are plenty of grey areas where human modelers are struggling to create optimal machine learning scenarios. This research is the first attempt to build machine level structuration where the human modelers’ continuous commitment to enhance machine learning models can be eliminated. In Artificial Social Intelligence Framework, those requirements are replaced at the machine level by adopting cloud native computing foundation (CNCF) with continuous integration and development. The suggested machine level structuration is demonstrated with hotel rate predictions.;https://link.springer.com/chapter/10.1007/978-3-030-39442-4_7;j9Y5fddqphIJ
Hummer, W., Muthusamy, V., Rausch, T., Dube, P., El Maghraoui, K., Murthi, A., & Oum, P. (2019, June). Modelops: Cloud-based lifecycle management for reliable and trusted ai. In 2019 IEEE International Conference on Cloud Engineering (IC2E) (pp. 113-120). IEEE.;1_ml_machine_data_learning;2019;Modelops: Cloud-based lifecycle management for reliable and trusted ai;Waldemar Hummer, Vinod Muthusamy, Thomas Rausch, Parijat Dube, Kaoutar El Maghraoui, Anupama Murthi, Punleuk Oum;2019 IEEE International Conference on Cloud Engineering (IC2E), 113-120, 2019;This paper proposes a cloud-based framework and platform for end-to-end development and lifecycle management of artificial intelligence (AI) applications. We build on our previous work on platform-level support for cloud-managed deep learning services, and show how the principles of software lifecycle management can be leveraged and extended to enable automation, trust, reliability, traceability, quality control, and reproducibility of AI pipelines. Based on a discussion of use cases and current challenges, we describe a framework for managingAI application lifecycles and its key components. We also show concrete examples that illustrate how this framework enables managing and executing model training and continuous learning pipelines while infusing trusted AI principles.;https://ieeexplore.ieee.org/abstract/document/8790192/;QmdGA7izEWUJ
Gerostathopoulos, I., Kugele, S., Segler, C., Bures, T., & Knoll, A. (2019, November). Automated trainability evaluation for smart software functions. In 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE) (pp. 998-1001). IEEE.;1_ml_machine_data_learning;2019;Automated trainability evaluation for smart software functions;Ilias Gerostathopoulos, Stefan Kugele, Christoph Segler, Tomas Bures, Alois Knoll;2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE), 998-1001, 2019;More and more software-intensive systems employ machine learning and runtime optimization to improve their functionality by providing advanced features (e. g. personal driving assistants or recommendation engines). Such systems incorporate a number of smart software functions (SSFs) which gradually learn and adapt to the users' preferences. A key property of SSFs is their ability to learn based on data resulting from the interaction with the user (implicit and explicit feedback)-which we call trainability. Newly developed and enhanced features in a SSF must be evaluated based on their effect on the trainability of the system. Despite recent approaches for continuous deployment of machine learning systems, trainability evaluation is not yet part of continuous integration and deployment (CID) pipelines. In this paper, we describe the different facets of trainability for the development of SSFs. We also present our approach for automated trainability evaluation within an automotive CID framework which proposes to use automated quality gates for the continuous evaluation of machine learning models. The results from our indicative evaluation based on real data from eight BMW cars highlight the importance of continuous and rigorous trainability evaluation in the development of SSFs.;https://ieeexplore.ieee.org/abstract/document/8952173/;oHhuqenHI3cJ
Zhang, H., Li, Y., Huang, Y., Wen, Y., Yin, J., & Guan, K. (2020, October). Mlmodelci: An automatic cloud platform for efficient mlaas. In Proceedings of the 28th ACM International Conference on Multimedia (pp. 4453-4456).;1_ml_machine_data_learning;2020;Mlmodelci: An automatic cloud platform for efficient mlaas;Huaizheng Zhang, Yuanming Li, Yizheng Huang, Yonggang Wen, Jianxiong Yin, Kyle Guan;Proceedings of the 28th ACM International Conference on Multimedia, 4453-4456, 2020;MLModelCI provides multimedia researchers and developers with a one-stop platform for efficient machine learning (ML) services. The system leverages DevOps techniques to optimize, test, and manage models. It also containerizes and deploys these optimized and validated models as cloud services (MLaaS). In its essence, MLModelCI serves as a housekeeper to help users publish models. The models are first automatically converted to optimized formats for production purpose and then profiled under different settings (e.g., batch size and hardware). The profiling information can be used as guidelines for balancing the trade-off between performance and cost of MLaaS. Finally, the system dockerizes the models for ease of deployment to cloud environments. A key feature of MLModelCI is the implementation of a controller, which allows elastic evaluation which only utilizes idle workers while maintaining online service quality. Our system bridges the gap between current ML training and serving systems and thus free developers from manual and tedious work often associated with service deployment. We release the platform as an open-source project on GitHub under Apache 2.0 license, with the aim that it will facilitate and streamline more large-scale ML applications and research projects.;https://dl.acm.org/doi/abs/10.1145/3394171.3414535;I5Ck84iW1M4J
Schreiber, M., Barkschat, K., & Kraft, B. (2014, April). Using Continuous Integration to organize and monitor the annotation process of domain specific corpora. In 2014 5th International Conference on Information and Communication Systems (ICICS) (pp. 1-6). IEEE.;9_data_science_software_process;2014;Using Continuous Integration to organize and monitor the annotation process of domain specific corpora;Marc Schreiber, Kai Barkschat, Bodo Kraft;2014 5th International Conference on Information and Communication Systems (ICICS), 1-6, 2014;Applications in the World Wide Web aggregate vast amounts of information from different data sources. The aggregation process is often implemented with Extract, Transform and Load (ETL) processes. Usually ETL processes require information for aggregation available in structured formats, e. g. XML or JSON. In many cases the information is provided in natural language text which makes the application of ETL processes impractical. Due to the fact that information is provided in natural language, Information Extraction (IE) systems have been evolved. They make use of Natural Language Processing (NLP) tools to derive meaning from natural language text. State-of-the-art NLP tools apply Machine Learning methods. These NLP tools perform on newspapers with good quality, but they drop accuracy in other domains. However, to improve the quality for IE systems in specific domains often NLP tools are trained on domain specific text which is a time consuming process. This paper introduces an approach using a Continuous Integration pipeline for organizing and monitoring the annotation process on domain specific corpora.;https://ieeexplore.ieee.org/abstract/document/6841958/;_8HKkIJfZVoJ
Aguilar Melgar, L., Dao, D., Gan, S., Gürel, N. M., Hollenstein, N., Jiang, J., ... & Zhang, C. (2021). Ease. ml: a lifecycle management system for machine learning. In Proceedings of the Annual Conference on Innovative Data Systems Research (CIDR), 2021. CIDR.;1_ml_machine_data_learning;2021;Ease. ml: a lifecycle management system for machine learning;Leonel Aguilar Melgar, David Dao, Shaoduo Gan, Nezihe M Gürel, Nora Hollenstein, Jiawei Jiang, Bojan Karlaš, Thomas Lemmin, Tian Li, Yang Li, Susie Rao, Johannes Rausch, Cedric Renggli, Luka Rimanic, Maurice Weber, Shuai Zhang, Zhikuan Zhao, Kevin Schawinski, Wentao Wu, Ce Zhang;Proceedings of the Annual Conference on Innovative Data Systems Research (CIDR), 2021, 2021;"We present Ease.ML, a lifecycle management system for machine learning (ML). Unlike many existing works, which focus on improving individual steps during the lifecycle of ML application development, Ease.ML focuses on managing and automating the entire lifecycle itself. We present user scenarios that have motivated the development of Ease.ML, the eight-step Ease.ML process that covers the lifecycle of ML application development; the foundation of Ease.ML in terms of a probabilistic database model and its connection to information theory; and our lessons learned, which hopefully can inspire future research.";https://www.research-collection.ethz.ch/handle/20.500.11850/458755;8NC9L1I5UPIJ
Lwakatare, L. E., Crnkovic, I., Rånge, E., & Bosch, J. (2020). From a data science driven process to a continuous delivery process for machine learning systems. In Product-Focused Software Process Improvement: 21st International Conference, PROFES 2020, Turin, Italy, November 25–27, 2020, Proceedings 21 (pp. 185-201). Springer International Publishing.;1_ml_machine_data_learning;2020;From a data science driven process to a continuous delivery process for machine learning systems;Lucy Ellen Lwakatare, Ivica Crnkovic, Ellinor Rånge, Jan Bosch;Product-Focused Software Process Improvement: 21st International Conference, PROFES 2020, Turin, Italy, November 25–27, 2020, Proceedings 21, 185-201, 2020;Development of machine learning (ML) enabled applications in real-world settings is challenging and requires the consideration of sound software engineering (SE) principles and practices. A large body of knowledge exists on the use of modern approaches to developing traditional software components, but not ML components. Using exploratory case study approach, this study investigates the adoption and use of existing software development approaches, specifically continuous delivery (CD), to development of ML components. Research data was collected using a multivocal literature review (MLR) and focus group technique with ten practitioners involved in developing ML-enabled systems at a large telecommunication company. The results of our MLR show that companies do not outright apply CD to the development of ML components rather as a result of improving their development practices and infrastructure over time. A process improvement conceptual model, that includes the description of CD application to ML components is developed and initially validated in the study.;https://link.springer.com/chapter/10.1007/978-3-030-64148-1_12;TcFehlosagkJ
Renggli, C., Hubis, F. A., Karlaš, B., Schawinski, K., Wu, W., & Zhang, C. (2019). Ease. ml/ci and ease. ml/meter in action: Towards data management for statistical generalization. Proceedings of the VLDB Endowment, 12(12), 1962-1965.;1_ml_machine_data_learning;2019;Ease. ml/ci and ease. ml/meter in action: Towards data management for statistical generalization;Cedric Renggli, Frances Ann Hubis, Bojan KarlaÅ¡, Kevin Schawinski, Wentao Wu, Ce Zhang;Proceedings of the VLDB Endowment 12 (12), 1962-1965, 2019;"Developing machine learning (ML) applications is similar to developing traditional software --- it is often an iterative process in which developers navigate within a rich space of requirements, design decisions, implementations, empirical quality, and performance. In traditional software development, software engineering is the field of study which provides principled guidelines for this iterative process. However, as of today, the counterpart of ""software engineering for ML"" is largely missing --- developers of ML applications are left with powerful tools (e.g., TensorFlow and PyTorch) but little guidance regarding the development lifecycle itself.In this paper, we view the management of ML development life-cycles from a data management perspective. We demonstrate two closely related systems, ease.ml/ci and ease.ml/meter, that provide some ""principled guidelines"" for ML application development: ci is a continuous integration engine for ML models and meter is a ""profiler"" for controlling overfitting of ML models. Both systems focus on managing the ""statistical generalization power"" of datasets used for assessing the quality of ML applications, namely, the validation set and the test set. By demonstrating these two systems we hope to spawn further discussions within our community on building this new type of data management systems for statistical generalization.";https://dl.acm.org/doi/abs/10.14778/3352063.3352110;zEuMIZOu-wUJ
Karlaš, B., Interlandi, M., Renggli, C., Wu, W., Zhang, C., Mukunthu Iyappan Babu, D., ... & Weimer, M. (2020, August). Building continuous integration services for machine learning. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp. 2407-2415).;1_ml_machine_data_learning;2020;Building continuous integration services for machine learning;Bojan KarlaÅ¡, Matteo Interlandi, Cedric Renggli, Wentao Wu, Ce Zhang, Deepak Mukunthu Iyappan Babu, Jordan Edwards, Chris Lauren, Andy Xu, Markus Weimer;Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2407-2415, 2020;Continuous integration (CI) has been a de facto standard for building industrial-strength software. Yet, there is little attention towards applying CI to the development of machine learning (ML) applications until the very recent effort on the theoretical side. In this paper, we take a step forward to bring the theory into practice.We develop the first CI system for ML, to the best of our knowledge, that integrates seamlessly with existing ML development tools. We present its design and implementation details.;https://dl.acm.org/doi/abs/10.1145/3394486.3403290;Vjtmu7rvh0MJ
Corbeil, J. P., & Daudens, F. (2020). Deploying a Cost-Effective and Production-Ready Deep News Recommender System in the Media Crisis Context. In ORSUM@ RecSys.;1_ml_machine_data_learning;2020;Deploying a Cost-Effective and Production-Ready Deep News Recommender System in the Media Crisis Context.;Jean-Philippe Corbeil, Florent Daudens;ORSUM@ RecSys, 2020;In the actual context of the media crisis, online media companies need cost-effective technological solutions to stay competitive against huge monopolistic software companies massively feeding content to users. News recommender systems are well-suited solutions, even if current commercial solutions are well above most online media’s budget. In this paper, we present a case study of our deployed deep news recommender system at Le Devoir, an independent french Canadian newspaper in the province of Quebec. We expose the software architecture and the issues we have met with their solutions. Furthermore, we present four qualitative and quantitative analyses done with our custom monitoring dashboard: offline performances of our models, embedding space analysis, fake-user testing and high-traffic simulations. For a tiny fraction of the available commercial solutions’ prices, our current simple software architecture based on the Docker, the Kubernetes and open-source technologies in the cloud has demonstrated to be easily maintainable, scalable, and cost-effective. It also shows excellent offline performance and generates high-quality embeddings as well as relevant recommendations.;https://ceur-ws.org/Vol-2715/paper2.pdf;pTL2UHaSLEcJ
Sangiovanni, M., Schouten, G., & van den Heuvel, W. J. (2020). An IoT beehive network for monitoring urban biodiversity: vision, method, and architecture. In Service-Oriented Computing: 14th Symposium and Summer School on Service-Oriented Computing, SummerSOC 2020, Crete, Greece, September 13-19, 2020 14 (pp. 33-42). Springer International Publishing.;1_ml_machine_data_learning;2020;An IoT beehive network for monitoring urban biodiversity: vision, method, and architecture;Mirella Sangiovanni, Gerard Schouten, Willem-Jan van den Heuvel;Service-Oriented Computing: 14th Symposium and Summer School on Service-Oriented Computing, SummerSOC 2020, Crete, Greece, September 13-19, 2020 14, 33-42, 2020;Environmental sustainability issues have received global attention in recent decades, both at scientific and administrative levels. Despite the scrupulous studies and initiatives around such issues, they remain largely unresolved, and sometimes even unknown. A complete understanding of the quality of our living environment that surrounds us, especially urban places, where we spend most of our lives would help improve living conditions for both humans and other species present. The concept of Intelligent Beehives for urban biodiversity encapsulates and leverages biotic elements such as bio-indicators (e.g. bees), and pollination, with technologies like AI and IoT instrumentation. Together they comprise a smart service that shapes the backbone of a real-time, AI-enabled environmental dashboard. In this vision paper, we outline and discuss our solution architecture and prototypization for such servified intelligent beehives. We focus our discussion on the hives’ predictive modelling abilities that enable Machine-Learning service operations – or MLOps – for increasing the sustainability of urban biodiversity.;https://link.springer.com/chapter/10.1007/978-3-030-64846-6_3;YGSBmqro8-4J
García, Á. L., De Lucas, J. M., Antonacci, M., Zu Castell, W., David, M., Hardt, M., ... & Wolniewicz, P. (2020). A cloud-based framework for machine learning workloads and applications. IEEE access, 8, 18681-18692.;1_ml_machine_data_learning;2020;A cloud-based framework for machine learning workloads and applications;Álvaro López García, Jesus Marco De Lucas, Marica Antonacci, Wolfgang Zu Castell, Mario David, Marcus Hardt, Lara Lloret Iglesias, Germán Moltó, Marcin Plociennik, Viet Tran, Andy S Alic, Miguel Caballer, Isabel Campos Plasencia, Alessandro Costantini, Stefan Dlugolinsky, Doina Cristina Duma, Giacinto Donvito, Jorge Gomes, Ignacio Heredia Cacha, Keiichi Ito, Valentin Y Kozlov, Giang Nguyen, Pablo Orviz Fernandez, Zděnek Šustr, Pawel Wolniewicz;IEEE access 8, 18681-18692, 2020;In this paper we propose a distributed architecture to provide machine learning practitioners with a set of tools and cloud services that cover the whole machine learning development cycle: ranging from the models creation, training, validation and testing to the models serving as a service, sharing and publication. In such respect, the DEEP-Hybrid-DataCloud framework allows transparent access to existing e-Infrastructures, effectively exploiting distributed resources for the most compute-intensive tasks coming from the machine learning development cycle. Moreover, it provides scientists with a set of Cloud-oriented services to make their models publicly available, by adopting a serverless architecture and a DevOps approach, allowing an easy share, publish and deploy of the developed models.;https://ieeexplore.ieee.org/abstract/document/8950411/;LeGHAGaDOKAJ
Liu, W. C., Chiang, Y. T., & Liang, T. Y. (2019, November). A development platform of intelligent mobile APP based on edge computing. In 2019 Seventh International Symposium on Computing and Networking Workshops (CANDARW) (pp. 235-241). IEEE.;7_edge_computing_deep_learning;2019;A development platform of intelligent mobile APP based on edge computing;Wei-Chen Liu, Yu Ting Chiang, Tyng-Yeu Liang;2019 Seventh International Symposium on Computing and Networking Workshops (CANDARW), 235-241, 2019;In this paper, we propose a platform based on the architecture of edge computing for the development of intelligent mobile APP. The proposed platform has a number of advantages as follows. First, it supports static and dynamic visualization for reducing the programming and training effort of deep learning models. Second, it offers a runtime library for mobile APPs to exploit external deep learning models and make use of edge servers for the execution of the models. Third, it provides a friendly user interface to partition a deep learning model and execute the model by different work modes for performance optimization. Forth, it can dynamically redirect the requests of mobile APPs for achieving load balance and minimizing the response time of edge servers.;https://ieeexplore.ieee.org/abstract/document/8951740/;s9SOmsHH1tMJ
Castellanos, C., Varela, C. A., & Correal, D. (2021). ACCORDANT: A domain specific-model and DevOps approach for big data analytics architectures. Journal of Systems and Software, 172, 110869.;9_data_science_software_process;2021;ACCORDANT: A domain specific-model and DevOps approach for big data analytics architectures;Camilo Castellanos, Carlos A Varela, Dario Correal;Journal of Systems and Software 172, 110869, 2021;Big data analytics (BDA) applications use machine learning algorithms to extract valuable insights from large, fast, and heterogeneous data sources. New software engineering challenges for BDA applications include ensuring performance levels of data-driven algorithms even in the presence of large data volume, velocity, and variety (3Vs). BDA software complexity frequently leads to delayed deployments, longer development cycles, and challenging performance assessment. This paper proposes a Domain-Specific Model (DSM), and DevOps practices to design, deploy, and monitor performance metrics in BDA applications. Our proposal includes a design process, and a framework to define architectural inputs, software components, and deployment strategies through integrated high-level abstractions to enable QS monitoring. We evaluate our approach with four use cases from different domains to demonstrate a high level of generalization. Our results show a shorter deployment and monitoring times, and a higher gain factor per iteration compared to similar approaches.;https://www.sciencedirect.com/science/article/pii/S0164121220302594;oI80bxAgS78J
Banerjee, A., Chen, C. C., Hung, C. C., Huang, X., Wang, Y., & Chevesaran, R. (2020). Challenges and Experiences with {MLOps} for Performance Diagnostics in {Hybrid-Cloud} Enterprise Software Deployments. In 2020 USENIX Conference on Operational Machine Learning (OpML 20).;1_ml_machine_data_learning;2020;Challenges and Experiences with {MLOps} for Performance Diagnostics in {Hybrid-Cloud} Enterprise Software Deployments;Amitabha Banerjee, Chien-Chia Chen, Chien-Chun Hung, Xiaobo Huang, Yifan Wang, Razvan Chevesaran;2020 USENIX Conference on Operational Machine Learning (OpML 20), 2020;This paper presents how VMware addressed the following challenges in operationalizing our ML-based performance diagnostics solution in enterprise hybrid-cloud environments: data governance, model serving and deployment, dealing with system performance drifts, selecting model features, centralized model training pipeline, setting the appropriate alarm threshold, and explainability. We also share the lessons and experiences we learned over the past four years in deploying ML operations at scale for enterprise customers.;https://www.usenix.org/conference/opml20/presentation/banerjee;5OA00yOIs0AJ
Tamburri, D. A. (2020, September). Sustainable mlops: Trends and challenges. In 2020 22nd international symposium on symbolic and numeric algorithms for scientific computing (SYNASC) (pp. 17-23). IEEE.;1_ml_machine_data_learning;2020;Sustainable mlops: Trends and challenges;Damian A Tamburri;2020 22nd international symposium on symbolic and numeric algorithms for scientific computing (SYNASC), 17-23, 2020;Even simply through a GoogleTrends search it becomes clear that Machine-Learning Operations-or MLOps, for short-are climbing in interest from both a scientific and practical perspective. On the one hand, software components and middleware are proliferating to support all manners of MLOps, from AutoML (i.e., software which enables developers with limited machine-learning expertise to train high-quality models specific to their domain or data) to feature-specific ML engineering, e.g., Explainability and Interpretability. On the other hand, the more these platforms penetrate the day-to-day activities of software operations, the more the risk for AI Software becoming unsustainable from a social, technical, or organisational perspective. This paper offers a concise definition of MLOps and AI Software Sustainability and outlines key challenges in its pursuit.;https://ieeexplore.ieee.org/abstract/document/9356947/;U0Yetx1xjBgJ
Fursin, G., Guillou, H., & Essayan, N. (2020). CodeReef: an open platform for portable MLOps, reusable automation actions and reproducible benchmarking. arXiv preprint arXiv:2001.07935.;1_ml_machine_data_learning;2020;CodeReef: an open platform for portable MLOps, reusable automation actions and reproducible benchmarking;Grigori Fursin, Herve Guillou, Nicolas Essayan;arXiv preprint arXiv:2001.07935, 2020;We present CodeReef - an open platform to share all the components necessary to enable cross-platform MLOps (MLSysOps), i.e. automating the deployment of ML models across diverse systems in the most efficient way. We also introduce the CodeReef solution - a way to package and share models as non-virtualized, portable, customizable and reproducible archive files. Such ML packages include JSON meta description of models with all dependencies, Python APIs, CLI actions and portable workflows necessary to automatically build, benchmark, test and customize models across diverse platforms, AI frameworks, libraries, compilers and datasets. We demonstrate several CodeReef solutions to automatically build, run and measure object detection based on SSD-Mobilenets, TensorFlow and COCO dataset from the latest MLPerf inference benchmark across a wide range of platforms from Raspberry Pi, Android phones and IoT devices to data centers. Our long-term goal is to help researchers share their new techniques as production-ready packages along with research papers to participate in collaborative and reproducible benchmarking, compare the different ML/software/hardware stacks and select the most efficient ones on a Pareto frontier using online CodeReef dashboards.;https://arxiv.org/abs/2001.07935;l9qvUBmVwToJ
Mäkinen, S., Skogström, H., Laaksonen, E., & Mikkonen, T. (2021, May). Who needs MLOps: What data scientists seek to accomplish and how can MLOps help?. In 2021 IEEE/ACM 1st Workshop on AI Engineering-Software Engineering for AI (WAIN) (pp. 109-112). IEEE.;1_ml_machine_data_learning;2021;Who needs MLOps: What data scientists seek to accomplish and how can MLOps help?;Sasu Mäkinen, Henrik Skogström, Eero Laaksonen, Tommi Mikkonen;2021 IEEE/ACM 1st Workshop on AI Engineering-Software Engineering for AI (WAIN), 109-112, 2021;"Following continuous software engineering practices, there has been an increasing interest in rapid deployment of machine learning (ML) features, called MLOps. In this paper, we study the importance of MLOps in the context of data scientists' daily activities, based on a survey where we collected responses from 331 professionals from 63 different countries in ML domain, indicating on what they were working on in the last three months. Based on the results, up to 40% respondents say that they work with both models and infrastructure; the majority of the work revolves around relational and time series data; and the largest categories of problems to be solved are predictive analysis, time series data, and computer vision. The biggest perceived problems revolve around data, although there is some awareness of problems related to deploying models to production and related procedures. To hypothesise, we believe that organisations represented in the survey can be divided to three categories - (i) figuring out how to best use data; (ii) focusing on building the first models and getting them to production; and (iii) managing several models, their versions and training datasets, as well as retraining and frequent deployment of retrained models. In the results, the majority of respondents are in category (i) or (ii), focusing on data and models; however the benefits of MLOps only emerge in category (iii) when there is a need for frequent retraining and redeployment. Hence, setting up an MLOps pipeline is a natural step to take, when an organization takes the step from ML as a proof-of-concept to ML as a part of nominal activities.";https://ieeexplore.ieee.org/abstract/document/9474355/;v8BetkJk4SoJ
Pölöskei, I. (2022). MLOps approach in the cloud-native data pipeline design. Acta Technica Jaurinensis, 15(1), 1-6.;9_data_science_software_process;2022;MLOps approach in the cloud-native data pipeline design;István Pölöskei;Acta Technica Jaurinensis 15 (1), 1-6, 2022;The data modeling process is challenging and involves hypotheses and trials. In the industry, a workflow has been constructed around data modeling. The offered modernized workflow expects to use of the cloud’s full abilities as cloud-native services. For a flourishing big data project, the organization should have analytics and information-technological know-how. MLOps approach concentrates on the modeling, eliminating the personnel and technology gap in the deployment. In this article, the paradigm will be verified with a case-study in the context of composing a data pipeline in the cloud-native ecosystem. Based on the analysis, the considered strategy is the recommended way for data pipeline design.;https://acta.sze.hu/index.php/acta/article/view/581;jciVqDD61MgJ
Bourgais, A., & Ibnouhsein, I. (2022). Ethics-by-design: the next frontier of industrialization. AI and Ethics, 2(2), 317-324.;12_ai_ethical_ethic_intelligence;2022;Ethics-by-design: the next frontier of industrialization;AurÃ©lien Bourgais, Issam Ibnouhsein;AI and Ethics 2 (2), 317-324, 2022;During the past few years, most companies have launched experiments on how they can use artificial intelligence (AI) to leverage their data. These experiments generally correspond to prototypes solving a specific business case, such as fraud detection in banking or predictive maintenance for industrial equipment. If the estimated return on investment of the prototype is positive, the technical and business teams start thinking about how to industrialize their experiments. Deployment of AI systems comes with a set of specific challenges, such as data governance, model lifecycle management, and collaborators training and onboarding, among others. Overcoming these challenges hedges most performance risks. However, a new set of risks and challenges, related to ethical considerations, is emerging. In this paper, we review in detail all these challenges, share our experience on best practices that help build well-integrated AI systems, and argue in favor of an ethics-by-design approach to prototyping.;https://link.springer.com/article/10.1007/s43681-021-00057-0;R6BDuDqMTSUJ
Benbya, H., Davenport, T. H., & Pachidi, S. (2020). Artificial intelligence in organizations: Current state and future opportunities. MIS Quarterly Executive, 19(4).;12_ai_ethical_ethic_intelligence;2020;Artificial intelligence in organizations: Current state and future opportunities;Hind Benbya, Thomas H Davenport, Stella Pachidi;MIS Quarterly Executive 19 (4), 2020;Artificial intelligence (AI) is currently viewed as the most important and disruptive new technology for large organizations. However, the technology is still in a relatively early state in large enterprises, and largely absent from smaller ones other than technology startups. Surveys suggest that fewer than half of large organizations have meaningful AI initiatives underway, although the percentage is increasing over time. This essay titled “AI in organizations: current state and future opportunities” details current challenges and implications that might arise from AI applications, and the ways to overcome such challenges to realize the potential of this emerging technology. First, the paper provide a brief history of AI and an overview of AI typologies. We discuss current challenges, implications and future opportunities regarding AI. Finally, we summarize the special issue articles and highlight the contributions each makes.;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3741983;MJ-1dZIVT-kJ
Yun, F., Shibahara, T., Ohsita, Y., Chiba, D., Akiyama, M., & Murata, M. (2020). Understanding machine learning model updates based on changes in feature attributions.;1_ml_machine_data_learning;2020;Understanding machine learning model updates based on changes in feature attributions;Fan Yun, Toshiki Shibahara, Yuichi Ohsita, Daiki Chiba, Mitsuaki Akiyama, Masayuki Murata;-;Machine learning operations (MLOps) are widely adopted in various real-world machine learning (ML) systems. To ensure ML model performance in such systems, it is crucial to minimize the influence of insufficient data and concept drift. One simple and effective solution is to update models with newly added data. In MLOps, after updating a model, accuracy scores are usually used to validate the model. However, it is hard to obtain detailed information regarding the causes of performance changes. We therefore propose a method for understanding ML model updates by using a feature attribution method called Shapley additive explanations (SHAP), which explains the output of a ML model by assigning an importance value called a SHAP value to each feature. We calculate SHAP values using models before and after updates to investigate changes in SHAP values. By analyzing the extent of changes, we can identify the slight changes in models due to updates and the data related to the changes.;https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=207987&item_no=1&page_id=13&block_id=8;k5hudarObygJ
Serban, A., van der Blom, K., Hoos, H., & Visser, J. (2020, October). Adoption and effects of software engineering best practices in machine learning. In Proceedings of the 14th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM) (pp. 1-12).;1_ml_machine_data_learning;2020;Adoption and effects of software engineering best practices in machine learning;Alex Serban, Koen van der Blom, Holger Hoos, Joost Visser;Proceedings of the 14th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM), 1-12, 2020;Background. The increasing reliance on applications with machine learning (ML) components calls for mature engineering techniques that ensure these are built in a robust and future-proof manner.Aim. We aim to empirically determine the state of the art in how teams develop, deploy and maintain software with ML components.Method. We mined both academic and grey literature and identified 29 engineering best practices for ML applications. We conducted a survey among 313 practitioners to determine the degree of adoption for these practices and to validate their perceived effects. Using the survey responses, we quantified practice adoption, differentiated along demographic characteristics, such as geography or team size. We also tested correlations and investigated linear and non-linear relationships between practices and their perceived effect using various statistical models.Results. Our findings indicate, for example, that larger teams tend to adopt more practices, and that traditional software engineering practices tend to have lower adoption than ML specific practices. Also, the statistical models can accurately predict perceived effects such as agility, software quality and traceability, from the degree of adoption for specific sets of practices. Combining practice adoption rates with practice importance, as revealed by statistical models, we identify practices that are important but have low adoption, as well as practices that are widely adopted but are less important for the effects we studied.Conclusion. Overall, our survey and the analysis of responses received provide a quantitative basis for assessment and step-wise improvement of practice adoption by ML teams.;https://dl.acm.org/doi/abs/10.1145/3382494.3410681;w6FljQGj5joJ
Amershi, S., Begel, A., Bird, C., DeLine, R., Gall, H., Kamar, E., ... & Zimmermann, T. (2019, May). Software engineering for machine learning: A case study. In 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) (pp. 291-300). IEEE.;1_ml_machine_data_learning;2019;Software engineering for machine learning: A case study;Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, Thomas Zimmermann;2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), 291-300, 2019;"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.";https://ieeexplore.ieee.org/abstract/document/8804457/;7n5WIuI2i5kJ
Baylor, D., Breck, E., Cheng, H. T., Fiedel, N., Foo, C. Y., Haque, Z., ... & Zinkevich, M. (2017, August). Tfx: A tensorflow-based production-scale machine learning platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1387-1395).;1_ml_machine_data_learning;2017;Tfx: A tensorflow-based production-scale machine learning platform;Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, Martin Zinkevich;Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1387-1395, 2017;Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful orchestration of many components---a learner for generating models based on training data, modules for analyzing and validating both data as well as models, and finally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such orchestration is often done ad hoc using glue code and custom scripts developed by individual teams for specific use cases, leading to duplicated effort and fragile systems with high technical debt.We present TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at Google. By integrating the aforementioned components into one platform, we were able to standardize the components, simplify the platform configuration, and reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions.We present the case study of one deployment of TFX in the Google Play app store, where the machine learning models are refreshed continuously as new data arrive. Deploying TFX led to reduced custom code, faster experiment cycles, and a 2% increase in app installs resulting from improved data and model analysis.;https://dl.acm.org/doi/abs/10.1145/3097983.3098021;7HIshLcOA08J
Brumbaugh, E., Bhushan, M., Cheong, A., Du, M. G. Q., Feng, J., Handel, N., ... & Zhu, Q. (2019, October). Bighead: a framework-agnostic, end-to-end machine learning platform. In 2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA) (pp. 551-560). IEEE.;1_ml_machine_data_learning;2019;Bighead: a framework-agnostic, end-to-end machine learning platform;Eli Brumbaugh, Mani Bhushan, Andrew Cheong, Michelle Gu-Qian Du, Jeff Feng, Nick Handel, Andrew Hoh, Jack Hone, Brad Hunter, Atul Kale, Alfredo Luque, Bahador Nooraei, John Park, Krishna Puttaswamy, Kyle Schiller, Evgeny Shapiro, Conglei Shi, Aaron Siegel, Nikhil Simha, Marie Sbrocca, Shi-Jing Yao, Patrick Yoon, Varant Zanoyan, Xiao-Han T Zeng, Qiang Zhu;2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA), 551-560, 2019;With the increasing need to build systems and products powered by machine learning inside organizations, it is critical to have a platform that provides machine learning practitioners with a unified environment to easily prototype, deploy, and maintain their models at scale. However, due to the diversity of machine learning libraries, the inconsistency between environments, and various scalability requirement, there is no existing work to date that addresses all of these challenges. Here, we introduce Bighead, a framework-agnostic, end-to-end platform for machine learning. It offers a seamless user experience requiring only minimal efforts that span feature set management, prototyping, training, batch (offline) inference, real-time (online) inference, evaluation, and model lifecycle management. In contrast to existing platforms, it is designed to be highly versatile and extensible, and supports all major machine learning frameworks, rather than focusing on one particular framework. It ensures consistency across different environments and stages of the model lifecycle, as well as across data sources and transformations. It scales horizontally and elastically in response to the workload such as dataset size and throughput. Its components include a feature management framework, a model development toolkit, a lifecycle management service with UI, an offline training and inference engine, an online inference service, an interactive prototyping environment, and a Docker image customization tool. It is the first platform to offer a feature management component that is a general-purpose aggregation framework with lambda architecture and temporal joins. Bighead is deployed and widely adopted at Airbnb, and has enabled the data science and engineering teams to develop and deploy machine learning models in a timely and reliable manner. Bighead has shortened the time to deploy a new model from months to days, ensured the stability of the models in production, facilitated adoption of cutting-edge models, and enabled advanced machine learning based product features of the Airbnb platform. We present two use cases of productionizing models of computer vision and natural language processing.;https://ieeexplore.ieee.org/abstract/document/8964147/;64xSrRkTxCEJ
Olston, C., Fiedel, N., Gorovoy, K., Harmsen, J., Lao, L., Li, F., ... & Soyke, J. (2017). Tensorflow-serving: Flexible, high-performance ml serving. arXiv preprint arXiv:1712.06139.;1_ml_machine_data_learning;2017;Tensorflow-serving: Flexible, high-performance ml serving;Christopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, Jordan Soyke;arXiv preprint arXiv:1712.06139, 2017;We describe TensorFlow-Serving, a system to serve machine learning models inside Google which is also available in the cloud and via open-source. It is extremely flexible in terms of the types of ML platforms it supports, and ways to integrate with systems that convey new models and updated versions from training to serving. At the same time, the core code paths around model lookup and inference have been carefully optimized to avoid performance pitfalls observed in naive implementations. Google uses it in many production deployments, including a multi-tenant model hosting service called TFS^2.;https://arxiv.org/abs/1712.06139;RGf1K-KWpjQJ
Sridhar, V., Subramanian, S., Arteaga, D., Sundararaman, S., Roselli, D., & Talagala, N. (2018). Model governance: Reducing the anarchy of production {ML}. In 2018 USENIX Annual Technical Conference (USENIX ATC 18) (pp. 351-358).;1_ml_machine_data_learning;2018;Model governance: Reducing the anarchy of production {ML};Vinay Sridhar, Sriram Subramanian, Dulcardo Arteaga, Swaminathan Sundararaman, Drew Roselli, Nisha Talagala;2018 USENIX Annual Technical Conference (USENIX ATC 18), 351-358, 2018;As the influence of machine learning grows over decisions in businesses and human life, so grows the need for Model Governance. In this paper, we motivate the need for, define the problem of, and propose a solution for Model Governance in production ML. We show that through our approach one can meaningfully track and understand the who, where, what, when, and how an ML prediction came to be. To the best of our knowledge, this is the first work providing a comprehensive framework for production Model Governance, building upon previous work in developer-focused Model Management.;https://www.usenix.org/conference/atc18/presentation/sridhar;HoZkMa8YJ38J
Miao, H., Li, A., Davis, L. S., & Deshpande, A. (2017, April). Modelhub: Deep learning lifecycle management. In 2017 IEEE 33rd International Conference on Data Engineering (ICDE) (pp. 1393-1394). IEEE.;1_ml_machine_data_learning;2017;Modelhub: Deep learning lifecycle management;Hui Miao, Ang Li, Larry S Davis, Amol Deshpande;2017 IEEE 33rd International Conference on Data Engineering (ICDE), 1393-1394, 2017;Deep learning has improved the state-of-the-art results in many domains, leading to the development of several systems for facilitating deep learning. Current systems, however, mainly focus on model building and training phases, while the issues of lifecycle management are largely ignored. Deep learning modeling lifecycle contains a rich set of artifacts and frequently conducted tasks, dealing with them is cumbersome and left to the users. To address these issues in a comprehensive manner, we demonstrate ModelHub, which includes a novel model versioning system (dlv), a domain-specific language for searching through model space (DQL), and a hosted service (ModelHub).;https://ieeexplore.ieee.org/abstract/document/7930088/;3YpkkqP7I04J
Lim, J., Lee, H., Won, Y., & Yeon, H. (2019). {MLOp} Lifecycle Scheme for Vision-based Inspection Process in Manufacturing. In 2019 USENIX Conference on Operational Machine Learning (OpML 19) (pp. 9-11).;1_ml_machine_data_learning;2019;{MLOp} Lifecycle Scheme for Vision-based Inspection Process in Manufacturing;Junsung Lim, Hoejoo Lee, Youngmin Won, Hunje Yeon;2019 USENIX Conference on Operational Machine Learning (OpML 19), 9-11, 2019;Recent advances in machine learning and the proliferation of edge computing have enabled manufacturing industry to integrate machine learning into its operation to boost productivity. In addition to building high performing machine learning models, stakeholders and infrastructures within the industry should be taken into an account in building an operational lifecycle. In this paper, a practical machine learning operation scheme to build the vision inspection process is proposed, which is mainly motivated from field experiences in applying the system in large scale corporate manufacturing plants. We evaluate our scheme in four defect inspection lines in production. The results show that deep neural network models outperform existing algorithms and the scheme is easily extensible to other manufacturing processes.;https://www.usenix.org/conference/opml19/presentation/lim;fbLcguHZN24J
Ciucu, R., Adochiei, F. C., Adochiei, I. R., Argatu, F., Seriţan, G. C., Enache, B., ... & Argatu, V. V. (2019). Innovative devops for artificial intelligence. The Scientific Bulletin of Electrical Engineering Faculty, 19(1), 58-63.;1_ml_machine_data_learning;2019;Innovative devops for artificial intelligence;R Ciucu, FC Adochiei, Ioana-Raluca Adochiei, F Argatu, GC SeriÅ£an, B Enache, S Grigorescu, Violeta Vasilica Argatu;The Scientific Bulletin of Electrical Engineering Faculty 19 (1), 58-63, 2019;Developing Artificial Intelligence is a laborintensive task. It implies both storage and computational resources. In this paper, we present a state-of-the-art servicebased infrastructure for deploying, managing and serving computational models alongside their respective data-sets and virtual environments. Our architecture uses key-based values to store specific graphs and datasets into memory for fast deployment and model training, furthermore leveraging the need for manual data reduction in the drafting and retraining stages. To develop the platform, we used clustering and orchestration to set up services and containers that allow deployment within seconds. In this article, we cover highperformance computing concepts such as swarming, GPU resource management for model implementation in production environments with emphasis on standardized development to reduce integration tasks and performance optimization.;https://sciendo.com/article/10.1515/sbeef-2019-0011;EfVaSRTWu0MJ
Boag, S., Dube, P., Herta, B., Hummer, W., Ishakian, V., Jayaram, K., ... & Rosenberg, F. (2017). Scalable multi-framework multi-tenant lifecycle management of deep learning training jobs. In Workshop on ML Systems, NIPS.;7_edge_computing_deep_learning;2017;Scalable multi-framework multi-tenant lifecycle management of deep learning training jobs;Scott Boag, Parijat Dube, Benjamin Herta, Waldemar Hummer, Vatche Ishakian, K Jayaram, Michael Kalantar, Vinod Muthusamy, Priya Nagpurkar, Florian Rosenberg;Workshop on ML Systems, NIPS, 2017;With the ongoing rise and phenomenal success of machine learning (ML), particularly deep learning, efficient training of large neural network models in scalable cloud infrastructures becomes a priority. ML workloads have traditionally been run in high-performance computing (HPC) environments, where users log in to dedicated machines and utilize the attached GPUs to run jobs that train models on huge datasets. Providing a similar user experience in a multi-tenant cloud environment comes with its own unique challenges regarding fault tolerance, performance, and security. We tackle these challenges and present a deep learning stack specifically designed for on-demand cloud environments. Based on a detailed discussion of the system architecture, we examine real usage data from internal users, and discuss performance experiments that illustrate the scalability of the system.;http://learningsys.org/nips17/assets/papers/paper_29.pdf;NXaEy9RaQooJ
Miao, H., Li, A., Davis, L. S., & Deshpande, A. (2017, April). Towards unified data and lifecycle management for deep learning. In 2017 IEEE 33rd International Conference on Data Engineering (ICDE) (pp. 571-582). IEEE.;1_ml_machine_data_learning;2017;Towards unified data and lifecycle management for deep learning;Hui Miao, Ang Li, Larry S Davis, Amol Deshpande;2017 IEEE 33rd International Conference on Data Engineering (ICDE), 571-582, 2017;Deep learning has improved state-of-the-art results in many important fields, and has been the subject of much research in recent years, leading to the development of several systems for facilitating deep learning. Current systems, however, mainly focus on model building and training phases, while the issues of data management, model sharing, and lifecycle management are largely ignored. Deep learning modeling lifecycle generates a rich set of data artifacts, e.g., learned parameters and training logs, and it comprises of several frequently conducted tasks, e.g., to understand the model behaviors and to try out new models. Dealing with such artifacts and tasks is cumbersome and largely left to the users. This paper describes our vision and implementation of a data and lifecycle management system for deep learning. First, we generalize model exploration and model enumeration queries from commonly conducted tasks by deep learning modelers, and propose a high-level domain specific language (DSL), inspired by SQL, to raise the abstraction level and thereby accelerate the modeling process. To manage the variety of data artifacts, especially the large amount of checkpointed float parameters, we design a novel model versioning system (dlv), and a read-optimized parameter archival storage system (PAS) that minimizes storage footprint and accelerates query workloads with minimal loss of accuracy. PAS archives versioned models using deltas in a multi-resolution fashion by separately storing the less significant bits, and features a novel progressive query (inference) evaluation algorithm. Third, we develop e cient algorithms for archiving versioned models using deltas under co-retrieval constraints. We conduct extensive experiments over several real datasets from computDeep learning has improved state-of-the-art results in many important fields, and has been the subject of much research in recent years, leading to the development of several systems for facilitating deep learning. Current systems, however, mainly focus on model building and training phases, while the issues of data management, model sharing, and lifecycle management are largely ignored. Deep learning modeling lifecycle generates a rich set of data artifacts, e.g., learned parameters and training logs, and it comprises of several frequently conducted tasks, e.g., to understand the model behaviors and to try out new models. Dealing with such artifacts and tasks is cumbersome and largely left to the users. This paper describes our vision and implementation of a data and lifecycle management system for deep learning. First, we generalize model exploration and model enumeration queries from commonly conducted tasks by deep learning modelers, and propose a high-level domain specific language (DSL), inspired by SQL, to raise the abstraction level and thereby accelerate the modeling process. To manage the variety of data artifacts, especially the large amount of checkpointed float parameters, we design a novel model versioning system (dlv), and a read-optimized parameter archival storage system (PAS) that minimizes storage footprint and accelerates query workloads with minimal loss of accuracy. PAS archives versioned models using deltas in a multi-resolution fashion by separately storing the less significant bits, and features a novel progressive query (inference) evaluation algorithm. Third, we develop efficient algorithms for archiving versioned models using deltas under co-retrieval constraints. We conduct extensive experiments over several real datasets from computer vision domain to show the efficiency of the proposed techniques.;https://ieeexplore.ieee.org/abstract/document/7930008/;E83oTR9YVlIJ
Renggli, C., Rimanic, L., Kolar, L., Wu, W., & Zhang, C. (2020). Ease. ml/snoopy in action: Towards automatic feasibility analysis for machine learning application development. Proceedings of the VLDB Endowment, 13(12), 2837-2840.;1_ml_machine_data_learning;2020;Ease. ml/snoopy in action: Towards automatic feasibility analysis for machine learning application development;Cedric Renggli, Luka Rimanic, Luka Kolar, Wentao Wu, Ce Zhang;Proceedings of the VLDB Endowment 13 (12), 2837-2840, 2020;"We demonstrate ease.ml/snoopy, a data analytics system that performs feasibility analysis for machine learning (ML) applications before they are developed. Given a performance target of an ML application (e.g., accuracy above 0.95), ease.ml/snoopy provides a decisive answer to ML developers regarding whether the target is achievable or not. We formulate the feasibility analysis problem as an instance of Bayes error estimation. That is, for a data (distribution) on which the ML application should be performed, ease.ml/snoopy provides an estimate of the Bayes error - the minimum error rate that can be achieved by any classifier. It is well-known that estimating the Bayes error is a notoriously hard task. In ease.ml/snoopy we explore and employ estimators based on the combination of (1) nearest neighbor (NN) classifiers and (2) pre-trained feature transformations. To the best of our knowledge, this is the first work on Bayes error estimation that combines (1) and (2). In today's cost-driven business world, feasibility of an ML project is an ideal piece of information for ML application developers - ease.ml/snoopy plays the role of a reliable ""consultant.""";https://dl.acm.org/doi/abs/10.14778/3415478.3415488;eaDQa4VSBWMJ
Chard, R., Li, Z., Chard, K., Ward, L., Babuji, Y., Woodard, A., ... & Foster, I. (2019, May). DLHub: Model and data serving for science. In 2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS) (pp. 283-292). IEEE.;1_ml_machine_data_learning;2019;DLHub: Model and data serving for science;Ryan Chard, Zhuozhao Li, Kyle Chard, Logan Ward, Yadu Babuji, Anna Woodard, Steven Tuecke, Ben Blaiszik, Michael J Franklin, Ian Foster;2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS), 283-292, 2019;While the Machine Learning (ML) landscape is evolving rapidly, there has been a relative lag in the development of the “learning systems” needed to enable broad adoption. Furthermore, few such systems are designed to support the specialized requirements of scientific ML. Here we present the Data and Learning Hub for science (DLHub), a multi-tenant system that provides both model repository and serving capabilities with a focus on science applications. DLHub addresses two significant shortcomings in current systems. First, its self-service model repository allows users to share, publish, verify, reproduce, and reuse models, and addresses concerns related to model reproducibility by packaging and distributing models and all constituent components. Second, it implements scalable and low-latency serving capabilities that can leverage parallel and distributed computing resources to democratize access to published models through a simple web interface. Unlike other model serving frameworks, DLHub can store and serve any Python 3-compatible model or processing function, plus multiple-function pipelines. We show that relative to other model serving systems including TensorFlow Serving, SageMaker, and Clipper, DLHub provides greater capabilities, comparable performance without memoization and batching, and significantly better performance when the latter two techniques can be employed. We also describe early uses of DLHub for scientific applications.;https://ieeexplore.ieee.org/abstract/document/8821027/;Wlue0mkq2YEJ
Polyzotis, N., Zinkevich, M., Roy, S., Breck, E., & Whang, S. (2019). Data validation for machine learning. Proceedings of Machine Learning and Systems, 1, 334-347.;1_ml_machine_data_learning;2019;Data validation for machine learning;Neoklis Polyzotis, Martin Zinkevich, Sudip Roy, Eric Breck, Steven Whang;Proceedings of Machine Learning and Systems 1, 334-347, 2019;Machine learning is a powerful tool for gleaning knowledge from massive amounts of data. While a great deal of machine learning research has focused on improving the accuracy and efficiency of training and inference algorithms, there is less attention in the equally important problem of monitoring the quality of data fed to machine learning. The importance of this problem is hard to dispute: errors in the input data can nullify any benefits on speed and accuracy for training and inference. This argument points to a data-centric approach to machine learning that treats training and serving data as an important production asset, on par with the algorithm and infrastructure used for learning.;https://proceedings.mlsys.org/paper_files/paper/2019/hash/928f1160e52192e3e0017fb63ab65391-Abstract.html;yvN4QjvCVzcJ
Karlaš, B., Liu, J., Wu, W., & Zhang, C. (2018). Ease. ml in action: Towards multi-tenant declarative learning services. Proceedings of the VLDB Endowment, 11(12), 2054-2057.;1_ml_machine_data_learning;2018;Ease. ml in action: Towards multi-tenant declarative learning services;Bojan KarlaÅ¡, Ji Liu, Wentao Wu, Ce Zhang;Proceedings of the VLDB Endowment 11 (12), 2054-2057, 2018;We demonstrate ease.ml, a multi-tenant machine learning service we host at ETH Zurich for various research groups. Unlike existing machine learning services, ease.ml presents a novel architecture that supports multi-tenant, cost-aware model selection that optimizes for minimizing total regrets of all users. Moreover, it provides a novel user interface that enables declarative machine learning at a higher level: Users only need to specify the input/output schemata of their learning tasks and ease.ml can handle the rest. In this demonstration, we present the design principles of ease.ml, highlight the implementation of its key components, and showcase how ease.ml can help ease machine learning tasks that often perplex even experienced users.;https://dl.acm.org/doi/abs/10.14778/3229863.3236258;pufZ9wG4GTcJ
Vartak, M., Subramanyam, H., Lee, W. E., Viswanathan, S., Husnoo, S., Madden, S., & Zaharia, M. (2016, June). ModelDB: a system for machine learning model management. In Proceedings of the Workshop on Human-In-the-Loop Data Analytics (pp. 1-3).;1_ml_machine_data_learning;2016;ModelDB: a system for machine learning model management;Manasi Vartak, Harihar Subramanyam, Wei-En Lee, Srinidhi Viswanathan, Saadiyah Husnoo, Samuel Madden, Matei Zaharia;Proceedings of the Workshop on Human-In-the-Loop Data Analytics, 1-3, 2016;"Building a machine learning model is an iterative process. A data scientist will build many tens to hundreds of models before arriving at one that meets some acceptance criteria (e.g. AUC cutoff, accuracy threshold). However, the current style of model building is ad-hoc and there is no practical way for a data scientist to manage models that are built over time. As a result, the data scientist must attempt to ""remember"" previously constructed models and insights obtained from them. This task is challenging for more than a handful of models and can hamper the process of sensemaking. Without a means to manage models, there is no easy way for a data scientist to answer questions such as ""Which models were built using an incorrect feature?"", ""Which model performed best on American customers?"" or ""How did the two top models compare?"" In this paper, we describe our ongoing work on ModelDB, a novel end-to-end system for the management of machine learning models. ModelDB clients automatically track machine learning models in their native environments (e.g. scikit-learn, spark.ml), the ModelDB backend introduces a common layer of abstractions to represent models and pipelines, and the ModelDB frontend allows visual exploration and analyses of models via a web-based interface.";https://dl.acm.org/doi/abs/10.1145/2939502.2939516;HghFYhX5mHgJ
Zaharia, M., Chen, A., Davidson, A., Ghodsi, A., Hong, S. A., Konwinski, A., ... & Zumar, C. (2018). Accelerating the machine learning lifecycle with MLflow. IEEE Data Eng. Bull., 41(4), 39-45.;1_ml_machine_data_learning;2018;Accelerating the machine learning lifecycle with MLflow.;Matei Zaharia, Andrew Chen, Aaron Davidson, Ali Ghodsi, Sue Ann Hong, Andy Konwinski, Siddharth Murching, Tomas Nykodym, Paul Ogilvie, Mani Parkhe, Fen Xie, Corey Zumar;IEEE Data Eng. Bull. 41 (4), 39-45, 2018;Machine learning development creates multiple new challenges that are not present in a traditional software development lifecycle. These include keeping track of the myriad inputs to an ML application (eg, data versions, code and tuning parameters), reproducing results, and production deployment. In this paper, we summarize these challenges from our experience with Databricks customers, and describe MLflow, an open source platform we recently launched to streamline the machine learning lifecycle. MLflow covers three key challenges: experimentation, reproducibility, and model deployment, using generic APIs that work with any ML library, algorithm and programming language. The project has a rapidly growing open source community, with over 50 contributors since its launch in June 2018.;https://www-cs.stanford.edu/people/matei/papers/2018/ieee_mlflow.pdf;LBk5KRblHTMJ
Gruendner, J., Schwachhofer, T., Sippl, P., Wolf, N., Erpenbeck, M., Gulden, C., ... & Toddenroth, D. (2019). KETOS: Clinical decision support and machine learning as a service–A training and deployment platform based on Docker, OMOP-CDM, and FHIR Web Services. PloS one, 14(10), e0223010.;1_ml_machine_data_learning;2019;KETOS: Clinical decision support and machine learning as a service–A training and deployment platform based on Docker, OMOP-CDM, and FHIR Web Services;Julian Gruendner, Thorsten Schwachhofer, Phillip Sippl, Nicolas Wolf, Marcel Erpenbeck, Christian Gulden, Lorenz A Kapsner, Jakob Zierk, Sebastian Mate, Michael Stürzl, Roland Croner, Hans-Ulrich Prokosch, Dennis Toddenroth;PloS one 14 (10), e0223010, 2019;To take full advantage of decision support, machine learning, and patient-level prediction models, it is important that models are not only created, but also deployed in a clinical setting. The KETOS platform demonstrated in this work implements a tool for researchers allowing them to perform statistical analyses and deploy resulting models in a secure environment.The proposed system uses Docker virtualization to provide researchers with reproducible data analysis and development environments, accessible via Jupyter Notebook, to perform statistical analysis and develop, train and deploy models based on standardized input data. The platform is built in a modular fashion and interfaces with web services using the Health Level 7 (HL7) Fast Healthcare Interoperability Resources (FHIR) standard to access patient data. In our prototypical implementation we use an OMOP common data model (OMOP-CDM) database. The architecture supports the entire research lifecycle from creating a data analysis environment, retrieving data, and training to final deployment in a hospital setting.We evaluated the platform by establishing and deploying an analysis and end user application for hemoglobin reference intervals within the University Hospital Erlangen. To demonstrate the potential of the system to deploy arbitrary models, we loaded a colorectal cancer dataset into an OMOP database and built machine learning models to predict patient outcomes and made them available via a web service. We demonstrated both the integration with FHIR as well as an example end user application. Finally, we integrated the platform with the open source DataSHIELD architecture to allow for distributed privacy preserving data analysis and training across networks of hospitals.The KETOS platform takes a novel approach to data analysis, training and deploying decision support models in a hospital or healthcare setting. It does so in a secure and privacy-preserving manner, combining the flexibility of Docker virtualization with the advantages of standardized vocabularies, a widely applied database schema (OMOP-CDM), and a standardized way to exchange medical data (FHIR).;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0223010;a1mEs8hyjeIJ
Jackson, S., Yaqub, M., & Li, C. X. (2019). The agile deployment of machine learning models in healthcare. Frontiers in big Data, 7.;1_ml_machine_data_learning;2019;The agile deployment of machine learning models in healthcare;Stuart Jackson, Maha Yaqub, Cheng-Xi Li;Frontiers in big Data, 7, 2019;The continuous delivery of applied machine learning models in healthcare is often hampered by the existence of isolated product deployments with poorly developed architectures and limited or non-existent maintenance plans. For example, actuarial models in healthcare are often trained in total separation from the client-facing software that implements the models in real-world settings. In practice, such systems prove difficult to maintain, to calibrate on new populations, and to re-engineer to include newer design features and capabilities. Here, we briefly describe our product teamâ€™s ongoing efforts at translating an existing research pipeline into an integrated, production-ready system for healthcare cost estimation, using an agile methodology. In doing so, we illustrate several nearly universal implementation challenges for machine learning models in healthcare, and provide concrete recommendations on how to proactively address these issues.;https://www.frontiersin.org/articles/10.3389/fdata.2018.00007/full?utm_source=F-NTF&utm_medium=EMLX&utm_campaign=PRD_FEOPS_20170000_ARTICLE?utm_source=JOURN&utm_medium=LKN&utm_campaign=ECO_FDATA_CON;ZYkuptBpWgkJ
Schleier-Smith, J. (2015, August). An architecture for agile machine learning in real-time applications. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 2059-2068).;1_ml_machine_data_learning;2015;An architecture for agile machine learning in real-time applications;Johann Schleier-Smith;Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2059-2068, 2015;Machine learning techniques have proved effective in recommender systems and other applications, yet teams working to deploy them lack many of the advantages that those in more established software disciplines today take for granted. The well-known Agile methodology advances projects in a chain of rapid development cycles, with subsequent steps often informed by production experiments. Support for such workflow in machine learning applications remains primitive.The platform developed at if(we) embodies a specific machine learning approach and a rigorous data architecture constraint, so allowing teams to work in rapid iterative cycles. We require models to consume data from a time-ordered event history, and we focus on facilitating creative feature engineering. We make it practical for data scientists to use the same model code in development and in production deployment, and make it practical for them to collaborate on complex models.We deliver real-time recommendations at scale, returning top results from among 10,000,000 candidates with sub-second response times and incorporating new updates in just a few seconds. Using the approach and architecture described here, our team can routinely go from ideas for new models to production-validated results within two weeks.;https://dl.acm.org/doi/abs/10.1145/2783258.2788628;-UlNL8k2uEgJ
Lwakatare, L. E., Raj, A., Bosch, J., Olsson, H. H., & Crnkovic, I. (2019). A taxonomy of software engineering challenges for machine learning systems: An empirical investigation. In Agile Processes in Software Engineering and Extreme Programming: 20th International Conference, XP 2019, Montréal, QC, Canada, May 21–25, 2019, Proceedings 20 (pp. 227-243). Springer International Publishing.;1_ml_machine_data_learning;2019;A taxonomy of software engineering challenges for machine learning systems: An empirical investigation;Lucy Ellen Lwakatare, Aiswarya Raj, Jan Bosch, Helena Holmström Olsson, Ivica Crnkovic;Agile Processes in Software Engineering and Extreme Programming: 20th International Conference, XP 2019, Montréal, QC, Canada, May 21–25, 2019, Proceedings 20, 227-243, 2019;Artificial intelligence enabled systems have been an inevitable part of everyday life. However, efficient software engineering principles and processes need to be considered and extended when developing AI-enabled systems. The objective of this study is to identify and classify software engineering challenges that are faced by different companies when developing software-intensive systems that incorporate machine learning components. Using case study approach, we explored the development of machine learning systems from six different companies across various domains and identified main software engineering challenges. The challenges are mapped into a proposed taxonomy that depicts the evolution of use of ML components in software-intensive system in industrial settings. Our study provides insights to software engineering community and research to guide discussions and future research into applied machine learning.;https://library.oapen.org/bitstream/handle/20.500.12657/23099/1007059.pdf?sequence=1#page=233;YYPZxRQZthEJ
Baylor, D., Haas, K., Katsiapis, K., Leong, S., Liu, R., Menwald, C., ... & Zinkevich, M. (2019). Continuous Training for Production {ML} in the {TensorFlow} Extended ({{{{{TFX}}}}}) Platform. In 2019 USENIX Conference on Operational Machine Learning (OpML 19) (pp. 51-53).;1_ml_machine_data_learning;2019;Continuous Training for Production {ML} in the {TensorFlow} Extended ({{{{{TFX}}}}}) Platform;Denis Baylor, Kevin Haas, Konstantinos Katsiapis, Sammy Leong, Rose Liu, Clemens Menwald, Hui Miao, Neoklis Polyzotis, Mitchell Trott, Martin Zinkevich;2019 USENIX Conference on Operational Machine Learning (OpML 19), 51-53, 2019;Large organizations rely increasingly on continuous ML pipelines in order to keep machine-learned models continuously up-to-date with respect to data. In this scenario, disruptions in the pipeline can increase model staleness and thus degrade the quality of downstream services supported by these models. In this paper we describe the operation of continuous pipelines in the Tensorflow Extended (TFX) platform that we developed and deployed at Google. We present the main mechanisms in TFX to support this type of pipelines in production and the lessons learned from the deployment of the platform internally at Google.;https://www.usenix.org/conference/opml19/presentation/baylor;D9ZcCcbu9UoJ
Li, Z., Chard, R., Ward, L., Chard, K., Skluzacek, T. J., Babuji, Y., ... & Foster, I. (2021). DLHub: Simplifying publication, discovery, and use of machine learning models in science. Journal of Parallel and Distributed Computing, 147, 64-76.;1_ml_machine_data_learning;2021;DLHub: Simplifying publication, discovery, and use of machine learning models in science;Zhuozhao Li, Ryan Chard, Logan Ward, Kyle Chard, Tyler J Skluzacek, Yadu Babuji, Anna Woodard, Steven Tuecke, Ben Blaiszik, Michael J Franklin, Ian Foster;Journal of Parallel and Distributed Computing 147, 64-76, 2021;Machine Learning (ML) has become a critical tool enabling new methods of analysis and driving deeper understanding of phenomena across scientific disciplines. There is a growing need for “learning systems” to support various phases in the ML lifecycle. While others have focused on supporting model development, training, and inference, few have focused on the unique challenges inherent in science, such as the need to publish and share models and to serve them on a range of available computing resources. In this paper, we present the Data …;https://www.sciencedirect.com/science/article/pii/S0743731520303464;ozDmVeBIVtAJ
Spell, D. C., Zeng, X. H. T., Chung, J. Y., Nooraei, B., Shomer, R. T., Wang, L. Y., ... & Kirsche, D. (2017, December). Flux: Groupon's automated, scalable, extensible machine learning platform. In 2017 IEEE International Conference on Big Data (Big Data) (pp. 1554-1559). IEEE.;1_ml_machine_data_learning;2017;Flux: Groupon's automated, scalable, extensible machine learning platform;Derrick C Spell, Xiao-Han T Zeng, Jae Young Chung, Bahador Nooraei, Richard T Shomer, Ling-Yong Wang, James C Gibson, Daniel Kirsche;2017 IEEE International Conference on Big Data (Big Data), 1554-1559, 2017;As machine learning becomes the driving force of the daily operation of companies within the information technology sector, infrastructure that enables automated, scalable machine learning is a core component of the systems of many large companies. Various systems and products are being built, offered, and open sourced. As an e-commerce company, numerous aspects of Groupon's business is driven by machine learning. To solve the scalability issue and provide a seamless collaboration between data scientists and engineers, we built Flux, a system that expedites the deployment, execution, and monitoring of machine learning models. Flux focuses on enabling data scientists to build model prototypes with languages and tools they are most proficient in, and integrating the models into the enterprise production system. It manages the life cycle of deployed models, and executes them in distributed batch mode, or exposes them as micro-services for real-time use cases. Its design focuses on automation and easy management, scalability, and extensibility. Flux is the central system for supervised machine learning tasks at Groupon and has been supporting multiple teams across the company.;https://ieeexplore.ieee.org/abstract/document/8258089/;D85ab02PgmgJ
Bhardwaj, A., Bhattacherjee, S., Chavan, A., Deshpande, A., Elmore, A. J., Madden, S., & Parameswaran, A. G. (2014). Datahub: Collaborative data science & dataset version management at scale. arXiv preprint arXiv:1409.0798.;9_data_science_software_process;2014;Datahub: Collaborative data science & dataset version management at scale;Anant Bhardwaj, Souvik Bhattacherjee, Amit Chavan, Amol Deshpande, Aaron J Elmore, Samuel Madden, Aditya G Parameswaran;arXiv preprint arXiv:1409.0798, 2014;Relational databases have limited support for data collaboration, where teams collaboratively curate and analyze large datasets. Inspired by software version control systems like git, we propose (a) a dataset version control system, giving users the ability to create, branch, merge, difference and search large, divergent collections of datasets, and (b) a platform, DataHub, that gives users the ability to perform collaborative data analysis building on this version control system. We outline the challenges in providing dataset version control at scale.;https://arxiv.org/abs/1409.0798;lK2BbyuxDIAJ
Rausch, T., Hummer, W., Muthusamy, V., Rashed, A., & Dustdar, S. (2019). Towards a serverless platform for edge {AI}. In 2nd USENIX Workshop on Hot Topics in Edge Computing (HotEdge 19).;7_edge_computing_deep_learning;2019;Towards a serverless platform for edge {AI};Thomas Rausch, Waldemar Hummer, Vinod Muthusamy, Alexander Rashed, Schahram Dustdar;2nd USENIX Workshop on Hot Topics in Edge Computing (HotEdge 19), 2019;This paper proposes a serverless platform for building and operating edge AI applications. We analyze edge AI use cases to illustrate the challenges in building and operating AI applications in edge cloud scenarios. By elevating concepts from AI lifecycle management into the established serverless model, we enable easy development of edge AI workflow functions. We take a deviceless approach, ie, we treat edge resources transparently as cluster resources, but give developers fine-grained control over scheduling constraints. Furthermore, we demonstrate the limitations of current serverless function schedulers, and present the current state of our prototype.;https://www.usenix.org/conference/hotedge19/presentation/rausch;KiyszhmgGTQJ
Díaz-de-Arcaya, J., Miñón, R., Torre-Bastida, A. I., Del Ser, J., & Almeida, A. (2020). PADL: A modeling and deployment language for advanced analytical services. Sensors, 20(23), 6712.;1_ml_machine_data_learning;2020;PADL: A modeling and deployment language for advanced analytical services;Josu Díaz-de-Arcaya, Raúl Miñón, Ana I Torre-Bastida, Javier Del Ser, Aitor Almeida;Sensors 20 (23), 6712, 2020;In the smart city context, Big Data analytics plays an important role in processing the data collected through IoT devices. The analysis of the information gathered by sensors favors the generation of specific services and systems that not only improve the quality of life of the citizens, but also optimize the city resources. However, the difficulties of implementing this entire process in real scenarios are manifold, including the huge amount and heterogeneity of the devices, their geographical distribution, and the complexity of the necessary IT infrastructures. For this reason, the main contribution of this paper is the PADL description language, which has been specifically tailored to assist in the definition and operationalization phases of the machine learning life cycle. It provides annotations that serve as an abstraction layer from the underlying infrastructure and technologies, hence facilitating the work of data scientists and engineers. Due to its proficiency in the operationalization of distributed pipelines over edge, fog, and cloud layers, it is particularly useful in the complex and heterogeneous environments of smart cities. For this purpose, PADL contains functionalities for the specification of monitoring, notifications, and actuation capabilities. In addition, we provide tools that facilitate its adoption in production environments. Finally, we showcase the usefulness of the language by showing the definition of PADL-compliant analytical pipelines over two uses cases in a smart city context (flood control and waste management), demonstrating that its adoption is simple and beneficial for the definition of information and process flows in such environments.;https://www.mdpi.com/1424-8220/20/23/6712;8qYCTKw8A04J
Martínez-Fernández, S., Franch, X., Jedlitschka, A., Oriol, M., & Trendowicz, A. (2021, May). Developing and operating artificial intelligence models in trustworthy autonomous systems. In International Conference on Research Challenges in Information Science (pp. 221-229). Cham: Springer International Publishing.;1_ml_machine_data_learning;2021;Developing and operating artificial intelligence models in trustworthy autonomous systems;Silverio Martínez-Fernández, Xavier Franch, Andreas Jedlitschka, Marc Oriol, Adam Trendowicz;International Conference on Research Challenges in Information Science, 221-229, 2021;"Companies dealing with Artificial Intelligence (AI) models in Autonomous Systems (AS) face several problems, such as users’ lack of trust in adverse or unknown conditions, gaps between software engineering and AI model development, and operation in a continuously changing operational environment. This work-in-progress paper aims to close the gap between the development and operation of trustworthy AI-based AS by defining an approach that coordinates both activities. We synthesize the main challenges of AI-based AS in industrial settings. We reflect on the research efforts required to overcome these challenges and propose a novel, holistic DevOps approach to put it into practice. We elaborate on four research directions: (a) increased users’ trust by monitoring operational AI-based AS and identifying self-adaptation needs in critical situations; (b) integrated agile process for the development and evolution of AI models and AS; (c) continuous deployment of different context-specific instances of AI models in a distributed setting of AS; and (d) holistic DevOps-based lifecycle for AI-based AS.";https://link.springer.com/chapter/10.1007/978-3-030-75018-3_14;383ZBNe69Y0J
John, M. M., Olsson, H. H., & Bosch, J. (2020, June). Developing ml/dl models: A design framework. In Proceedings of the International Conference on Software and System Processes (pp. 1-10).;1_ml_machine_data_learning;2020;Developing ml/dl models: A design framework;Meenu Mary John, Helena HolmstrÃ¶m Olsson, Jan Bosch;Proceedings of the International Conference on Software and System Processes, 1-10, 2020;Artificial Intelligence is becoming increasingly popular with organizations due to the success of Machine Learning and Deep Learning techniques. Using these techniques, data scientists learn from vast amounts of data to enhance behaviour in software-intensive systems. Despite the attractiveness of these techniques, however, there is a lack of systematic and structured design process for developing ML/DL models. The study uses a multiple-case study approach to explore the different activities and challenges data scientists face when developing ML/DL models in software-intensive embedded systems. In addition, we have identified seven different phases in the proposed design process leading to effective model development based on the case study. Iterations identified between phases and events which trigger these iterations optimize the design process for ML/DL models. Lessons learned from this study allow data scientists and engineers to develop high-performance ML/DL models and also bridge the gap between high demand and low supply of data scientists.;https://dl.acm.org/doi/abs/10.1145/3379177.3388892;uEiEP5A6xFAJ
Janardhanan, P. S. (2020). Project repositories for machine learning with TensorFlow. Procedia Computer Science, 171, 188-196.;1_ml_machine_data_learning;2020;Project repositories for machine learning with TensorFlow;PS Janardhanan;Procedia Computer Science 171, 188-196, 2020;"In machine learning, models capture intelligence from data using algorithms implemented on frameworks like TensorFlow. Models learn during the training phase; an iterative process in which parameters are tuned to improve the prediction accuracy. Software repositories are used to save the artifacts of model development so that they can be modified in subsequent releases and shared between development teams. Issues in saving the state of machine learning projects is different from standard software practices. Machine learning models are equivalent to binary executable programs and ideally one should be able to recreate the model from the information preserved in project repositories. Recreation of the model becomes necessary when there is a change in the development team as part of the product transition. Retraining of models also becomes necessary when the inaccuracy in the predictions made on new data increases beyond the acceptable limit. When traditional source code management systems are used for maintaining repositories of machine learning projects, we face challenges in keeping complete information required for model development starting from saved states. This paper presents the results of the studies conducted to identify the challenges in maintaining TensorFlow machine learning projects in repositories. Some of the existing tools are compared and recommendations are made to improve the ease of recreation of machine learning models by saving complete information in project repositories maintained in normal source code control systems.";https://www.sciencedirect.com/science/article/pii/S1877050920309856;vzjzbIQxi28J
Rivero, L., Diniz, J., Silva, G., Borralho, G., Braz Junior, G., Paiva, A., ... & Oliveira, M. (2020, December). Deployment of a machine learning system for predicting lawsuits against power companies: lessons learned from an agile testing experience for improving software quality. In Proceedings of the XIX Brazilian Symposium on Software Quality (pp. 1-10).;1_ml_machine_data_learning;2020;Deployment of a machine learning system for predicting lawsuits against power companies: lessons learned from an agile testing experience for improving software quality;Luis Rivero, JoÃ£o Diniz, Giovanni Silva, Gabriel Borralho, Geraldo Braz Junior, Anselmo Paiva, Erika Alves, Milton Oliveira;Proceedings of the XIX Brazilian Symposium on Software Quality, 1-10, 2020;"The advances in Machine Learning (ML) require software organizations to evolve their development processes in order to improve the quality of ML systems. Within the software development process, the testing stage of an ML system is more critical, considering that it is necessary to add data validation, trained model quality evaluation, and model validation to traditional unit, integration tests and system tests. In this paper, we focus on reporting the lessons learned of using model testing and exploratory testing within the context of the agile development process of an ML system that predicts lawsuits proneness in energy supply companies. Through the development of the project, the SCRUM agile methodology was applied and activities related to the development of the ML model and the development of the end-user application were defined. After the testing process of the ML model, we managed to achieve 93.89 accuracy; 95.58 specificity; 88.84 sensitivity; and 87.09 precision. Furthermore, we focused on the quality of use of the application embedding the ML model, by carrying out exploratory testing. As a result, through several iterations, different types of defects were identified and corrected. Our lessons learned support software engineers willing to develop ML systems that consider both the ML model and the end-user application.";https://dl.acm.org/doi/abs/10.1145/3439961.3439991;gogRad4tqZEJ
Azimi, S., & Pahl, C. (2021). Continuous Data Quality Management for Machine Learning based Data-as-a-Service Architectures. In CLOSER (pp. 328-335).;1_ml_machine_data_learning;2021;Continuous Data Quality Management for Machine Learning based Data-as-a-Service Architectures.;Shelernaz Azimi, Claus Pahl;CLOSER, 328-335, 2021;Data-as-a-Service (DaaS) solutions make raw source data accessible in the form of processable information. Machine learning (ML) allows to produce meaningful information and knowledge based on raw data. Thus, quality is a major concern that applies to raw data as well as to information provided by ML-generated models. At the core of the solution is a conceptual framework that links input data quality and the machine learned data service quality, specifically inferring raw data problems as root causes from observed data service deficiency symptoms. This allows to deduce the hidden origins of quality problems observable by users of DaaS offerings. We analyse the quality framework through an extensive case study from an edge cloud and Internet-of-Thingsbased traffic application. We determine quality assessment mechanisms for symptom and cause analysis in different quality dimensions.;https://www.scitepress.org/PublishedPapers/2021/105095/105095.pdf;CsaV3btAYdYJ
Raj, A., Bosch, J., Olsson, H. H., & Wang, T. J. (2020, August). Modelling data pipelines. In 2020 46th Euromicro conference on software engineering and advanced applications (SEAA) (pp. 13-20). IEEE.;9_data_science_software_process;2020;Modelling Data Pipelines;Aiswarya Raj, Jan Bosch, Helena Holmström Olsson, Tian J. Wang;2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA);Data is the new currency and key to success. However, collecting high-quality data from multiple distributed sources requires much effort. In addition, there are several other challenges involved while transporting data from its source to the destination. Data pipelines are implemented in order to increase the overall efficiency of data-flow from the source to the destination since it is automated and reduces the human involvement which is required otherwise.Despite existing research on ETL (Extract-Transform-Load) and ELT (Extract-Load-Transform) pipelines, the research on this topic is limited. ETL/ELT pipelines are abstract representations of the end-to-end data pipelines. To utilize the full potential of the data pipeline, we should understand the activities in it and how they are connected in an end-to-end data pipeline. This study gives an overview of how to design a conceptual model of data pipeline which can be further used as a language of communication between different data teams. Furthermore, it can be used for automation of monitoring, fault detection, mitigation and alarming at different steps of data pipeline.;https://ieeexplore.ieee.org/abstract/document/9226314;Todo
Gharibi, G., Walunj, V., Nekadi, R., Marri, R., & Lee, Y. (2021). Automated end-to-end management of the modeling lifecycle in deep learning. Empirical Software Engineering, 26, 1-33.;1_ml_machine_data_learning;2021;Automated end-to-end management of the modeling lifecycle in deep learning;Gharib Gharibi, Vijay Walunj, Raju Nekadi, Raj Marri, Yugyung Lee;Empirical Software Engineering 26, 1-33, 2021;"Deep learning has improved the state-of-the-art results in an ever-growing number of domains. This success heavily relies on the development and training of deep learning modelsâ€“an experimental, iterative process that produces tens to hundreds of models before arriving at a satisfactory result. While there has been a surge in the number of tools and frameworks that aim at facilitating deep learning, the process of managing the models and their artifacts is still surprisingly challenging and time-consuming. Existing model-management solutions are either tailored for commercial platforms or require significant code changes. Moreover, most of the existing solutions address a single phase of the modeling lifecycle, such as experiment monitoring, while ignoring other essential tasks, such as model deployment. In this paper, we present a software system to facilitate and accelerate the deep learning lifecycle, named ModelKB. ModelKB can automatically manage the modeling lifecycle end-to-end, including (1) monitoring and tracking experiments; (2) visualizing, searching for, and comparing models and experiments; (3) deploying models locally and on the cloud; and (4) sharing and publishing trained models. Moreover, our system provides a stepping-stone for enhanced reproducibility. ModelKB currently supports TensorFlow 2.0, Keras, and PyTorch, and it can be extended to other deep learning frameworks easily.";https://link.springer.com/article/10.1007/s10664-020-09894-9;JengDGeOJDcJ
Caveness, E., GC, P. S., Peng, Z., Polyzotis, N., Roy, S., & Zinkevich, M. (2020, June). Tensorflow data validation: Data analysis and validation in continuous ml pipelines. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data (pp. 2793-2796).;1_ml_machine_data_learning;2020;Tensorflow data validation: Data analysis and validation in continuous ml pipelines;Emily Caveness, Paul Suganthan GC, Zhuo Peng, Neoklis Polyzotis, Sudip Roy, Martin Zinkevich;Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data, 2793-2796, 2020;Machine Learning (ML) research has primarily focused on improving the accuracy and efficiency of the training algorithms while paying much less attention to the equally important problem of understanding, validating, and monitoring the data fed to ML. Irrespective of the ML algorithms used, data errors can adversely affect the quality of the generated model. This indicates that we need to adopt a data-centric approach to ML that treats data as a first-class citizen, on par with algorithms and infrastructure which are the typical building blocks of ML pipelines. In this demonstration we showcase TensorFlow Data Validation (TFDV), a scalable data analysis and validation system for ML that we have developed at Google and recently open-sourced. This system is deployed in production as an integral part of TFX - an end-to-end machine learning platform at Google. It is used by hundreds of product teams at Google and has received significant attention from the open-source community as well.;https://dl.acm.org/doi/abs/10.1145/3318464.3384707;M4H_PfLh8b0J
Polyzotis, N., Roy, S., Whang, S. E., & Zinkevich, M. (2018). Data lifecycle challenges in production machine learning: a survey. ACM SIGMOD Record, 47(2), 17-28.;1_ml_machine_data_learning;2018;Data lifecycle challenges in production machine learning: a survey;Neoklis Polyzotis, Sudip Roy, Steven Euijong Whang, Martin Zinkevich;ACM SIGMOD Record 47 (2), 17-28, 2018;Machine learning has become an essential tool for gleaning knowledge from data and tackling a diverse set of computationally hard tasks. However, the accuracy of a machine learned model is deeply tied to the data that it is trained on. Designing and building robust processes and tools that make it easier to analyze, validate, and transform data that is fed into large-scale machine learning systems poses data management challenges. Drawn from our experience in developing data-centric infrastructure for a production machine learning platform at Google, we summarize some of the interesting research challenges that we encountered, and survey some of the relevant literature from the data management and machine learning communities. Specifically, we explore challenges in three main areas of focus - data understanding, data validation and cleaning, and data preparation. In each of these areas, we try to explore how different constraints are imposed on the solutions depending on where in the lifecycle of a model the problems are encountered and who encounters them.;https://dl.acm.org/doi/abs/10.1145/3299887.3299891;MUSGcrRFTeQJ
Schelter, S., Biessmann, F., Januschowski, T., Salinas, D., Seufert, S., & Szarvas, G. (2015). On challenges in machine learning model management.;1_ml_machine_data_learning;2015;On challenges in machine learning model management;Sebastian Schelter, Felix Biessmann, Tim Januschowski, David Salinas, Stephan Seufert, Gyuri Szarvas;-;The training, maintenance, deployment, monitoring, organization and documentation of machine learning (ML) models–in short model management–is a critical task in virtually all production ML use cases. Wrong model management decisions can lead to poor performance of a ML system and result in high maintenance cost. As both research on infrastructure as well as on algorithms is quickly evolving, there is a lack of understanding of challenges and best practices for ML model management. Therefore, this field is receiving increased attention in recent years, both from the data management as well as from the ML community. In this paper, we discuss a selection of ML use cases, develop an overview over conceptual, engineering, and data-related challenges arising in the management of the corresponding ML models, and point out future research directions.;https://www.amazon.science/publications/on-challenges-in-machine-learning-model-management;4rfAku5R-SgJ
Boovaraghavan, S., Maravi, A., Mallela, P., & Agarwal, Y. (2021, May). MLIoT: An end-to-end machine learning system for the Internet-of-Things. In Proceedings of the International Conference on Internet-of-Things Design and Implementation (pp. 169-181).;7_edge_computing_deep_learning;2021;MLIoT: An end-to-end machine learning system for the Internet-of-Things;"Sudershan Boovaraghavan, Anurag Maravi, Prahaladha Mallela, Yuvraj Agarwal
";IoTDI '21: Proceedings of the International Conference on Internet-of-Things Design and Implementation;Modern Internet of Things (IoT) applications, from contextual sensing to voice assistants, rely on ML-based training and serving systems using pre-trained models to render predictions. However, real-world IoT environments are diverse, with rich IoT sensors and need ML models to be personalized for each setting using relatively less training data. Most existing general-purpose ML systems are optimized for specific and dedicated hardware resources and do not adapt to changing resources and different IoT application requirements. To address this gap, we propose MLIoT, an end-to-end Machine Learning System tailored towards supporting the entire lifecycle of IoT applications. MLIoT adapts to different IoT data sources, IoT tasks, and compute resources by automatically training, optimizing, and serving models based on expressive application-specific policies. MLIoT also adapts to changes in IoT environments or compute resources by enabling re-training, and updating models served on the fly while maintaining accuracy and performance. Our evaluation across a set of benchmarks show that MLIoT can handle multiple IoT tasks, each with individual requirements, in a scalable manner while maintaining high accuracy and performance. We compare MLIoT with two state-of-the-art hand-tuned systems and a commercial ML system showing that MLIoT improves accuracy from 50% - 75% while reducing or maintaining latency.;https://dl.acm.org/doi/abs/10.1145/3450268.3453522;TODO
Maskey, M., Ramachandran, R., Gurung, I., Freitag, B., Miller, J. J., Ramasubramanian, M., ... & Hain, C. (2019, July). Machine learning lifecycle for earth science application: a practical insight into production deployment. In IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium (pp. 10043-10046). IEEE.;1_ml_machine_data_learning;2019;Machine learning lifecycle for earth science application: a practical insight into production deployment;Manil Maskey, Rahul Ramachandran, Iksha Gurung, Brian Freitag, JJ Miller, Muthukumaran Ramasubramanian, Drew Bollinger, Ricardo Mestre, Daniel Cecil, Andrew Molthan, Chris Hain;IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, 10043-10046, 2019;Enterprises are making machine learning for production as an integral part of their future roadmaps and Earth science domain is no exception. However, there is common problem in transitioning machine learning from science to production due to a major difference in constructing a model versus deploying it for people to use to make decisions. Phases of machine learning lifecycle that includes model transition to production using a successful application is discussed.;https://ieeexplore.ieee.org/abstract/document/8899031/;KRKQ2267nLwJ
Garcia, R., Sreekanti, V., Yadwadkar, N., Crankshaw, D., Gonzalez, J. E., & Hellerstein, J. M. (2018). Context: The missing piece in the machine learning lifecycle. In KDD CMI Workshop (Vol. 114, pp. 1-4).;1_ml_machine_data_learning;2018;Context: The missing piece in the machine learning lifecycle;Rolando Garcia, Vikram Sreekanti, Neeraja Yadwadkar, Daniel Crankshaw, Joseph E Gonzalez, Joseph M Hellerstein;KDD CMI Workshop 114, 1-4, 2018;Machine learning models have become ubiquitous in modern applications. The ML Lifecycle describes a three-phase process used by data scientists and data engineers to develop, train, and serve models. Unfortunately, context around the data, code, people, and systems involved in these pipelines is not captured today. In this paper, we first discuss common pitfalls that missing context creates. Some examples where context is missing include tracking the relationships between code and data and capturing experimental processes over time. We then discuss techniques to address these challenges and briefly mention future work around designing and implementing systems in this space.;https://rlnsanz.github.io/dat/Flor_CMI_18_CameraReady.pdf;Zd3FafErMFwJ
Nashaat, M., Ghosh, A., Miller, J., Quader, S., & Marston, C. (2019). M-Lean: An end-to-end development framework for predictive models in B2B scenarios. Information and Software Technology, 113, 131-145.;1_ml_machine_data_learning;2019;M-Lean: An end-to-end development framework for predictive models in B2B scenarios;Mona Nashaat, Aindrila Ghosh, James Miller, Shaikh Quader, Chad Marston;Information and Software Technology 113, 131-145, 2019;The need for business intelligence has led to advances in machine learning in the business domain, especially with the rise of big data analytics. However, the resulting predictive systems often fail to maintain a satisfactory level of performance in production. Besides, for predictive systems used in business-to-business scenarios, user trust is subject to the model performance. Therefore, the processes of creating, evaluating, and deploying machine learning systems in the business domain need innovative solutions to solve the critical challenges of assuring the quality of the resulting systems.Applying machine learning in business-to-business situations imposes specific requirements. This paper aims at providing an integrated solution to businesses to help them transform their data into actions.The paper presents MLean, an end-to-end framework, that aims at guiding businesses in designing, developing, evaluating, and deploying business-to-business predictive systems. The framework employs the Lean Startup methodology and aims at maximizing the business value while eliminating wasteful development practices.To evaluate the proposed framework, with the help of our industrial partner, we applied the framework to a case study to build a predictive product. The case study resulted in a predictive system to predict the risks of software license cancellations. The system was iteratively developed and evaluated while adopting the management and end-user perspectives.It is concluded that, in industry, it is important to be aware of the businesses requirements before considering the application of machine learning. The framework accommodates business perspective from the beginning to produce a holistic product. From the results of the case study, we think that this framework can help businesses define the right opportunities for applying machine learning, developing solutions, evaluating the effectiveness of these solutions, and maintaining their performance in production.;https://www.sciencedirect.com/science/article/pii/S0950584919301247;CrSrml0fkkAJ
Peili, Y., Xuezhen, Y., Jian, Y., Lingfeng, Y., Hui, Z., & Jimin, L. (2018, April). Deep learning model management for coronary heart disease early warning research. In 2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA) (pp. 552-557). IEEE.;11_deep_network_model_layer;2018;Deep learning model management for coronary heart disease early warning research;Yang Peili, Yin Xuezhen, Ye Jian, Yang Lingfeng, Zhao Hui, Liang Jimin;2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA), 552-557, 2018;Coronary Heart Disease (CHD) is one of the common diseases that threaten people's health and life. To facilitate the CHD early warning research, the deep learning based methods have drawn much attention. However, the literature mostly focuses on how to establish and optimize the CHD early warning models, while overlooking the training data-model-experimental results modeling lifecycle management. Aiming to promote the early warning research of CHD, we contribute a data management system integrated the CHD patient data with the deep learning model data. In the system, a deep learning model version tree is established to represent the relationship between the models. Tracking-Ancestors algorithm and Find-Specified-Ancestor algorithm are designed to conduct the lineage management of the deep learning model. Considering the big data characteristics of the patient data and deep learning model data, we compare the query response time and select MongoDB as the DBMS for the Pdmdims (Patient Data & Deep Learning Model Data Integrated Management System). The research results show that Pdmdims can provide an effective integrated data management platform for CHD early warning researchers.;https://ieeexplore.ieee.org/abstract/document/8386577/;9JIBfJo8aa4J
Kronberger, G., Bachinger, F., & Affenzeller, M. (2020). Smart manufacturing and continuous improvement and adaptation of predictive models. Procedia Manufacturing, 42, 528-531.;1_ml_machine_data_learning;2020;Smart manufacturing and continuous improvement and adaptation of predictive models;Gabriel Kronberger, Florian Bachinger, Michael Affenzeller;Procedia Manufacturing 42, 528-531, 2020;Predictive models are an important success factor for smart manufacturing. Accordingly, purely data-driven models as well as hybrid models are increasingly deployed within manufacturing environments for optimal control of plants. However, long-term monitoring and adaptation of predictive models has not been a focus of studies so far but will likely become increasingly more important as more and more predictive models are deployed. We give a number of recommendations for effectively managing predictive models in smart manufacturing environments.;https://www.sciencedirect.com/science/article/pii/S2351978920305862;RMq8uqiGV84J
Bachinger, F., & Kronberger, G. (2019, February). Concept for a technical infrastructure for management of predictive models in industrial applications. In International Conference on Computer Aided Systems Theory (pp. 263-270). Cham: Springer International Publishing.;1_ml_machine_data_learning;2019;Concept for a technical infrastructure for management of predictive models in industrial applications;Florian Bachinger, Gabriel Kronberger;International Conference on Computer Aided Systems Theory, 263-270, 2019;With the increasing number of created and deployed prediction models and the complexity of machine learning workflows we require so called model management systems to support data scientists in their tasks. In this work we describe our technological concept for such a model management system. This concept includes versioned storage of data, support for different machine learning algorithms, fine tuning of models, subsequent deployment of models and monitoring of model performance after deployment. We describe this concept with a close focus on model lifecycle requirements stemming from our industry application cases, but generalize key features that are relevant for all applications of machine learning.;https://link.springer.com/chapter/10.1007/978-3-030-45093-9_32;Nxl6vtAFJC8J
Munappy, A. R., Mattos, D. I., Bosch, J., Olsson, H. H., & Dakkak, A. (2020, June). From ad-hoc data analytics to dataops. In Proceedings of the International Conference on Software and System Processes (pp. 165-174).;9_data_science_software_process;2020;From ad-hoc data analytics to dataops;Aiswarya Raj Munappy, David Issa Mattos, Jan Bosch, Helena HolmstrÃ¶m Olsson, Anas Dakkak;Proceedings of the International Conference on Software and System Processes, 165-174, 2020;The collection of high-quality data provides a key competitive advantage to companies in their decision-making process. It helps to understand customer behavior and enables the usage and deployment of new technologies based on machine learning. However, the process from collecting the data, to clean and process it to be used by data scientists and applications is often manual, non-optimized and error-prone. This increases the time that the data takes to deliver value for the business. To reduce this time companies are looking into automation and validation of the data processes. Data processes are the operational side of data analytic workflow.DataOps, a recently coined term by data scientists, data analysts and data engineers refer to a general process aimed to shorten the end-to-end data analytic life-cycle time by introducing automation in the data collection, validation, and verification process. Despite its increasing popularity among practitioners, research on this topic has been limited and does not provide a clear definition for the term or how a data analytic process evolves from ad-hoc data collection to fully automated data analytics as envisioned by DataOps.This research provides three main contributions. First, utilizing multi-vocal literature we provide a definition and a scope for the general process referred to as DataOps. Second, based on a case study with a large mobile telecommunication organization, we analyze how multiple data analytic teams evolve their infrastructure and processes towards DataOps. Also, we provide a stairway showing the different stages of the evolution process. With this evolution model, companies can identify the stage which they belong to and also, can try to move to the next stage by overcoming the challenges they encounter in the current stage.;https://dl.acm.org/doi/abs/10.1145/3379177.3388909;HlQ4Wan1Oa0J
Chen, A., Chow, A., Davidson, A., DCunha, A., Ghodsi, A., Hong, S. A., ... & Zumar, C. (2020, June). Developments in mlflow: A system to accelerate the machine learning lifecycle. In Proceedings of the fourth international workshop on data management for end-to-end machine learning (pp. 1-4).;1_ml_machine_data_learning;2020;Developments in mlflow: A system to accelerate the machine learning lifecycle;Andrew Chen, Andy Chow, Aaron Davidson, Arjun DCunha, Ali Ghodsi, Sue Ann Hong, Andy Konwinski, Clemens Mewald, Siddharth Murching, Tomas Nykodym, Paul Ogilvie, Mani Parkhe, Avesh Singh, Fen Xie, Matei Zaharia, Richard Zang, Juntai Zheng, Corey Zumar;Proceedings of the fourth international workshop on data management for end-to-end machine learning, 1-4, 2020;MLflow is a popular open source platform for managing ML development, including experiment tracking, reproducibility, and deployment. In this paper, we discuss user feedback collected since MLflow was launched in 2018, as well as three major features we have introduced in response to this feedback: a Model Registry for collaborative model management and review, tools for simplifying ML code instrumentation, and experiment analytics functions for extracting insights from millions of ML experiments.;https://dl.acm.org/doi/abs/10.1145/3399579.3399867;4snotEsdI5QJ
Ismail, B. I., Khalid, M. F., Kandan, R., & Hoe, O. H. (2019, August). On-Premise AI Platform: From DC to Edge. In Proceedings of the 2019 2nd International Conference on Robot Systems and Applications (pp. 40-45).;1_ml_machine_data_learning;2019;On-Premise AI Platform: From DC to Edge;Bukhary Ikhwan Ismail, Mohammad Fairus Khalid, Rajendar Kandan, Ong Hong Hoe;Proceedings of the 2019 2nd International Conference on Robot Systems and Applications, 40-45, 2019;Artificial Intelligence (AI) is powering everything from devices, applications and services. Machine learning a branch of AI requires powerful infrastructure platform to do training and to serve the AI model. In this paper, we share our blueprint to build and host internal on-premise AI platform. We make use of our existing services such as private cloud, distributed storage, unified authentication platform, and build the AI platform on top of it. We discuss the requirements gathered from user, the technologies to make it possible, implementation and lesson learned from hosting it internally. Based on our evaluation, based on specific need, it is economical and viable option to host on-premise AI Platform.;https://dl.acm.org/doi/abs/10.1145/3378891.3378899;uA7CyhWoCpkJ
Chen, Z., Cao, Y., Liu, Y., Wang, H., Xie, T., & Liu, X. (2020, November). A comprehensive study on challenges in deploying deep learning based software. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 750-762).;4_dl_testing_deep_network;2020;A comprehensive study on challenges in deploying deep learning based software;Zhenpeng Chen, Yanbin Cao, Yuanqiang Liu, Haoyu Wang, Tao Xie, Xuanzhe Liu;Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 750-762, 2020;Deep learning (DL) becomes increasingly pervasive, being used in a wide range of software applications. These software applications, named as DL based software (in short as DL software), integrate DL models trained using a large data corpus with DL programs written based on DL frameworks such as TensorFlow and Keras. A DL program encodes the network structure of a desirable DL model and the process by which the model is trained using the training data. To help developers of DL software meet the new challenges posed by DL, enormous research efforts in software engineering have been devoted. Existing studies focus on the development of DL software and extensively analyze faults in DL programs. However, the deployment of DL software has not been comprehensively studied. To fill this knowledge gap, this paper presents a comprehensive study on understanding challenges in deploying DL software. We mine and analyze 3,023 relevant posts from Stack Overflow, a popular Q&A website for developers, and show the increasing popularity and high difficulty of DL software deployment among developers. We build a taxonomy of specific challenges encountered by developers in the process of DL software deployment through manual inspection of 769 sampled posts and report a series of actionable implications for researchers, developers, and DL framework vendors.;https://dl.acm.org/doi/abs/10.1145/3368089.3409759;8Jr4FudfQbYJ
Dhanorkar, S., Wolf, C. T., Qian, K., Xu, A., Popa, L., & Li, Y. (2021, June). Who needs to know what, when?: Broadening the Explainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycle. In Designing Interactive Systems Conference 2021 (pp. 1591-1602).;3_explanation_model_machine_learning;2021;Who needs to know what, when?: Broadening the Explainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycle;Shipi Dhanorkar, Christine T Wolf, Kun Qian, Anbang Xu, Lucian Popa, Yunyao Li;Designing Interactive Systems Conference 2021, 1591-1602, 2021;The interpretability or explainability of AI systems (XAI) has been a topic gaining renewed attention in recent years across AI and HCI communities. Recent work has drawn attention to the emergent explainability requirements of in situ, applied projects, yet further exploratory work is needed to more fully understand this space. This paper investigates applied AI projects and reports on a qualitative interview study of individuals working on AI projects at a large technology and consulting company. Presenting an empirical understanding of the range of stakeholders in industrial AI projects, this paper also draws out the emergent explainability practices that arise as these projects unfold, highlighting the range of explanation audiences (who), as well as how their explainability needs evolve across the AI project lifecycle (when). We discuss the importance of adopting a sociotechnical lens in designing AI systems, noting how the “AI lifecycle” can serve as a design metaphor to further the XAI design field.;https://dl.acm.org/doi/abs/10.1145/3461778.3462131;JU0fyQFrms8J
Silva, L. C., Zagatti, F. R., Sette, B. S., dos Santos Silva, L. N., Lucrédio, D., Silva, D. F., & de Medeiros Caseli, H. (2020, December). Benchmarking machine learning solutions in production. In 2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA) (pp. 626-633). IEEE.;1_ml_machine_data_learning;2020;Benchmarking machine learning solutions in production;Lucas Cardoso Silva, Fernando Rezende Zagatti, Bruno Silva Sette, Lucas Nildaimon dos Santos Silva, Daniel LucrÃ©dio, Diego Furtado Silva, Helena de Medeiros Caseli;2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA), 626-633, 2020;Machine learning (ML) is becoming critical to many businesses. Keeping an ML solution online and responding is therefore a necessity, and is part of the MLOps (Machine Learning operationalization) movement. One aspect for this process is monitoring not only prediction quality, but also system resources. This is important to correctly provide the necessary infrastructure, either using a fully-managed cloud platform or a local solution. This is not a difficult task, as there are many tools available. However, it requires some planning and knowledge about what to monitor. Also, many ML professionals are not experts in system operations and may not have the skills to easily setup a monitoring and benchmarking environment. In the spirit of MLOps, this paper presents an approach, based on a simple API and set of tools, to monitor ML solutions. The approach was tested with 9 different solutions. The results indicate that the approach can deliver useful information to help in decision making, proper resource provision and operation of ML systems.;https://ieeexplore.ieee.org/abstract/document/9356298/;AYz2N1TQlUoJ
Granlund, T., Kopponen, A., Stirbu, V., Myllyaho, L., & Mikkonen, T. (2021, May). MLOps challenges in multi-organization setup: Experiences from two real-world cases. In 2021 IEEE/ACM 1st Workshop on AI Engineering-Software Engineering for AI (WAIN) (pp. 82-88). IEEE.;1_ml_machine_data_learning;2021;MLOps challenges in multi-organization setup: Experiences from two real-world cases;Tuomas Granlund, Aleksi Kopponen, Vlad Stirbu, Lalli Myllyaho, Tommi Mikkonen;2021 IEEE/ACM 1st Workshop on AI Engineering-Software Engineering for AI (WAIN), 82-88, 2021;The emerging age of connected, digital world means that there are tons of data, distributed to various organizations and their databases. Since this data can be confidential in nature, it cannot always be openly shared in seek of artificial intelligence (AI) and machine learning (ML) solutions. Instead, we need integration mechanisms, analogous to integration patterns in information systems, to create multi-organization AI/ML systems. In this paper, we present two real-world cases. First, we study integration between two organizations in detail. Second, we address scaling of AI/ML to multi-organization context. The setup we assume is that of continuous deployment, often referred to DevOps in software development. When also ML components are deployed in a similar fashion, term MLOps is used. Towards the end of the paper, we list the main observations and draw some final conclusions. Finally, we propose some directions for future work.;https://ieeexplore.ieee.org/abstract/document/9474388/;Zc5P0G3KSm4J
Souza, R., Azevedo, L., Lourenço, V., Soares, E., Thiago, R., Brandão, R., ... & Netto, M. A. (2019, November). Provenance data in the machine learning lifecycle in computational science and engineering. In 2019 IEEE/ACM Workflows in Support of Large-Scale Science (WORKS) (pp. 1-10). IEEE.;1_ml_machine_data_learning;2019;Provenance data in the machine learning lifecycle in computational science and engineering;Renan Souza, Leonardo Azevedo, Vítor Lourenço, Elton Soares, Raphael Thiago, Rafael Brandão, Daniel Civitarese, Emilio Brazil, Marcio Moreno, Patrick Valduriez, Marta Mattoso, Renato Cerqueira, Marco AS Netto;2019 IEEE/ACM Workflows in Support of Large-Scale Science (WORKS), 1-10, 2019;"Machine Learning (ML) has become essential in several industries. In Computational Science and Engineering (CSE), the complexity of the ML lifecycle comes from the large variety of data, scientists' expertise, tools, and workflows. If data are not tracked properly during the lifecycle, it becomes unfeasible to recreate a ML model from scratch or to explain to stackholders how it was created. The main limitation of provenance tracking solutions is that they cannot cope with provenance capture and integration of domain and ML data processed in the multiple workflows in the lifecycle, while keeping the provenance capture overhead low. To handle this problem, in this paper we contribute with a detailed characterization of provenance data in the ML lifecycle in CSE; a new provenance data representation, called PROV-ML, built on top of W3C PROV and ML Schema; and extensions to a system that tracks provenance from multiple workflows to address the characteristics of ML and CSE, and to allow for provenance queries with a standard vocabulary. We show a practical use in a real case in the O&G industry, along with its evaluation using 239,616 CUDA cores in parallel.";https://ieeexplore.ieee.org/abstract/document/8943505/;yTq2p8rkE78J
Dang, Y., Lin, Q., & Huang, P. (2019, May). Aiops: real-world challenges and research innovations. In 2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion) (pp. 4-5). IEEE.;1_ml_machine_data_learning;2019;Aiops: real-world challenges and research innovations;Yingnong Dang, Qingwei Lin, Peng Huang;2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion);AIOps is about empowering software and service engineers (e.g., developers, program managers, support engineers, site reliability engineers) to efficiently and effectively build and operate online services and Apps at scale with artificial intelligence (AI) and machine learning (ML) techniques. AIOps can help achieve higher service quality and customer satisfaction, engineering productivity boost, and cost reduction. In this technical briefing, we summarize the real-world challenges on building AIOps solutions based on our practice and experience in Microsoft, propose a roadmap of AIOps related research directions, and share a few successful AIOps solutions we have built for Microsoft service products.;https://ieeexplore.ieee.org/abstract/document/8802836;Todo
Liu, Y., Ling, Z., Huo, B., Wang, B., Chen, T., & Mouine, E. (2020). Building a platform for machine learning operations from open source frameworks. IFAC-PapersOnLine, 53(5), 704-709.;1_ml_machine_data_learning;2020;Building a platform for machine learning operations from open source frameworks;Yan Liu, Zhijing Ling, Boyu Huo, Boqian Wang, Tianen Chen, Esma Mouine;IFAC-PapersOnLine 53 (5), 704-709, 2020;Machine Learning Operations (MLOps) aim to establish a set of practices that put tools, pipelines, and processes to build fast time-to-value machine learning development projects. The lifecycle of machine learning project development encompasses a set of roles, stacks of software frameworks and multiple types of computing resources. Such complexity makes MLOps support usually bundled with commercial cloud platforms that is referred as vendor lock. In this paper, we provide an alternative solution that devises a MLOps platform with open …;https://www.sciencedirect.com/science/article/pii/S2405896321003013;J-T3SmrsNJwJ
Giray, G. (2021). A software engineering perspective on engineering machine learning systems: State of the art and challenges. Journal of Systems and Software, 180, 111031.;1_ml_machine_data_learning;2021;A software engineering perspective on engineering machine learning systems: State of the art and challenges;GÃ¶rkem Giray;Journal of Systems and Software 180, 111031, 2021;"Advancements in machine learning (ML) lead to a shift from the traditional view of software development, where algorithms are hard-coded by humans, to ML systems materialized through learning from data. Therefore, we need to revisit our ways of developing software systems and consider the particularities required by these new types of systems.The purpose of this study is to systematically identify, analyze, summarize, and synthesize the current state of software engineering (SE) research for engineering ML systems.I performed a systematic literature review (SLR). I systematically selected a pool of 141 studies from SE venues and then conducted a quantitative and qualitative analysis using the data extracted from these studies.The non-deterministic nature of ML systems complicates all SE aspects of engineering ML systems. Despite increasing interest from 2018 onwards, the results reveal that none of the SE aspects have a mature set of tools and techniques. Testing is by far the most popular area among researchers. Even for testing ML systems, engineers have only some tool prototypes and solution proposals with weak experimental proof. Many of the challenges of ML systems engineering were identified through surveys and interviews. Researchers should conduct experiments and case studies, ideally in industrial environments, to further understand these challenges and propose solutions.The results may benefit (1) practitioners in foreseeing the challenges of ML systems engineering; (2) researchers and academicians in identifying potential research questions; and (3) educators in designing or updating SE courses to cover ML systems engineering.";https://www.sciencedirect.com/science/article/pii/S016412122100128X;rxsRXE58easJ
Martín, C., Langendoerfer, P., Zarrin, P. S., Díaz, M., & Rubio, B. (2022). Kafka-ML: Connecting the data stream with ML/AI frameworks. Future Generation Computer Systems, 126, 15-33.;1_ml_machine_data_learning;2022;Kafka-ML: Connecting the data stream with ML/AI frameworks;Cristian Martín, Peter Langendoerfer, Pouya Soltani Zarrin, Manuel Díaz, Bartolomé Rubio;Future Generation Computer Systems 126, 15-33, 2022;Machine Learning (ML) and Artificial Intelligence (AI) depend on data sources to train, improve, and make predictions through their algorithms. With the digital revolution and current paradigms like the Internet of Things, this information is turning from static data to continuous data streams. However, most of the ML/AI frameworks used nowadays are not fully prepared for this revolution. In this paper, we propose Kafka-ML, a novel and open-source framework that enables the management of ML/AI pipelines through data streams. Kafka-ML provides an accessible and user-friendly Web user interface where users can easily define ML models, to then train, evaluate, and deploy them for inferences. Kafka-ML itself and the components it deploys are fully managed through containerization technologies, which ensure their portability, easy distribution, and other features such as fault-tolerance and high availability. Finally, a novel approach has been introduced to manage and reuse data streams, which may eliminate the need for data storage or file systems.;https://www.sciencedirect.com/science/article/pii/S0167739X21002995;YwN_8IYiqxAJ
Lwakatare, L. E., Raj, A., Crnkovic, I., Bosch, J., & Olsson, H. H. (2020). Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions. Information and software technology, 127, 106368.;1_ml_machine_data_learning;2020;Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions;Lucy Ellen Lwakatare, Aiswarya Raj, Ivica Crnkovic, Jan Bosch, Helena HolmstrÃ¶m Olsson;Information and software technology 127, 106368, 2020;Background: Developing and maintaining large scale machine learning (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems.Objective: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges.Method: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four quality attributes: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment.Results: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions.Conclusion: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in ML system development practice and the lack of solutions point to directions for future research.;https://www.sciencedirect.com/science/article/pii/S0950584920301373;_eaX9C-stLcJ
van den Heuvel, W. J., & Tamburri, D. A. (2020). Model-driven ML-Ops for intelligent enterprise applications: vision, approaches and challenges. In Business Modeling and Software Design: 10th International Symposium, BMSD 2020, Berlin, Germany, July 6-8, 2020, Proceedings 10 (pp. 169-181). Springer International Publishing.;1_ml_machine_data_learning;2020;Model-driven ML-Ops for intelligent enterprise applications: vision, approaches and challenges;Willem-Jan van den Heuvel, Damian A Tamburri;Business Modeling and Software Design: 10th International Symposium, BMSD 2020, Berlin, Germany, July 6-8, 2020, Proceedings 10, 169-181, 2020;This paper explores a novel vision for the disciplined, repeatable, and transparent model-driven development and Machine-Learning operations (ML-Ops) of intelligent enterprise applications.The proposed framework treats model abstractions of AI/ML models (named AI/ML Blueprints) as first-class citizens and promotes end-to-end transparency and portability from raw data detection- to model verification, and, policy-driven model management.This framework is grounded on the intelligent Application Architecture (iA2) and entails a first attempt to incorporate requirements stemming from (more) intelligent enterprise applications into a logically-structured architecture. The logical separation is grounded on the need to enact MLOps and logically separate basic data manipulation requirements (data-processing layer), from more advanced functionality needed to instrument applications with intelligence (data intelligence layer), and continuous deployment, testing and monitoring of intelligent application (knowledge-driven application layer).Finally, the paper sets out exploring a foundational metamodel underpinning blueprint-model-driven MLOps for iA2 applications, and presents its main findings and open research agenda.;https://link.springer.com/chapter/10.1007/978-3-030-52306-0_11;IKkbjTimxc8J
Figalist, I., Elsner, C., Bosch, J., & Olsson, H. H. (2020). An end-to-end framework for productive use of machine learning in software analytics and business intelligence solutions. In Product-Focused Software Process Improvement: 21st International Conference, PROFES 2020, Turin, Italy, November 25–27, 2020, Proceedings 21 (pp. 217-233). Springer International Publishing.;1_ml_machine_data_learning;2020;An end-to-end framework for productive use of machine learning in software analytics and business intelligence solutions;Iris Figalist, Christoph Elsner, Jan Bosch, Helena Holmström Olsson;Product-Focused Software Process Improvement: 21st International Conference, PROFES 2020, Turin, Italy, November 25–27, 2020, Proceedings 21, 217-233, 2020;Nowadays, machine learning (ML) is an integral component in a wide range of areas, including software analytics (SA) and business intelligence (BI). As a result, the interest in custom ML-based software analytics and business intelligence solutions is rising. In practice, however, such solutions often get stuck in a prototypical stage because setting up an infrastructure for deployment and maintenance is considered complex and time-consuming. For this reason, we aim at structuring the entire process and making it more transparent by deriving an end-to-end framework from existing literature for building and deploying ML-based software analytics and business intelligence solutions. The framework is structured in three iterative cycles representing different stages in a model’s lifecycle: prototyping, deployment, update. As a result, the framework specifically supports the transitions between these stages while also covering all important activities from data collection to retraining deployed ML models. To validate the applicability of the framework in practice, we compare it to and apply it in a real-world ML-based SA/BI solution.;https://link.springer.com/chapter/10.1007/978-3-030-64148-1_14;d12E4RdUukkJ
Akkiraju, R., Sinha, V., Xu, A., Mahmud, J., Gundecha, P., Liu, Z., ... & Schumacher, J. (2020). Characterizing machine learning processes: A maturity framework. In Business Process Management: 18th International Conference, BPM 2020, Seville, Spain, September 13–18, 2020, Proceedings 18 (pp. 17-31). Springer International Publishing.;1_ml_machine_data_learning;2020;Characterizing machine learning processes: A maturity framework;Rama Akkiraju, Vibha Sinha, Anbang Xu, Jalal Mahmud, Pritam Gundecha, Zhe Liu, Xiaotong Liu, John Schumacher;Business Process Management: 18th International Conference, BPM 2020, Seville, Spain, September 13–18, 2020, Proceedings 18, 17-31, 2020;Academic literature on machine learning modeling fails to address how to make machine learning models work for enterprises. For example, existing machine learning processes cannot address how to define business use cases for an AI application, how to convert business requirements from product managers into data requirements for data scientists, and how to continuously improve AI applications in term of accuracy and fairness, how to customize general purpose machine learning models with industry, domain, and use case specific data to make them more accurate for specific situations etc. Making AI work for enterprises requires special considerations, tools, methods and processes. In this paper we present a maturity framework for machine learning model lifecycle management for enterprises. Our framework is a re-interpretation of the software Capability Maturity Model (CMM) for machine learning model development process. We present a set of best practices from authors’ personal experience of building large scale real-world machine learning models to help organizations achieve higher levels of maturity independent of their starting point.;https://link.springer.com/chapter/10.1007/978-3-030-58666-9_2;FHACJ_qG3IQJ
Ashmore, R., Calinescu, R., & Paterson, C. (2021). Assuring the machine learning lifecycle: Desiderata, methods, and challenges. ACM Computing Surveys (CSUR), 54(5), 1-39.;2_safety_system_autonomous_vehicle;2021;Assuring the machine learning lifecycle: Desiderata, methods, and challenges;Rob Ashmore, Radu Calinescu, Colin Paterson;ACM Computing Surveys (CSUR) 54 (5), 1-39, 2021;Machine learning has evolved into an enabling technology for a wide range of highly successful applications. The potential for this success to continue and accelerate has placed machine learning (ML) at the top of research, economic, and political agendas. Such unprecedented interest is fuelled by a vision of ML applicability extending to healthcare, transportation, defence, and other domains of great societal importance. Achieving this vision requires the use of ML in safety-critical applications that demand levels of assurance beyond those needed for current ML applications. Our article provides a comprehensive survey of the state of the art in the assurance of ML, i.e., in the generation of evidence that ML is sufficiently safe for its intended use. The survey covers the methods capable of providing such evidence at different stages of the machine learning lifecycle, i.e., of the complex, iterative process that starts with the collection of the data used to train an ML component for a system, and ends with the deployment of that component within the system. The article begins with a systematic presentation of the ML lifecycle and its stages. We then define assurance desiderata for each stage, review existing methods that contribute to achieving these desiderata, and identify open challenges that require further research.;https://dl.acm.org/doi/abs/10.1145/3453444;i-72pbV3PKsJ
Kanagal, B., & Tata, S. (2018, April). Recommendations for all: Solving thousands of recommendation problems daily. In 2018 IEEE 34th International Conference on Data Engineering (ICDE) (pp. 1404-1413). IEEE.;1_ml_machine_data_learning;2018;Recommendations for all: Solving thousands of recommendation problems daily;Bhargav Kanagal, Sandeep Tata;2018 IEEE 34th International Conference on Data Engineering (ICDE), 1404-1413, 2018;Recommender systems are a key technology for many online services including e-commerce, movies, music, and news. Online retailers use product recommender systems to help users discover items that they may like. However, building a large-scale product recommender system is a challenging task. The problems of sparsity and cold-start are much more pronounced in this domain. Large online retailers have used good recommendations to drive user engagement and improve revenue, but the complexity involved is a roadblock to widespread adoption by smaller retailers. In this paper, we tackle the problem of generating product recommendations for tens of thousands of online retailers. Sigmund is an industrial-scale system for providing recommendations as a service. Sigmund was deployed to production in early 2014 and has been serving retailers every day. We describe the design choices that we made in order to train accurate matrix factorization models at minimal cost. We also share the lessons we learned from this experience - both from a machine learning perspective and a systems perspective. We hope that these lessons are useful for building future machine-learning services.;https://ieeexplore.ieee.org/abstract/document/8509380/;lv3kCWcfMYQJ
Wang, J., Huang, P., Zhao, H., Zhang, Z., Zhao, B., & Lee, D. L. (2018, July). Billion-scale commodity embedding for e-commerce recommendation in alibaba. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining (pp. 839-848).;1_ml_machine_data_learning;2018;Billion-scale commodity embedding for e-commerce recommendation in alibaba;Jizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, Dik Lun Lee;Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, 839-848, 2018;Recommender systems (RSs) have been the most important technology for increasing the business in Taobao, the largest online consumer-to-consumer (C2C) platform in China. There are three major challenges facing RS in Taobao: scalability, sparsity and cold start. In this paper, we present our technical solutions to address these three challenges. The methods are based on a well-known graph embedding framework. We first construct an item graph from users' behavior history, and learn the embeddings of all items in the graph. The item embeddings are employed to compute pairwise similarities between all items, which are then used in the recommendation process. To alleviate the sparsity and cold start problems, side information is incorporated into the graph embedding framework. We propose two aggregation methods to integrate the embeddings of items and the corresponding side information. Experimental results from offline experiments show that methods incorporating side information are superior to those that do not. Further, we describe the platform upon which the embedding methods are deployed and the workflow to process the billion-scale data in Taobao. Using A/B test, we show that the online Click-Through-Rates (CTRs) are improved comparing to the previous collaborative filtering based methods widely used in Taobao, further demonstrating the effectiveness and feasibility of our proposed methods in Taobao's live production environment.;https://dl.acm.org/doi/abs/10.1145/3219819.3219869;ycoRdqPt7YUJ
Zheng, L., Tan, Z., Han, K., & Mao, R. (2018). Collaborative multi-modal deep learning for the personalized product retrieval in facebook marketplace. arXiv preprint arXiv:1805.12312.;1_ml_machine_data_learning;2018;Collaborative multi-modal deep learning for the personalized product retrieval in facebook marketplace;Lu Zheng, Zhao Tan, Kun Han, Ren Mao;arXiv preprint arXiv:1805.12312, 2018;"Facebook Marketplace is quickly gaining momentum among consumers as a favored customer-to-customer (C2C) product trading platform. The recommendation system behind it helps to significantly improve the user experience. Building the recommendation system for Facebook Marketplace is challenging for two reasons: 1) Scalability: the number of products in Facebook Marketplace is huge. Tens of thousands of products need to be scored and recommended within a couple hundred milliseconds for millions of users every day; 2) Cold start: the life span of the C2C products is very short and the user activities on the products are sparse. Thus it is difficult to accumulate enough product level signals for recommendation and we are facing a significant cold start issue. In this paper, we propose to address both the scalability and the cold-start issue by building a collaborative multi-modal deep learning based retrieval system where the compact embeddings for the users and the products are trained with the multi-modal content information. This system shows significant improvement over the benchmark in online and off-line experiments: In the online experiment, it increases the number of messages initiated by the buyer to the seller by +26.95%; in the off-line experiment, it improves the prediction accuracy by +9.58%.";https://arxiv.org/abs/1805.12312;u_KKIbwmLpUJ
Saitta, L., & Neri, F. (1998). Learning in the “real world”. Machine learning, 30, 133-163.;1_ml_machine_data_learning;1998;Learning in the “Real World”;Lorenza Saitta, Filippo Neri ;Machine learning, 1998•Springer;In this paper we define and characterize the process of developing a “real-world” Machine Learning application, with its difficulties and relevant issues, distinguishing it from the popular practice of exploiting ready-to-use data sets. To this aim, we analyze and summarize the lessons learned from applying Machine Learning techniques to a variety of problems. We believe that these lessons, though primarily based on our personal experience, can be generalized to a wider range of situations and are supported by the reported experiences of other researchers.;https://link.springer.com/article/10.1023/A:1007448122119;TODO
Cheng, H. T., Koc, L., Harmsen, J., Shaked, T., Chandra, T., Aradhye, H., ... & Shah, H. (2016, September). Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems (pp. 7-10).;1_ml_machine_data_learning;2016;Wide & deep learning for recommender systems;Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah;Proceedings of the 1st workshop on deep learning for recommender systems, 7-10, 2016;Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.;https://dl.acm.org/doi/abs/10.1145/2988450.2988454;7Vyg-6xNzw0J
Frey, R. M., Xu, R., Ammendola, C., Moling, O., Giglio, G., & Ilic, A. (2017). Mobile recommendations based on interest prediction from consumer's installed apps–insights from a large-scale field study. Information Systems, 71, 152-163.;1_ml_machine_data_learning;2017;Mobile recommendations based on interest prediction from consumer's installed apps–insights from a large-scale field study;Remo Manuel Frey, Runhua Xu, Christian Ammendola, Omar Moling, Giuseppe Giglio, Alexander Ilic;Information Systems 71, 152-163, 2017;Recommender systems are essential in mobile commerce to benefit both companies and individuals by offering highly personalized products and services. One key pre-requirement of applying such systems is to gain decent knowledge about each individual consumer through user profiling. However, most existing profiling approaches on mobile suffer problems such as non-real-time, intrusive, cold-start, and non-scalable, which prevents them from being adopted in reality. To tackle the problems, this work developed real-time machine-learning …;https://www.sciencedirect.com/science/article/pii/S0306437917304611;0WQ8t-f9iGcJ
Covington, P., Adams, J., & Sargin, E. (2016, September). Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems (pp. 191-198).;1_ml_machine_data_learning;2016;Deep neural networks for youtube recommendations;Paul Covington, Jay Adams, Emre Sargin;Proceedings of the 10th ACM conference on recommender systems, 191-198, 2016;YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous user-facing impact.;https://dl.acm.org/doi/abs/10.1145/2959100.2959190;4Kd-j2QZlp0J
Tata, S., Popescul, A., Najork, M., Colagrosso, M., Gibbons, J., Green, A., ... & Kan, R. (2017, August). Quick access: building a smart experience for Google drive. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1643-1651).;1_ml_machine_data_learning;2017;Quick access: building a smart experience for Google drive;Sandeep Tata, Alexandrin Popescul, Marc Najork, Mike Colagrosso, Julian Gibbons, Alan Green, Alexandre Mah, Michael Smith, Divanshu Garg, Cayden Meyer, Reuben Kan;Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1643-1651, 2017;Google Drive is a cloud storage and collaboration service used by hundreds of millions of users around the world. Quick Access is a new feature in Google Drive that surfaces the most relevant documents when a user visits the home screen. Our metrics show that users locate their documents in half the time with this feature compared to previous approaches. The development of Quick Access illustrates many general challenges and constraints associated with practical machine learning such as protecting user privacy, working with data services that are not designed with machine learning in mind, and evolving product definitions. We believe that the lessons learned from this experience will be useful to practitioners tackling a wide range of applied machine learning problems.;https://dl.acm.org/doi/abs/10.1145/3097983.3098048;R96IRZl4c9MJ
Liu, D. C., Rogers, S., Shiau, R., Kislyuk, D., Ma, K. C., Zhong, Z., ... & Jing, Y. (2017, April). Related pins at pinterest: The evolution of a real-world recommender system. In Proceedings of the 26th international conference on world wide web companion (pp. 583-592).;1_ml_machine_data_learning;2017;Related pins at pinterest: The evolution of a real-world recommender system;David C Liu, Stephanie Rogers, Raymond Shiau, Dmitry Kislyuk, Kevin C Ma, Zhigang Zhong, Jenny Liu, Yushi Jing;Proceedings of the 26th international conference on world wide web companion, 583-592, 2017;Related Pins is the Web-scale recommender system that powers over 40% of user engagement on Pinterest. This paper is a longitudinal study of three years of its development, exploring the evolution of the system and its components from prototypes to present state. Each component was originally built with many constraints on engineering effort and computational resources, so we prioritized the simplest and highest-leverage solutions. We show how organic growth led to a complex system and how we managed this complexity. Many challenges arose while building this system, such as avoiding feedback loops, evaluating performance, activating content, and eliminating legacy heuristics. Finally, we offer suggestions for tackling these challenges when engineering Web-scale recommender systems.;https://dl.acm.org/doi/abs/10.1145/3041021.3054202;v4vVV4Zim3QJ
Eksombatchai, C., Jindal, P., Liu, J. Z., Liu, Y., Sharma, R., Sugnet, C., ... & Leskovec, J. (2018, April). Pixie: A system for recommending 3+ billion items to 200+ million users in real-time. In Proceedings of the 2018 world wide web conference (pp. 1775-1784).;1_ml_machine_data_learning;2018;Pixie: A system for recommending 3+ billion items to 200+ million users in real-time;Chantat Eksombatchai, Pranav Jindal, Jerry Zitao Liu, Yuchen Liu, Rahul Sharma, Charles Sugnet, Mark Ulrich, Jure Leskovec;Proceedings of the 2018 world wide web conference, 1775-1784, 2018;User experience in modern content discovery applications critically depends on high-quality personalized recommendations. However, building systems that provide such recommendations presents a major challenge due to a massive pool of items, a large number of users, and requirements for recommendations to be responsive to user actions and generated on demand in real-time. Here we present Pixie, a scalable graph-based real-time recommender system that we developed and deployed at Pinterest. Given a set of user-specific pins as a query, Pixie selects in real-time from billions of possible pins those that are most related to the query. To generate recommendations, we develop Pixie Random Walk algorithm that utilizes the Pinterest object graph of 3 billion nodes and 17 billion edges. Experiments show that recommendations provided by Pixie lead up to 50% higher user engagement when compared to the previous Hadoop-based production system. Furthermore, we develop a graph pruning strategy at that leads to an additional 58% improvement in recommendations. Last, we discuss system aspects of Pixie, where a single server executes 1,200 recommendation requests per second with 60 millisecond latency. Today, systems backed by Pixie contribute to more than 80% of all user engagement on Pinterest.;https://dl.acm.org/doi/abs/10.1145/3178876.3186183;7wg0CSQ5WtYJ
Sharma, A., Jiang, J., Bommannavar, P., Larson, B., & Lin, J. (2016). GraphJet: Real-time content recommendations at Twitter. Proceedings of the VLDB Endowment, 9(13), 1281-1292.;1_ml_machine_data_learning;2016;GraphJet: Real-time content recommendations at Twitter;Aneesh Sharma, Jerry Jiang, Praveen Bommannavar, Brian Larson, Jimmy Lin;Proceedings of the VLDB Endowment 9 (13), 1281-1292, 2016;This paper presents GraphJet, a new graph-based system for generating content recommendations at Twitter. As motivation, we trace the evolution of our formulation and approach to the graph recommendation problem, embodied in successive generations of systems. Two trends can be identified: supplementing batch with real-time processing and a broadening of the scope of recommendations from users to content. Both of these trends come together in Graph-Jet, an in-memory graph processing engine that maintains a real-time bipartite interaction graph between users and tweets. The storage engine implements a simple API, but one that is sufficiently expressive to support a range of recommendation algorithms based on random walks that we have refined over the years. Similar to Cassovary, a previous graph recommendation engine developed at Twitter, GraphJet assumes that the entire graph can be held in memory on a single server. The system organizes the interaction graph into temporally-partitioned index segments that hold adjacency lists. GraphJet is able to support rapid ingestion of edges while concurrently serving lookup queries through a combination of compact edge encoding and a dynamic memory allocation scheme that exploits power-law characteristics of the graph. Each GraphJet server ingests up to one million graph edges per second, and in steady state, computes up to 500 recommendations per second, which translates into several million edge read operations per second.;https://dl.acm.org/doi/abs/10.14778/3007263.3007267;bocg5KirHqQJ
Gupta, P., Goel, A., Lin, J., Sharma, A., Wang, D., & Zadeh, R. (2013, May). Wtf: The who to follow service at twitter. In Proceedings of the 22nd international conference on World Wide Web (pp. 505-514).;1_ml_machine_data_learning;2013;Wtf: The who to follow service at twitter;Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, Reza Zadeh;Proceedings of the 22nd international conference on World Wide Web, 505-514, 2013;"WTF (""Who to Follow"") is Twitter's user recommendation service, which is responsible for creating millions of connections daily between users based on shared interests, common connections, and other related factors. This paper provides an architectural overview and shares lessons we learned in building and running the service over the past few years. Particularly noteworthy was our design decision to process the entire Twitter graph in memory on a single server, which significantly reduced architectural complexity and allowed us to develop and deploy the service in only a few months. At the core of our architecture is Cassovary, an open-source in-memory graph processing engine we built from scratch for WTF. Besides powering Twitter's user recommendations, Cassovary is also used for search, discovery, promoted products, and other services as well. We describe and evaluate a few graph recommendation algorithms implemented in Cassovary, including a novel approach based on a combination of random walks and SALSA. Looking into the future, we revisit the design of our architecture and comment on its limitations, which are presently being addressed in a second-generation system under development.";https://dl.acm.org/doi/abs/10.1145/2488388.2488433;zdncYcK8Vl0J
Gupta, R., Liang, G., Tseng, H. P., Holur Vijay, R. K., Chen, X., & Rosales, R. (2016, August). Email volume optimization at LinkedIn. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 97-106).;1_ml_machine_data_learning;2016;Email volume optimization at LinkedIn;Rupesh Gupta, Guanfeng Liang, Hsiao-Ping Tseng, Ravi Kiran Holur Vijay, Xiaoyu Chen, Romer Rosales;KDD '16: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;Online social networking services distribute various types of messages to their members. Common types of messages include news, connection requests, membership notifications, promotions and event notifications. Such communication, if used judiciously, can provide an enormous value to members thereby keeping them engaged. However sending a message for every instance of news, connection request, or the like can result in an overwhelming number of messages in a member's mailbox. This may result in reduced effectiveness of communication if the messages are not sufficiently relevant to the member's interests. It may also result in a poor brand perception of the networking service. In this paper we discuss our strategy and experience with regard to the problem of email volume optimization at LinkedIn. In particular, we present a cost-benefit analysis of sending emails, the key factors to administer an effective volume optimization, our algorithm for volume optimization, the architecture of the supporting system and experimental results from online A/B tests.;https://dl.acm.org/doi/abs/10.1145/2939672.2939692;Todo
Zhao, B., Narita, K., Orten, B., & Egan, J. (2018, July). Notification volume control and optimization system at Pinterest. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp. 1012-1020).;1_ml_machine_data_learning;2018;Notification volume control and optimization system at Pinterest;Bo Zhao, Koichiro Narita, Burkay Orten, John Egan;Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 1012-1020, 2018;Notifications (including emails, mobile / desktop push notifications, SMS, etc.) are very effective channels for online services to engage with users and drive user engagement metrics and other business metrics. One of the most important and challenging problems in a production notification system is to decide the right frequency for each user. In this paper, we propose a novel machine learning approach to decide notification volume for each user such that long term user engagement is optimized. We will also discuss a few practical issues and design choices we have made. The new system has been deployed to production at Pinterest in mid 2017 and significantly reduced notification volume and improved CTR of notifications and site engagement metrics compared with the previous machine learning approach.;https://dl.acm.org/doi/abs/10.1145/3219819.3219906;1rnej9UAgl8J
Gupta, R., Liang, G., & Rosales, R. (2017, November). Optimizing email volume for sitewide engagement. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management (pp. 1947-1955).;1_ml_machine_data_learning;2017;Optimizing email volume for sitewide engagement;Rupesh Gupta, Guanfeng Liang, RÃ³mer Rosales;Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, 1947-1955, 2017;"In this paper we focus on the problem of optimizing email volume for maximizing sitewide engagement of an online social networking service. Email volume optimization approaches published in the past have proposed optimization of email volume for maximization of engagement metrics which are impacted exclusively by email; for example, the number of sessions that begin with clicks on links within emails. The impact of email on such downstream engagement metrics can be estimated easily because of the ease of attribution of such an engagement event to an email. However, this framework is limited in its view of the ecosystem of the networking service which comprises of several tools and utilities that contribute towards delivering value to members; with email being just one such utility. Thus, in this paper we depart from previous approaches by exploring and optimizing the contribution of email to this ecosystem. In particular, we present and contrast the differential impact of email on sitewide engagement metrics for various types of users. We propose a new email volume optimization approach which maximizes sitewide engagement metrics, such as the total number of active users. This is in sharp contrast to the previous approaches whose objective has been maximization of downstream engagement metrics. We present details of our prediction function for predicting the impact of emails on a user's activeness on the mobile or web application. We describe how certain approximations to this prediction function can be made for solving the volume optimization problem, and present results from online A/B tests.";https://dl.acm.org/doi/abs/10.1145/3132847.3132849;EHkwJANSpYEJ
Liu, S., Xiao, F., Ou, W., & Si, L. (2017, August). Cascade ranking for operational e-commerce search. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1557-1565).;1_ml_machine_data_learning;2017;Cascade ranking for operational e-commerce search;Shichen Liu, Fei Xiao, Wenwu Ou, Luo Si;Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1557-1565, 2017;"In the 'Big Data' era, many real-world applications like search involve the ranking problem for a large number of items. It is important to obtain effective ranking results and at the same time obtain the results efficiently in a timely manner for providing good user experience and saving computational costs. Valuable prior research has been conducted for learning to efficiently rank like the cascade ranking (learning) model, which uses a sequence of ranking functions to progressively filter some items and rank the remaining items. However, most existing research of learning to efficiently rank in search is studied in a relatively small computing environments with simulated user queries.This paper presents novel research and thorough study of designing and deploying a Cascade model in a Large-scale Operational E-commerce Search application (CLOES), which deals with hundreds of millions of user queries per day with hundreds of servers. The challenge of the real-world application provides new insights for research: 1). Real-world search applications often involve multiple factors of preferences or constraints with respect to user experience and computational costs such as search accuracy, search latency, size of search results and total CPU cost, while most existing search solutions only address one or two factors; 2). Effectiveness of e-commerce search involves multiple types of user behaviors such as click and purchase, while most existing cascade ranking in search only models the click behavior. Based on these observations, a novel cascade ranking model is designed and deployed in an operational e-commerce search application. An extensive set of experiments demonstrate the advantage of the proposed work to address multiple factors of effectiveness, efficiency and user experience in the real-world application.";https://dl.acm.org/doi/abs/10.1145/3097983.3098011;H54JY6V3930J
Ni, Y., Ou, D., Liu, S., Li, X., Ou, W., Zeng, A., & Si, L. (2018, July). Perceive your users in depth: Learning universal user representations from multiple e-commerce tasks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp. 596-605).;1_ml_machine_data_learning;2018;Perceive your users in depth: Learning universal user representations from multiple e-commerce tasks;Yabo Ni, Dan Ou, Shichen Liu, Xiang Li, Wenwu Ou, Anxiang Zeng, Luo Si;Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 596-605, 2018;Tasks such as search and recommendation have become increasingly important for E-commerce to deal with the information overload problem. To meet the diverse needs of different users, personalization plays an important role. In many large portals such as Taobao and Amazon, there are a bunch of different types of search and recommendation tasks operating simultaneously for personalization. However, most of current techniques address each task separately. This is suboptimal as no information about users shared across different tasks.In this work, we propose to learn universal user representations across multiple tasks for more effective personalization. In particular, user behavior sequences (e.g., click, bookmark or purchase of products) are modeled by LSTM and attention mechanism by integrating all the corresponding content, behavior and temporal information. User representations are shared and learned in an end-to-end setting across multiple tasks. Benefiting from better information utilization of multiple tasks, the user representations are more effective to reflect their interests and are more general to be transferred to new tasks. We refer this work as Deep User Perception Network (DUPN) and conduct an extensive set of offline and online experiments. Across all tested five different tasks, our DUPN consistently achieves better results by giving more effective user representations. Moreover, we deploy DUPN in large scale operational tasks in Taobao. Detailed implementations, e.g., incremental model updating, are also provided to address the practical issues for the real world applications.;https://dl.acm.org/doi/abs/10.1145/3219819.3219828;9wo_rvEqNKgJ
Arya, D., Ha-Thuc, V., & Sinha, S. (2015, October). Personalized federated search at linkedin. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management (pp. 1699-1702).;1_ml_machine_data_learning;2015;Personalized federated search at linkedin;Dhruv Arya, Viet Ha-Thuc, Shakti Sinha;Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, 1699-1702, 2015;"LinkedIn has grown to become a platform hosting diverse sources of information ranging from member profiles, jobs, professional groups, slideshows etc. Given the existence of multiple sources, when a member issues a query like ""software engineer"", the member could look for software engineer profiles, jobs or professional groups. To tackle this problem, we exploit a data-driven approach that extracts searcher intents from their profile data and recent activities at a large scale. The intents such as job seeking, hiring, content consuming are used to construct features to personalize federated search experience. We tested the approach on the LinkedIn homepage and A/B tests show significant improvements in member engagement. As of writing this paper, the approach powers all of federated search on LinkedIn homepage.";https://dl.acm.org/doi/abs/10.1145/2806416.2806615;s9LkNbgNl0cJ
Li, C., Lu, Y., Mei, Q., Wang, D., & Pandey, S. (2015, August). Click-through prediction for advertising in twitter timeline. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1959-1968).;1_ml_machine_data_learning;2015;Click-through prediction for advertising in twitter timeline;Cheng Li, Yue Lu, Qiaozhu Mei, Dong Wang, Sandeep Pandey;Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1959-1968, 2015;We present the problem of click-through prediction for advertising in Twitter timeline, which displays a stream of Tweets from accounts a user choose to follow. Traditional computational advertising usually appears in two forms: sponsored search that places ads onto the search result page when a query is issued to a search engine, and contextual advertising that places ads onto a regular, usually static Web page. Compared with these two paradigms, placing ads into a Tweet stream is particularly challenging given the nature of the data stream: the context into which an ad can be placed updates dynamically and never replicates. Every ad is therefore placed into a unique context. This makes the information available for training a machine learning model extremely sparse. In this study, we propose a learning-to-rank method which not only addresses the sparsity of training signals but also can be trained and updated online. The proposed method is evaluated using both offline experiments and online A/B tests, which involve very large collections of Twitter data and real Twitter users. Results of the experiments prove the effectiveness and efficiency of our solution, and its superiority over the current production model adopted by Twitter.;https://dl.acm.org/doi/abs/10.1145/2783258.2788582;UGq40yOthYkJ
Raeder, T., Stitelman, O., Dalessandro, B., Perlich, C., & Provost, F. (2012, August). Design principles of massive, robust prediction systems. In Proceedings of the 18th ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1357-1365).;1_ml_machine_data_learning;2012;Design principles of massive, robust prediction systems;Troy Raeder, Ori Stitelman, Brian Dalessandro, Claudia Perlich, Foster Provost;Proceedings of the 18th ACM SIGKDD international conference on knowledge discovery and data mining, 1357-1365, 2012;Most data mining research is concerned with building high-quality classification models in isolation. In massive production systems, however, the ability to monitor and maintain performance over time while growing in size and scope is equally important. Many external factors may degrade classification performance including changes in data distribution, noise or bias in the source data, and the evolution of the system itself. A well-functioning system must gracefully handle all of these. This paper lays out a set of design principles for large-scale autonomous data mining systems and then demonstrates our application of these principles within the m6d automated ad targeting system. We demonstrate a comprehensive set of quality control processes that allow us monitor and maintain thousands of distinct classification models automatically, and to add new models, take on new data, and correct poorly-performing models without manual intervention or system disruption.;https://dl.acm.org/doi/abs/10.1145/2339530.2339740;S_L7W_ycUjUJ
Perlich, C., Dalessandro, B., Raeder, T., Stitelman, O., & Provost, F. (2014). Machine learning for targeted display advertising: Transfer learning in action. Machine learning, 95(1), 103-127.;1_ml_machine_data_learning;2014;Machine learning for targeted display advertising: Transfer learning in action;Claudia Perlich, Brian Dalessandro, Troy Raeder, Ori Stitelman, Foster Provost;Machine learning 95 (1), 103-127, 2014;This paper presents the design of a fully deployed multistage transfer learning system for targeted display advertising, highlighting the important role of problem formulation and the sampling of data from distributions different from that of the target environment. Notably, the machine learning system itself is deployed and has been in continual use for years for thousands of advertising campaignsâ€”in contrast to the more common case where predictive models are built outside the system, curated, and then deployed. In this domain, acquiring sufficient data for training from the ideal sampling distribution is prohibitively expensive. Instead, data are drawn from surrogate distributions and learning tasks, and then transferred to the target task. We present the design of the transfer learning system We then present a detailed experimental evaluation, showing that the different transfer stages indeed each add value. We also present production results across a variety of advertising clients from a variety of industries, illustrating the performance of the system in use. We close the paper with a collection of lessons learned from over half a decade of research and development on this complex, deployed, and intensely used machine learning system.;https://link.springer.com/article/10.1007/s10994-013-5375-2;1C7vopnFzKgJ
Rendle, S., Fetterly, D., Shekita, E. J., & Su, B. Y. (2016, August). Robust large-scale machine learning in the cloud. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1125-1134).;0_federated_learning_data_privacy;2016;Robust large-scale machine learning in the cloud;Steffen Rendle, Dennis Fetterly, Eugene J Shekita, Bor-yiing Su;Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1125-1134, 2016;The convergence behavior of many distributed machine learning (ML) algorithms can be sensitive to the number of machines being used or to changes in the computing environment. As a result, scaling to a large number of machines can be challenging. In this paper, we describe a new scalable coordinate descent (SCD) algorithm for generalized linear models whose convergence behavior is always the same, regardless of how much SCD is scaled out and regardless of the computing environment. This makes SCD highly robust and enables it to scale to massive datasets on low-cost commodity servers. Experimental results on a real advertising dataset in Google are used to demonstrate SCD's cost effectiveness and scalability. Using Google's internal cloud, we show that SCD can provide near linear scaling using thousands of cores for 1 trillion training examples on a petabyte of compressed data. This represents 10,000x more training examples than the 'large-scale' Netflix prize dataset. We also show that SCD can learn a model for 20 billion training examples in two hours for about $10.;https://dl.acm.org/doi/abs/10.1145/2939672.2939790;c2MwDLZ3oycJ
Molino, P., Zheng, H., & Wang, Y. C. (2018, July). Cota: Improving the speed and accuracy of customer support through ranking and deep networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp. 586-595).;1_ml_machine_data_learning;2018;Cota: Improving the speed and accuracy of customer support through ranking and deep networks;Piero Molino, Huaixiu Zheng, Yi-Chia Wang;Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 586-595, 2018;For a company looking to provide delightful user experiences, it is of paramount importance to take care of any customer issues. This paper proposes COTA, a system to improve speed and reliability of customer support for end users through automated ticket classification and answers selection for support representatives. Two machine learning and natural language processing techniques are demonstrated: one relying on feature engineering (COTA v1) and the other exploiting raw signals through deep learning architectures (COTA v2). COTA v1 employs a new approach that converts the multi-classification task into a ranking problem, demonstrating significantly better performance in the case of thousands of classes. For COTA v2, we propose an Encoder-Combiner-Decoder, a novel deep learning architecture that allows for heterogeneous input and output feature types and injection of prior knowledge through network architecture choices. This paper compares these models and their variants on the task of ticket classification and answer selection, showing model COTA v2 outperforms COTA v1, and analyzes their inner workings and shortcomings. Finally, an A/B test is conducted in a production setting validating the real-world impact of COTA in reducing issue resolution time by 10 percent without reducing customer satisfaction.;https://dl.acm.org/doi/abs/10.1145/3219819.3219851;SgNCV4wCQVQJ
Stein, T., Chen, E., & Mangla, K. (2011, April). Facebook immune system. In Proceedings of the 4th workshop on social network systems (pp. 1-8).;5_adversarial_attack_example_model;2011;Facebook immune system;Tao Stein, Erdong Chen, Karan Mangla;Proceedings of the 4th workshop on social network systems, 1-8, 2011;Popular Internet sites are under attack all the time from phishers, fraudsters, and spammers. They aim to steal user information and expose users to unwanted spam. The attackers have vast resources at their disposal. They are well-funded, with full-time skilled labor, control over compromised and infected accounts, and access to global botnets. Protecting our users is a challenging adversarial learning problem with extreme scale and load requirements. Over the past several years we have built and deployed a coherent, scalable, and extensible realtime system to protect our users and the social graph. This Immune System performs realtime checks and classifications on every read and write action. As of March 2011, this is 25B checks per day, reaching 650K per second at peak. The system also generates signals for use as feedback in classifiers and other components. We believe this system has contributed to making Facebook the safest place on the Internet for people and their information. This paper outlines the design of the Facebook Immune System, the challenges we have faced and overcome, and the challenges we continue to face.;https://dl.acm.org/doi/abs/10.1145/1989656.1989664;MnziXjduxacJ
Leontjeva, A., Goldszmidt, M., Xie, Y., Yu, F., & Abadi, M. (2013, November). Early security classification of skype users via machine learning. In Proceedings of the 2013 ACM workshop on Artificial intelligence and security (pp. 35-44).;1_ml_machine_data_learning;2013;Early security classification of skype users via machine learning;Anna Leontjeva, Moises Goldszmidt, Yinglian Xie, Fang Yu, MartÃ­n Abadi;Proceedings of the 2013 ACM workshop on Artificial intelligence and security, 35-44, 2013;We investigate possible improvements in online fraud detection based on information about users and their interactions. We develop, apply, and evaluate our methods in the context of Skype. Specifically, in Skype, we aim to provide tools that identify fraudsters that have eluded the first line of detection systems and have been active for months. Our approach to automation is based on machine learning methods. We rely on a variety of features present in the data, including static user profiles (e.g., age), dynamic product usage (e.g., time series of calls), local social behavior (addition/deletion of friends), and global social features (e.g., PageRank). We introduce new techniques for pre-processing the dynamic (time series) features and fusing them with social features. We provide a thorough analysis of the usefulness of the different categories of features and of the effectiveness of our new techniques.;https://dl.acm.org/doi/abs/10.1145/2517312.2517322;qTiUrUg71VQJ
Wang, S., Liu, C., Gao, X., Qu, H., & Xu, W. (2017). Session-based fraud detection in online e-commerce transactions using recurrent neural networks. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2017, Skopje, Macedonia, September 18–22, 2017, Proceedings, Part III 10 (pp. 241-252). Springer International Publishing.;1_ml_machine_data_learning;2017;Session-based fraud detection in online e-commerce transactions using recurrent neural networks;Shuhao Wang, Cancheng Liu, Xiang Gao, Hongtao Qu, Wei Xu;Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2017, Skopje, Macedonia, September 18–22, 2017, Proceedings, Part III 10, 241-252, 2017;Transaction frauds impose serious threats onto e-commerce. We present CLUE, a novel deep-learning-based transaction fraud detection system we design and deploy at JD.com, one of the largest e-commerce platforms in China with over 220 million active users. CLUE captures detailed information on users’ click actions using neural-network based embedding, and models sequences of such clicks using the recurrent neural network. Furthermore, CLUE provides application-specific design optimizations including imbalanced learning, real-time detection, and incremental model update. Using real production data for over eight months, we show that CLUE achieves over 3x improvement over the existing fraud detection approaches.;https://link.springer.com/chapter/10.1007/978-3-319-71273-4_20;kadIZLyW8OYJ
Zhou, H., Chai, H. F., & Qiu, M. L. (2018). Fraud detection within bankcard enrollment on mobile device based payment using machine learning. Frontiers of Information Technology & Electronic Engineering, 19, 1537-1545.;1_ml_machine_data_learning;2018;Fraud detection within bankcard enrollment on mobile device based payment using machine learning;Hao Zhou, Hong-feng Chai, Mao-lin Qiu;Frontiers of Information Technology & Electronic Engineering 19, 1537-1545, 2018;The rapid growth of mobile Internet technologies has induced a dramatic increase in mobile payments as well as concomitant mobile transaction fraud. As the first step of mobile transactions, bankcard enrollment on mobile devices has become the primary target of fraud attempts. Although no immediate financial loss is incurred after a fraud attempt, subsequent fraudulent transactions can be quickly executed and could easily deceive the fraud detection systems if the fraud attempt succeeds at the bankcard enrollment step. In recent years, financial institutions and service providers have implemented rule-based expert systems and adopted short message service (SMS) user authentication to address this problem. However, the above solution is inadequate to face the challenges of data loss and social engineering. In this study, we introduce several traditional machine learning algorithms and finally choose the improved gradient boosting decision tree (GBDT) algorithm software library for use in a real system, namely, XGBoost. We further expand multiple features based on analysis of the enrollment behavior and plan to add historical transactions in future studies. Subsequently, we use a real card enrollment dataset covering the year 2017, provided by a worldwide payment processor. The results and framework are adopted and absorbed into a new design for a mobile payment fraud detection system within the Chinese payment processor.;https://link.springer.com/article/10.1631/FITEE.1800580;aib87ZcMBuoJ
Rigoll, G. (2007). The connecteddrive context server–flexible software architecture for a context aware vehicle. Advanced Microsystems for Automotive Applications 2007, 201.;2_safety_system_autonomous_vehicle;2007;The connecteddrive context server–flexible software architecture for a context aware vehicle;G Rigoll;Advanced Microsystems for Automotive Applications 2007 201, 2007;Focus is pointed on the ConnectedDrive Context Server (CDCS), a central server component managing the situational context of an automotive human-machine-interaction, that is capable of providing key functionalities for a flexible prototyping process in the development and evaluation of “intelligent” vehicle behaviour. The main features are an object-oriented, shared knowledge management database and both generic, flexible I/O and knowledge analysis interfaces, offering a high compatibility for the connection to existing applications and for the implementation of intelligent reasoning algorithms. Our prototyping architecture is currently used in the development and evaluation of several context aware applications like a context-sensitive lane departure warning system (LDWS).;https://link.springer.com/content/pdf/10.1007/978-3-540-71325-8.pdf#page=203;TVMLUHnWirUJ
Zhang, Z., Stenneth, L., Marappan, R., Sebastian, Z., & Yu, P. S. (2018, November). Insert beyond the traffic sign recognition: constructing an auto-pilot map for autonomous vehicles. In Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (pp. 468-471).;2_safety_system_autonomous_vehicle;2018;Insert beyond the traffic sign recognition: constructing an auto-pilot map for autonomous vehicles;Zhenhua Zhang, Leon Stenneth, Ram Marappan, Zaba Sebastian, Philip S Yu;Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, 468-471, 2018;"Traffic sign recognition (TSR) systems on the vehicles can collect posted speed limit sign information and have been in commercial usage since 2008. A daily-updated auto-pilot map can be constructed based on the massive amounts of TSR observations from multiple consumer vehicles; the data is then aggregated, filtered and processed, and the learned posted speed limit signs can be finally transferred to vehicles with high-coverage and real-time speed limit information. Compared with the direct sign detection by TSR systems, the auto-pilot map can complement the current detection errors, reduce the camera cost and provide a continuous speed limit information for autonomous vehicle applications. A pipeline of methods are specifically designed to deliver our research purpose by making full utilization of TSR observations and HERE map. Experimental results indicate that our proposed algorithms and methods can construct an auto-pilot map with an overall accuracy of 95.8%. It is also expected to update the speed limit information in a map at a faster pace than the traditional map since we are using sensors of customer vehicles instead of dedicated map construction vehicles. The utility of our proposed auto-pilot map opens a new perspective in autonomous driving.";https://dl.acm.org/doi/abs/10.1145/3274895.3274951;arX8trSEZoEJ
Lin, S. C., Hsu, C. H., Talamonti, W., Zhang, Y., Oney, S., Mars, J., & Tang, L. (2018, October). Adasa: A conversational in-vehicle digital assistant for advanced driver assistance features. In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology (pp. 531-542).;2_safety_system_autonomous_vehicle;2018;Adasa: A conversational in-vehicle digital assistant for advanced driver assistance features;Shih-Chieh Lin, Chang-Hong Hsu, Walter Talamonti, Yunqi Zhang, Steve Oney, Jason Mars, Lingjia Tang;Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology, 531-542, 2018;Advanced Driver Assistance Systems (ADAS) come equipped on most modern vehicles and are intended to assist the driver and enhance the driving experience through features such as lane keeping system and adaptive cruise control. However, recent studies show that few people utilize these features for several reasons. First, ADAS features were not common until recently. Second, most users are unfamiliar with these features and do not know what to expect. Finally, the interface for operating these features is not intuitive. To help drivers understand ADAS features, we present a conversational in-vehicle digital assistant that responds to drivers' questions and commands in natural language. With the system prototyped herein, drivers can ask questions or command using unconstrained natural language in the vehicle, and the assistant trained by using advanced machine learning techniques, coupled with access to vehicle signals, responds in real-time based on conversational context. Results of our system prototyped on a production vehicle are presented, demonstrating its effectiveness in improving driver understanding and usability of ADAS.;https://dl.acm.org/doi/abs/10.1145/3242587.3242593;KyT7bhseV0EJ
Etherington, T. J., Kramer, L. J., Le Vie, L. R., Last, M. C., Kennedy, K. D., Bailey, R. E., & Houston, V. (2018, September). Impact of Advanced Synoptics and Simplified Checklists during Aircraft Systems Failures. In 2018 IEEE/AIAA 37th Digital Avionics Systems Conference (DASC) (pp. 1-9). IEEE.;2_safety_system_autonomous_vehicle;2018;Impact of Advanced Synoptics and Simplified Checklists during Aircraft Systems Failures;Timothy J Etherington, Lynda J Kramer, Lisa R Le Vie, Mary Carolyn Last, Kellie D Kennedy, Randall E Bailey, Vincent Houston;2018 IEEE/AIAA 37th Digital Avionics Systems Conference (DASC), 1-9, 2018;Natural human capacities are becoming increasingly mismatched to the enormous data volumes, processing capabilities, and decision speeds demanded in today's aviation environment. Increasingly Autonomous Systems (IAS) are uniquely suited to solve this problem. NASA is conducting research and development of IAS - hardware and software systems, utilizing machine learning algorithms, seamlessly integrated with humans whereby task performance of the combined system is significantly greater than the individual components. IAS offer the potential for significantly improved levels of performance and safety that are superior to either human or automation alone. A human-in-the-loop test was conducted in NASA Langley's Integration Flight Deck B-737-800 simulator to evaluate advanced synoptic pages with simplified interactive electronic checklists as an IAS for routine air carrier flight operations and in response to aircraft system failures. Twelve U.S. airline crews flew various normal and non-normal procedures and their actions and performance were recorded in response to failures. These data are fundamental to and critical for the design and development of future increasingly autonomous systems that can better support the human in the cockpit. Synoptic pages and electronic checklists significantly improved pilot responses to non-normal scenarios, but implementation of these aids and other intelligent assistants have barriers to implementation (e.g., certification cost) that must be overcome.;https://ieeexplore.ieee.org/abstract/document/8569559/;ikc-lCYrkFAJ
Muñoz, A., Scarlatti, D., & Costas, P. (2018, September). Real-time prediction of flight arrival times using surveillance information. In Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings (pp. 1-4).;1_ml_machine_data_learning;2018;Real-time prediction of flight arrival times using surveillance information;Andrés Muñoz, David Scarlatti, Pablo Costas;Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings, 1-4, 2018;Accurate estimations of the time of arrival of a flight prior to its landing are beneficial for most of the stakeholders in the air traffic management and control industry, as they could lead to reductions in potential safety risks and improvements in resources allocation. In this paper, and within the European Commission funded Transforming Transport project, we propose a methodology for real-time prediction of flight arrival times based on the application of machine learning techniques. For this purpose, we employ state-of-the-art data warehousing and broadcasting processes, that allow both the training of a regression machine learning model and the integration of its predictions of current flights on a real-time visualization tool set up for customer usage. The model only makes use of the information included in aircraft surveillance messages. Predictions obtained with such model are compared to those provided by other current services to observe the added value of the application of the proposed system on real-time operations.;https://dl.acm.org/doi/abs/10.1145/3241403.3241434;RjoO7Pve-MgJ
Bosneag, A. M., & Wang, M. X. (2017, November). Intelligent network management mechanisms as a step towards SG. In 2017 8th International Conference on the Network of the Future (NOF) (pp. 52-57). IEEE.;1_ml_machine_data_learning;2017;Intelligent network management mechanisms as a step towards SG;Anne-Marie Bosneag, Ming Xue Wang;2017 8th International Conference on the Network of the Future (NOF), 52-57, 2017;The evolution of telecom networks towards 5G comes with challenges that permeate the entire network structure and its management principles. In this paper, we outline how challenges such as the need to support different types of users and new network business models, the movement towards virtualization, as well as the need for more automation drive the requirement for a holistic and flexible way of managing the networks. We detail the architectural principles that underline such a cognitive management framework and we exemplify its use through a scenario based on software defined networking, where we combine machine learning, control and automation in the context of flexible resource provisioning in the Radio Access Network. Our experiments were conducted in collaboration with a major telecom operator and clearly show the advantages of introducing intelligence and automation into the network.;https://ieeexplore.ieee.org/abstract/document/8251220/;ATW0du5nCssJ
Mijumbi, R., Asthana, A., Koivunen, M., Haiyong, F., & Norman, Z. (2018, June). Darn: Dynamic baselines for real-time network monitoring. In 2018 4th ieee conference on network softwarization and workshops (netsoft) (pp. 37-45). IEEE.;1_ml_machine_data_learning;2018;Darn: Dynamic baselines for real-time network monitoring;Rashid Mijumbi, Abhaya Asthana, Markku Koivunen, Fu Haiyong, Zhu Norman;2018 4th ieee conference on network softwarization and workshops (netsoft), 37-45, 2018;Network monitoring is necessary so as to ensure high reliability and availability in telecom networks. One of the main challenges posed by state-of-the-art monitoring tools is the creation of network baselines. Such baselines include thresholds that can be used to determine whether monitored values (with a given context, e.g. time) represent normal network operation or not. The size and complexity of current (and future) networks makes it infeasible to manually determine and set baselines for each network operator and metric, let alone adapting the thresholds to changes in network conditions. This leads to the use of default baselines and/or setting baselines only once and never changing them throughout the lifetime of network elements. This does not only cause inefficient operation, but could have implications for network reliability and availability. In this paper, we present the design, implementation, and evaluation of DARN: a collection of analytics and machine learning-based algorithms aimed at ensuring that network baselines are automatically adapted to different metric evolution. DARN has been comprehensively evaluated on a deployment with real traffic to confirm accuracy of generated baselines, a 22% improvement in accuracy due to baseline adaptation, and a 72% reduction in false alarms.;https://ieeexplore.ieee.org/abstract/document/8460047/;Awx7QwQIHpYJ
Magnusson, J., & Kvernvik, T. (2012, August). Subscriber classification within telecom networks utilizing big data technologies and machine learning. In Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications (pp. 77-84).;1_ml_machine_data_learning;2012;Subscriber classification within telecom networks utilizing big data technologies and machine learning;Jonathan Magnusson, Tor Kvernvik;Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications, 77-84, 2012;This paper describes a scalable solution for identifying influential subscribers in for example telecom networks. The solution estimates one weighted value of influence out of several Social Network Analysis(SNA) metrics. The novel method for aggregation of several metrics utilizes machine learning to train models. A prototype solution has been implemented on a Hadoop platform to support scalability and to reduce hard ware cost by enabling the usage of commodity computers. The SNA algorithms have been adapted to efficiently execute on the MapReduce distributed platform. The prototype solution has been tested on a Hadoop cluster. The tests have verified that the solution can scale to support networks with millions of subscribers. Both real data from a telecom network operator with 2.4 million subscribers and synthetic data for networks up to 100 million subscribers have been used to verify the scalability and accuracy of the solution. The correlation between metrics have been analyzed to identify the information gain from each metric.;https://dl.acm.org/doi/abs/10.1145/2351316.2351327;mFRp3KzWx6oJ
Fadida-Specktor, B. (2018). Preprocessing prediction of advanced algorithms for medical imaging. Journal of Digital Imaging, 31(1), 42-50.;1_ml_machine_data_learning;2018;Preprocessing prediction of advanced algorithms for medical imaging;Bella Fadida-Specktor;Journal of Digital Imaging 31 (1), 42-50, 2018;Advanced medical imaging algorithms (such as bone removal, vessel segmentation, or a lung nodule detection) can provide extremely valuable information to the radiologists, but they might sometimes be very time consuming. Being able to run the algorithms in advance can be a possible solution. However, we do not know which algorithm to run on a given dataset before it is actually used. It is possible to manually insert matching rules for preprocessing algorithms, but it requires high maintenance and does not work well in practice. This paper presents a dynamic machine learning solution for predicting which advanced visualization (AV) algorithm needs to be applied on a given series. The system gets a handful of free text DICOM tags as an input and builds a model in the clinical setting. It incorporates a Bag of Words (BOW) feature extractor and a Random Forest classifier. The approach was tested on two datasets from clinical sites which use different languages and varying scanner models. We show that even without feature extraction, sensitivity of above 90% can be reached on both of them. By using BOW feature extractor, precision and sensitivity can usually be further improved. Even on a noisy and highly unbalanced dataset, only around 100 samples were needed to reach sensitivity of above 80% and specificity of above 97%. We show how the solution can be part of a Smart Preprocessing mechanism in a viewing software. Using such a system will ultimately minimize the time to launch studies and improve radiologists reading time efficiency.;https://link.springer.com/article/10.1007/s10278-017-9999-9;bWGaaer8t9UJ
Pastor, M., Quintana, J., & Sanz, F. (2018). Development of an infrastructure for the prediction of biological endpoints in industrial environments. Lessons learned at the eTOX project. Frontiers in Pharmacology, 9, 1147.;1_ml_machine_data_learning;2018;Development of an infrastructure for the prediction of biological endpoints in industrial environments. Lessons learned at the eTOX project;Manuel Pastor, Jordi Quintana, Ferran Sanz;Frontiers in Pharmacology 9, 1147, 2018;In silico methods are increasingly being used for assessing the chemical safety of substances, as a part of integrated approaches involving in vitro and in vivo experiments. A paradigmatic example of these strategies is the eTOX project http://www.etoxproject.eu, funded by the European Innovative Medicines Initiative (IMI), which aimed at producing high quality predictions of in vivo toxicity of drug candidates and resulted in generating about 200 models for diverse endpoints of toxicological interest. In an industry-oriented project like eTOX, apart from the predictive quality, the models need to meet other quality parameters related to the procedures for their generation and their intended use. For example, when the models are used for predicting the properties of drug candidates, the prediction system must guarantee the complete confidentiality of the compound structures. The interface of the system must be designed to provide non-expert users all the information required to choose the models and appropriately interpret the results. Moreover, procedures like installation, maintenance, documentation, validation and versioning, which are common in software development, must be also implemented for the models and for the prediction platform in which they are implemented. In this article we describe our experience in the eTOX project and the lessons learned after 7 years of close collaboration between industrial and academic partners. We believe that some of the solutions found and the tools developed could be useful for supporting similar initiatives in the future.;https://www.frontiersin.org/articles/10.3389/fphar.2018.01147/full;S4yyJgGmNlgJ
Lee, J., Noh, S. D., Kim, H. J., & Kang, Y. S. (2018). Implementation of cyber-physical production systems for quality prediction and operation control in metal casting. Sensors, 18(5), 1428.;1_ml_machine_data_learning;2018;Implementation of cyber-physical production systems for quality prediction and operation control in metal casting;JuneHyuck Lee, Sang Do Noh, Hyun-Jung Kim, Yong-Shin Kang;Sensors 18 (5), 1428, 2018;The prediction of internal defects of metal casting immediately after the casting process saves unnecessary time and money by reducing the amount of inputs into the next stage, such as the machining process, and enables flexible scheduling. Cyber-physical production systems (CPPS) perfectly fulfill the aforementioned requirements. This study deals with the implementation of CPPS in a real factory to predict the quality of metal casting and operation control. First, a CPPS architecture framework for quality prediction and operation control in metal-casting production was designed. The framework describes collaboration among internet of things (IoT), artificial intelligence, simulations, manufacturing execution systems, and advanced planning and scheduling systems. Subsequently, the implementation of the CPPS in actual plants is described. Temperature is a major factor that affects casting quality, and thus, temperature sensors and IoT communication devices were attached to casting machines. The well-known NoSQL database, HBase and the high-speed processing/analysis tool, Spark, are used for IoT repository and data pre-processing, respectively. Many machine learning algorithms such as decision tree, random forest, artificial neural network, and support vector machine were used for quality prediction and compared with R software. Finally, the operation of the entire system is demonstrated through a CPPS dashboard. In an era in which most CPPS-related studies are conducted on high-level abstract models, this study describes more specific architectural frameworks, use cases, usable software, and analytical methodologies. In addition, this study verifies the usefulness of CPPS by estimating quantitative effects. This is expected to contribute to the proliferation of CPPS in the industry.;https://www.mdpi.com/1424-8220/18/5/1428;brX0dZa6H6MJ
Zissis, D. (2017, June). Intelligent security on the edge of the cloud. In 2017 International Conference on Engineering, Technology and Innovation (ICE/ITMC) (pp. 1066-1070). IEEE.;7_edge_computing_deep_learning;2017;Intelligent security on the edge of the cloud;Dimitrios Zissis;2017 International Conference on Engineering, Technology and Innovation (ICE/ITMC), 1066-1070, 2017;Edge or Fog computing is a relatively new architectural deployment model, ideally fit for the unique requirements of the Internet of Things. This paper presents a novel solution, which leverages the architectural characteristics of edge computing for security reasons. Machine learning models (specifically Support Vector Machines) are employed on the edge of the cloud, to perform low footprint unsupervised learning and analysis of sensor data for anomaly detection purposes. To this end, a proof of concept system is developed, capable of detecting anomalies in real world vessel sensor streams (big data) in a smart port environment. We report on early results, that validate the potential of the solution. The quality and performance of the model is investigated in real world conditions.;https://ieeexplore.ieee.org/abstract/document/8279999/;tmxNO4Jlio0J
Kumar, R. S. S., Wicker, A., & Swann, M. (2017, November). Practical machine learning for cloud intrusion detection: Challenges and the way forward. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (pp. 81-90).;5_adversarial_attack_example_model;2017;Practical machine learning for cloud intrusion detection: Challenges and the way forward;Ram Shankar Siva Kumar, Andrew Wicker, Matt Swann;Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, 81-90, 2017;"Operationalizing machine learning based security detections is extremely challenging, especially in a continuously evolving cloud environment. Conventional anomaly detection does not produce satisfactory results for analysts that are investigating security incidents in the cloud. Model evaluation alone presents its own set of problems due to a lack of benchmark datasets. When deploying these detections, we must deal with model compliance, localization, and data silo issues, among many others. We pose the problem of ""attack disruption"" as a way forward in the security data science space. In this paper, we describe the framework, challenges, and open questions surrounding the successful operationalization of machine learning based security detections in a cloud environment and provide some insights on how we have addressed them.";https://dl.acm.org/doi/abs/10.1145/3128572.3140445;ieeQEkp-b34J
Hynes, N., Sculley, D., & Terry, M. (2017). The data linter: Lightweight, automated sanity checking for ml data sets. In NIPS MLSys Workshop (Vol. 1, No. 5).;1_ml_machine_data_learning;2017;The data linter: Lightweight, automated sanity checking for ml data sets;Nick Hynes, D Sculley, Michael Terry;NIPS MLSys Workshop 1 (5), 2017;"Data cleaning and feature engineering are both common practices when developing machine learning (ML) models. However, developers are not always aware of best practices for preparing or transforming data for a given model type, which can lead to suboptimal representations of input features. To address this issue, we introduce the data linter, a new class of ML tool that automatically inspects ML data sets to 1) identify potential issues in the data and 2) suggest potentially useful feature transforms, for a given model type. As with traditional code linting, data linting automatically identifies potential issues or inefficiencies; codifies best practices and educates end-users about these practices through tool use; and can lead to quality improvements. In this paper, we provide a detailed description of data linting, describe our initial implementation of a data linter for deep neural networks, and report results suggesting the utility of using a data linter during ML model design.";http://learningsys.org/nips17/assets/papers/paper_19.pdf;PmvydmnAUTgJ
"Biessmann, F., Salinas, D., Schelter, S., Schmidt, P., & Lange, D. (2018, October). "" Deep"" Learning for Missing Value Imputationin Tables with Non-numerical Data. In Proceedings of the 27th ACM international conference on information and knowledge management (pp. 2017-2025).";1_ml_machine_data_learning;2018;""" Deep"" Learning for Missing Value Imputationin Tables with Non-numerical Data";Felix Biessmann, David Salinas, Sebastian Schelter, Philipp Schmidt, Dustin Lange;Proceedings of the 27th ACM international conference on information and knowledge management, 2017-2025, 2018;The success of applications that process data critically depends on the quality of the ingested data. Completeness of a data source is essential in many cases. Yet, most missing value imputation approaches suffer from severe limitations. They are almost exclusively restricted to numerical data, and they either offer only simple imputation methods or are difficult to scale and maintain in production. Here we present a robust and scalable approach to imputation that extends to tables with non-numerical values, including unstructured text data in diverse languages. Experiments on public data sets as well as data sets sampled from a large product catalog in different languages (English and Japanese) demonstrate that the proposed approach is both scalable and yields more accurate imputations than previous approaches. Training on data sets with several million rows is a matter of minutes on a single machine. With a median imputation F1 score of 0.93 across a broad selection of data sets our approach achieves on average a 23-fold improvement compared to mode imputation. While our system allows users to apply state-of-the-art deep learning models if needed, we find that often simple linear n-gram models perform on par with deep learning methods at a much lower operational cost. The proposed method learns all parameters of the entire imputation pipeline automatically in an end-to-end fashion, rendering it attractive as a generic plugin both for engineers in charge of data pipelines where data completeness is relevant, as well as for practitioners without expertise in machine learning who need to impute missing values in tables with non-numerical data.;https://dl.acm.org/doi/abs/10.1145/3269206.3272005;UbgXxs6ioYMJ
Lane, N. D., Bhattacharya, S., Georgiev, P., Forlivesi, C., Jiao, L., Qendro, L., & Kawsar, F. (2016, April). Deepx: A software accelerator for low-power deep learning inference on mobile devices. In 2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN) (pp. 1-12). IEEE.;7_edge_computing_deep_learning;2016;Deepx: A software accelerator for low-power deep learning inference on mobile devices;Nicholas D Lane, Sourav Bhattacharya, Petko Georgiev, Claudio Forlivesi, Lei Jiao, Lorena Qendro, Fahim Kawsar;2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN), 1-12, 2016;"Breakthroughs from the field of deep learning are radically changing how sensor data are interpreted to extract the high-level information needed by mobile apps. It is critical that the gains in inference accuracy that deep models afford become embedded in future generations of mobile apps. In this work, we present the design and implementation of DeepX, a software accelerator for deep learning execution. DeepX signif- icantly lowers the device resources (viz. memory, computation, energy) required by deep learning that currently act as a severe bottleneck to mobile adoption. The foundation of DeepX is a pair of resource control algorithms, designed for the inference stage of deep learning, that: (1) decompose monolithic deep model network architectures into unit- blocks of various types, that are then more efficiently executed by heterogeneous local device processors (e.g., GPUs, CPUs); and (2), perform principled resource scaling that adjusts the architecture of deep models to shape the overhead each unit-blocks introduces. Experiments show, DeepX can allow even large-scale deep learning models to execute efficently on modern mobile processors and significantly outperform existing solutions, such as cloud-based offloading.";https://ieeexplore.ieee.org/abstract/document/7460664/;pSe6L38_rP4J
Chen, D., Bellamy, R. K., Malkin, P. K., & Erickson, T. (2016, September). Diagnostic visualization for non-expert machine learning practitioners: A design study. In 2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC) (pp. 87-95). IEEE.;3_explanation_model_machine_learning;2016;Diagnostic visualization for non-expert machine learning practitioners: A design study;Dong Chen, Rachel KE Bellamy, Peter K Malkin, Thomas Erickson;2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC), 87-95, 2016;"As machine learning (ML) becomes increasingly popular, developers without deep experience in ML - who we will refer to as ML practitioners - are facing the need to diagnose problems with ML models. Yet successful diagnosis requires high-level expertise that practitioners lack. As in many complex data-oriented domains, visualization could help. This two-phase study explored the design of visualizations to aid ML diagnosis. In phase 1, twelve ML practitioners were asked to diagnose a model using ten state-of-the-art visualizations; seven design themes were identified. In phase 2, several design themes were embodied in an interactive visualization. The visualization was used to engage practitioners in a participatory design exercise that explored how they would carry out multi-step diagnosis using the visualization. Our findings provide design implications for tools that better support ML diagnosis by non-expert practitioners.";https://ieeexplore.ieee.org/abstract/document/7739669/;m6C0QrWee50J
Yu, Y., Abadi, M., Barham, P., Brevdo, E., Burrows, M., Davis, A., ... & Zheng, X. (2018, April). Dynamic control flow in large-scale machine learning. In Proceedings of the Thirteenth EuroSys Conference (pp. 1-15).;7_edge_computing_deep_learning;2018;Dynamic control flow in large-scale machine learning;Yuan Yu, MartÃ­n Abadi, Paul Barham, Eugene Brevdo, Mike Burrows, Andy Davis, Jeff Dean, Sanjay Ghemawat, Tim Harley, Peter Hawkins, Michael Isard, Manjunath Kudlur, Rajat Monga, Derek Murray, Xiaoqiang Zheng;Proceedings of the Thirteenth EuroSys Conference, 1-15, 2018;Many recent machine learning models rely on fine-grained dynamic control flow for training and inference. In particular, models based on recurrent neural networks and on reinforcement learning depend on recurrence relations, data-dependent conditional execution, and other features that call for dynamic control flow. These applications benefit from the ability to make rapid control-flow decisions across a set of computing devices in a distributed system. For performance, scalability, and expressiveness, a machine learning system must support dynamic control flow in distributed and heterogeneous environments.This paper presents a programming model for distributed machine learning that supports dynamic control flow. We describe the design of the programming model, and its implementation in TensorFlow, a distributed machine learning system. Our approach extends the use of dataflow graphs to represent machine learning models, offering several distinctive features. First, the branches of conditionals and bodies of loops can be partitioned across many machines to run on a set of heterogeneous devices, including CPUs, GPUs, and custom ASICs. Second, programs written in our model support automatic differentiation and distributed gradient computations, which are necessary for training machine learning models that use control flow. Third, our choice of non-strict semantics enables multiple loop iterations to execute in parallel across machines, and to overlap compute and I/O operations.We have done our work in the context of TensorFlow, and it has been used extensively in research and production. We evaluate it using several real-world applications, and demonstrate its performance and scalability.;https://dl.acm.org/doi/abs/10.1145/3190508.3190551;7BkgT4ZCsyMJ
Gujarati, A., Elnikety, S., He, Y., McKinley, K. S., & Brandenburg, B. B. (2017, December). Swayam: distributed autoscaling to meet slas of machine learning inference services with resource efficiency. In Proceedings of the 18th ACM/IFIP/USENIX middleware conference (pp. 109-120).;8_serverless_cloud_cost_computing;2017;Swayam: distributed autoscaling to meet slas of machine learning inference services with resource efficiency;Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S McKinley, BjÃ¶rn B Brandenburg;Proceedings of the 18th ACM/IFIP/USENIX middleware conference, 109-120, 2017;Developers use Machine Learning (ML) platforms to train ML models and then deploy these ML models as web services for inference (prediction). A key challenge for platform providers is to guarantee response-time Service Level Agreements (SLAs) for inference workloads while maximizing resource efficiency. Swayam is a fully distributed autoscaling framework that exploits characteristics of production ML inference workloads to deliver on the dual challenge of resource efficiency and SLA compliance. Our key contributions are (1) model-based autoscaling that takes into account SLAs and ML inference workload characteristics, (2) a distributed protocol that uses partial load information and prediction at frontends to provision new service instances, and (3) a backend self-decommissioning protocol for service instances. We evaluate Swayam on 15 popular services that were hosted on a production ML-as-a-service platform, for the following service-specific SLAs: for each service, at least 99% of requests must complete within the response-time threshold. Compared to a clairvoyant autoscaler that always satisfies the SLAs (i.e., even if there is a burst in the request rates), Swayam decreases resource utilization by up to 27%, while meeting the service-specific SLAs over 96% of the time during a three hour window. Microsoft Azure's Swayam-based framework was deployed in 2016 and has hosted over 100,000 services.;https://dl.acm.org/doi/abs/10.1145/3135974.3135993;icYIj9SImXkJ
Goli, M., Iwanski, L., Lawson, J., Dolinsky, U., & Richards, A. (2018, May). TensorFlow Acceleration on ARM Hikey Board. In Proceedings of the International Workshop on OpenCL (pp. 1-4).;7_edge_computing_deep_learning;2018;TensorFlow Acceleration on ARM Hikey Board;Mehdi Goli, Luke Iwanski, John Lawson, Uwe Dolinsky, Andrew Richards;Proceedings of the International Workshop on OpenCL, 1-4, 2018;There is huge demand for targeting complex and large-scale machine learning applications particularly those based on popular actively-maintained frameworks such as TensorFlow and CAFFE to a variety of platforms with accelerators ranging from high-end desktop GPUs to resource-constrained embedded or mobile GPUs, FPGAs, and DSPs. However, to deliver good performance different platforms may require different algorithms or data structures, yet code should be easily portable and reused as much as possible across different devices. The open SYCL standard addresses this by providing parallel processing through a single-source programming model enabling the same standard C++ code to be used on the CPU and accelerator. This allows high-level C++ abstractions and templates to be used to quickly configure device and host code to cover specific features of the platform. By targeting OpenCL, SYCL enables C++ applications such as TensorFlow to run efficiently on OpenCL devices without having to write OpenCL code.;https://dl.acm.org/doi/abs/10.1145/3204919.3204926;YZScNPhLRagJ
Cai, S., Breck, E., Nielsen, E., Salib, M., & Sculley, D. (2016). Tensorflow debugger: Debugging dataflow graphs for machine learning.;1_ml_machine_data_learning;2016;Tensorflow debugger: Debugging dataflow graphs for machine learning;Shanqing Cai, Eric Breck, Eric Nielsen, Michael Salib, D Sculley;-;"Debuggability is important in the development of machine-learning (ML) systems. Several widely-used ML libraries, such as TensorFlow and Theano, are based on dataflow graphs. While offering important benefits such as facilitating distributed training, the dataflow graph paradigm makes the debugging of model issues more challenging compared to debugging in the more conventional procedural paradigm. In this paper, we present the design of the TensorFlow Debugger (tfdbg), a specialized debugger for ML models written in TensorFlow. tfdbg provides features to inspect runtime dataflow graphs and the state of the intermediate graph elements ("" tensors""), as well as simulating stepping on the graph. We will discuss the application of this debugger in development and testing use cases.";http://research.google/pubs/pub45789/;aobCw5l6Q8YJ
Zhang, W., Feng, M., Zheng, Y., Ren, Y., Wang, Y., Liu, J., ... & Wang, F. (2017, November). Gadei: On scale-up training as a service for deep learning. In 2017 IEEE International Conference on Data Mining (ICDM) (pp. 1195-1200). IEEE.;7_edge_computing_deep_learning;2017;Gadei: On scale-up training as a service for deep learning;Wei Zhang, Minwei Feng, Yunhui Zheng, Yufei Ren, Yandong Wang, Ji Liu, Peng Liu, Bing Xiang, Li Zhang, Bowen Zhou, Fei Wang;2017 IEEE International Conference on Data Mining (ICDM), 1195-1200, 2017;Deep learning (DL) training-as-a-service (TaaS) is an important emerging industrial workload. TaaS must satisfy a wide range of customers who have no experience and/or resources to tune DL hyper-parameters (e.g., mini-batch size and learning rate), and meticulous tuning for each user's dataset is prohibitively expensive. Therefore, TaaS hyper-parameters must be fixed with values that are applicable to all users. Unfortunately, few research papers have studied how to design a system for TaaS workloads. By evaluating the IBM Watson Natural Language Classfier (NLC) workloads, the most popular IBM cognitive service used by thousands of enterprise-level clients globally, we provide empirical evidence that only the conservative hyper-parameter setup (e.g., small mini-batch size) can guarantee acceptable model accuracy for a wide range of customers. Unfortunately, smaller mini-batch size requires higher communication bandwidth in a parameter-server based DL training system. In this paper, we characterize the exceedingly high communication bandwidth requirement of TaaS using representative industrial deep learning workloads. We then present GaDei, a highly optimized shared-memory based scale-up parameter server design. We evaluate GaDei using both commercial benchmarks and public benchmarks and demonstrate that GaDei significantly outperforms the state-of-the-art parameter-server based implementation while maintaining the required accuracy. GaDei achieves near-best-possible runtime performance, constrained only by the hardware limitation. Furthermore, to the best of our knowledge, GaDei is the only scale-up DL system that provides fault-tolerance.;https://ieeexplore.ieee.org/abstract/document/8215624/;-nujDbjjbRYJ
Schelter, S., Böse, J. H., Kirschnick, J., Klein, T., & Seufert, S. (2018). Declarative metadata management: A missing piece in end-to-end machine learning.;1_ml_machine_data_learning;2018;Declarative metadata management: A missing piece in end-to-end machine learning;Sebastian Schelter, Joos-Hendrik BÃ¶se, Johannes Kirschnick, Thoralf Klein, Stephan Seufert;-;We argue for the necessity of managing the metadata and lineage of common artifacts in machine learning (ML). We discuss a recently presented lightweight system built for this task, which accelerates users in their ML workflows, and provides a basis for comparability and repeatability of ML experiments. This system tracks the lineage of produced artifacts in ML workloads and automatically extracts metadata such as hyperparameters of models, schemas of datasets and layouts of deep neural networks. It provides a general declarative representation of common ML artifacts, is integrated with popular frameworks such as MXNet, SparkML and scikit-learn, and meets the demands of various production use cases at Amazon.;https://www.amazon.science/publications/declarative-metadata-management-a-missing-piece-in-end-to-end-machine-learning;inJT4_R6V-YJ
Bhattacharjee, B., Boag, S., Doshi, C., Dube, P., Herta, B., Ishakian, V., ... & Zhang, L. (2017). IBM deep learning service. IBM Journal of Research and Development, 61(4/5), 10-1.;7_edge_computing_deep_learning;2017;IBM deep learning service;Bishwaranjan Bhattacharjee, Scott Boag, Chandani Doshi, Parijat Dube, Ben Herta, Vatche Ishakian, KR Jayaram, Rania Khalaf, Avesh Krishna, Yu Bo Li, Vinod Muthusamy, Ruchir Puri, Yufei Ren, Florian Rosenberg, Seetharami R Seelam, Yandong Wang, Jian Ming Zhang, Li Zhang;IBM Journal of Research and Development 61 (4/5), 10: 1-10: 11, 2017;Deep learning, driven by large neural network models, is overtaking traditional machine learning methods for understanding unstructured and perceptual data domains such as speech, text, and vision. At the same time, the “As-a-Service”-based business model for the cloud is fundamentally transforming the information technology industry. These two trends, deep learning and “As-a-Service,” are colliding to give rise to a new business model for cognitive application delivery: deep learning as a service in the cloud. In this paper, we discuss the details of the software architecture behind IBM's deep learning as a service (DLaaS). DLaaS provides developers the flexibility to use popular deep learning libraries—such as Caffe, Torch, and TensorFlow—in the cloud in a scalable and resilient manner with minimal effort. The platform uses a distribution and orchestration layer that facilitates learning from a large amount of data in a reasonable amount of time across compute nodes. A resource provisioning layer enables flexible job management on heterogeneous resources, such as graphics processing units and central processing units, in an infrastructure-as-a-service cloud.;https://ieeexplore.ieee.org/abstract/document/8030274/;2_G8yAiJx5oJ
Wang, Y., Zhang, L., Ren, Y., & Zhang, W. (2017, September). Nexus: bringing efficient and scalable training to deep learning frameworks. In 2017 IEEE 25th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS) (pp. 12-21). IEEE.;7_edge_computing_deep_learning;2017;Nexus: bringing efficient and scalable training to deep learning frameworks;Yandong Wang, Li Zhang, Yufei Ren, Wei Zhang;2017 IEEE 25th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS), 12-21, 2017;Demand is mounting in the industry for scalable GPU-based deep learning systems. Unfortunately, existing training applications built atop popular deep learning frameworks, including Caffe, Theano, and Torch, etc, are incapable of conducting distributed GPU training over large-scale clusters. To remedy such a situation, this paper presents Nexus, a platform that allows existing deep learning frameworks to easily scale out to multiple machines without sacrificing model accuracy. Nexus leverages recently proposed distributed parameter management architecture to orchestrate distributed training by a large number of learners spread across the cluster. Through characterizing the run-time behavior of existing single-node based applications, Nexus is equipped with a suite of optimization schemes, including hierarchical and hybrid parameter aggregation, enhanced network and computation layer, and quality-guided communication adjustment, etc, to strengthen the communication channels and resource utilization. Empirical evaluations with a diverse set of deep learning applications demonstrate that Nexus is easy to integrate and can deliver efficient distributed training services to major deep learning frameworks. In addition, Nexus's optimization schemes are highly effective to shorten the training time with targeted accuracy bounds.;https://ieeexplore.ieee.org/abstract/document/8107427/;V5eUyhe8YRQJ
Tsay, J., Mummert, T., Bobroff, N., Braz, A., Westerink, P., & Hirzel, M. (2018). Runway: machine learning model experiment management tool. In Conference on systems and machine learning (sysML).;1_ml_machine_data_learning;2018;Runway: machine learning model experiment management tool;Jason Tsay, Todd Mummert, Norman Bobroff, Alan Braz, Peter Westerink, Martin Hirzel;Conference on systems and machine learning (sysML), 2018;Runway is a cloud-native tool for managing machine learning experiments and their associated models. The iterative nature of developing models results in a large number of experiments and models that are often managed in an ad hoc manner. Runway is a workflow and framework independent tool that centrally manages and maintains metadata and links to artifacts needed to reproduce models and experiments. Runway provides a web dashboard with multiple levels of visualizations to evaluate performance and enable side-by-side comparisons of models and experiments.;https://www.researchgate.net/profile/Alan-Braz-2/publication/324415101_Runway_machine_learning_model_experiment_management_tool/links/5accd7980f7e9b1896543ae7/Runway-machine-learning-model-experiment-management-tool.pdf;mw-nSDAHKP4J
Breck, E., Cai, S., Nielsen, E., Salib, M., & Sculley, D. (2017, December). The ML test score: A rubric for ML production readiness and technical debt reduction. In 2017 IEEE International Conference on Big Data (Big Data) (pp. 1123-1132). IEEE.;1_ml_machine_data_learning;2017;The ML test score: A rubric for ML production readiness and technical debt reduction;Eric Breck, Shanqing Cai, Eric Nielsen, Michael Salib, D Sculley;2017 IEEE International Conference on Big Data (Big Data), 1123-1132, 2017;Creating reliable, production-level machine learning systems brings on a host of concerns not found in small toy examples or even large offline research experiments. Testing and monitoring are key considerations for ensuring the production-readiness of an ML system, and for reducing technical debt of ML systems. But it can be difficult to formulate specific tests, given that the actual prediction behavior of any given model is difficult to specify a priori. In this paper, we present 28 specific tests and monitoring needs, drawn from experience with a wide range of production ML systems to help quantify these issues and present an easy to follow road-map to improve production readiness and pay down ML technical debt.;https://ieeexplore.ieee.org/abstract/document/8258038/;AEU2RJyAo1QJ
Schelter, S., Lange, D., Schmidt, P., Celikel, M., Biessmann, F., & Grafberger, A. (2018). Automating large-scale data quality verification. Proceedings of the VLDB Endowment, 11(12), 1781-1794.;1_ml_machine_data_learning;2018;Automating large-scale data quality verification;Sebastian Schelter, Dustin Lange, Philipp Schmidt, Meltem Celikel, Felix Biessmann, Andreas Grafberger;Proceedings of the VLDB Endowment 11 (12), 1781-1794, 2018;Modern companies and institutions rely on data to guide every single business process and decision. Missing or incorrect information seriously compromises any decision process downstream. Therefore, a crucial, but tedious task for everyone involved in data processing is to verify the quality of their data. We present a system for automating the verification of data quality at scale, which meets the requirements of production use cases. Our system provides a declarative API, which combines common quality constraints with user-defined validation code, and thereby enables 'unit tests' for data. We efficiently execute the resulting constraint validation workload by translating it to aggregation queries on Apache Spark. Our platform supports the incremental validation of data quality on growing datasets, and leverages machine learning, e.g., for enhancing constraint suggestions, for estimating the 'predictability' of a column, and for detecting anomalies in historic data quality time series. We discuss our design decisions, describe the resulting system architecture, and present an experimental evaluation on various datasets.;https://dl.acm.org/doi/abs/10.14778/3229863.3229867;oMc8jzKASP4J
Burton, S., Gauerhof, L., & Heinzemann, C. (2017). Making the case for safety of machine learning in highly automated driving. In Computer Safety, Reliability, and Security: SAFECOMP 2017 Workshops, ASSURE, DECSoS, SASSUR, TELERISE, and TIPS, Trento, Italy, September 12, 2017, Proceedings 36 (pp. 5-16). Springer International Publishing.;2_safety_system_autonomous_vehicle;2017;Making the case for safety of machine learning in highly automated driving;Simon Burton, Lydia Gauerhof, Christian Heinzemann;Computer Safety, Reliability, and Security: SAFECOMP 2017 Workshops, ASSURE, DECSoS, SASSUR, TELERISE, and TIPS, Trento, Italy, September 12, 2017, Proceedings 36, 5-16, 2017;This paper describes the challenges involved in arguing the safety of highly automated driving functions which make use of machine learning techniques. An assurance case structure is used to highlight the systems engineering and validation considerations when applying machine learning methods for highly automated driving. Particular focus is placed on addressing functional insufficiencies in the perception functions based on convolutional neural networks and possible types of evidence that can be used to mitigate against such risks.;https://link.springer.com/chapter/10.1007/978-3-319-66284-8_1;jueW2copmTEJ
Delaye, E., Sirasao, A., Dudha, C., & Das, S. (2017, November). Deep learning challenges and solutions with xilinx fpgas. In 2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD) (pp. 908-913). IEEE.;7_edge_computing_deep_learning;2017;Deep learning challenges and solutions with xilinx fpgas;Elliott Delaye, Ashish Sirasao, Chaithanya Dudha, Sabya Das;2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), 908-913, 2017;In this paper, we will describe the architectural, software, performance, and implementation challenges and solutions and current research on the use of programmable logic to enable deep learning applications. First a discussion of characteristics of building a deep learning system will described. Next architectural choices will be explained for how a FPGA fabric can efficiently solve deep learning tasks. Finally specific techniques for how DSPs, memories and are used in high performance applications will be described.;https://ieeexplore.ieee.org/abstract/document/8203877/;W1fAzCFyn8AJ
Xiaojing, X. I. E., & Govardhan, S. S. (2020, May). A service mesh-based load balancing and task scheduling system for deep learning applications. In 2020 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing (CCGRID) (pp. 843-849). IEEE.;7_edge_computing_deep_learning;2020;A service mesh-based load balancing and task scheduling system for deep learning applications;XIE Xiaojing, Shyam S Govardhan;2020 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing (CCGRID), 843-849, 2020;In recent years, there are increasing technologies that benefit from cloud computing and edge computing, especially the application in the Deep Learning and Internet of Things topics. Functionalities such as load balancing, traffic routing, and task scheduling, which used to be part of the software, are now enabled and provided as API or microservices on the underlying infrastructure. Therefore, when we want to deliver a container application to the users either on the cloud cluster or edge cluster, developers do not need to worry about these issues and only focus on the development of the functionality. Nevertheless, for the deployment stage, the system architect should design the fundamental architecture to build an ecosystem for the application. In this paper, a practical approach is proposed to develop a flexible and on-demand system for deep learning applications based on container and service mesh technologies. Our experiment chooses Kubernetes as the container orchestration platform and introduces Istio to implement the service mesh. We packaged one Flask based Deep learning model into a Docker container, and successfully deploy and provision this image classifier application to the public users with Functionalities like load balancing and task scheduling. Furthermore, we empower the system with an evaluation of the resource utilization services, and traffic flows inside the Kubernetes cluster in a visualized manner.;https://ieeexplore.ieee.org/abstract/document/9139676/;yZse6cpKpaQJ
Serebryakov, S., Milojicic, D., Vassilieva, N., Fleischman, S., & Clark, R. D. (2019, November). Deep learning cookbook: recipes for your AI infrastructure and applications. In 2019 IEEE International Conference on Rebooting Computing (ICRC) (pp. 1-9). IEEE.;1_ml_machine_data_learning;2019;Deep learning cookbook: recipes for your AI infrastructure and applications;Sergey Serebryakov, Dejan Milojicic, Natalia Vassilieva, Stephen Fleischman, Robert D Clark;2019 IEEE International Conference on Rebooting Computing (ICRC), 1-9, 2019;Deep Learning (DL) has gained wide adoption and different DL models have been deployed for an expanding number of applications. It is being used both for inference at the edge and for training in datacenters. Applications include image recognition, video analytics, pattern recognition in networking traffic, and many others. Different applications rely on different neural network models, and it has proven difficult to predict resource requirements for different models and applications. This leads to the nonsystematic and suboptimal selection of computational resources for DL applications resulting in overpaying for underutilized infrastructure or, even worse, the deployment of models on underpowered hardware and missed service level objectives. In this paper we present the DL Cookbook, a toolset that a) helps with benchmarking models on different hardware, b) guides the use of DL and c) provides reference designs. Automated benchmarking collects performance data for different DL workloads (training and inference with different models) on various hardware and software configurations. A web-based tool guides a choice of optimal hardware and software configuration via analysis of collected performance data and applying performance models. And finally, it offers reference hardware/software stacks for particular classes of deep learning workloads. This way the DL Cookbook helps both customers and hardware vendors match optimal DL models to the available hardware and vice versa, in case of acquisition, specify required hardware to models in question. Finally, DL Cookbook helps with reproducibility of results.;https://ieeexplore.ieee.org/abstract/document/8914704/;1l0Qx8y4WLAJ
Li, J., Li, J., & Zhang, H. (2018, August). Deep learning based parking prediction on cloud platform. In 2018 4th International Conference on Big Data Computing and Communications (BIGCOM) (pp. 132-137). IEEE.;7_edge_computing_deep_learning;2018;Deep learning based parking prediction on cloud platform;Jiachang Li, Jiming Li, Haitao Zhang;2018 4th International Conference on Big Data Computing and Communications (BIGCOM), 132-137, 2018;"With the explosive growth of the urban population, an increasing number of cities face the parking problem. Studies shows that more than 30% of the urban traffic is due to these ""search berth"" traffic, which aggravate the traffic congestion and create more traffic pollutants. The prediction for parking availability plays more and more important role in the urban traffic management system. However, existing parking prediction methods have lower accuracy or require a large number of data and computation cost, which make it difficult for parking prediction to be widely used. In this paper, we propose a deep learning based parking prediction system architecture on cloud platform. The LSTM network is used to predict the availability of parking space, which performs well in time series prediction. In order to improve the prediction accuracy, we comprehensively take more factors into account such as time of day, weather and holiday. In addition, we also propose an economical workflow based on the elastic computing service (ECS) provided by cloud platform. In the optimized workflow, the model training and updating processes need not to be in running state all the time, which can reduce avoidable computation and cost significantly. We implement the proposed system and deploy it on the Ali Could platform. Experimental results show that our prediction model based on LSTM network outperforms BP neural network, and our system has good scalability to apply to the cloud platform.";https://ieeexplore.ieee.org/abstract/document/8488638/;sEy6Ig0tP_EJ
Brayford, D., Vallecorsa, S., Atanasov, A., Baruffa, F., & Riviera, W. (2019, September). Deploying AI frameworks on secure HPC systems with containers. In 2019 ieee high performance extreme computing conference (hpec) (pp. 1-6). IEEE.;1_ml_machine_data_learning;2019;Deploying AI frameworks on secure HPC systems with containers;David Brayford, Sofia Vallecorsa, Atanas Atanasov, Fabio Baruffa, Walter Riviera;2019 ieee high performance extreme computing conference (hpec), 1-6, 2019;The increasing interest in the usage of Artificial Intelligence (AI) techniques from the research community and industry to tackle “real world” problems, requires High Performance Computing (HPC) resources to efficiently compute and scale complex algorithms across thousands of nodes. Unfortunately, typical data scientists are not familiar with the unique requirements and characteristics of HPC environments. They usually develop their applications with high level scripting languages or frameworks such as TensorFlow and the installation processes often require connection to external systems to download open source software during the build. HPC environments, on the other hand, are often based on closed source applications that incorporate parallel and distributed computing API’s such as MPI and OpenMP, while users have restricted administrator privileges, and face security restrictions such as not allowing access to external systems. In this paper we discuss the issues associated with the deployment of AI frameworks in a secure HPC environment and how we successfully deploy AI frameworks on SuperMUC-NG with Charliecloud.;https://ieeexplore.ieee.org/abstract/document/8916576/;TyimjH_49MgJ
Tuli, S., Basumatary, N., & Buyya, R. (2019, November). Edgelens: Deep learning based object detection in integrated iot, fog and cloud computing environments. In 2019 4th International Conference on Information Systems and Computer Networks (ISCON) (pp. 496-502). IEEE.;7_edge_computing_deep_learning;2019;Edgelens: Deep learning based object detection in integrated iot, fog and cloud computing environments;Shreshth Tuli, Nipam Basumatary, Rajkumar Buyya;2019 4th International Conference on Information Systems and Computer Networks (ISCON), 496-502, 2019;Data-intensive applications are growing at an increasing rate and there is a growing need to solve scalability and high-performance issues in them. By the advent of Cloud computing paradigm, it became possible to harness remote resources to build and deploy these applications. In recent years, new set of applications and services based on Internet of Things (IoT) paradigm, require to process large amount of data in very less time. Among them surveillance and object detection have gained prime importance, but cloud is unable to bring down the network latencies to meet the response time requirements. This problem is solved by Fog computing which harnesses resources in the edge of the network along with remote cloud resources as required. However, there is still a lack of frameworks that are successfully able to integrate sophisticated software and applications, especially deep learning, with fog and cloud computing environments. In this work, we propose a framework to deploy deep learning-based applications in fog-cloud environments to harness edge and cloud resources to provide better service quality for such applications. Our proposed framework, called EdgeLens, adapts to the application or user requirements to provide high accuracy or low latency modes of services. We also tested the performance of the software in terms of accuracy, response time, jitter, network bandwidth and power consumption and show how EdgeLens adapts to different service requirements.;https://ieeexplore.ieee.org/abstract/document/9036216/;26A9nF9-Cf8J
John, M. M., Holmström Olsson, H., & Bosch, J. (2021). Architecting AI Deployment: A Systematic Review of State-of-the-art and State-of-practice Literature. In Software Business: 11th International Conference, ICSOB 2020, Karlskrona, Sweden, November 16–18, 2020, Proceedings 11 (pp. 14-29). Springer International Publishing.;7_edge_computing_deep_learning;2021;Architecting AI Deployment: A Systematic Review of State-of-the-art and State-of-practice Literature;Meenu Mary John, Helena Holmström Olsson, Jan Bosch;Software Business: 11th International Conference, ICSOB 2020, Karlskrona, Sweden, November 16–18, 2020, Proceedings 11, 14-29, 2021;Companies across domains are rapidly engaged in shifting computational power and intelligence from centralized cloud to fully decentralized edges to maximize value delivery, strengthen security and reduce latency. However, most companies have only recently started pursuing this opportunity and are therefore at the early stage of the cloud-to-edge transition. To provide an overview of AI deployment in the context of edge/cloud/hybrid architectures, we conduct a systematic literature review and a grey literature review. To advance understanding of how to integrate, deploy, operationalize and evolve AI models, we derive a framework from existing literature to accelerate the end-to-end deployment process. The framework is organized into five phases: Design, Integration, Deployment, Operation and Evolution. We make an attempt to analyze the extracted results by comparing and contrasting them to derive insights. The contribution of the paper is threefold. First, we conduct a systematic literature review in which we review the contemporary scientific literature and provide a detailed overview of the state-of-the-art of AI deployment. Second, we review the grey literature and present the state-of-practice and experience of practitioners while deploying AI models. Third, we present a framework derived from existing literature for the end-to-end deployment process and attempt to compare and contrast SLR and GLR results.;https://link.springer.com/chapter/10.1007/978-3-030-67292-8_2;KX20qprbNVoJ
Gupta, N., Anantharaj, K., & Subramani, K. (2020, January). Containerized architecture for edge computing in smart home: A consistent architecture for model deployment. In 2020 International Conference on Computer Communication and Informatics (ICCCI) (pp. 1-8). IEEE.;7_edge_computing_deep_learning;2020;Containerized architecture for edge computing in smart home: A consistent architecture for model deployment;Nitu Gupta, Katpagavalli Anantharaj, Karthikeyan Subramani;2020 International Conference on Computer Communication and Informatics (ICCCI), 1-8, 2020;Network bandwidth and high latency are the main bottlenecks of cloud computing. To combat such scenarios, new paradigm Edge Computing is used. Edge computing shifts the computation of resources from centralized cloud closer to the devices which generates data. Edge Computing reduces the response time, latency and improves the battery life while maintaining data safety and privacy. The distributed architecture of edge computing makes resource management an important aspect of edge computing. In such a resource constrained environment edge devices should be capable of processing all types of request coming from IoT devices. With the advancement in Data science and Machine learning models in different domains, many intelligent services have emerged to provide better user experience. These deep learning models have frequent updates to adapt to new hardware and software requirements. These models also have some installation dependencies and requirement of cross platform compatibility for training and prediction. In home edge environment, these issues are more important due to resource constraint in terms of memory and computing power. Also, due to the availability of extensive list of deep learning models for different service, it is difficult to maintain an environment supporting such models across various devices in the smart home to support all services. To solve this issue in smart home environment, this paper proposes architecture, using containerization techniques to deploy and manage the deep learning models. This paper also explains about the steps to convert the existing model into containers. Minimal space requirement on the edge device, data privacy, low latency along with device independence for deep learning models are prime benefits of the architecture proposed. To test the performance of the architecture, deep learning model was containerized and compared with the actual model deployed in the same edge environment. The experimental results demonstrated the performance is almost similar of the containerized architecture in terms of model execution time and CPU load vs execution time. But it comes with ease of model deployment and cross platform model execution.;https://ieeexplore.ieee.org/abstract/document/9104073/;i4J_cC_Ce8cJ
Cartas, A., Kocour, M., Raman, A., Leontiadis, I., Luque, J., Sastry, N., ... & Segura, C. (2019, March). A reality check on inference at mobile networks edge. In Proceedings of the 2nd International Workshop on Edge Systems, Analytics and Networking (pp. 54-59).;7_edge_computing_deep_learning;2019;A reality check on inference at mobile networks edge;Alejandro Cartas, Martin Kocour, Aravindh Raman, Ilias Leontiadis, Jordi Luque, Nishanth Sastry, Jose NuÃ±ez-Martinez, Diego Perino, Carlos Segura;Proceedings of the 2nd International Workshop on Edge Systems, Analytics and Networking, 54-59, 2019;Edge computing is considered a key enabler to deploy Artificial Intelligence platforms to provide real-time applications such as AR/VR or cognitive assistance. Previous works show computing capabilities deployed very close to the user can actually reduce the end-to-end latency of such interactive applications. Nonetheless, the main performance bottleneck remains in the machine learning inference operation. In this paper, we question some assumptions of these works, as the network location where edge computing is deployed, and considered software architectures within the framework of a couple of popular machine learning tasks. Our experimental evaluation shows that after performance tuning that leverages recent advances in deep learning algorithms and hardware, network latency is now the main bottleneck on end-to-end application performance. We also report that deploying computing capabilities at the first network node still provides latency reduction but, overall, it is not required by all applications. Based on our findings, we overview the requirements and sketch the design of an adaptive architecture for general machine learning inference across edge locations.;https://dl.acm.org/doi/abs/10.1145/3301418.3313946;9NccywocNxMJ
Zhang, X., Wang, Y., Lu, S., Liu, L., & Shi, W. (2019, July). OpenEI: An open framework for edge intelligence. In 2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS) (pp. 1840-1851). IEEE.;7_edge_computing_deep_learning;2019;OpenEI: An open framework for edge intelligence;Xingzhou Zhang, Yifan Wang, Sidi Lu, Liangkai Liu, Weisong Shi;2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS), 1840-1851, 2019;In the last five years, edge computing has attracted tremendous attention from industry and academia due to its promise to reduce latency, save bandwidth, improve availability, and protect data privacy to keep data secure. At the same time, we have witnessed the proliferation of AI algorithms and models which accelerate the successful deployment of intelligence mainly in cloud services. These two trends, combined together, have created a new horizon: Edge Intelligence (EI). The development of EI requires much attention from both the computer systems research community and the AI community to meet these demands. However, existing computing techniques used in the cloud are not applicable to edge computing directly due to the diversity of computing sources and the distribution of data sources. We envision that there missing a framework that can be rapidly deployed on edge and enable edge AI capabilities. To address this challenge, in this paper we first present the definition and a systematic review of EI. Then, we introduce an Open Framework for Edge Intelligence (OpenEI), which is a lightweight software platform to equip edges with intelligent processing and data sharing capability. We analyze four fundamental EI techniques which are used to build OpenEI and identify several open problems based on potential research directions. Finally, four typical application scenarios enabled by OpenEI are presented.;https://ieeexplore.ieee.org/abstract/document/8885335/;UgiUN3gUYpoJ
Rovnyagin, M. M., Timofeev, K. V., Elenkin, A. A., & Shipugin, V. A. (2019, January). Cloud computing architecture for high-volume ML-based solutions. In 2019 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus) (pp. 315-318). IEEE.;1_ml_machine_data_learning;2019;Cloud computing architecture for high-volume ML-based solutions;Mikhail M Rovnyagin, Kirill V Timofeev, Aleksandr A Elenkin, Vladislav A Shipugin;2019 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus), 315-318, 2019;A large number of modern projects use machine learning technology to perform a variety of business calculations. There are two main ways to integrate machine-learning models into the logic of industrial applications. The first way is to rewrite models from the data analysis language (for example R or Python) to the industrial development language (for example Java, Go or Scala). The second way is to equip models with a web-interface and integrate it into the calculation. In this article, we explore the second method. A deployment architecture for machine learning in the clouds is proposed. The possibilities of the proposed scheme for scaling are described. Examples of practical use of the proposed architecture for organizing data storage with compression are also given.;https://ieeexplore.ieee.org/abstract/document/8656765/;yFZyaKmGQ-8J
Pääkkönen, P., & Pakkala, D. (2020). Extending reference architecture of big data systems towards machine learning in edge computing environments. Journal of Big Data, 7(1), 1-29.;7_edge_computing_deep_learning;2020;Extending reference architecture of big data systems towards machine learning in edge computing environments;Pekka Pääkkönen, Daniel Pakkala;Journal of Big Data 7 (1), 1-29, 2020;Augmented reality, computer vision and other (e.g. network functions, Internet-of-Things (IoT)) use cases can be realised in edge computing environments with machine learning (ML) techniques. For realisation of the use cases, it has to be understood how data is collected, stored, processed, analysed, and visualised in big data systems. In order to provide services with low latency for end users, often utilisation of ML techniques has to be optimized. Also, software/service developers have to understand, how to develop and deploy ML models in edge computing environments. Therefore, architecture design of big data systems to edge computing environments may be challenging. The contribution of this paper is reference architecture (RA) design of a big data system utilising ML techniques in edge computing environments. An earlier version of the RA has been extended based on 16 realised implementation architectures, which have been developed to edge/distributed computing environments. Also, deployment of architectural elements in different environments is described. Finally, a system view is provided of the software engineering aspects of ML model development and deployment. The presented RA may facilitate concrete architecture design of use cases in edge computing environments. The value of RAs is reduction of development and maintenance costs of systems, reduction of risks, and facilitation of communication between different stakeholders.;https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00303-y;z1DyFwV-2rgJ
Li, Y., Han, Z., Zhang, Q., Li, Z., & Tan, H. (2020, July). Automating cloud deployment for deep learning inference of real-time online services. In IEEE INFOCOM 2020-IEEE Conference on Computer Communications (pp. 1668-1677). IEEE.;8_serverless_cloud_cost_computing;2020;Automating cloud deployment for deep learning inference of real-time online services;Yang Li, Zhenhua Han, Quanlu Zhang, Zhenhua Li, Haisheng Tan;IEEE INFOCOM 2020-IEEE Conference on Computer Communications, 1668-1677, 2020;Real-time online services using pre-trained deep neural network (DNN) models, e.g., Siri and Instagram, require low-latency and cost-efficiency for quality-of-service and commercial competitiveness. When deployed in a cloud environment, such services call for an appropriate selection of cloud configurations (i.e., specific types of VM instances), as well as a considerate device placement plan that places the operations of a DNN model to multiple computation devices like GPUs and CPUs. Currently, the deployment mainly relies on service providersâ€™ manual efforts, which is not only onerous but also far from satisfactory oftentimes (for a same service, a poor deployment can incur significantly more costs by tens of times). In this paper, we attempt to automate the cloud deployment for real-time online DNN inference with minimum costs under the constraint of acceptably low latency. This attempt is enabled by jointly leveraging the Bayesian Optimization and Deep Reinforcement Learning to adaptively unearth the (nearly) optimal cloud configuration and device placement with limited search time. We implement a prototype system of our solution based on TensorFlow and conduct extensive experiments on top of Microsoft Azure. The results show that our solution essentially outperforms the nontrivial baselines in terms of inference speed and cost-efficiency.;https://ieeexplore.ieee.org/abstract/document/9155267/;qSRu59G6HogJ
Hazelwood, K., Bird, S., Brooks, D., Chintala, S., Diril, U., Dzhulgakov, D., ... & Wang, X. (2018, February). Applied machine learning at facebook: A datacenter infrastructure perspective. In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA) (pp. 620-629). IEEE.;7_edge_computing_deep_learning;2018;Applied machine learning at facebook: A datacenter infrastructure perspective;Kim Hazelwood, Sarah Bird, David Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhulgakov, Mohamed Fawzy, Bill Jia, Yangqing Jia, Aditya Kalro, James Law, Kevin Lee, Jason Lu, Pieter Noordhuis, Misha Smelyanskiy, Liang Xiong, Xiaodong Wang;2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), 620-629, 2018;Machine learning sits at the core of many essential products and services at Facebook. This paper describes the hardware and software infrastructure that supports machine learning at global scale. Facebook's machine learning workloads are extremely diverse: services require many different types of models in practice. This diversity has implications at all layers in the system stack. In addition, a sizable fraction of all data stored at Facebook flows through machine learning pipelines, presenting significant challenges in delivering data to high-performance distributed training flows. Computational requirements are also intense, leveraging both GPU and CPU platforms for training and abundant CPU capacity for real-time inference. Addressing these and other emerging challenges continues to require diverse efforts that span machine learning algorithms, software, and hardware design.;https://ieeexplore.ieee.org/abstract/document/8327042/;M2mwOjd9CQQJ
Krishnamurthi, R., Maheshwari, R., & Gulati, R. (2019, August). Deploying deep learning models via iot deployment tools. In 2019 Twelfth International Conference on Contemporary Computing (IC3) (pp. 1-6). IEEE.;7_edge_computing_deep_learning;2019;Deploying deep learning models via iot deployment tools;Rajalakshmi Krishnamurthi, Raghav Maheshwari, Rishabh Gulati;2019 Twelfth International Conference on Contemporary Computing (IC3), 1-6, 2019;Deep Learning and Internet of Things (IoT) are some top fields of research in computer science now days. Many researches are going on to incorporate the best of both the fields and to collaborate them into one. Inspired from this, this paper works on using and creating an efficient Deep Learning model for colorizing the image and transport them to remote systems through IoT deployment tools. This paper develops two models, namely Alpha and Beta, for the colorization of the greyscale images. Efficient models are developed to lessen the loss rate to around 0.005. Then the paper uses tools like AWS Greengrass and Docker for the deployment of the model, thus combining Deep Learning with IoT deployment tools.;https://ieeexplore.ieee.org/abstract/document/8844946/;9psB8fAZRl8J
Bosch, J., Olsson, H. H., & Crnkovic, I. (2021). Engineering ai systems: A research agenda. Artificial Intelligence Paradigms for Smart Cyber-Physical Systems, 1-19.;1_ml_machine_data_learning;2021;Engineering ai systems: A research agenda;Jan Bosch, Helena HolmstrÃ¶m Olsson, Ivica Crnkovic;Artificial Intelligence Paradigms for Smart Cyber-Physical Systems, 1-19, 2021;Artificial intelligence (AI) and machine learning (ML) are increasingly broadly adopted in industry. However, based on well over a dozen case studies, we have learned that deploying industry-strength, production quality ML models in systems proves to be challenging. Companies experience challenges related to data quality, design methods and processes, performance of models as well as deployment and compliance. We learned that a new, structured engineering approach is required to construct and evolve systems that contain ML/DL components. In this chapter, the authors provide a conceptualization of the typical evolution patterns that companies experience when employing ML as well as an overview of the key problems experienced by the companies that they have studied. The main contribution of the chapter is a research agenda for AI engineering that provides an overview of the key engineering challenges surrounding ML solutions and an overview of open items that need to be addressed by the research community at large.;https://www.igi-global.com/chapter/engineering-ai-systems/266130;heAwDA5EsuAJ
Ruf, P., Madan, M., Reich, C., & Ould-Abdeslam, D. (2021). Demystifying mlops and presenting a recipe for the selection of open-source tools. Applied Sciences, 11(19), 8861.;1_ml_machine_data_learning;2021;Demystifying mlops and presenting a recipe for the selection of open-source tools;Philipp Ruf, Manav Madan, Christoph Reich, Djaffar Ould-Abdeslam;Applied Sciences 11 (19), 8861, 2021;Nowadays, machine learning projects have become more and more relevant to various real-world use cases. The success of complex Neural Network models depends upon many factors, as the requirement for structured and machine learning-centric project development management arises. Due to the multitude of tools available for different operational phases, responsibilities and requirements become more and more unclear. In this work, Machine Learning Operations (MLOps) technologies and tools for every part of the overall project pipeline, as well as involved roles, are examined and clearly defined. With the focus on the inter-connectivity of specific tools and comparison by well-selected requirements of MLOps, model performance, input data, and system quality metrics are briefly discussed. By identifying aspects of machine learning, which can be reused from project to project, open-source tools which help in specific parts of the pipeline, and possible combinations, an overview of support in MLOps is given. Deep learning has revolutionized the field of Image processing, and building an automated machine learning workflow for object detection is of great interest for many organizations. For this, a simple MLOps workflow for object detection with images is portrayed.;https://www.mdpi.com/2076-3417/11/19/8861;9_rPnizax5YJ
Granlund, T., Stirbu, V., & Mikkonen, T. (2021). Towards regulatory-compliant MLOps: Oravizio’s journey from a machine learning experiment to a deployed certified medical product. SN computer Science, 2(5), 342.;1_ml_machine_data_learning;2021;Towards regulatory-compliant MLOps: Oravizioâ€™s journey from a machine learning experiment to a deployed certified medical product;Tuomas Granlund, Vlad Stirbu, Tommi Mikkonen;SN computer Science 2 (5), 342, 2021;Agile software development embraces change and manifests working software over comprehensive documentation and responding to change over following a plan. The ability to continuously release software has enabled a development approach where experimental features are put to use, and, if they stand the test of real use, they remain in production. Examples of such features include machine learning (ML) models, which are usually pre-trained, but can still evolve in production. However, many domains require more plan-driven approach to avoid hazard to environment and humans, and to mitigate risks in the process. In this paper, we start by presenting continuous software engineering practices in a regulated context, and then apply the results to the emerging practice of MLOps, or continuous delivery of ML features. Furthermore, as a practical contribution, we present a case study regarding Oravizio, first CE-certified medical software for assessing the risks of joint replacement surgeries. Towards the end of the paper, we also reflect the Oravizio experiences to MLOps in regulatory context.;https://link.springer.com/article/10.1007/s42979-021-00726-1;9fo9ATOH1AkJ
Peticolas, D., Kirmayer, R., & Turaga, D. (2019, November). Mímir: Building and Deploying an ML Framework for Industrial IoT. In 2019 International Conference on Data Mining Workshops (ICDMW) (pp. 399-406). IEEE.;1_ml_machine_data_learning;2019;Mímir: Building and Deploying an ML Framework for Industrial IoT;Devon Peticolas, Russell Kirmayer, Deepak Turaga;2019 International Conference on Data Mining Workshops (ICDMW), 399-406, 2019;In this paper we describe Mímir, a production grade cloud and edge spanning ML framework for Industrial IoT applications. We first describe our infrastructure for optimized capture, streaming and multi-resolution storage of manufacturing data and its context. We then describe our workflow for scalable ML model training, validation, and deployment that leverages a manufacturing taxonomy and parameterized ML pipelines to determine the best metrics, hyper-parameters and models to use for a given task. We also discuss our design decisions on model deployment for real-time and batch data in the cloud and at the edge. Finally, we describe the use of the framework in building and deploying an application for Predictive Quality monitoring during a Plastics Extrusion manufacturing process.;https://ieeexplore.ieee.org/abstract/document/8955638/;6QEa77u3ywsJ
Richins, D., Doshi, D., Blackmore, M., Nair, A. T., Pathapati, N., Patel, A., ... & Reddi, V. J. (2020, February). Missing the forest for the trees: End-to-end ai application performance in edge data centers. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA) (pp. 515-528). IEEE.;7_edge_computing_deep_learning;2020;Missing the forest for the trees: End-to-end ai application performance in edge data centers;Daniel Richins, Dharmisha Doshi, Matthew Blackmore, Aswathy Thulaseedharan Nair, Neha Pathapati, Ankit Patel, Brainard Daguman, Daniel Dobrijalowski, Ramesh Illikkal, Kevin Long, David Zimmerman, Vijay Janapa Reddi;2020 IEEE International Symposium on High Performance Computer Architecture (HPCA), 515-528, 2020;"Artificial intelligence and machine learning are experiencing widespread adoption in the industry, academia, and even public consciousness. This has been driven by the rapid advances in the applications and accuracy of AI through increasingly complex algorithms and models; this, in turn, has spurred research into developing specialized hardware AI accelerators. The rapid pace of the advances makes it easy to miss the forest for the trees: they are often developed and evaluated in a vacuum without considering the full application environment in which they must eventually operate. In this paper, we deploy and characterize Face Recognition, an AI-centric edge video analytics application built using open source and widely adopted infrastructure and ML tools. We evaluate its holistic, end-to-end behavior in a production-size edge data center and reveal the ""AI tax"" for all the processing that is involved. Even though the application is built around state-of-the-art AI and ML algorithms, it relies heavily on pre-and post-processing code which must be executed on a general-purpose CPU. As AI-centric applications start to reap the acceleration promised by so many accelerators, we find they impose stresses on the underlying software infrastructure and the data center's capabilities: storage and network bandwidth become major bottlenecks with increasing AI acceleration. By not having to serve a wide variety of applications, we show that a purpose-built edge data center can be designed to accommodate the stresses of accelerated AI at 15% lower TCO than one derived from homogeneous servers and infrastructure. We also discuss how our conclusions generalize beyond Face Recognition as many AI-centric applications at the edge rely upon the same underlying software and hardware infrastructure.";https://ieeexplore.ieee.org/abstract/document/9065599/;Z1cUnqTJvysJ
Li, L. E., Chen, E., Hermann, J., Zhang, P., & Wang, L. (2017, July). Scaling machine learning as a service. In International Conference on Predictive Applications and APIs (pp. 14-29). PMLR.;1_ml_machine_data_learning;2017;Scaling Machine Learning as a Service;Li Erran Li, Eric Chen, Jeremy Hermann, Pusheng Zhang, Luming Wang;Proceedings of The 3rd International Conference on Predictive Applications and APIs, PMLR 67:14-29, 2017;Machine learning as a service (MLaaS) is imperative to the success of many companies as they need to gain business intelligence from big data. Building a scalable MLaaS for mission-critical and real-time applications is a very challenging problem. In this paper, we present the scalable MLaaS we built for Uber that operates globally. We focus on several scalability challenges. First, how to scale feature computation for many machine learning use cases. Second, how to build accurate models using global data and account for individual city or region characteristics. Third, how to enable scalable model deployment and real-time serving for hundreds of thousands of models across multiple data centers. Our technical solutions are the design and implementation of a scalable feature computing engine and feature store, a framework to manage and train a hierarchy of models as a single logical entity, and an automated one-click deployment system and scalable real-time serving service. ;http://proceedings.mlr.press/v67/li17a.html;Todo
Crankshaw, D., Wang, X., Zhou, G., Franklin, M. J., Gonzalez, J. E., & Stoica, I. (2017). Clipper: A {Low-Latency} online prediction serving system. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17) (pp. 613-627).;1_ml_machine_data_learning;2017;Clipper: A {Low-Latency} online prediction serving system;Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E Gonzalez, Ion Stoica;14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17), 613-627, 2017;Machine learning is being deployed in a growing number of applications which demand real-time, accurate, and robust predictions under heavy query load. However, most machine learning frameworks and systems only address model training and not deployment.;https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/crankshaw;zoZ_85jjlAgJ
Bernardi, L., Mavridis, T., & Estevez, P. (2019, July). 150 successful machine learning models: 6 lessons learned at booking. com. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining (pp. 1743-1751).;1_ml_machine_data_learning;2019;150 successful machine learning models: 6 lessons learned at booking. com;Lucas Bernardi, Themistoklis Mavridis, Pablo Estevez;Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, 1743-1751, 2019;Booking.com is the world's largest online travel agent where millions of guests find their accommodation and millions of accommodation providers list their properties including hotels, apartments, bed and breakfasts, guest houses, and more. During the last years we have applied Machine Learning to improve the experience of our customers and our business. While most of the Machine Learning literature focuses on the algorithmic or mathematical aspects of the field, not much has been published about how Machine Learning can deliver meaningful impact in an industrial environment where commercial gains are paramount. We conducted an analysis on about 150 successful customer facing applications of Machine Learning, developed by dozens of teams in Booking.com, exposed to hundreds of millions of users worldwide and validated through rigorous Randomized Controlled Trials. Following the phases of a Machine Learning project we describe our approach, the many challenges we found, and the lessons we learned while scaling up such a complex technology across our organization. Our main conclusion is that an iterative, hypothesis driven process, integrated with other disciplines was fundamental to build 150 successful products enabled by Machine Learning.;https://dl.acm.org/doi/abs/10.1145/3292500.3330744;qAW07VXlQKcJ
Yadwadkar, N. J., Romero, F., Li, Q., & Kozyrakis, C. (2019, May). A case for managed and model-less inference serving. In Proceedings of the Workshop on Hot Topics in Operating Systems (pp. 184-191).;1_ml_machine_data_learning;2019;A case for managed and model-less inference serving;Neeraja J Yadwadkar, Francisco Romero, Qian Li, Christos Kozyrakis;Proceedings of the Workshop on Hot Topics in Operating Systems, 184-191, 2019;The number of applications relying on inference from machine learning models, especially neural networks, is already large and expected to keep growing. For instance, Facebook applications issue tens-of-trillions of inference queries per day with varying performance, accuracy, and cost constraints. Unfortunately, today's inference serving systems are neither easy to use nor cost effective. Developers must manually match the performance, accuracy, and cost constraints of their applications to a large design space that includes decisions such as selecting the right model and model optimizations, selecting the right hardware architecture, selecting the right scale-out factor, and avoiding cold-start effects. These interacting decisions are difficult to make, especially when the application load varies over time, applications evolve over time, and the available resources vary over time.If we want an increasing number of applications to use machine learning, we must automate issues that affect ease-of-use, performance, and cost efficiency for both users and providers. Hence, we define and make the case for managed and model-less inference serving. In this paper, we identify and discuss open research directions to realize this vision.;https://dl.acm.org/doi/abs/10.1145/3317550.3321443;cMxycwsQ61QJ
Chahal, D., Ojha, R., Choudhury, S. R., & Nambiar, M. (2020, April). Migrating a recommendation system to cloud using ml workflow. In Companion of the ACM/SPEC International Conference on Performance Engineering (pp. 1-4).;1_ml_machine_data_learning;2020;Migrating a recommendation system to cloud using ml workflow;Dheeraj Chahal, Ravi Ojha, Sharod Roy Choudhury, Manoj Nambiar;Companion of the ACM/SPEC International Conference on Performance Engineering, 1-4, 2020;Inference is the production stage of machine learning workflow in which a trained model is used to infer or predict with real world data. A recommendation system improves customer experience by displaying most relevant items based on historical behavior of a customer. Machine learning models built for recommendation systems are deployed either on-premise or migrated to a cloud for inference in real time or batch. A recommendation system should be cost effective while honoring service level agreements (SLAs). In this work we discuss on-premise implementation of our recommendation system called iPrescribe. We show a methodology to migrate on-premise implementation of recommendation system to a cloud using ML workflow. We also present our study on performance of recommendation system model when deployed on different types of virtual instances.;https://dl.acm.org/doi/abs/10.1145/3375555.3384423;F4xyJs0GKCgJ
Choi, Y., Kim, Y., & Rhu, M. (2021, February). Lazy batching: An SLA-aware batching system for cloud machine learning inference. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA) (pp. 493-506). IEEE.;8_serverless_cloud_cost_computing;2021;Lazy batching: An SLA-aware batching system for cloud machine learning inference;Yujeong Choi, Yunseong Kim, Minsoo Rhu;2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), 493-506, 2021;In cloud ML inference systems, batching is an essential technique to increase throughput which helps optimize total-cost-of-ownership. Prior graph batching combines the individual DNN graphs into a single one, allowing multiple inputs to be concurrently executed in parallel. We observe that the coarse-grained graph batching becomes suboptimal in effectively handling the dynamic inference request traffic, leaving significant performance left on the table. This paper proposes LazyBatching, an SLA-aware batching system that considers both scheduling and batching in the granularity of individual graph nodes, rather than the entire graph for flexible batching. We show that LazyBatching can intelligently determine the set of nodes that can be efficiently batched together, achieving an average 15×, 1.5×, and 5.5 × improvement than graph batching in terms of average response time, throughput, and SLA satisfaction, respectively.;https://ieeexplore.ieee.org/abstract/document/9407131/;gd2P8lY1QnwJ
Zhang, J., Elnikety, S., Zarar, S., Gupta, A., & Garg, S. (2020). {Model-Switching}: Dealing with Fluctuating Workloads in {Machine-Learning-as-a-Service} Systems. In 12th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 20).;8_serverless_cloud_cost_computing;2020;{Model-Switching}: Dealing with Fluctuating Workloads in {Machine-Learning-as-a-Service} Systems;Jeff Zhang, Sameh Elnikety, Shuayb Zarar, Atul Gupta, Siddharth Garg;12th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 20), 2020;Machine learning (ML) based prediction models, and especially deep neural networks (DNNs) are increasingly being served in the cloud in order to provide fast and accurate inferences. However, existing service ML serving systems have trouble dealing with fluctuating workloads and either drop requests or significantly expand hardware resources in response to load spikes. In this paper, we introduce Model-Switching, a new approach to dealing with fluctuating workloads when serving DNN models. Motivated by the observation that end-users of ML primarily care about the accuracy of responses that are returned within the deadline (which we refer to as effective accuracy), we propose to switch from complex and highly accurate DNN models to simpler but less accurate models in the presence of load spikes. We show that the flexibility introduced by enabling online model switching provides higher effective accuracy in the presence of fluctuating workloads compared to serving using any single model. We implement Model-Switching within Clipper, a state-of-art DNN model serving system, and demonstrate its advantages over baseline approaches.;https://www.usenix.org/conference/hotcloud20/presentation/zhang;bii1_303sUgJ
Pääkkönen, P., Pakkala, D., Kiljander, J., & Sarala, R. (2020). Architecture for enabling edge inference via model transfer from cloud domain in a kubernetes environment. Future Internet, 13(1), 5.;7_edge_computing_deep_learning;2020;Architecture for enabling edge inference via model transfer from cloud domain in a kubernetes environment;Pekka Pääkkönen, Daniel Pakkala, Jussi Kiljander, Roope Sarala;Future Internet 13 (1), 5, 2020;The current approaches for energy consumption optimisation in buildings are mainly reactive or focus on scheduling of daily/weekly operation modes in heating. Machine Learning (ML)-based advanced control methods have been demonstrated to improve energy efficiency when compared to these traditional methods. However, placing of ML-based models close to the buildings is not straightforward. Firstly, edge-devices typically have lower capabilities in terms of processing power, memory, and storage, which may limit execution of ML-based inference at the edge. Secondly, associated building information should be kept private. Thirdly, network access may be limited for serving a large number of edge devices. The contribution of this paper is an architecture, which enables training of ML-based models for energy consumption prediction in private cloud domain, and transfer of the models to edge nodes for prediction in Kubernetes environment. Additionally, predictors at the edge nodes can be automatically updated without interrupting operation. Performance results with sensor-based devices (Raspberry Pi 4 and Jetson Nano) indicated that a satisfactory prediction latency (~7–9 s) can be achieved within the research context. However, model switching led to an increase in prediction latency (~9–13 s). Partial evaluation of a Reference Architecture for edge computing systems, which was used as a starting point for architecture design, may be considered as an additional contribution of the paper.;https://www.mdpi.com/1999-5903/13/1/5;SSaktkUMgUIJ
Nascimento, E., Nguyen-Duc, A., Sundbø, I., & Conte, T. (2020). Software engineering for artificial intelligence and machine learning software: A systematic literature review. arXiv preprint arXiv:2011.03751.;1_ml_machine_data_learning;2020;Software engineering for artificial intelligence and machine learning software: A systematic literature review;Elizamary Nascimento, Anh Nguyen-Duc, Ingrid SundbÃ¸, Tayana Conte;arXiv preprint arXiv:2011.03751, 2020;Artificial Intelligence (AI) or Machine Learning (ML) systems have been widely adopted as value propositions by companies in all industries in order to create or extend the services and products they offer. However, developing AI/ML systems has presented several engineering problems that are different from those that arise in, non-AI/ML software development. This study aims to investigate how software engineering (SE) has been applied in the development of AI/ML systems and identify challenges and practices that are applicable and determine whether they meet the needs of professionals. Also, we assessed whether these SE practices apply to different contexts, and in which areas they may be applicable. We conducted a systematic review of literature from 1990 to 2019 to (i) understand and summarize the current state of the art in this field and (ii) analyze its limitations and open challenges that will drive future research. Our results show these systems are developed on a lab context or a large company and followed a research-driven development process. The main challenges faced by professionals are in areas of testing, AI software quality, and data management. The contribution types of most of the proposed SE practices are guidelines, lessons learned, and tools.;https://arxiv.org/abs/2011.03751;QbmGMcjqyNoJ
de Souza Nascimento, E., Ahmed, I., Oliveira, E., Palheta, M. P., Steinmacher, I., & Conte, T. (2019, September). Understanding development process of machine learning systems: Challenges and solutions. In 2019 acm/ieee international symposium on empirical software engineering and measurement (esem) (pp. 1-6). IEEE.;1_ml_machine_data_learning;2019;Understanding development process of machine learning systems: Challenges and solutions;Elizamary de Souza Nascimento, Iftekhar Ahmed, Edson Oliveira, MÃ¡rcio Piedade Palheta, Igor Steinmacher, Tayana Conte;2019 acm/ieee international symposium on empirical software engineering and measurement (esem), 1-6, 2019;"Background
The number of Machine Learning (ML) systems developed in the industry is increasing rapidly. Since ML systems are different from traditional systems, these differences are clearly visible in different activities pertaining to ML systems software development process. These differences make the Software Engineering (SE) activities more challenging for ML systems because not only the behavior of the system is data dependent, but also the requirements are data dependent. In such scenario, how can Software Engineering better support the development of ML systems?
Aim
Our objective is twofold. First, better understand the process that developers use to build ML systems. Second, identify the main challenges that developers face, proposing ways to overcome these challenges.
Method
We conducted interviews with seven developers from three software small companies that develop ML systems. Based on the challenges uncovered, we proposed a set of checklists to support the developers. We assessed the checklists by using a focus group.
Results
We found that the ML systems development follow a 4-stage process in these companies. These stages are: understanding the problem, data handling, model building, and model monitoring. The main challenges faced by the developers are: identifying the clients' business metrics, lack of a defined development process, and designing the database structure. We have identified in the focus group that our proposed checklists provided support during identification of the client's business metrics and in increasing visibility of the progress of the project tasks.
Conclusions
Our research is an initial step towards supporting the development of ML systems, suggesting checklists that support developers in essential development tasks, and also serve as a basis for future research in the area.";https://ieeexplore.ieee.org/abstract/document/8870157/;T-_kkC5ut4kJ
Ishikawa, F., & Yoshioka, N. (2019, May). How do engineers perceive difficulties in engineering of machine-learning systems?-questionnaire survey. In 2019 IEEE/ACM Joint 7th International Workshop on Conducting Empirical Studies in Industry (CESI) and 6th International Workshop on Software Engineering Research and Industrial Practice (SER&IP) (pp. 2-9). IEEE.;1_ml_machine_data_learning;2019;How do engineers perceive difficulties in engineering of machine-learning systems?-questionnaire survey;Fuyuki Ishikawa, Nobukazu Yoshioka;2019 IEEE/ACM Joint 7th International Workshop on Conducting Empirical Studies in Industry (CESI) and 6th International Workshop on Software Engineering Research and Industrial …, 2019;There is increasing interest in machine learning (ML) techniques and their applications in recent years. Although there has been intensive support by frameworks and libraries for the implementation of ML-based systems, investigation into engineering disciplines and methods is still at the early phase. The most pressing issue in this field is identifying the essential challenges for the software engineering research community as engineering of ML-based systems requires novel approaches due to the essentially different nature of ML-based systems. In this paper, we analyze the results of a questionnaire administered to 278 people who have worked on ML-based systems in practice, clarify the essential difficulties and their causes as perceived by practitioners, and suggest potential research directions.;https://ieeexplore.ieee.org/abstract/document/8836142/;UJsHRNpdPtAJ
Munappy, A., Bosch, J., Olsson, H. H., Arpteg, A., & Brinne, B. (2019, August). Data management challenges for deep learning. In 2019 45th Euromicro Conference on Software Engineering and Advanced Applications (SEAA) (pp. 140-147). IEEE.;1_ml_machine_data_learning;2019;Data management challenges for deep learning;Aiswarya Munappy, Jan Bosch, Helena Holmström Olsson, Anders Arpteg, Björn Brinne;2019 45th Euromicro Conference on Software Engineering and Advanced Applications (SEAA);Deep learning is one of the most exciting and fast-growing techniques in Artificial Intelligence. The unique capacity of deep learning models to automatically learn patterns from the data differentiates it from other machine learning techniques. Deep learning is responsible for a significant number of recent breakthroughs in AI. However, deep learning models are highly dependent on the underlying data. So, consistency, accuracy, and completeness of data is essential for a deep learning model. Thus, data management principles and practices need to be adopted throughout the development process of deep learning models. The objective of this study is to identify and categorise data management challenges faced by practitioners in different stages of end-to-end development. In this paper, a case study approach is employed to explore the data management issues faced by practitioners across various domains when they use real-world data for training and deploying deep learning models. Our case study is intended to provide valuable insights to the deep learning community as well as for data scientists to guide discussion and future research in applied deep learning with real-world data.;https://ieeexplore.ieee.org/abstract/document/8906736;
Vakkuri, V., Kemell, K. K., & Abrahamsson, P. (2019, November). Implementing ethics in AI: initial results of an industrial multiple case study. In International Conference on Product-Focused Software Process Improvement (pp. 331-338). Cham: Springer International Publishing.;12_ai_ethical_ethic_intelligence;2019;Implementing ethics in AI: initial results of an industrial multiple case study;Ville Vakkuri, Kai-Kristian Kemell, Pekka Abrahamsson;International Conference on Product-Focused Software Process Improvement, 331-338, 2019;Artificial intelligence (AI) is becoming increasingly widespread in system development endeavors. As AI systems affect various stakeholders due to their unique nature, the growing influence of these systems calls for ethical considerations. Academic discussion and practical examples of autonomous system failures have highlighted the need for implementing ethics in software development. However, research on methods and tools for implementing ethics into AI system design and development in practice is still lacking. This paper begins to address this focal problem by providing elements needed for producing a baseline for ethics in AI based software development. We do so by means of an industrial multiple case study on AI systems development in the healthcare sector. Using a research model based on extant, conceptual AI ethics literature, we explore the current state of practice out on the field in the absence of formal methods and tools for ethically aligned design.;https://link.springer.com/chapter/10.1007/978-3-030-35333-9_24;pw7Q6FiFmHYJ
Vakkuri, V., & Kemell, K. K. (2019). Implementing AI ethics in practice: An empirical evaluation of the RESOLVEDD strategy. In Software Business: 10th International Conference, ICSOB 2019, Jyväskylä, Finland, November 18–20, 2019, Proceedings 10 (pp. 260-275). Springer International Publishing.;12_ai_ethical_ethic_intelligence;2019;Implementing AI ethics in practice: An empirical evaluation of the RESOLVEDD strategy;Ville Vakkuri, Kai-Kristian Kemell;Software Business: 10th International Conference, ICSOB 2019, Jyväskylä, Finland, November 18–20, 2019, Proceedings 10, 260-275, 2019;As Artificial Intelligence (AI) systems exert a growing influence on society, real-life incidents begin to underline the importance of AI Ethics. Though calls for more ethical AI systems have been voiced by scholars and the general public alike, few empirical studies on the topic exist. Similarly, few tools and methods designed for implementing AI ethics into practice currently exist. To provide empirical data into this on-going discussion, we empirically evaluate an existing method from the field of business ethics, the RESOLVEDD strategy, in the context of ethical system development. We evaluated RESOLVEDD by means of a multiple case study of five student projects where its use was given as one of the design requirements for the projects. One of our key findings is that, even though the use of the ethical method was forced upon the participants, its utilization nonetheless facilitated of ethical consideration in the projects. Specifically, it resulted in the developers displaying more responsibility, even though the use of the tool did not stem from intrinsic motivation.;https://link.springer.com/chapter/10.1007/978-3-030-33742-1_21;I2DKFC4m9w0J
Chen, M., Knapp, A., Pohl, M., & Dietmayer, K. (2018, June). Taming functional deficiencies of automated driving systems: A methodology framework toward safety validation. In 2018 IEEE Intelligent Vehicles Symposium (IV) (pp. 1918-1924). IEEE.;2_safety_system_autonomous_vehicle;2018;Taming functional deficiencies of automated driving systems: A methodology framework toward safety validation;Meng Chen, Andreas Knapp, Martin Pohl, Klaus Dietmayer;2018 IEEE Intelligent Vehicles Symposium (IV), 1918-1924, 2018;Safety is one of the key aspects of road vehicles. With applications of machine learning and artificial intelligence (AI) technologies, driver assistance and automated driving systems have been rapidly developed. This paper identifies one of the emerging safety issues of automated driving systems: functional deficiencies resulting from limited sensing abilities and algorithmic performance. Safety validation problem and challenges for some methodologies provided by ISO 26262 are addressed. To this end, we provide a methodology framework for identifying functional deficiencies during system development. A novel methodology based on possibility theory and a fuzzy relation model, Causal Scenario Analysis (CSA), is introduced as one essential part in this framework. A traffic light handling case study is presented.;https://ieeexplore.ieee.org/abstract/document/8500679/;ZyhP81TBpbIJ
Mattos, D. I., Bosch, J., & Olsson, H. H. (2017, August). Your system gets better every day you use it: towards automated continuous experimentation. In 2017 43rd Euromicro Conference on Software Engineering and Advanced Applications (SEAA) (pp. 256-265). IEEE.;1_ml_machine_data_learning;2017;Your system gets better every day you use it: towards automated continuous experimentation;David Issa Mattos, Jan Bosch, Helena HolmstrÃ¶m Olsson;2017 43rd Euromicro Conference on Software Engineering and Advanced Applications (SEAA), 256-265, 2017;Innovation and optimization in software systems can occur from pre-development to post-deployment stages. Companies are increasingly reporting the use of experiments with customers in their systems in the post-deployment stage. Experiments with customers and users are can lead to a significant learning and return-on-investment. Experiments are used for both validation of manual hypothesis testing and feature optimization, linked to business goals. Automated experimentation refers to having the system controlling and running the experiments, opposed to having the R&D organization in control. Currently, there are no systematic approaches that combine manual hypothesis validation and optimization in automated experiments. This paper presents concepts related to automated experimentation, as controlled experiments, machine learning and software architectures for adaptation. However, this paper focuses on how architectural aspects that can contribute to support automated experimentation. A case study using an autonomous system is used to demonstrate the developed initial architecture framework. The contributions of this paper are threefold. First, it identifies software architecture qualities to support automated experimentation. Second, it develops an initial architecture framework that supports automated experiments and validates the framework with an autonomous mobile robot. Third, it identifies key research challenges that need to be addressed to support further development of automated experimentation.;https://ieeexplore.ieee.org/abstract/document/8051357/;EUCb7yfyfR4J
Zhang, Y., Zhang, T., Jia, Y., Sun, J., Xu, F., & Xu, W. (2017, May). DataLab: Introducing software engineering thinking into data science education at scale. In 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering Education and Training Track (ICSE-SEET) (pp. 47-56). IEEE.;9_data_science_software_process;2017;DataLab: Introducing software engineering thinking into data science education at scale;Yang Zhang, Tingjian Zhang, Yongzheng Jia, Jiao Sun, Fangzhou Xu, Wei Xu;2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering Education and Training Track (ICSE-SEET), 47-56, 2017;Data science education is a new area in computer science that has attracted increasing attention in recent years. However, currently, data science educators lack good tools and methodologies. In particular, they lack integrated tools through which their students can acquire hands-on software engineering experience. To address these problems, we designed and implemented DataLab, a web-based tool for data science education that integrates code, data and execution management into one system. The goal of DataLab is to provide a hands-on online lab environment to train students to have basic software engineering thinking and habits while maintaining a focus on the core data science contents. In this paper, we present the user-experience design and system-level implementation of DataLab. Further, we evaluate DataLab's performance through an in-classroom use case. Finally, using objective log-based learning behavior analysis and a subjective survey, we demonstrate DataLab's effectiveness.;https://ieeexplore.ieee.org/abstract/document/7964329/;TcATcXyz2CgJ
Hill, C., Bellamy, R., Erickson, T., & Burnett, M. (2016, September). Trials and tribulations of developers of intelligent systems: A field study. In 2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC) (pp. 162-170). IEEE.;1_ml_machine_data_learning;2016;Trials and tribulations of developers of intelligent systems: A field study;Charles Hill, Rachel Bellamy, Thomas Erickson, Margaret Burnett;2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC), 162-170, 2016;Intelligent systems are gaining in popularity and receiving increased media attention, but little is known about how people actually go about developing them. In this paper, we attempt to fill this gap through a set of field interviews that investigate how people develop intelligent systems that incorporate machine learning algorithms. The developers we interviewed were experienced at working with machine learning algorithms and dealing with the large amounts of data needed to develop intelligent systems. Despite their level of experience, we learned that they struggle to establish a repeatable process. They described problems with each step of the processes they perform, as well as cross-cutting issues that pervade multiple steps of their processes. The unique difficulties that developers like these face seem to point to a need for software engineering advances that address such machine learning systems, and we conclude by discussing this need and some of its implications.;https://ieeexplore.ieee.org/abstract/document/7739680/;P62U_alZ3gkJ
Shcherbakov, M., Shcherbakova, N., Brebels, A., Janovsky, T., & Kamaev, V. (2014). Lean data science research life cycle: A concept for data analysis software development. In Knowledge-Based Software Engineering: 11th Joint Conference, JCKBSE 2014, Volgograd, Russia, September 17-20, 2014. Proceedings 11 (pp. 708-716). Springer International Publishing.;9_data_science_software_process;2014;Lean data science research life cycle: A concept for data analysis software development;Maxim Shcherbakov, Nataliya Shcherbakova, Adriaan Brebels, Timur Janovsky, Valery Kamaev;Knowledge-Based Software Engineering: 11th Joint Conference, JCKBSE 2014, Volgograd, Russia, September 17-20, 2014. Proceedings 11, 708-716, 2014;Data Science is a new study that combines computer science, data mining, data engineering and software development. Based on the concept of lean software development we propose an idea of lean data science research as a technology for data analysis software development. This concept includes the mandatory stages of the life cycle that meet the lean manufacturing principles. We have defined the business understanding stage with defining the targeted questions, the set of lean data analysis sprints and a decision support stage. Each lean data analysis sprint contents of the task statement step, a step of data integration, a step of data analysis and the interpretation of the results. This approach allows to build data analysis software with iterative improvement quality of the results. Some case study have been suggested as examples of the proposed concept.;https://link.springer.com/chapter/10.1007/978-3-319-11854-3_61;R17nwvWup5EJ
Thung, F., Wang, S., Lo, D., & Jiang, L. (2012, November). An empirical study of bugs in machine learning systems. In 2012 IEEE 23rd International Symposium on Software Reliability Engineering (pp. 271-280). IEEE.;1_ml_machine_data_learning;2012;An empirical study of bugs in machine learning systems;Ferdian Thung, Shaowei Wang, David Lo, Lingxiao Jiang;2012 IEEE 23rd International Symposium on Software Reliability Engineering, 271-280, 2012;Many machine learning systems that include various data mining, information retrieval, and natural language processing code and libraries are used in real world applications. Search engines, internet advertising systems, product recommendation systems are sample users of these algorithm-intensive code and libraries. Machine learning code and toolkits have also been used in many recent studies on software mining and analytics that aim to automate various software engineering tasks. With the increasing number of important applications of machine learning systems, the reliability of such systems is also becoming increasingly important. A necessary step for ensuring reliability of such systems is to understand the features and characteristics of bugs occurred in the systems. A number of studies have investigated bugs and fixes in various software systems, but none focuses on machine learning systems. Machine learning systems are unique due to their algorithm-intensive nature and applications to potentially large-scale data, and thus deserve a special consideration. In this study, we fill the research gap by performing an empirical study on the bugs in machine learning systems. We analyze three systems, Apache Mahout, Lucene, and OpenNLP, which are data mining, information retrieval, and natural language processing tools respectively. We look into their bug databases and code repositories, analyze a sample set of bugs and corresponding fixes, and label the bugs into various categories. Our study finds that 22.6% of the bugs belong to the algorithm/method category, 15.6% of the bugs belong to the non-functional category, and 13% of the bugs belong to the assignment/initialization category. We also report the relationship between bug categories and bug severities, the time and effort needed to fix the bugs, and bug impacts. We highlight several bug categories that deserve attention in future research.;https://ieeexplore.ieee.org/abstract/document/6405375/;WCnELNFZyhoJ
Martin, C., & Schreckenghost, D. (2004). Beyond the Prototype: The Design Evolution of a Deployed AI System.;1_ml_machine_data_learning;2004;Beyond the Prototype: The Design Evolution of a Deployed AI System;Cheryl Martin, Debra Schreckenghost;-;This paper describes the evolution, from prototype to deployment, of the software engineering approach for a personal assistant agent-based system. This discussion is presented as a case study, relating experiences and lessons learned from our work with the Distributed Collaboration and Interaction (DCI) environment. Our development of this system is based on the spiral software engineering methodology, which incorporates iterative cycles of improved design, implementation, and evaluation. We first describe the techniques we used to bootstrap the implementation of this large, distributed AI system, and then we describe how we accommodated more advanced requirements as the system matured.;https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=512a34e90d1a9f0b452381d399718f83349d8161;j4vL0cGNh5UJ
Kouba, Z., Lažansky, J., Marik, V., Quirchmayr, G., Retschitzegger, W., Vlcek, T., & Wagner, R. (1992). Software development for cim—a case study. Annual Review in Automatic Programming, 16, 11-19.;1_ml_machine_data_learning;1992;Software development for cim—a case study;Z Kouba, J Lažansky, V Marik, G Quirchmayr, W Retschitzegger, T Vlcek, R Wagner;Annual Review in Automatic Programming 16, 11-19, 1992;The paper describes a CIM system designed to meet the needs of a Czechoslovak industrial company. After a brief analysis of the particular system, general considerations on software development and rapid prototyping are presented. Software engineering and artificial intelligence points of view are discussed.;https://www.sciencedirect.com/science/article/pii/0066413892900038;EdlwyF6wF1kJ
Guo, Q., Chen, S., Xie, X., Ma, L., Hu, Q., Liu, H., ... & Li, X. (2019, November). An empirical study towards characterizing deep learning development and deployment across different frameworks and platforms. In 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE) (pp. 810-822). IEEE.;4_dl_testing_deep_network;2019;An empirical study towards characterizing deep learning development and deployment across different frameworks and platforms;Qianyu Guo, Sen Chen, Xiaofei Xie, Lei Ma, Qiang Hu, Hongtao Liu, Yang Liu, Jianjun Zhao, Xiaohong Li;2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE), 810-822, 2019;Deep Learning (DL) has recently achieved tremendous success. A variety of DL frameworks and platforms play a key role to catalyze such progress. However, the differences in architecture designs and implementations of existing frameworks and platforms bring new challenges for DL software development and deployment. Till now, there is no study on how various mainstream frameworks and platforms influence both DL software development and deployment in practice. To fill this gap, we take the first step towards understanding how the most widely-used DL frameworks and platforms support the DL software development and deployment. We conduct a systematic study on these frameworks and platforms by using two types of DNN architectures and three popular datasets. (1) For development process, we investigate the prediction accuracy under the same runtime training configuration or same model weights/biases. We also study the adversarial robustness of trained models by leveraging the existing adversarial attack techniques. The experimental results show that the computing differences across frameworks could result in an obvious prediction accuracy decline, which should draw the attention of DL developers. (2) For deployment process, we investigate the prediction accuracy and performance (refers to time cost and memory consumption) when the trained models are migrated/quantized from PC to real mobile devices and web browsers. The DL platform study unveils that the migration and quantization still suffer from compatibility and reliability issues. Meanwhile, we find several DL software bugs by using the results as a benchmark. We further validate the results through bug confirmation from stakeholders and industrial positive feedback to highlight the implications of our study. Through our study, we summarize practical guidelines, identify challenges and pinpoint new research directions, such as understanding the characteristics of DL frameworks and platforms, avoiding compatibility and reliability issues, detecting DL software bugs, and reducing time cost and memory consumption towards developing and deploying high quality DL systems effectively.;https://ieeexplore.ieee.org/abstract/document/8952401/;7Ds0WK8ar5wJ
Ghofrani, J., Kozegar, E., Bozorgmehr, A., & Soorati, M. D. (2019, September). Reusability in artificial neural networks: An empirical study. In Proceedings of the 23rd International Systems and Software Product Line Conference-Volume B (pp. 122-129).;1_ml_machine_data_learning;2019;Reusability in artificial neural networks: An empirical study;Javad Ghofrani, Ehsan Kozegar, Arezoo Bozorgmehr, Mohammad Divband Soorati;SPLC '19: Proceedings of the 23rd International Systems and Software Product Line Conference - Volume BSeptember 2019Pages 122–129;Machine learning, especially deep learning has aroused interests of researchers and practitioners for the last few years in development of intelligent systems such as speech, natural language, and image processing. Software solutions based on machine learning techniques attract more attention as alternatives to conventional software systems. In this paper, we investigate how reusability techniques are applied in implementation of artificial neural networks (ANNs). We conducted an empirical study with an online survey among experts with experience in developing solutions with ANNs. We analyze the feedback of more than 100 experts to our survey. The results show existing challenges and some of the applied solutions in an intersection between reusability and ANNs.;https://dl.acm.org/doi/abs/10.1145/3307630.3342419;TODO
Damasceno, L., Werneck, V. M. B., & Schots, M. (2018, May). Metric-based evaluation of multiagent systems models. In Proceedings of the 10th International Workshop on Modelling in Software Engineering (pp. 67-74).;1_ml_machine_data_learning;2018;Metric-based evaluation of multiagent systems models;Lidiane Damasceno, Vera Maria B Werneck, Marcelo Schots;Proceedings of the 10th International Workshop on Modelling in Software Engineering, 67-74, 2018;The use of Multiagent Systems (MAS) has been increasing over the years due to their capacity of dealing with problems in a variety of domains. The modeling of such systems is not trivial: besides the knowledge and skills on agent-oriented software engineering and a basic understanding of the target domain to be modeled, it demands familiarity with agent-oriented modeling methodologies. This is not always the case, though, especially for newcomers in the field. This work proposes a set of guidelines in the form of a questionnaire for the evaluation of MAS models, aiming at supporting the verification of their quality. The questionnaire is built upon the results of a systematic mapping conducted to identify how MAS models have been evaluated and what metrics have been used. The proposed guidelines were evaluated through (i) a peer review by experts and (ii) its actual application by graduate students applying modeling methodologies in the context of Guardian Angel (GA), a patient-centered health system that automatically supports patients suffering from chronic diseases. Participants provided an overall positive feedback and proposed some improvements on the questionnaire, most of which were promptly incorporated.;https://dl.acm.org/doi/abs/10.1145/3193954.3193960;NJQU702_wogJ
Kim, J., Feldt, R., & Yoo, S. (2019, May). Guiding deep learning system testing using surprise adequacy. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) (pp. 1039-1049). IEEE.;4_dl_testing_deep_network;2019;Guiding deep learning system testing using surprise adequacy;Jinhan Kim, Robert Feldt, Shin Yoo;2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 1039-1049, 2019;Deep Learning (DL) systems are rapidly being adopted in safety and security critical domains, urgently calling for ways to test their correctness and robustness. Testing of DL systems has traditionally relied on manual collection and labelling of data. Recently, a number of coverage criteria based on neuron activation values have been proposed. These criteria essentially count the number of neurons whose activation during the execution of a DL system satisfied certain properties, such as being above predefined thresholds. However, existing coverage criteria are not sufficiently fine grained to capture subtle behaviours exhibited by DL systems. Moreover, evaluations have focused on showing correlation between adversarial examples and proposed criteria rather than evaluating and guiding their use for actual testing of DL systems. We propose a novel test adequacy criterion for testing of DL systems, called Surprise Adequacy for Deep Learning Systems (SADL), which is based on the behaviour of DL systems with respect to their training data. We measure the surprise of an input as the difference in DL system's behaviour between the input and the training data (i.e., what was learnt during training), and subsequently develop this as an adequacy criterion: a good test input should be sufficiently but not overtly surprising compared to training data. Empirical evaluation using a range of DL systems from simple image classifiers to autonomous driving car platforms shows that systematic sampling of inputs based on their surprise can improve classification accuracy of DL systems against adversarial examples by up to 77.5% via retraining.;https://ieeexplore.ieee.org/abstract/document/8812069/;W49xkvO14c4J
Vassev, E., Hinchey, M., Gaudin, B., & Nixon, P. (2011, May). Requirements and initial model for knowlang: a language for knowledge representation in autonomic service-component ensembles. In Proceedings of The Fourth International C* Conference on Computer Science and Software Engineering (pp. 35-42).;1_ml_machine_data_learning;2011;Requirements and initial model for knowlang: a language for knowledge representation in autonomic service-component ensembles;Emil Vassev, Mike Hinchey, Benoit Gaudin, Paddy Nixon;Proceedings of The Fourth International C* Conference on Computer Science and Software Engineering, 35-42, 2011;Autonomic Service-Component Ensembles (ASCENS) is a class of multi-agent systems formed as mobile, intelligent and open-ended swarms of special autonomic service components capable of local and distributed reasoning. Such components encapsulate rules, constraints and mechanisms for self-adaptation and acquire and process knowledge about themselves, other service components and their environment. ASCENS systems pose distinct challenges for knowledge representation languages. In this paper, we present requirements and an initial model for such a language called KnowLang. KnowLang is intended to provide for formal specification of distinct knowledge models each representing a different knowledge domain of an ASCENS system, such as the internal world of a service component, the world of a service-component ensemble, the surrounding external world and information of special situations related to state changes and operations of service components. KnowLang provides the necessary constructs and mechanisms for specifying such knowledge models at two main levels -- an ontology level and a logic-foundations level, where the latter is formed by special facts, rules, constraints and inter-ontology operators. In this paper, we also survey one of the ASCENS case studies to derive some of the requirements for KnowLang.;https://dl.acm.org/doi/abs/10.1145/1992896.1992901;8h2ZnZRsyIQJ
Aggarwal, A., Lohia, P., Nagar, S., Dey, K., & Saha, D. (2019, August). Black box fairness testing of machine learning models. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 625-635).;6_fairness_discrimination_bias_decision;2019;Black box fairness testing of machine learning models;Aniya Aggarwal, Pranay Lohia, Seema Nagar, Kuntal Dey, Diptikalyan Saha;Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 625-635, 2019;Any given AI system cannot be accepted unless its trustworthiness is proven. An important characteristic of a trustworthy AI system is the absence of algorithmic bias. 'Individual discrimination' exists when a given individual different from another only in 'protected attributes' (e.g., age, gender, race, etc.) receives a different decision outcome from a given machine learning (ML) model as compared to the other individual. The current work addresses the problem of detecting the presence of individual discrimination in given ML models. Detection of individual discrimination is test-intensive in a black-box setting, which is not feasible for non-trivial systems. We propose a methodology for auto-generation of test inputs, for the task of detecting individual discrimination. Our approach combines two well-established techniques - symbolic execution and local explainability for effective test case generation. We empirically show that our approach to generate test cases is very effective as compared to the best-known benchmark systems that we examine.;https://dl.acm.org/doi/abs/10.1145/3338906.3338937;xuo68P6VtboJ
Barash, G., Farchi, E., Jayaraman, I., Raz, O., Tzoref-Brill, R., & Zalmanovici, M. (2019, August). Bridging the gap between ml solutions and their business requirements using feature interactions. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 1048-1058).;1_ml_machine_data_learning;2019;Bridging the gap between ml solutions and their business requirements using feature interactions;Guy Barash, Eitan Farchi, Ilan Jayaraman, Orna Raz, Rachel Tzoref-Brill, Marcel Zalmanovici;Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 1048-1058, 2019;Machine Learning (ML) based solutions are becoming increasingly popular and pervasive. When testing such solutions, there is a tendency to focus on improving the ML metrics such as the F1-score and accuracy at the expense of ensuring business value and correctness by covering business requirements. In this work, we adapt test planning methods of classical software to ML solutions. We use combinatorial modeling methodology to define the space of business requirements and map it to the ML solution data, and use the notion of data slices to identify the weaker areas of the ML solution and strengthen them. We apply our approach to three real-world case studies and demonstrate its value.;https://dl.acm.org/doi/abs/10.1145/3338906.3340442;YFmc7l__vVgJ
Li, Z., Ma, X., Xu, C., Cao, C., Xu, J., & Lü, J. (2019, August). Boosting operational DNN testing efficiency through conditioning. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 499-509).;4_dl_testing_deep_network;2019;Boosting operational DNN testing efficiency through conditioning;Zenan Li, Xiaoxing Ma, Chang Xu, Chun Cao, Jingwei Xu, Jian LÃ¼;Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 499-509, 2019;With the increasing adoption of Deep Neural Network (DNN) models as integral parts of software systems, efficient operational testing of DNNs is much in demand to ensure these models' actual performance in field conditions. A challenge is that the testing often needs to produce precise results with a very limited budget for labeling data collected in field.Viewing software testing as a practice of reliability estimation through statistical sampling, we re-interpret the idea behind conventional structural coverages as conditioning for variance reduction. With this insight we propose an efficient DNN testing method based on the conditioning on the representation learned by the DNN model under testing. The representation is defined by the probability distribution of the output of neurons in the last hidden layer of the model. To sample from this high dimensional distribution in which the operational data are sparsely distributed, we design an algorithm leveraging cross entropy minimization.Experiments with various DNN models and datasets were conducted to evaluate the general efficiency of the approach. The results show that, compared with simple random sampling, this approach requires only about a half of labeled inputs to achieve the same level of precision.;https://dl.acm.org/doi/abs/10.1145/3338906.3338930;hgK1f8h_FGcJ
Chakravarty, A. (2010, April). Stress testing an AI based web service: A case study. In 2010 Seventh International Conference on Information Technology: New Generations (pp. 1004-1008). IEEE.;1_ml_machine_data_learning;2010;Stress testing an AI based web service: A case study;Anand Chakravarty;2010 Seventh International Conference on Information Technology: New Generations, 1004-1008, 2010;The stress testing of AI-based systems differs from the approach taken for more traditional Web services, both in terms of the design of test cases and the metrics used to measure quality. The expected variability in responses of an AI-based system to the same request adds a level of complexity to stress testing, when compared to more standard systems where the system response is deterministic and any deviations may easily be characterized as product defects. Generating test cases for AI-based systems requires balancing breadth of test cases with depth of response quality: most AI-systems may not return a perfect answer. An example of a machine learning translation system is considered, and the approach used for stress testing it is presented, alongside comparisons with a more traditional approach. The challenges of shipping such a system to support a growing set of features and language pairs necessitate a mature prioritization of test cases. This approach has been successful in shipping a Web service that currently serves millions of users per day.;https://ieeexplore.ieee.org/abstract/document/5501500/;Y-wqtyT_41AJ
Kim, M., Zimmermann, T., DeLine, R., & Begel, A. (2016, May). The emerging role of data scientists on software development teams. In Proceedings of the 38th International Conference on Software Engineering (pp. 96-107).;9_data_science_software_process;2016;The emerging role of data scientists on software development teams;Miryung Kim, Thomas Zimmermann, Robert DeLine, Andrew Begel;Proceedings of the 38th International Conference on Software Engineering, 96-107, 2016;"Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their missions in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists: (1) Insight Providers, who work with engineers to collect the data needed to inform decisions that managers make; (2) Modeling Specialists, who use their machine learning expertise to build predictive models; (3) Platform Builders, who create data platforms, balancing both engineering and data analysis concerns; (4) Polymaths, who do all data science activities themselves; and (5) Team Leaders, who run teams of data scientists and spread best practices. We further describe a set of strategies that they employ to increase the impact and actionability of their work.";https://dl.acm.org/doi/abs/10.1145/2884781.2884783;Emnv0E1wb_MJ
Motoda, H. (1990). The current status of expert system development and related technologies in Japan. IEEE Intelligent Systems, 5(04), 3-11.;1_ml_machine_data_learning;1990;The current status of expert system development and related technologies in Japan;Hiroshi Motoda;IEEE Intelligent Systems 5 (04), 3-11, 1990;General trends and statistics obtained from responses to a questionnaire are reviewed, showing that more than 400 expert systems are now under development or in practical use in Japan. Three expert systems selected as representative examples from among planning-and-design-system candidates are described: IBM Japan's Scheplan, Kayaba's OHCS, and Hitachi's automatic pipe-routing system for a power plant. Current Japanese research in related methodologies, including knowledge compilation, knowledge acquisition, generic methods for building expert systems, the automatic generation of task-oriented expert systems, and machine learning, is examined. It is noted that AI is spreading into almost every Japanese industry, and Japanese companies are seeking the most efficient way of introducing AI technologies, particularly expert systems.;https://www.computer.org/csdl/magazine/ex/1990/04/x4003/13rRUx0gemh;9HRH3B_zM2oJ
Kusmenko, E., Pavlitskaya, S., Rumpe, B., & Stüber, S. (2019, November). On the engineering of AI-powered systems. In 2019 34th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW) (pp. 126-133). IEEE.;4_dl_testing_deep_network;2019;On the engineering of AI-powered systems;Evgeny Kusmenko, Svetlana Pavlitskaya, Bernhard Rumpe, Sebastian StÃ¼ber;2019 34th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW), 126-133, 2019;More and more tasks become solvable using deep learning technology nowadays. Consequently, the amount of neural network code in software rises continuously. To make the new paradigm more accessible, frameworks, languages, and tools keep emerging. Although, the maturity of these tools is steadily increasing, we still lack appropriate domain specific languages and a high degree of automation when it comes to deep learning for productive systems. In this paper we present a multi-paradigm language family allowing the AI engineer to model and train deep neural networks as well as to integrate them into software architectures containing classical code. Using input and output layers as strictly typed interfaces enables a seamless embedding of neural networks into component-based models. The lifecycle of deep learning components can then be governed by a compiler accordingly, e.g. detecting when (re-)training is necessary or when network weights can be shared between different network instances. We provide a compelling case study, where we train an autonomous vehicle for the TORCS simulator. Furthermore, we discuss how the methodology automates the AI development process if neural networks are changed or added to the system.;https://ieeexplore.ieee.org/abstract/document/8967413/;qniZpOdYGNYJ
Alshangiti, M., Sapkota, H., Murukannaiah, P. K., Liu, X., & Yu, Q. (2019, September). Why is developing machine learning applications challenging? a study on stack overflow posts. In 2019 acm/ieee international symposium on empirical software engineering and measurement (esem) (pp. 1-11). IEEE.;1_ml_machine_data_learning;2019;Why is developing machine learning applications challenging? a study on stack overflow posts;Moayad Alshangiti, Hitesh Sapkota, Pradeep K Murukannaiah, Xumin Liu, Qi Yu;2019 acm/ieee international symposium on empirical software engineering and measurement (esem), 1-11, 2019;"Background
As smart and automated applications pervade our lives, an increasing number of software developers are required to incorporate machine learning (ML) techniques into application development. However, acquiring the ML skill set can be nontrivial for software developers owing to both the breadth and depth of the ML domain.
Aims
We seek to understand the challenges developers face in the process of ML application development and offer insights to simplify the process. Despite its importance, there has been little research on this topic. A few existing studies on development challenges with ML are outdated, small scale, or they do no involve a representative set of developers.
Method
We conduct an empirical study of ML-related developer posts on Stack Overflow. We perform in-depth quantitative and qualitative analyses focusing on a series of research questions related to the challenges of developing ML applications and the directions to address them.
Results
Our findings include: (1) ML questions suffer from a much higher percentage of unanswered questions on Stack Overflow than other domains; (2) there is a lack of ML experts in the Stack Overflow QA community; (3) the data preprocessing and model deployment phases are where most of the challenges lay; and (4) addressing most of these challenges require more ML implementation knowledge than ML conceptual knowledge.
Conclusions
Our findings suggest that most challenges are under the data preparation and model deployment phases, i.e., early and late stages. Also, the implementation aspect of ML shows much higher difficulty level among developers than the conceptual aspect.";https://ieeexplore.ieee.org/abstract/document/8870187/;aQ4shbXvD6wJ
Li, C., & Li, K. (2011, December). A practical framework for agent-based hybrid intelligent systems. In 2011 Seventh International Conference on Computational Intelligence and Security (pp. 199-203). IEEE.;1_ml_machine_data_learning;2011;A practical framework for agent-based hybrid intelligent systems;Chunsheng Li, Kan Li;2011 Seventh International Conference on Computational Intelligence and Security, 199-203, 2011;The design and development of hybrid intelligent systems are difficult because there are many interactions between various components. Existing software development techniques cannot manage those complex interactions efficiently as those interactions may occur at unpredictable times, for unpredictable reasons, between unpredictable components. In this paper, we contribute a multi-agent framework to organize those components and interactions. The framework consists of user interface, decision making, knowledge discovering, information management facilitators, and distributed heterogeneous data resources. We employ the middle agent concept to match task requesters with specific intelligent agents. We demonstrate the potentials of the framework by case study and present theoretical and empirical evidence that our framework is available and robust.;https://ieeexplore.ieee.org/abstract/document/6128105/;Hc4NKxPUKF8J
Washizaki, H., Uchida, H., Khomh, F., & Guéhéneuc, Y. G. (2019, December). Studying software engineering patterns for designing machine learning systems. In 2019 10th International Workshop on Empirical Software Engineering in Practice (IWESEP) (pp. 49-495). IEEE.;1_ml_machine_data_learning;2019;Studying software engineering patterns for designing machine learning systems;Hironori Washizaki, Hiromu Uchida, Foutse Khomh, Yann-Gaël Guéhéneuc;2019 10th International Workshop on Empirical Software Engineering in Practice (IWESEP), 49-495, 2019;Machine-learning (ML) techniques are becoming more prevalent. ML techniques rely on mathematics and software engineering. Researchers and practitioners studying best practices strive to design ML systems and software that address software complexity and quality issues. Such design practices are often formalized as architecture and design patterns by encapsulating reusable solutions to common problems within given contexts. However, a systematic study to collect, classify, and discuss these software-engineering (SE) design patterns for ML techniques have yet to be reported. Our research collects good/bad SE design patterns for ML techniques to provide developers with a comprehensive classification of such patterns. Herein we report the preliminary results of a systematic-literature review (SLR) of good/bad design patterns for ML.;https://ieeexplore.ieee.org/abstract/document/8945075/;bzLYE1EZBVgJ
John, M. M., Olsson, H. H., & Bosch, J. (2020, December). Ai deployment architecture: Multi-case study for key factor identification. In 2020 27th Asia-Pacific Software Engineering Conference (APSEC) (pp. 395-404). IEEE.;1_ml_machine_data_learning;2020;Ai deployment architecture: Multi-case study for key factor identification;Meenu Mary John, Helena HolmstrÃ¶m Olsson, Jan Bosch;2020 27th Asia-Pacific Software Engineering Conference (APSEC), 395-404, 2020;Machine learning and deep learning techniques are becoming increasingly popular and critical for companies as part of their systems. However, although the development and prototyping of ML/DL systems are common across companies, the transition from prototype to production-quality deployment models are challenging. One of the key challenges is how to determine the selection of an optimal architecture for AI deployment. Based on our previous research, and to offer support and guidance to practitioners, we developed a framework in which we present five architectural alternatives for AI deployment ranging from centralized to fully decentralized edge architectures. As part of our research, we validated the framework in software-intensive embedded system companies and identified key challenges they face when deploying ML/DL models. In this paper, and to further advance our research on this topic, we identify factors that help practitioners determine what architecture to select for the ML/D L model deployment. For this, we conducted a follow-up study involving interviews and workshops in seven case companies in the embedded systems domain. Based on our findings, we identify three key factors and develop a framework in which we outline how prioritization and trade-offs between these results in certain architecture. The contribution of the paper is threefold. First, we identify key factors critical for AI system deployment. Second, we present the architecture selection framework that explains how prioritization and trade-offs between key factors result in the selection of a certain architecture. Third, we discuss additional factors that mayor may not influence the selection of an optimal architecture.;https://ieeexplore.ieee.org/abstract/document/9359253/;jglRSnq1heEJ
Arpteg, A., Brinne, B., Crnkovic-Friis, L., & Bosch, J. (2018, August). Software engineering challenges of deep learning. In 2018 44th euromicro conference on software engineering and advanced applications (SEAA) (pp. 50-59). IEEE.;4_dl_testing_deep_network;2018;Software engineering challenges of deep learning;Anders Arpteg, BjÃ¶rn Brinne, Luka Crnkovic-Friis, Jan Bosch;2018 44th euromicro conference on software engineering and advanced applications (SEAA), 50-59, 2018;Surprisingly promising results have been achieved by deep learning (DL) systems in recent years. Many of these achievements have been reached in academic settings, or by large technology companies with highly skilled research groups and advanced supporting infrastructure. For companies without large research groups or advanced infrastructure, building high-quality production-ready systems with DL components has proven challenging. There is a clear lack of well-functioning tools and best practices for building DL systems. It is the goal of this research to identify what the main challenges are, by applying an interpretive research approach in close collaboration with companies of varying size and type. A set of seven projects have been selected to describe the potential with this new technology and to identify associated main challenges. A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges. Furthermore, a mapping between the challenges and the projects is defined, together with selected motivating descriptions of how and why the challenges apply to specific projects. Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of high-quality systems. The challenges identified in this paper can be used to guide future research by the software engineering and DL communities. Together, we could enable a large number of companies to start taking advantage of the high potential of the DL technology.;https://ieeexplore.ieee.org/abstract/document/8498185/;inCJnff_hTAJ
Sculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner, D., ... & Dennison, D. (2015). Hidden technical debt in machine learning systems. Advances in neural information processing systems, 28.;1_ml_machine_data_learning;2015;Hidden technical debt in machine learning systems;David Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, Dan Dennison;Advances in neural information processing systems 28, 2015;Machine learning offers a fantastically powerful toolkit for building useful complexprediction systems quickly. This paper argues it is dangerous to think ofthese quick wins as coming for free. Using the software engineering frameworkof technical debt, we find it is common to incur massive ongoing maintenancecosts in real-world ML systems. We explore several ML-specific risk factors toaccount for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configurationissues, changes in the external world, and a variety of system-level anti-patterns.;https://proceedings.neurips.cc/paper/5656-hidden-technical-debt-in-machine-learning-sy;BU2pKLq2Sx8J
Selsam, D., Liang, P., & Dill, D. L. (2017, July). Developing bug-free machine learning systems with formal mathematics. In International Conference on Machine Learning (pp. 3047-3056). PMLR.;2_safety_system_autonomous_vehicle;2017;Developing bug-free machine learning systems with formal mathematics;Daniel Selsam, Percy Liang, David L Dill;International Conference on Machine Learning, 3047-3056, 2017;Noisy data, non-convex objectives, model misspecification, and numerical instability can all cause undesired behaviors in machine learning systems. As a result, detecting actual implementation errors can be extremely difficult. We demonstrate a methodology in which developers use an interactive proof assistant to both implement their system and to state a formal theorem defining what it means for their system to be correct. The process of proving this theorem interactively in the proof assistant exposes all implementation errors since any error in the program would cause the proof to fail. As a case study, we implement a new system, Certigrad, for optimizing over stochastic computation graphs, and we generate a formal (ie machine-checkable) proof that the gradients sampled by the system are unbiased estimates of the true mathematical gradients. We train a variational autoencoder using Certigrad and find the performance comparable to training the same model in TensorFlow.;http://proceedings.mlr.press/v70/selsam17a.html;1lEjWA1Fuv0J
Yokoyama, H. (2019, March). Machine learning system architectural pattern for improving operational stability. In 2019 IEEE International Conference on Software Architecture Companion (ICSA-C) (pp. 267-274). IEEE.;1_ml_machine_data_learning;2019;Machine learning system architectural pattern for improving operational stability;Haruki Yokoyama;2019 IEEE International Conference on Software Architecture Companion (ICSA-C), 267-274, 2019;Recently, machine learning systems with inference engines have been widely used for a variety of purposes (such as prediction and classification) in our society. While it is quite important to keep the services provided by these machine learning systems stable, maintaining stability can be difficult given the nature of machine learning systems whose behaviors can be determined by program codes and input data. Therefore, quick troubleshooting (problem localization, rollback, etc.) is necessary. However, common machine learning systems with three-layer architectural patterns complicate the troubleshooting process because of their tightly coupled functions (e.g., business logic coded from design and inference engine derived from data). To solve the problem, we propose a novel architectural pattern for machine learning systems in which components for business logic and components for machine learning are separated. This architectural pattern helps operators break down the failures into a business logic part and a ML-specific part, and they can rollback the inference engine independent of the business logic when the inference engine has some problems. Through a practical case study scenario, we will show how our architectural pattern can make troubleshooting easier than common three-layer architecture.;https://ieeexplore.ieee.org/abstract/document/8712157/;ssPYwst3X_UJ
Braiek, H. B., & Khomh, F. (2020). On testing machine learning programs. Journal of Systems and Software, 164, 110542.;1_ml_machine_data_learning;2020;On testing machine learning programs;Houssem Ben Braiek, Foutse Khomh;Journal of Systems and Software 164, 110542, 2020;Nowadays, we are witnessing a wide adoption of Machine learning (ML) models in many software systems. They are even being tested in safety-critical systems, thanks to recent breakthroughs in deep learning and reinforcement learning. Many people are now interacting with systems based on ML every day, e.g., voice recognition systems used by virtual personal assistants like Amazon Alexa or Google Home. As the field of ML continues to grow, we are likely to witness transformative advances in a wide range of areas, from finance, energy, to health and transportation. Given this growing importance of ML-based systems in our daily life, it is becoming utterly important to ensure their reliability. Recently, software researchers have started adapting concepts from the software testing domain (e.g., code coverage, mutation testing, or property-based testing) to help ML engineers detect and correct faults in ML programs. This paper reviews current existing testing practices for ML programs. First, we identify and explain challenges that should be addressed when testing ML programs. Next, we report existing solutions found in the literature for testing ML programs. Finally, we identify gaps in the literature related to the testing of ML programs and make recommendations of future research directions for the scientific community. We hope that this comprehensive review of software testing practices will help ML engineers identify the right approach to improve the reliability of their ML-based systems. We also hope that the research community will act on our proposed research directions to advance the state of the art of testing for ML programs.;https://www.sciencedirect.com/science/article/pii/S0164121220300248;hi6eCAxf14IJ
Goodfellow, I., & Papernot, N. (2017). The challenge of verification and testing of machine learning. Cleverhans-blog.;5_adversarial_attack_example_model;2017;The challenge of verification and testing of machine learning;Ian Goodfellow, Nicolas Papernot;Cleverhans-blog, 2017;In our second post, we gave some background explaining why attacking machine learning is often easier than defending it. We saw some of the reasons why we do not yet have completely effective defenses against adversarial examples, and we speculated about whether we can ever expect such a defense.In this post, we explore the types of guarantees one can expect a machine learning model to possess. We argue that the limitations of existing defenses point to the lack of verification of machine learning models. Indeed, to design reliable systems, engineers typically engage in both testing and verification:;http://www.cleverhans.io/security/privacy/ml/2017/06/14/verification.html;3IEWgZ_d4BgJ
Bryson, J., & Winfield, A. (2017). Standardizing ethical design for artificial intelligence and autonomous systems. Computer, 50(5), 116-119.;12_ai_ethical_ethic_intelligence;2017;Standardizing ethical design for artificial intelligence and autonomous systems;Joanna Bryson, Alan Winfield;Computer 50 (5), 116-119, 2017;AI is here now, available to anyone with access to digital technology and the Internet. But its consequences for our social order aren't well understood. How can we guide the way technology impacts society?;https://ieeexplore.ieee.org/abstract/document/7924235/;9hT5uSUyxQoJ
Hannay, J. E., MacLeod, C., Singer, J., Langtangen, H. P., Pfahl, D., & Wilson, G. (2009, May). How do scientists develop and use scientific software?. In 2009 ICSE workshop on software engineering for computational science and engineering (pp. 1-8). Ieee.;9_data_science_software_process;2009;How do scientists develop and use scientific software?;Jo Erskine Hannay, Carolyn MacLeod, Janice Singer, Hans Petter Langtangen, Dietmar Pfahl, Greg Wilson;2009 ICSE workshop on software engineering for computational science and engineering, 1-8, 2009;"New knowledge in science and engineering relies increasingly on results produced by scientific software. Therefore, knowing how scientists develop and use software in their research is critical to assessing the necessity for improving current development practices and to making decisions about the future allocation of resources. To that end, this paper presents the results of a survey conducted online in October-December 2008 which received almost 2000 responses. Our main conclusions are that (1) the knowledge required to develop and use scientific software is primarily acquired from peers and through self-study, rather than from formal education and training; (2) the number of scientists using supercomputers is small compared to the number using desktop or intermediate computers; (3) most scientists rely primarily on software with a large user base; (4) while many scientists believe that software testing is important, a smaller number believe they have sufficient understanding about testing concepts; and (5) that there is a tendency for scientists to rank standard software engineering concepts higher if they work in large software development projects and teams, but that there is no uniform trend of association between rank of importance of software engineering concepts and project/team size.";https://ieeexplore.ieee.org/abstract/document/5069155/;e_AmslVY1RcJ
Ding, J., Kang, X., & Hu, X. H. (2017, May). Validating a deep learning framework by metamorphic testing. In 2017 IEEE/ACM 2nd International Workshop on Metamorphic Testing (MET) (pp. 28-34). IEEE.;4_dl_testing_deep_network;2017;Validating a deep learning framework by metamorphic testing;Junhua Ding, Xiaojun Kang, Xin-Hua Hu;2017 IEEE/ACM 2nd International Workshop on Metamorphic Testing (MET), 28-34, 2017;Deep learning has become an important tool for image classification and natural language processing. However, the effectiveness of deep learning is highly dependent on the quality of the training data as well as the net model for the learning. The training data set for deep learning normally is fairly large, and the net model is pretty complex. It is necessary to validate the deep learning framework including the net model, executing environment, and training data set before it is used for any applications. In this paper, we propose an approach for validating the classification accuracy of a deep learning framework that includes a convolutional neural network, a deep learning executing environment, and a massive image data set. The framework is first validated with a classifier built on support vector machine, and then it is tested using a metamorphic validation approach. The effectiveness of the approach is demonstrated by validating a deep learning classifier for automated classification of biology cell images. The proposed approach can be used for validating other deep learning framework for different applications.;https://ieeexplore.ieee.org/abstract/document/7961649/;EK4_ebMaTNgJ
Kim, M., Zimmermann, T., DeLine, R., & Begel, A. (2017). Data scientists in software teams: State of the art and challenges. IEEE Transactions on Software Engineering, 44(11), 1024-1038.;9_data_science_software_process;2017;Data scientists in software teams: State of the art and challenges;Miryung Kim, Thomas Zimmermann, Robert DeLine, Andrew Begel;IEEE Transactions on Software Engineering ( Volume: 44, Issue: 11, 01 November 2018) ;The demand for analyzing large scale telemetry, machine, and quality data is rapidly increasing in software industry. Data scientists are becoming popular within software teams, e.g., Facebook, LinkedIn and Microsoft are creating a new career path for data scientists. In this paper, we present a large-scale survey with 793 professional data scientists at Microsoft to understand their educational background, problem topics that they work on, tool usages, and activities. We cluster these data scientists based on the time spent for various activities and identify 9 distinct clusters of data scientists, and their corresponding characteristics. We also discuss the challenges that they face and the best practices they share with other data scientists. Our study finds several trends about data scientists in the software engineering context at Microsoft, and should inform managers on how to leverage data science capability effectively within their teams.;https://ieeexplore.ieee.org/abstract/document/8046093;Todo
Varshney, K. R. (2016, January). Engineering safety in machine learning. In 2016 Information Theory and Applications Workshop (ITA) (pp. 1-5). IEEE.;2_safety_system_autonomous_vehicle;2016;Engineering safety in machine learning;Kush R Varshney;2016 Information Theory and Applications Workshop (ITA), 1-5, 2016;Machine learning algorithms are increasingly influencing our decisions and interacting with us in all parts of our daily lives. Therefore, just like for power plants, highways, and myriad other engineered sociotechnical systems, we must consider the safety of systems involving machine learning. In this paper, we first discuss the definition of safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. Then we examine dimensions, such as the choice of cost function and the appropriateness of minimizing the empirical average training cost, along which certain real-world applications may not be completely amenable to the foundational principle of modern statistical machine learning: empirical risk minimization. In particular, we note an emerging dichotomy of applications: ones in which safety is important and risk minimization is not the complete story (we name these Type A applications), and ones in which safety is not so critical and risk minimization is sufficient (we name these Type B applications). Finally, we discuss how four different strategies for achieving safety in engineering (inherently safe design, safety reserves, safe fail, and procedural safeguards) can be mapped to the machine learning context through interpretability and causality of predictive models, objectives beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software.;https://ieeexplore.ieee.org/abstract/document/7888195/;aKx5n8Xg6AMJ
Pei, K., Cao, Y., Yang, J., & Jana, S. (2017, October). Deepxplore: Automated whitebox testing of deep learning systems. In proceedings of the 26th Symposium on Operating Systems Principles (pp. 1-18).;4_dl_testing_deep_network;2017;Deepxplore: Automated whitebox testing of deep learning systems;Kexin Pei, Yinzhi Cao, Junfeng Yang, Suman Jana;proceedings of the 26th Symposium on Operating Systems Principles, 1-18, 2017;Deep learning (DL) systems are increasingly deployed in safety- and security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system's behavior for corner case inputs are of great importance. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose erroneous behaviors for rare inputs.We design, implement, and evaluate DeepXplore, the first whitebox framework for systematically testing real-world DL systems. First, we introduce neuron coverage for systematically measuring the parts of a DL system exercised by test inputs. Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles to avoid manual checking. Finally, we demonstrate how finding inputs for DL systems that both trigger many differential behaviors and achieve high neuron coverage can be represented as a joint optimization problem and solved efficiently using gradient-based search techniques.DeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models with thousands of neurons trained on five popular datasets including ImageNet and Udacity self-driving challenge data. For all tested DL models, on average, DeepXplore generated one test input demonstrating incorrect behavior within one second while running only on a commodity laptop. We further show that the test inputs generated by DeepXplore can also be used to retrain the corresponding DL model to improve the model's accuracy by up to 3%.;https://dl.acm.org/doi/abs/10.1145/3132747.3132785;f7MnkEbCBLQJ
Ma, L., Juefei-Xu, F., Zhang, F., Sun, J., Xue, M., Li, B., ... & Wang, Y. (2018, September). Deepgauge: Multi-granularity testing criteria for deep learning systems. In Proceedings of the 33rd ACM/IEEE international conference on automated software engineering (pp. 120-131).;4_dl_testing_deep_network;2018;Deepgauge: Multi-granularity testing criteria for deep learning systems;Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, Yadong Wang;Proceedings of the 33rd ACM/IEEE international conference on automated software engineering, 120-131, 2018;Deep learning (DL) defines a new data-driven programming paradigm that constructs the internal system logic of a crafted neuron network through a set of training data. We have seen wide adoption of DL in many safety-critical scenarios. However, a plethora of studies have shown that the state-of-the-art DL systems suffer from various vulnerabilities which can lead to severe consequences when applied to real-world applications. Currently, the testing adequacy of a DL system is usually measured by the accuracy of test data. Considering the limitation of accessible high quality test data, good accuracy performance on test data can hardly provide confidence to the testing adequacy and generality of DL systems. Unlike traditional software systems that have clear and controllable logic and functionality, the lack of interpretability in a DL system makes system analysis and defect detection difficult, which could potentially hinder its real-world deployment. In this paper, we propose DeepGauge, a set of multi-granularity testing criteria for DL systems, which aims at rendering a multi-faceted portrayal of the testbed. The in-depth evaluation of our proposed testing criteria is demonstrated on two well-known datasets, five DL systems, and with four state-of-the-art adversarial attack techniques against DL. The potential usefulness of DeepGauge sheds light on the construction of more generic and robust DL systems.;https://dl.acm.org/doi/abs/10.1145/3238147.3238202;myE021i_F38J
Rahman, M. S., Rivera, E., Khomh, F., Guéhéneuc, Y. G., & Lehnert, B. (2019). Machine learning software engineering in practice: An industrial case study. arXiv preprint arXiv:1906.07154.;1_ml_machine_data_learning;2019;Machine learning software engineering in practice: An industrial case study;Md Saidur Rahman, Emilio Rivera, Foutse Khomh, Yann-Gaël Guéhéneuc, Bernd Lehnert;arXiv preprint arXiv:1906.07154, 2019;SAP is the market leader in enterprise software offering an end-to-end suite of applications and services to enable their customers worldwide to operate their business. Especially, retail customers of SAP deal with millions of sales transactions for their day-to-day business. Transactions are created during retail sales at the point of sale (POS) terminals and then sent to some central servers for validations and other business operations. A considerable proportion of the retail transactions may have inconsistencies due to many technical and human errors. SAP provides an automated process for error detection but still requires a manual process by dedicated employees using workbench software for correction. However, manual corrections of these errors are time-consuming, labor-intensive, and may lead to further errors due to incorrect modifications. This is not only a performance overhead on the customers' business workflow but it also incurs high operational costs. Thus, automated detection and correction of transaction errors are very important regarding their potential business values and the improvement in the business workflow. In this paper, we present an industrial case study where we apply machine learning (ML) to automatically detect transaction errors and propose corrections. We identify and discuss the challenges that we faced during this collaborative research and development project, from three distinct perspectives: Software Engineering, Machine Learning, and industry-academia collaboration. We report on our experience and insights from the project with guidelines for the identified challenges. We believe that our findings and recommendations can help researchers and practitioners embarking into similar endeavors.;https://arxiv.org/abs/1906.07154;Orcc9YHbtkoJ
Wicker, M., Huang, X., & Kwiatkowska, M. (2018). Feature-guided black-box safety testing of deep neural networks. In Tools and Algorithms for the Construction and Analysis of Systems: 24th International Conference, TACAS 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings, Part I 24 (pp. 408-426). Springer International Publishing.;5_adversarial_attack_example_model;2018;Feature-guided black-box safety testing of deep neural networks;Matthew Wicker, Xiaowei Huang, Marta Kwiatkowska;Tools and Algorithms for the Construction and Analysis of Systems: 24th International Conference, TACAS 2018, Held as Part of the European Joint Conferences on Theory and …, 2018;Despite the improved accuracy of deep neural networks, the discovery of adversarial examples has raised serious safety concerns. Most existing approaches for crafting adversarial examples necessitate some knowledge (architecture, parameters, etc) of the network at hand. In this paper, we focus on image classifiers and propose a feature-guided black-box approach to test the safety of deep neural networks that requires no such knowledge. Our algorithm employs object detection techniques such as SIFT (Scale Invariant Feature Transform) to extract features from an image. These features are converted into a mutable saliency distribution, where high probability is assigned to pixels that affect the composition of the image with respect to the human visual system. We formulate the crafting of adversarial examples as a two-player turn-based stochastic game, where the first player’s objective is to minimise the distance to an adversarial example by manipulating the features, and the second player can be cooperative, adversarial, or random. We show that, theoretically, the two-player game can converge to the optimal strategy, and that the optimal strategy represents a globally minimal adversarial image. For Lipschitz networks, we also identify conditions that provide safety guarantees that no adversarial examples exist. Using Monte Carlo tree search we gradually explore the game state space to search for adversarial examples. Our experiments show that, despite the black-box setting, manipulations guided by a perception-based saliency distribution are competitive with state-of-the-art methods that rely on white-box saliency matrices or sophisticated optimization procedures. Finally, we show how our method can be used to evaluate robustness of neural networks in safety-critical applications such as traffic sign recognition in self-driving cars.;https://link.springer.com/chapter/10.1007/978-3-319-89960-2_22;qTit8ZVP2tkJ
Zhang, M., Zhang, Y., Zhang, L., Liu, C., & Khurshid, S. (2018, September). DeepRoad: GAN-based metamorphic testing and input validation framework for autonomous driving systems. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering (pp. 132-142).;4_dl_testing_deep_network;2018;DeepRoad: GAN-based metamorphic testing and input validation framework for autonomous driving systems;Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, Sarfraz Khurshid;Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering, 132-142, 2018;While Deep Neural Networks (DNNs) have established the fundamentals of image-based autonomous driving systems, they may exhibit erroneous behaviors and cause fatal accidents. To address the safety issues in autonomous driving systems, a recent set of testing techniques have been designed to automatically generate artificial driving scenes to enrich test suite, e.g., generating new input images transformed from the original ones. However, these techniques are insufficient due to two limitations: first, many such synthetic images often lack diversity of driving scenes, and hence compromise the resulting efficacy and reliability. Second, for machine-learning-based systems, a mismatch between training and application domain can dramatically degrade system accuracy, such that it is necessary to validate inputs for improving system robustness.In this paper, we propose DeepRoad, an unsupervised DNN-based framework for automatically testing the consistency of DNN-based autonomous driving systems and online validation. First, DeepRoad automatically synthesizes large amounts of diverse driving scenes without using image transformation rules (e.g. scale, shear and rotation). In particular, DeepRoad is able to produce driving scenes with various weather conditions (including those with rather extreme conditions) by applying Generative Adversarial Networks (GANs) along with the corresponding real-world weather scenes. Second, DeepRoad utilizes metamorphic testing techniques to check the consistency of such systems using synthetic images. Third, DeepRoad validates input images for DNN-based systems by measuring the distance of the input and training images using their VGGNet features. We implement DeepRoad to test three well-recognized DNN-based autonomous driving systems in Udacity self-driving car challenge. The experimental results demonstrate that DeepRoad can detect thousands of inconsistent behaviors for these systems, and effectively validate input images to potentially enhance the system robustness as well.;https://dl.acm.org/doi/abs/10.1145/3238147.3238187;-OuetrtI-agJ
Lertpalangsunti, N., & Chan, C. W. (1998). An architectural framework for the construction of hybrid intelligent forecasting systems: application for electricity demand prediction. Engineering Applications of Artificial Intelligence, 11(4), 549-565.;1_ml_machine_data_learning;1998;An architectural framework for the construction of hybrid intelligent forecasting systems: application for electricity demand prediction;N Lertpalangsunti, CW Chan;Engineering Applications of Artificial Intelligence 11 (4), 549-565, 1998;This paper presents an implemented architectural framework for the construction of hybrid intelligent forecasters for utility demand prediction. The framework has been implemented as the intelligent forecasters construction set (IFCS), which supports the intelligent techniques of fuzzy logic, artificial neural networks, and knowledge-based and case-based reasoning. IFCS is also a hybrid programming tool, which allows the developer to implement forecasters by means of object-oriented visual programming, knowledge-based programming and procedural programming. The system was implemented on the real-time expert-system shell G2, with the G2 Diagnostic Assistant (GDA) and NeurOn-Line (NOL) modules. Rules, procedures and flow diagrams are organized into a hierarchy of workspaces. The modularity of IFCS allows the subsequent addition of other modules of intelligent techniques. IFCS was applied for daily power-load prediction in the city of Regina. The power-load data set was separated into subclasses, and a neural-network module consisting of backpropagation networks was applied to each of them. The data set was also modeled using a linear regression (LR) and a case-based reasoning (CBR) program, and their results were compared to those from the neural-network approach.;https://www.sciencedirect.com/science/article/pii/S095219769800013X;MpEOQeGtjDcJ
Nalchigar, S., Yu, E., Obeidi, Y., Carbajales, S., Green, J., & Chan, A. (2019). Solution patterns for machine learning. In Advanced Information Systems Engineering: 31st International Conference, CAiSE 2019, Rome, Italy, June 3–7, 2019, Proceedings 31 (pp. 627-642). Springer International Publishing.;1_ml_machine_data_learning;2019;Solution patterns for machine learning;Soroosh Nalchigar, Eric Yu, Yazan Obeidi, Sebastian Carbajales, John Green, Allen Chan;Advanced Information Systems Engineering: 31st International Conference, CAiSE 2019, Rome, Italy, June 3â€“7, 2019, Proceedings 31, 627-642, 2019;Despite the hype around machine learning (ML), many organizations are struggling to derive business value from ML capabilities. Design patterns have long been used in software engineering to enhance design effectiveness and to speed up the development process. The contribution of this paper is two-fold. First, it introduces solution patterns as an explicit way of representing generic and well-proven ML designs for commonly-known and recurring business analytics problems. Second, it reports on the feasibility, expressiveness, and usefulness of solution patterns for ML, in collaboration with an industry partner. It provides a prototype architecture for supporting the use of solution patterns in real world scenarios. It presents a proof-of-concept implementation of the architecture and illustrates its feasibility. Findings from the collaboration suggest that solution patterns can have a positive impact on ML design and development efforts.;https://link.springer.com/chapter/10.1007/978-3-030-21290-2_39;Fhx9tUdeZwYJ
Ma, S., Liu, Y., Lee, W. C., Zhang, X., & Grama, A. (2018, October). MODE: automated neural network model debugging via state differential analysis and input selection. In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 175-186).;10_testing_test_machine_metamorphic;2018;MODE: automated neural network model debugging via state differential analysis and input selection;Shiqing Ma, Yingqi Liu, Wen-Chuan Lee, Xiangyu Zhang, Ananth Grama;Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 175-186, 2018;Artificial intelligence models are becoming an integral part of modern computing systems. Just like software inevitably has bugs, models have bugs too, leading to poor classification/prediction accuracy. Unlike software bugs, model bugs cannot be easily fixed by directly modifying models. Existing solutions work by providing additional training inputs. However, they have limited effectiveness due to the lack of understanding of model misbehaviors and hence the incapability of selecting proper inputs. Inspired by software debugging, we propose a novel model debugging technique that works by first conducting model state differential analysis to identify the internal features of the model that are responsible for model bugs and then performing training input selection that is similar to program input selection in regression testing. Our evaluation results on 29 different models for 6 different applications show that our technique can fix model bugs effectively and efficiently without introducing new bugs. For simple applications (e.g., digit recognition), MODE improves the test accuracy from 75% to 93% on average whereas the state-of-the-art can only improve to 85% with 11 times more training time. For complex applications and models (e.g., object recognition), MODE is able to improve the accuracy from 75% to over 91% in minutes to a few hours, whereas state-of-the-art fails to fix the bug or even degrades the test accuracy.;https://dl.acm.org/doi/abs/10.1145/3236024.3236082;3UQdF3gT8SIJ
Xie, X., Ho, J. W., Murphy, C., Kaiser, G., Xu, B., & Chen, T. Y. (2011). Testing and validating machine learning classifiers by metamorphic testing. Journal of Systems and Software, 84(4), 544-558.;10_testing_test_machine_metamorphic;2011;Testing and validating machine learning classifiers by metamorphic testing;Xiaoyuan Xie, Joshua WK Ho, Christian Murphy, Gail Kaiser, Baowen Xu, Tsong Yueh Chen;Journal of Systems and Software 84 (4), 544-558, 2011;Machine learning algorithms have provided core functionality to many application domains – such as bioinformatics, computational linguistics, etc. However, it is difficult to detect faults in such applications because often there is no “test oracle” to verify the correctness of the computed outputs. To help address the software quality, in this paper we present a technique for testing the implementations of machine learning classification algorithms which support such applications. Our approach is based on the technique “metamorphic testing”, which has been shown to be effective to alleviate the oracle problem. Also presented include a case study on a real-world machine learning application framework, and a discussion of how programmers implementing machine learning algorithms can avoid the common pitfalls discovered in our study. We also conduct mutation analysis and cross-validation, which reveal that our method has high effectiveness in killing mutants, and that observing expected cross-validation result alone is not sufficiently effective to detect faults in a supervised classification program. The effectiveness of metamorphic testing is further confirmed by the detection of real faults in a popular open-source classification program.;https://www.sciencedirect.com/science/article/pii/S0164121210003213;by3kT6GTqw8J
Sun, Y., Wu, M., Ruan, W., Huang, X., Kwiatkowska, M., & Kroening, D. (2018, September). Concolic testing for deep neural networks. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering (pp. 109-119).;4_dl_testing_deep_network;2018;Concolic testing for deep neural networks;Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska, Daniel Kroening;ASE '18: Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering;Concolic testing combines program execution and symbolic analysis to explore the execution paths of a software program. In this paper, we develop the first concolic testing approach for Deep Neural Networks (DNNs). More specifically, we utilise quantified linear arithmetic over rationals to express test requirements that have been studied in the literature, and then develop a coherent method to perform concolic testing with the aim of better coverage. Our experimental results show the effectiveness of the concolic testing approach in both achieving high coverage and finding adversarial examples.;https://dl.acm.org/doi/abs/10.1145/3238147.3238172;Todo
Sun, Y., Huang, X., Kroening, D., Sharp, J., Hill, M., & Ashmore, R. (2018). Testing deep neural networks. arXiv preprint arXiv:1803.04792.;4_dl_testing_deep_network;2018;Testing deep neural networks;Youcheng Sun, Xiaowei Huang, Daniel Kroening, James Sharp, Matthew Hill, Rob Ashmore;arXiv preprint arXiv:1803.04792, 2018;Deep neural networks (DNNs) have a wide range of applications, and software employing them must be thoroughly tested, especially in safety-critical domains. However, traditional software test coverage metrics cannot be applied directly to DNNs. In this paper, inspired by the MC/DC coverage criterion, we propose a family of four novel test criteria that are tailored to structural features of DNNs and their semantics. We validate the criteria by demonstrating that the generated test inputs guided via our proposed coverage criteria are able to capture undesired behaviours in a DNN. Test cases are generated using a symbolic approach and a gradient-based heuristic search. By comparing them with existing methods, we show that our criteria achieve a balance between their ability to find bugs (proxied using adversarial examples) and the computational cost of test case generation. Our experiments are conducted on state-of-the-art DNNs obtained using popular open source datasets, including MNIST, CIFAR-10 and ImageNet.;https://arxiv.org/abs/1803.04792;2Gg951do_xkJ
Tian, Y., Pei, K., Jana, S., & Ray, B. (2018, May). Deeptest: Automated testing of deep-neural-network-driven autonomous cars. In Proceedings of the 40th international conference on software engineering (pp. 303-314).;4_dl_testing_deep_network;2018;Deeptest: Automated testing of deep-neural-network-driven autonomous cars;Yuchi Tian, Kexin Pei, Suman Jana, Baishakhi Ray;Proceedings of the 40th international conference on software engineering, 303-314, 2018;Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads.However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner-case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases.In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.;https://dl.acm.org/doi/abs/10.1145/3180155.3180220;P9lYhHEdd1kJ
Zinkevich, M. (2017). Rules of machine learning: Best practices for ML engineering. URL: https://developers. google. com/machine-learning/guides/rules-of-ml.;1_ml_machine_data_learning;2017;Rules of machine learning: Best practices for ML engineering;Martin Zinkevich;URL: https://developers. google. com/machine-learning/guides/rules-of-ml, 2017;This document is intended to help those with a basic knowledge of machine learning get the benefit of best practices in machine learning from around Google. It presents a style for machine learning, similar to the Google C++ Style Guide and other popular guides to practical programming. If you have taken a class in machine learning, or built or worked on a machinelearned model, then you have the necessary background to read this document.;https://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf;ZfI-95g_j4kJ
Benni, B., Blay-Fornarino, M., Mosser, S., Precisio, F., & Jungbluth, G. (2019, September). When DevOps meets meta-learning: A portfolio to rule them all. In 2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C) (pp. 605-612). IEEE.;1_ml_machine_data_learning;2019;When DevOps meets meta-learning: A portfolio to rule them all;Benjamin Benni, Mireille Blay-Fornarino, Sébastien Mosser, Frederic Precisio, Günther Jungbluth;2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C), 605-612, 2019;The Machine Learning (ML) world is in constant evolution, as the amount of different algorithms in this context is evolving quickly. Until now, it is the responsibility of data scientists to create ad-hoc ML pipelines for each situation they encounter, gaining knowledge about the adequacy between their context and the chosen pipeline. Considering that it is not possible at a human scale to analyze the exponential number of potential pipelines, picking the right pipeline that combines the proper preprocessing and algorithms is a hard task that requires knowledge and experience. In front of the complexity of building a right ML pipeline, algorithm portfolios aim to drive algorithm selection, learning from the past in a continuous process. However, building a portfolio requires that (i) data scientists develop and test pipelines and (ii) portfolio maintainers ensure the quality of the portfolio and enrich it. The firsts are the developers, while the seconds are the operators. In this paper, we present a set of criteria to be respected, and propose a pipeline-based meta-model, to support a DevOps approach in the context of Machine Learning Pipelines. The exploitation of this meta-model, both as a graph and as a logical expression, serves to ensure continuity between Dev and Ops. We depict our proposition through the simplified study of two primary use cases, one with developer's point-of-view, the other with ops'.;https://ieeexplore.ieee.org/abstract/document/8904866/;VX6NunF_t1cJ
Fursin, G. (2021). Collective knowledge: organizing research projects as a database of reusable components and portable workflows with common interfaces. Philosophical Transactions of the Royal Society A, 379(2197), 20200211.;1_ml_machine_data_learning;2021;Collective knowledge: organizing research projects as a database of reusable components and portable workflows with common interfaces;Grigori Fursin;Philosophical Transactions of the Royal Society A 379 (2197), 20200211, 2021;This article provides the motivation and overview of the Collective Knowledge Framework (CK or cKnowledge). The CK concept is to decompose research projects into reusable components that encapsulate research artifacts and provide unified application programming interfaces (APIs), command-line interfaces (CLIs), meta descriptions and common automation actions for related artifacts. The CK framework is used to organize and manage research projects as a database of such components. Inspired by the USB ‘plug and play’ approach for hardware, CK also helps to assemble portable workflows that can automatically plug in compatible components from different users and vendors (models, datasets, frameworks, compilers, tools). Such workflows can build and run algorithms on different platforms and environments in a unified way using the customizable CK program pipeline with software detection plugins and the automatic installation of missing packages. This article presents a number of industrial projects in which the modular CK approach was successfully validated in order to automate benchmarking, auto-tuning and co-design of efficient software and hardware for machine learning and artificial intelligence in terms of speed, accuracy, energy, size and various costs. The CK framework also helped to automate the artifact evaluation process at several computer science conferences as well as to make it easier to reproduce, compare and reuse research techniques from published papers, deploy them in production, and automatically adapt them to continuously changing datasets, models and systems. The long-term goal is to accelerate innovation by connecting researchers and practitioners to share and reuse all their knowledge, best practices, artifacts, workflows and experimental results in a common, portable and reproducible format at cKnowledge.io.This article is part of the theme issue ‘Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification in silico’.;https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2020.0211;w-hfgUXsXugJ
"A. Goyal, ""MLOps machine learning operations"", Int. J. Inf. Technol. Insights Transformations, vol. 4, no. 2, Apr. 2020, [online] Available: https://technology.eurekajournals.com/index.php/IJITIT/article/view/655.";1_ml_machine_data_learning;2020;Machine Learning Operations;Akshita Goyal;Vol 4 No 2 (2020): International Journal of Information Technology Insights & Transformations;MLOps (Machine Learning Operations) are relatively a new concept in Machine Learning that came into existence between 2018 and 2019. MLOps helps Data scientists and Engineers to generate long-term value and reduce risk in data science, machine learning, and AI initiatives by standardizing and streamlining Machine Leaning Lifecycle. This chapter delves into Explaining MLOps, Differentiating MLOps with DevOps, Working, and implementation of MLOps, Reason for MLOps Adoption, Its benefits, Challenges to MLOps, Standard practices, and tools used for operations.;https://technology.eurekajournals.com/index.php/IJITIT/article/view/655;TODO
Karamitsos, I., Albarhami, S., & Apostolopoulos, C. (2020). Applying DevOps practices of continuous automation for machine learning. Information, 11(7), 363.;1_ml_machine_data_learning;2020;Applying DevOps practices of continuous automation for machine learning;Ioannis Karamitsos, Saeed Albarhami, Charalampos Apostolopoulos;Information 11 (7), 363, 2020;This paper proposes DevOps practices for machine learning application, integrating both the development and operation environment seamlessly. The machine learning processes of development and deployment during the experimentation phase may seem easy. However, if not carefully designed, deploying and using such models may lead to a complex, time-consuming approaches which may require significant and costly efforts for maintenance, improvement, and monitoring. This paper presents how to apply continuous integration (CI) and continuous delivery (CD) principles, practices, and tools so as to minimize waste, support rapid feedback loops, explore the hidden technical debt, improve value delivery and maintenance, and improve operational functions for real-world machine learning applications.;https://www.mdpi.com/2078-2489/11/7/363;WGRrJkHKkokJ
Karn, R. R., Kudva, P., & Elfadel, I. A. M. (2018). Dynamic autoselection and autotuning of machine learning models for cloud network analytics. IEEE Transactions on Parallel and Distributed Systems, 30(5), 1052-1064.;1_ml_machine_data_learning;2018;Dynamic autoselection and autotuning of machine learning models for cloud network analytics;Rupesh Raj Karn, Prabhakar Kudva, Ibrahim Abe M Elfadel;IEEE Transactions on Parallel and Distributed Systems 30 (5), 1052-1064, 2018;Cloud network monitoring data is dynamic and distributed. Signals to monitor the cloud can appear, disappear or change their importance and clarity over time. Machine learning (ML) models tuned to a given data set can therefore quickly become inadequate. A model might be highly accurate at one point in time but may lose its accuracy at a later time due to changes in input data and their features. Distributed learning with dynamic model selection is therefore often required. Under such selection, poorly performing models (although aggressively tuned for the prior data) are retired or put on standby while new or standby models are brought in. The well-known method of Ensemble ML (EML) may potentially be applied to improve the overall accuracy of a family of ML models. Unfortunately, EML has several disadvantages, including the need for continuous training, excessive computational resources, requirement for large training datasets, high risks of overfitting, and a time-consuming model-building process. In this paper, we propose a novel cloud methodology for automatic ML model selection and tuning that automates model building and selection and is competitive with existing methods. We use unsupervised learning to better explore the data space before the generation of targeted supervised learning models in an automated fashion. In particular, we create a Cloud DevOps architecture for autotuning and selection based on container orchestration and messaging between containers, and take advantage of a new autoscaling method to dynamically create and evaluate instantiations of ML algorithms. The proposed methodology and tool are demonstrated on cloud network security datasets.;https://ieeexplore.ieee.org/abstract/document/8500348/;T4UGr1ritLsJ
Domenech, A. M., & Guillén, A. (2020, September). ml-experiment: A Python framework for reproducible data science. In Journal of Physics: Conference Series (Vol. 1603, No. 1, p. 012025). IOP Publishing.;1_ml_machine_data_learning;2020;ml-experiment: A Python framework for reproducible data science;Antonio Molner Domenech, Alberto GuillÃ©n;Journal of Physics: Conference Series 1603 (1), 012025, 2020;Nowadays, data science projects are usually developed in an unstructured way, which makes it difficult to reproduce. It is also hard to move from an experimental environment to production. Operational workflows such as containerization, continuous deployment, and cloud orchestration allow data science researchers to move a pipeline from a local environment to the cloud. Being aware of the difficulties of setting those workflows up, this paper presents a framework to ease experiment tracking and operationalizing machine learning by combining existent and well-supported technologies. These technologies include Docker, Mlflow, Ray, among others. The framework provides an opinionated workflow to design and execute experiments either on a local environment or the cloud. ml-experiment includes: an automatic tracking system for the most famous machine learning libraries: Tensorflow, Keras, Fastai, Xgboost and Lightgdm, first-class support for distributed training and hyperparameter optimization, and a Command Line Interface (CLI) for packaging and running projects inside containers.;https://iopscience.iop.org/article/10.1088/1742-6596/1603/1/012025/meta;jyF4KRMs2CMJ
Posoldova, A. (2020). Machine learning pipelines: From research to production. IEEE Potentials, 39(6), 38-42.;1_ml_machine_data_learning;2020;Machine learning pipelines: From research to production;Alexandra Posoldova;IEEE Potentials 39 (6), 38-42, 2020;Machine learning (ML) and artificial intelligence (AI) are getting lot of attention these days, and an increasing number of people are interested in becoming data scientists. Many imagine ML/AI as a black box that does its magic and is somehow able to make predictions. Well, the magic is that an ML algorithm is designed in such a way that it is able to find a pattern in data that have the correct answers and use that to predict the next answer. The learning task may vary from predicting the next word in a sentence, a sentiment, or what else would you like to buy based on what you already have in your basket.;https://ieeexplore.ieee.org/abstract/document/9258455/;hf8qXRu2zqoJ
Renggli, C., Rimanic, L., Gürel, N. M., Karlaš, B., Wu, W., & Zhang, C. (2021). A data quality-driven view of mlops. arXiv preprint arXiv:2102.07750.;1_ml_machine_data_learning;2021;A data quality-driven view of mlops;Cedric Renggli, Luka Rimanic, Nezihe Merve Gürel, Bojan Karlaš, Wentao Wu, Ce Zhang;arXiv preprint arXiv:2102.07750, 2021;Developing machine learning models can be seen as a process similar to the one established for traditional software development. A key difference between the two lies in the strong dependency between the quality of a machine learning model and the quality of the data used to train or perform evaluations. In this work, we demonstrate how different aspects of data quality propagate through various stages of machine learning development. By performing a joint analysis of the impact of well-known data quality dimensions and the downstream machine learning process, we show that different components of a typical MLOps pipeline can be efficiently designed, providing both a technical and theoretical perspective.;https://arxiv.org/abs/2102.07750;Kps7-A7vtGUJ
Spjuth, O., Frid, J., & Hellander, A. (2021). The machine learning life cycle and the cloud: implications for drug discovery. Expert opinion on drug discovery, 16(9), 1071-1079.;1_ml_machine_data_learning;2021;The machine learning life cycle and the cloud: implications for drug discovery;Ola Spjuth, Jens Frid, Andreas Hellander;Expert opinion on drug discovery 16 (9), 1071-1079, 2021;"Introduction: Artificial intelligence (AI) and machine learning (ML) are increasingly used in many aspects of drug discovery. Larger data sizes and methods such as Deep Neural Networks contribute to challenges in data management, the required software stack, and computational infrastructure. There is an increasing need in drug discovery to continuously re-train models and make them available in production environments.Areas covered: This article describes how cloud computing can aid the ML life cycle in drug discovery. The authors discuss opportunities with containerization and scientific workflows and introduce the concept of MLOps and describe how it can facilitate reproducible and robust ML modeling in drug discovery organizations. They also discuss ML on private, sensitive and regulated data.Expert opinion: Cloud computing offers a compelling suite of building blocks to sustain the ML life cycle integrated in iterative drug discovery. Containerization and platforms such as Kubernetes together with scientific workflows can enable reproducible and resilient analysis pipelines, and the elasticity and flexibility of cloud infrastructures enables scalable and efficient access to compute resources. Drug discovery commonly involves working with sensitive or private data, and cloud computing and federated learning can contribute toward enabling collaborative drug discovery within and between organizations.Abbreviations: AI = Artificial Intelligence; DL = Deep Learning; GPU = Graphics Processing Unit; IaaS = Infrastructure as a Service; K8S = Kubernetes; ML = Machine Learning; MLOps = Machine Learning and Operations; PaaS = Platform as a Service; QC = Quality Control; SaaS = Software as a Service";https://www.tandfonline.com/doi/abs/10.1080/17460441.2021.1932812;BOWQQcfBsbgJ
Wolf, C. T. (2020). AI models and their worlds: Investigating data-driven, AI/ML ecosystems through a work practices lens. In Sustainable Digital Communities: 15th International Conference, iConference 2020, Boras, Sweden, March 23–26, 2020, Proceedings 15 (pp. 651-664). Springer International Publishing.;1_ml_machine_data_learning;2020;AI models and their worlds: Investigating data-driven, AI/ML ecosystems through a work practices lens;Christine T Wolf;Sustainable Digital Communities: 15th International Conference, iConference 2020, Boras, Sweden, March 23–26, 2020, Proceedings 15, 651-664, 2020;"When we invoke the “future of work,” to whose work do we refer? This paper considers everyday work practices through which contemporary artificial intelligence (AI) and machine learning (ML) ecosystems are made possible. The “future of work” is often talked about in relation to the anticipated domain settings where AI/ML systems might be implemented and the labor conditions such implementations might re-configure (foreseeing or noting changes in medical/health, legal, or manufacturing work, for example). This paper turns our attention to the various forms of labor that must be undertaken to conceive of, train/test, deploy, and ongoingly maintain AI/ML systems in practice. In particular, this paper draws on an ongoing ethnographic endeavor in a large, global technology and consulting corporation and leverages a work practices lens to examine three themes: curating datasets (everyday work practices of data pre-processing); tending models (everyday work practices of training, deploying, and maintaining predictive models); and configuring compute (everyday work practices of back-end infrastructuring, commonly called “DevOps”). This paper considers the value of a work practices lens in studying contemporary sociotechnical labor ecosystems. By locating the work practices through which AI/ML systems emerge, this paper shows that these technologies indeed require considerable human labor, at the same time they are often talked about as drivers of automation and displacers of work. This extends discourses around the “future of work,” giving light to the various standpoints and experiences of labor such imaginaries implicate and ongoingly re-configure.";https://link.springer.com/chapter/10.1007/978-3-030-43687-2_55;vmGChy4eU8UJ
Wu, C., Haihong, E., & Song, M. (2020, January). An Automatic Artificial Intelligence Training Platform Based on Kubernetes. In Proceedings of the 2020 2nd International Conference on Big Data Engineering and Technology (pp. 58-62).;1_ml_machine_data_learning;2020;An Automatic Artificial Intelligence Training Platform Based on Kubernetes;Chaoyu Wu, E Haihong, Meina Song;Proceedings of the 2020 2nd International Conference on Big Data Engineering and Technology, 58-62, 2020;For large-scale AI training, the manual allocation of GPU resources is too inefficient, and it faces the problems of task allocation and fault restart. In this paper, a fully automatic machine learning platform is designed, which manages server resources uniformly, and users describe the required resources through configuration files. The platform automatically performs AI task allocation and scheduling based on the cluster load, which solves the problems of low cluster resource utilization and uneven machine load distribution. The platform also provides an automatic release and continuous integration of the model, which greatly simplifies the configuration of the model's operating environment and external release process, enabling researchers to focus more on model adjustments. Finally, it is verified by experiments that the extra time spent on AI task training through this platform is negligible, which confirms the feasibility of the platform.;https://dl.acm.org/doi/abs/10.1145/3378904.3378921;hu092NH6eWgJ
Yoon, G., Han, J., Lee, S., & Kim, J. (2020). DevOps Portal Design for SmartX AI Cluster Employing Cloud-Native Machine Learning Workflows. In Advances in Internet, Data and Web Technologies: The 8th International Conference on Emerging Internet, Data and Web Technologies (EIDWT-2020) (pp. 532-539). Springer International Publishing.;1_ml_machine_data_learning;2020;DevOps Portal Design for SmartX AI Cluster Employing Cloud-Native Machine Learning Workflows;GeumSeong Yoon, Jungsu Han, Seunghyung Lee, JongWon Kim;Advances in Internet, Data and Web Technologies: The 8th International Conference on Emerging Internet, Data and Web Technologies (EIDWT-2020), 532-539, 2020;This paper introduces DevOps Portal for AI multi-cluster environment and management using Kubernetes (K8S), a representative container orchestration technology based on cloud-native. Specifically, we propose and verify the concept of DevOps Portal that provides the management function for multi-cluster operators in DevOps aspect and enables the operation of multiple clusters through the support of cluster selection from the developerâ€™s point of view. In addition, after using DevOps Portal, developers can create an environment in which Machine Learning (ML) workflows can be performed according to the processing of data through a web dashboard. This allows partial validation of cloud-native based HPC/HPDA/AI workloads.;https://link.springer.com/chapter/10.1007/978-3-030-39746-3_54;buVaDpooOYsJ
Correia, J. L., Pereira, J. A., Mello, R., Garcia, A., Fonseca, B., Ribeiro, M., ... & Tiengo, W. (2020, December). Brazilian data scientists: Revealing their challenges and practices on machine learning model development. In Proceedings of the XIX Brazilian Symposium on Software Quality (pp. 1-10).;1_ml_machine_data_learning;2020;Brazilian data scientists: Revealing their challenges and practices on machine learning model development;João Lucas Correia, Juliana Alves Pereira, Rafael Mello, Alessandro Garcia, Baldoino Fonseca, Márcio Ribeiro, Rohit Gheyi, Marcos Kalinowski, Renato Cerqueira, Willy Tiengo;Proceedings of the XIX Brazilian Symposium on Software Quality, 1-10, 2020;"Data scientists often develop machine learning models to solve a variety of problems in the industry and academy. To build these models, these professionals usually perform activities that are also performed in the traditional software development lifecycle, such as eliciting and implementing requirements. One might argue that data scientists could rely on the engineering of traditional software development to build machine learning models. However, machine learning development presents certain characteristics, which may raise challenges that lead to the need for adopting new practices. The literature lacks in characterizing this knowledge from the perspective of the data scientists. In this paper, we characterize challenges and practices addressing the engineering of machine learning models that deserve attention from the research community. To this end, we performed a qualitative study with eight data scientists across five different companies having different levels of experience in developing machine learning models. Our findings suggest that: (i) data processing and feature engineering are the most challenging stages in the development of machine learning models; (ii) it is essential synergy between data scientists and domain experts in most of stages; and (iii) the development of machine learning models lacks the support of a well-engineered process.";https://dl.acm.org/doi/abs/10.1145/3439961.3439971;TvxxXe0VU4oJ
Zhang, T., Gao, C., Ma, L., Lyu, M., & Kim, M. (2019, October). An empirical study of common challenges in developing deep learning applications. In 2019 IEEE 30th International Symposium on Software Reliability Engineering (ISSRE) (pp. 104-115). IEEE.;4_dl_testing_deep_network;2019;An empirical study of common challenges in developing deep learning applications;Tianyi Zhang, Cuiyun Gao, Lei Ma, Michael Lyu, Miryung Kim;2019 IEEE 30th International Symposium on Software Reliability Engineering (ISSRE), 104-115, 2019;Recent advances in deep learning promote the innovation of many intelligent systems and applications such as autonomous driving and image recognition. Despite enormous efforts and investments in this field, a fundamental question remains under-investigated - what challenges do developers commonly face when building deep learning applications? To seek an answer, this paper presents a large-scale empirical study of deep learning questions in a popular Q&A website, Stack Overflow. We manually inspect a sample of 715 questions and identify seven kinds of frequently asked questions. We further build a classification model to quantify the distribution of different kinds of deep learning questions in the entire set of 39,628 deep learning questions. We find that program crashes, model migration, and implementation questions are the top three most frequently asked questions. After carefully examining accepted answers of these questions, we summarize five main root causes that may deserve attention from the research community, including API misuse, incorrect hyperparameter selection, GPU computation, static graph computation, and limited debugging and profiling support. Our results highlight the need for new techniques such as cross-framework differential testing to improve software development productivity and software reliability in deep learning.;https://ieeexplore.ieee.org/abstract/document/8987482/;Cu4GOTKnS8oJ
Hesenius, M., Schwenzfeier, N., Meyer, O., Koop, W., & Gruhn, V. (2019, May). Towards a software engineering process for developing data-driven applications. In 2019 IEEE/ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE) (pp. 35-41). IEEE.;1_ml_machine_data_learning;2019;Towards a software engineering process for developing data-driven applications;Marc Hesenius, Nils Schwenzfeier, Ole Meyer, Wilhelm Koop, Volker Gruhn;2019 IEEE/ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE), 35-41, 2019;Machine Learning and Artificial Intelligence allow the development of a new type of applications that automatically identify hidden patterns, process large amounts of data, and classify data according to aforementioned patterns. While they offer interesting solutions for several problems, they also impose challenges on software engineers in charge of structuring the development effort. The new applications require to incorporate additional specialists and their work into an overall development effort. We thus propose a software engineering process for data-driven applications.;https://ieeexplore.ieee.org/abstract/document/8823665/;D4m4RVtPwtIJ
Reimann, L., & Kniesel-Wünsche, G. (2020, March). Achieving guidance in applied machine learning through software engineering techniques. In Companion Proceedings of the 4th International Conference on Art, Science, and Engineering of Programming (pp. 7-12).;1_ml_machine_data_learning;2020;Achieving guidance in applied machine learning through software engineering techniques;Lars Reimann, Günter Kniesel-Wünsche;Companion Proceedings of the 4th International Conference on Art, Science, and Engineering of Programming, 7-12, 2020;Development of machine learning (ML) applications is hard. Producing successful applications requires, among others, being deeply familiar with a variety of complex and quickly evolving application programming interfaces (APIs). It is therefore critical to understand what prevents developers from learning these APIs, using them properly at development time, and understanding what went wrong when it comes to debugging. We look at the (lack of) guidance that currently used development environments and ML APIs provide to developers of ML applications, contrast these with software engineering best practices, and identify gaps in the current state of the art. We show that current ML tools fall short of fulfilling some basic software engineering gold standards and point out ways in which software engineering concepts, tools and techniques need to be extended and adapted to match the special needs of ML application development. Our findings point out ample opportunities for research on ML-specific software engineering.;https://dl.acm.org/doi/abs/10.1145/3397537.3397552;3FMbemcJUSEJ
Hutchinson, B., Smart, A., Hanna, A., Denton, E., Greer, C., Kjartansson, O., ... & Mitchell, M. (2021, March). Towards accountability for machine learning datasets: Practices from software engineering and infrastructure. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (pp. 560-575).;1_ml_machine_data_learning;2021;Towards accountability for machine learning datasets: Practices from software engineering and infrastructure;Ben Hutchinson, Andrew Smart, Alex Hanna, Emily Denton, Christina Greer, Oddur Kjartansson, Parker Barnes, Margaret Mitchell;Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 560-575, 2021;Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.;https://dl.acm.org/doi/abs/10.1145/3442188.3445918;hJDIJnpyIgoJ
Ole, M., & Volker, G. (2019, May). Towards concept based software engineering for intelligent agents. In 2019 IEEE/ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE) (pp. 42-48). IEEE.;1_ml_machine_data_learning;2019;Towards concept based software engineering for intelligent agents;Meyer Ole, Gruhn Volker;2019 IEEE/ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE), 42-48, 2019;The development of AI and machine learning applications at an industry mature level while maintaining quality and productivity goals is one of today's major challenges. Research in the field of intelligent agents has achieved many successes in recent years, especially due to various reinforcement learning techniques, and promises a high benefit in times of automation and autonomous systems. Bringing them into production, however, requires optimization against many other criteria than just accuracy. This leads to the emerging field of machine teaching. We already know many of the objectives used there from software engineering research, which has led to many well-established principles in recent decades. One of them is the component-based development whose idea finds an interesting counterpart in hierarchical reinforcement learning. We show that both areas can benefit from each other and introduce our approach of Concept Based Software Engineering, which is focused on supporting productivity and quality goals during the development of such systems.;https://ieeexplore.ieee.org/abstract/document/8823769/;O0tLNzfbNZoJ
Wan, Z., Xia, X., Lo, D., & Murphy, G. C. (2019). How does machine learning change software development practices?. IEEE Transactions on Software Engineering, 47(9), 1857-1871.;1_ml_machine_data_learning;2019;How does machine learning change software development practices?;Zhiyuan Wan, Xin Xia, David Lo, Gail C Murphy;IEEE Transactions on Software Engineering 47 (9), 1857-1871, 2019;Adding an ability for a system to learn inherently adds uncertainty into the system. Given the rising popularity of incorporating machine learning into systems, we wondered how the addition alters software development practices. We performed a mixture of qualitative and quantitative studies with 14 interviewees and 342 survey respondents from 26 countries across four continents to elicit significant differences between the development of machine learning systems and the development of non-machine-learning systems. Our study uncovers significant differences in various aspects of software engineering (e.g., requirements, design, testing, and process) and work characteristics (e.g., skill variety, problem solving and task identity). Based on our findings, we highlight future research directions and provide recommendations for practitioners.;https://ieeexplore.ieee.org/abstract/document/8812912/;TQmH-YSXX4AJ
Wolf, C. T., & Paine, D. (2020, June). Sensemaking practices in the everyday work of AI/ML software engineering. In Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops (pp. 86-92).;1_ml_machine_data_learning;2020;Sensemaking practices in the everyday work of AI/ML software engineering;Christine T Wolf, Drew Paine;Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops, 86-92, 2020;"This paper considers sensemaking as it relates to everyday software engineering (SE) work practices and draws on a multi-year ethnographic study of SE projects at a large, global technology company building digital services infused with artificial intelligence (AI) and machine learning (ML) capabilities. Our findings highlight the breadth of sensemaking practices in AI/ML projects, noting developers' efforts to make sense of AI/ML environments (e.g., algorithms/methods and libraries), of AI/ML model ecosystems (e.g., pre-trained models and ""upstream"" models), and of business-AI relations (e.g., how the AI/ML service relates to the domain context and business problem at hand). This paper builds on recent scholarship drawing attention to the integral role of sensemaking in everyday SE practices by empirically investigating how and in what ways AI/ML projects present software teams with emergent sensemaking requirements and opportunities.";https://dl.acm.org/doi/abs/10.1145/3387940.3391496;vDy1YtwcyaUJ
Foidl, H., & Felderer, M. (2019, August). Risk-based data validation in machine learning-based software systems. In proceedings of the 3rd ACM SIGSOFT international workshop on machine learning techniques for software quality evaluation (pp. 13-18).;1_ml_machine_data_learning;2019;Risk-based data validation in machine learning-based software systems;Harald Foidl, Michael Felderer;proceedings of the 3rd ACM SIGSOFT international workshop on machine learning techniques for software quality evaluation, 13-18, 2019;Data validation is an essential requirement to ensure the reliability and quality of Machine Learning-based Software Systems. However, an exhaustive validation of all data fed to these systems (i.e. up to several thousand features) is practically unfeasible. In addition, there has been little discussion about methods that support software engineers of such systems in determining how thorough to validate each feature (i.e. data validation rigor). Therefore, this paper presents a conceptual data validation approach that prioritizes features based on their estimated risk of poor data quality. The risk of poor data quality is determined by the probability that a feature is of low data quality and the impact of this low (data) quality feature on the result of the machine learning model. Three criteria are presented to estimate the probability of low data quality (Data Source Quality, Data Smells, Data Pipeline Quality). To determine the impact of low (data) quality features, the importance of features according to the performance of the machine learning model (i.e. Feature Importance) is utilized. The presented approach provides decision support (i.e. data validation prioritization and rigor) for software engineers during the implementation of data validation techniques in the course of deploying a trained machine learning model and its software stack.;https://dl.acm.org/doi/abs/10.1145/3340482.3342743;x7tI7-GHQDAJ
Götz, M., Book, M., Bodenstein, C., & Riedel, M. (2017, November). Supporting software engineering practices in the development of data-intensive hpc applications with the juml framework. In Proceedings of the 1st International Workshop on Software Engineering for High Performance Computing in Computational and Data-enabled Science & Engineering (pp. 1-8).;1_ml_machine_data_learning;2017;Supporting software engineering practices in the development of data-intensive hpc applications with the juml framework;Markus GÃ¶tz, Matthias Book, Christian Bodenstein, Morris Riedel;Proceedings of the 1st International Workshop on Software Engineering for High Performance Computing in Computational and Data-enabled Science & Engineering, 1-8, 2017;The development of high performance computing applications is considerably different from traditional software development. This distinction is due to the complex hardware systems, inherent parallelism, different software lifecycle and workflow, as well as (especially for scientific computing applications) partially unknown requirements at design time. This makes the use of software engineering practices challenging, so only a small subset of them are actually applied. In this paper, we discuss the potential for applying software engineering techniques to an emerging field in high performance computing, namely large-scale data analysis and machine learning. We argue for the employment of software engineering techniques in the development of such applications from the start, and the design of generic, reusable components. Using the example of the Juelich Machine Learning Library (JuML), we demonstrate how such a framework can not only simplify the design of new parallel algorithms, but also increase the productivity of the actual data analysis workflow. We place particular focus on the abstraction from heterogeneous hardware, the architectural design as well as aspects of parallel and distributed unit testing.;https://dl.acm.org/doi/abs/10.1145/3144763.3144765;QJWkCBJ__qEJ
Tsay, J., Braz, A., Hirzel, M., Shinnar, A., & Mummert, T. (2020, June). Aimmx: Artificial intelligence model metadata extractor. In Proceedings of the 17th international conference on mining software repositories (pp. 81-92).;1_ml_machine_data_learning;2020;Aimmx: Artificial intelligence model metadata extractor;Jason Tsay, Alan Braz, Martin Hirzel, Avraham Shinnar, Todd Mummert;Proceedings of the 17th international conference on mining software repositories, 81-92, 2020;Despite all of the power that machine learning and artificial intelligence (AI) models bring to applications, much of AI development is currently a fairly ad hoc process. Software engineering and AI development share many of the same languages and tools, but AI development as an engineering practice is still in early stages. Mining software repositories of AI models enables insight into the current state of AI development. However, much of the relevant metadata around models are not easily extractable directly from repositories and require deduction or domain knowledge. This paper presents a library called AIMMX that enables simplified AI Model Metadata eXtraction from software repositories. The extractors have five modules for extracting AI model-specific metadata: model name, associated datasets, references, AI frameworks used, and model domain. We evaluated AIMMX against 7,998 open-source models from three sources: model zoos, arXiv AI papers, and state-of-the-art AI papers. Our platform extracted metadata with 87% precision and 83% recall. As preliminary examples of how AI model metadata extraction enables studies and tools to advance engineering support for AI development, this paper presents an exploratory analysis for data and method reproducibility over the models in the evaluation dataset and a catalog tool for discovering and managing models. Our analysis suggests that while data reproducibility may be relatively poor with 42% of models in our sample citing their datasets, method reproducibility is more common at 72% of models in our sample, particularly state-of-the-art models. Our collected models are searchable in a catalog that uses existing metadata to enable advanced discovery features for efficiently finding models.;https://dl.acm.org/doi/abs/10.1145/3379597.3387448;GrKp-COlReEJ
Simmons, A. J., Barnett, S., Rivera-Villicana, J., Bajaj, A., & Vasa, R. (2020, October). A large-scale comparative analysis of coding standard conformance in open-source data science projects. In Proceedings of the 14th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM) (pp. 1-11).;9_data_science_software_process;2020;A large-scale comparative analysis of coding standard conformance in open-source data science projects;Andrew J Simmons, Scott Barnett, Jessica Rivera-Villicana, Akshat Bajaj, Rajesh Vasa;Proceedings of the 14th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM), 1-11, 2020;Background Meeting the growing industry demand for Data Science requires cross-disciplinary teams that can translate machine learning research into production-ready code. Software engineering teams value adherence to coding standards as an indication of code readability, maintainability, and developer expertise. However, there are no large-scale empirical studies of coding standards focused specifically on Data Science projects. Aims This study investigates the extent to which Data Science projects follow code standards. In particular, which standards are followed, which are ignored, and how does this differ to traditional software projects? Method We compare a corpus of 1048 Open-Source Data Science projects to a reference group of 1099 non-Data Science projects with a similar level of quality and maturity. Results Data Science projects suffer from a significantly higher rate of functions that use an excessive numbers of parameters and local variables. Data Science projects also follow different variable naming conventions to non-Data Science projects. Conclusions The differences indicate that Data Science codebases are distinct from traditional software codebases and do not follow traditional software engineering conventions. Our conjecture is that this may be because traditional software engineering conventions are inappropriate in the context of Data Science projects.;https://dl.acm.org/doi/abs/10.1145/3382494.3410680;7tlT7b6QTWcJ
Kim, M. (2020). Software engineering for data analytics. IEEE Software, 37(4), 36-42.;9_data_science_software_process;2020;Software engineering for data analytics;Miryung Kim;IEEE Software 37 (4), 36-42, 2020;We are at an inflection point where software engineering meets the data-centric world of big data, machine learning, and artificial intelligence. In this article, I summarize findings from studies of professional data scientists and discuss my perspectives on open research problems to improve data-centric software development.;https://ieeexplore.ieee.org/abstract/document/9056482/;6mJ0gEvaZg0J
Keele, S. (2007). Guidelines for performing systematic literature reviews in software engineering.;1_ml_machine_data_learning;2007;Guidelines for performing systematic literature reviews in software engineering;Staffs Keele;Technical report, ver. 2.3 ebse technical report. ebse, 2007;"This document presents general guidelines for undertaking systematic reviews. The goal of this document is to introduce the methodology for performing rigorous reviews of current empirical evidence to the software engineering community. It is aimed primarily at software engineering researchers including PhD students. It does not cover details of meta-analysis (a statistical procedure for synthesising quantitative results from different studies), nor does it discuss the implications that different types of systematic review questions have on research procedures.The original impetus for employing systematic literature review practice was to support evidence-based medicine, and many guidelines reflect this viewpoint. This document attempts to construct guidelines for performing systematic literature reviews that are appropriate to the needs of software engineering researchers. It discusses a number of issues where software engineering research differs from medical research. In particular, software engineering research has relatively little empirical research compared with the medical domain; research methods used by software engineers are not as generally rigorous as those used by medical researchers; and much empirical data in software engineering is proprietary.";https://www.researchgate.net/profile/Barbara-Kitchenham/publication/302924724_Guidelines_for_performing_Systematic_Literature_Reviews_in_Software_Engineering/links/61712932766c4a211c03a6f7/Guidelines-for-performing-Systematic-Literature-Reviews-in-Software-Engineering.pdf;XkCAL56caPUJ
Nascimento, N., Alencar, P., Lucena, C., & Cowan, D. (2018, December). Toward human-in-the-loop collaboration between software engineers and machine learning algorithms. In 2018 IEEE International Conference on Big Data (Big Data) (pp. 3534-3540). IEEE.;1_ml_machine_data_learning;2018;Toward human-in-the-loop collaboration between software engineers and machine learning algorithms;Nathalia Nascimento, Paulo Alencar, Carlos Lucena, Donald Cowan;2018 IEEE International Conference on Big Data (Big Data), 3534-3540, 2018;Several papers have recently contained reports on applying machine learning (ML) to the automation of software engineering (SE) tasks, such as project management, modeling and development. However, there appear to be no approaches comparing how software engineers fare against machine-learning algorithms as applied to specific software development tasks. Such a comparison is essential to gain insight into which tasks are better performed by humans and which by machine learning and how cooperative work or human-in-the-loop processes can be implemented more effectively. In this paper, we present an empirical study that compares how software engineers and machine-learning algorithms perform and reuse tasks. The empirical study involves the synthesis of the control structure of an autonomous streetlight application.;https://ieeexplore.ieee.org/abstract/document/8622107/;emRVvUdrlYoJ
Banimustafa, A., & Hardy, N. (2020). A scientific knowledge discovery and data mining process model for metabolomics. Ieee Access, 8, 209964-210005.;1_ml_machine_data_learning;2020;A scientific knowledge discovery and data mining process model for metabolomics;Ahmed Banimustafa, Nigel Hardy;Ieee Access 8, 209964-210005, 2020;This work presents a scientific data mining process model for metabolomics that provides a systematic and formalised framework for guiding and performing metabolomics data analysis in a justifiable and traceable manner. The process model is designed to promote the achievement of the analytical objectives of metabolomics investigations and to ensure the validity, interpretability and reproducibility of their results. It satisfies the requirements of metabolomics data mining, focuses on the contextual meaning of metabolomics knowledge, and addresses the shortcomings of existing data mining process models, while paying attention to the practical aspects of metabolomics investigations and other desirable features. The process model development involved investigating the ontologies and standards of science, data mining and metabolomics and its design was based on the principles, best practices and inspirations from Process Engineering, Software Engineering, Scientific Methodology and Machine Learning. A software environment was built to realise and automate the process model execution and was then applied to a number of metabolomics datasets to demonstrate and evaluate its applicability to different metabolomics investigations, approaches and data acquisition instruments on one hand, and to different data mining approaches, goals, tasks and techniques on the other. The process model was successful in satisfying the requirements of metabolomics data mining and can be generalised to perform data mining in other scientific disciplines.;https://ieeexplore.ieee.org/abstract/document/9263253/;fM5egF9DSdEJ
Singla, K., Bose, J., & Naik, C. (2018, December). Analysis of software engineering for agile machine learning projects. In 2018 15th IEEE India Council International Conference (INDICON) (pp. 1-5). IEEE.;1_ml_machine_data_learning;2018;Analysis of software engineering for agile machine learning projects;Kushal Singla, Joy Bose, Chetan Naik;2018 15th IEEE India Council International Conference (INDICON), 1-5, 2018;The number of machine learning, artificial intelligence or data science related software engineering projects using Agile methodology is increasing. However, there are very few studies on how such projects work in practice. In this paper, we analyze project issues tracking data taken from Scrum (a popular tool for Agile) for several machine learning projects. We compare this data with corresponding data from non-machine learning projects, in an attempt to analyze how machine learning projects are executed differently from normal software engineering projects. On analysis, we find that machine learning project issues use different kinds of words to describe issues, have higher number of exploratory or research oriented tasks as compared to implementation tasks, and have a higher number of issues in the product backlog after each sprint, denoting that it is more difficult to estimate the duration of machine learning project related tasks in advance. After analyzing this data, we propose a few ways in which Agile machine learning projects can be better logged and executed, given their differences with normal software engineering projects.;https://ieeexplore.ieee.org/abstract/document/8987154/;o8EhLxlPrT8J
Kriens, P., & Verbelen, T. (2019). Software engineering practices for machine learning. arXiv preprint arXiv:1906.10366.;1_ml_machine_data_learning;2019;Software engineering practices for machine learning;Peter Kriens, Tim Verbelen;arXiv preprint arXiv:1906.10366, 2019;In the last couple of years we have witnessed an enormous increase of machine learning (ML) applications. More and more program functions are no longer written in code, but learnt from a huge amount of data samples using an ML algorithm. However, what is often overlooked is the complexity of managing the resulting ML models as well as bringing these into a real production system. In software engineering, we have spent decades on developing tools and methodologies to create, manage and assemble complex software modules. We present an overview of current techniques to manage complex software, and how this applies to ML models.;https://arxiv.org/abs/1906.10366;kkAOL9JGwrUJ
Lo, S. K., Lu, Q., Wang, C., Paik, H. Y., & Zhu, L. (2021). A systematic literature review on federated machine learning: From a software engineering perspective. ACM Computing Surveys (CSUR), 54(5), 1-39.;0_federated_learning_data_privacy;2021;A systematic literature review on federated machine learning: From a software engineering perspective;Sin Kit Lo, Qinghua Lu, Chen Wang, Hye-Young Paik, Liming Zhu;ACM Computing Surveys (CSUR) 54 (5), 1-39, 2021;Federated learning is an emerging machine learning paradigm where clients train models locally and formulate a global model based on the local model updates. To identify the state-of-the-art in federated learning and explore how to develop federated learning systems, we perform a systematic literature review from a software engineering perspective, based on 231 primary studies. Our data synthesis covers the lifecycle of federated learning system development that includes background understanding, requirement analysis, architecture design, implementation, and evaluation. We highlight and summarise the findings from the results and identify future trends to encourage researchers to advance their current work.;https://dl.acm.org/doi/abs/10.1145/3450288;cTcBo-BPbwwJ
Wang, S., Huang, L., Ge, J., Zhang, T., Feng, H., Li, M., ... & Ng, V. (2020). Synergy between machine/deep learning and software engineering: How far are we?. arXiv preprint arXiv:2008.05515.;4_dl_testing_deep_network;2020;Synergy between machine/deep learning and software engineering: How far are we?;Simin Wang, Liguo Huang, Jidong Ge, Tengfei Zhang, Haitao Feng, Ming Li, He Zhang, Vincent Ng;arXiv preprint arXiv:2008.05515, 2020;Since 2009, the deep learning revolution, which was triggered by the introduction of ImageNet, has stimulated the synergy between Machine Learning (ML)/Deep Learning (DL) and Software Engineering (SE). Meanwhile, critical reviews have emerged that suggest that ML/DL should be used cautiously. To improve the quality (especially the applicability and generalizability) of ML/DL-related SE studies, and to stimulate and enhance future collaborations between SE/AI researchers and industry practitioners, we conducted a 10-year Systematic Literature Review (SLR) on 906 ML/DL-related SE papers published between 2009 and 2018. Our trend analysis demonstrated the mutual impacts that ML/DL and SE have had on each other. At the same time, however, we also observed a paucity of replicable and reproducible ML/DL-related SE studies and identified five factors that influence their replicability and reproducibility. To improve the applicability and generalizability of research results, we analyzed what ingredients in a study would facilitate an understanding of why a ML/DL technique was selected for a specific SE problem. In addition, we identified the unique trends of impacts of DL models on SE tasks, as well as five unique challenges that needed to be met in order to better leverage DL to improve the productivity of SE tasks. Finally, we outlined a road-map that we believe can facilitate the transfer of ML/DL-based SE research results into real-world industry practices.;https://arxiv.org/abs/2008.05515;SVIrI4Pxa5wJ
Khomh, F., Adams, B., Cheng, J., Fokaefs, M., & Antoniol, G. (2018). Software engineering for machine-learning applications: The road ahead. IEEE Software, 35(5), 81-84.;1_ml_machine_data_learning;2018;Software engineering for machine-learning applications: The road ahead;Foutse Khomh, Bram Adams, Jinghui Cheng, Marios Fokaefs, Giuliano Antoniol;IEEE Software 35 (5), 81-84, 2018;The First Symposium on Software Engineering for Machine Learning Applications (SEMLA) aimed to create a space in which machine learning (ML) and software engineering (SE) experts could come together to discuss challenges, new insights, and practical ideas regarding the engineering of ML and AI-based systems. Key challenges discussed included the accuracy of systems built using ML and AI models, the testing of those systems, industrial applications of AI, and the rift between the ML and SE communities. This article is part of a theme issue on software engineeringâ€™s 50th anniversary.;https://ieeexplore.ieee.org/abstract/document/8474484/;rTAdmYw6o1EJ
Henriksson, J., Borg, M., & Englund, C. (2018, May). Automotive safety and machine learning: Initial results from a study on how to adapt the ISO 26262 safety standard. In Proceedings of the 1st International Workshop on Software Engineering for AI in Autonomous Systems (pp. 47-49).;2_safety_system_autonomous_vehicle;2018;Automotive safety and machine learning: initial results from a study on how to adapt the ISO 26262 safety standard;Jens Henriksson, Markus Borg, Cristofer Englund;SEFAIS '18: Proceedings of the 1st International Workshop on Software Engineering for AI in Autonomous SystemsMay 2018Pages 47–49;Machine learning (ML) applications generate a continuous stream of success stories from various domains. ML enables many novel applications, also in safety-critical contexts. However, the functional safety standards such as ISO 26262 did not evolve to cover ML. We conduct an exploratory study on which parts of ISO 26262 represent the most critical gaps between safety engineering and ML development. While this paper only reports the first steps toward a larger research endeavor, we report three adaptations that are critically needed to allow ISO 26262 compliant engineering, and related suggestions on how to evolve the standard.;https://dl.acm.org/doi/abs/10.1145/3194085.3194090;TODO
Scheerer, M., Klamroth, J., Reussner, R., & Beckert, B. (2020, June). Towards classes of architectural dependability assurance for machine-learning-based systems. In Proceedings of the IEEE/ACM 15th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (pp. 31-37).;2_safety_system_autonomous_vehicle;2020;Towards classes of architectural dependability assurance for machine-learning-based systems;Max Scheerer, Jonas Klamroth, Ralf Reussner, Bernhard Beckert;Proceedings of the IEEE/ACM 15th International Symposium on Software Engineering for Adaptive and Self-Managing Systems, 31-37, 2020;Advances in Machine Learning (ML) have brought previously hard to handle problems within arm's reach. However, this power comes at the cost of unassured reliability and lacking transparency. Overcoming this drawback is very hard due to the probabilistic nature of ML. Current approaches mainly tackle this problem by developing more robust learning procedures. Such algorithmic approaches, however, are limited to certain types of uncertainties and cannot deal with all of them, e.g., hardware failure. This paper discusses how this problem can be addressed at architectural rather than algorithmic level to assess systems dependability properties in early development stages. Moreover, we argue that Self-Adaptive Systems (SAS) are more suited to safeguard ML w.r.t. various uncertainties. As a step towards this we propose classes of dependability in which ML-based systems may be categorized and discuss which and how assurances can be made for each class.;https://dl.acm.org/doi/abs/10.1145/3387939.3388613;Cw4PAlGBcioJ
Cummaudo, A., Vasa, R., Barnett, S., Grundy, J., & Abdelrazek, M. (2020, June). Interpreting cloud computer vision pain-points: A mining study of Stack Overflow. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (pp. 1584-1596).;1_ml_machine_data_learning;2020;Interpreting cloud computer vision pain-points: A mining study of Stack Overflow;Alex Cummaudo, Rajesh Vasa, Scott Barnett, John Grundy, Mohamed Abdelrazek;Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, 1584-1596, 2020;"Intelligent services are becoming increasingly more pervasive; application developers want to leverage the latest advances in areas such as computer vision to provide new services and products to users, and large technology firms enable this via RESTful APIs. While such APIs promise an easy-to-integrate on-demand machine intelligence, their current design, documentation and developer interface hides much of the underlying machine learning techniques that power them. Such APIs look and feel like conventional APIs but abstract away data-driven probabilistic behaviour---the implications of a developer treating these APIs in the same way as other, traditional cloud services, such as cloud storage, is of concern. The objective of this study is to determine the various pain-points developers face when implementing systems that rely on the most mature of these intelligent services, specifically those that provide computer vision. We use Stack Overflow to mine indications of the frustrations that developers appear to face when using computer vision services, classifying their questions against two recent classification taxonomies (documentation-related and general questions). We find that, unlike mature fields like mobile development, there is a contrast in the types of questions asked by developers. These indicate a shallow understanding of the underlying technology that empower such systems. We discuss several implications of these findings via the lens of learning taxonomies to suggest how the software engineering community can improve these services and comment on the nature by which developers use them.";https://dl.acm.org/doi/abs/10.1145/3377811.3380404;Y_3BCk-WrqoJ
Gerasimou, S., Eniser, H. F., Sen, A., & Cakan, A. (2020, June). Importance-driven deep learning system testing. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (pp. 702-713).;4_dl_testing_deep_network;2020;Importance-driven deep learning system testing;Simos Gerasimou, Hasan Ferit Eniser, Alper Sen, Alper Cakan;Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, 702-713, 2020;Deep Learning (DL) systems are key enablers for engineering intelligent applications due to their ability to solve complex tasks such as image recognition and machine translation. Nevertheless, using DL systems in safety- and security-critical applications requires to provide testing evidence for their dependable operation. Recent research in this direction focuses on adapting testing criteria from traditional software engineering as a means of increasing confidence for their correct behaviour. However, they are inadequate in capturing the intrinsic properties exhibited by these systems. We bridge this gap by introducing DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems, across multiple DL datasets and with state-of-the-art adversarial generation techniques demonstrates the usefulness and effectiveness of DeepImportance and its ability to support the engineering of more robust DL systems.;https://dl.acm.org/doi/abs/10.1145/3377811.3380391;b5KoIfR3mpIJ
Shams, R. (2018). Developing machine learning products better and faster at startups. IEEE Engineering Management Review, 46(3), 36-39.;1_ml_machine_data_learning;2018;Developing machine learning products better and faster at startups;Rushdi Shams;IEEE Engineering Management Review 46 (3), 36-39, 2018;There are many uncertainties in developing machine-learning (ML)-based products. Due to the gap between research and development, the overall progress becomes slow, and experiences many failures and learnings only to see an initial idea not working or generating no significant revenue. To minimize these drastic effects, there are continuing studies to make a balanced handshake between research and development to shorten the time span of ML-based products from idea generation to deployment. This paper demonstrates a three-phase ML product development workflow at OneClass. The workflow is a combination of multiple best-practices in the innovation-based startup industry. The first phase of the workflow considers the pivotal idea generation for products that involves data reliability assessment, idea prioritization, expectation setting, and building trust among users. The second phase concentrates on several state-of-the-art strategies for planning and future re-use of several product components. Finally, the actual research and development phase describes the fail-fast method practiced by OneClass to learn quickly from failures and act accordingly. The workflow is followed by the company to develop many sophisticated ML-based products successfully within a very short period of time.;https://ieeexplore.ieee.org/abstract/document/8486814/;hiFLRQ4T8yMJ
Idowu, S., Strüber, D., & Berger, T. (2021, May). Asset management in machine learning: A survey. In 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) (pp. 51-60). IEEE.;1_ml_machine_data_learning;2021;Asset management in machine learning: A survey;Samuel Idowu, Daniel StrÃ¼ber, Thorsten Berger;2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), 51-60, 2021;Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.;https://ieeexplore.ieee.org/abstract/document/9401964/;ltvAYYBKs0AJ
Rahimi, M., Guo, J. L., Kokaly, S., & Chechik, M. (2019, September). Toward requirements specification for machine-learned components. In 2019 IEEE 27th International Requirements Engineering Conference Workshops (REW) (pp. 241-244). IEEE.;2_safety_system_autonomous_vehicle;2019;Toward requirements specification for machine-learned components;Mona Rahimi, Jin LC Guo, Sahar Kokaly, Marsha Chechik;2019 IEEE 27th International Requirements Engineering Conference Workshops (REW), 241-244, 2019;"In current practice, the behavior of Machine-Learned Components (MLCs) is not sufficiently specified by the predefined requirements. Instead, they ""learn"" existing patterns from the available training data, and make predictions for unseen data when deployed. On the surface, their ability to extract patterns and to behave accordingly is specifically useful for hard-to-specify concepts in certain safety critical domains (e.g., the definition of a pedestrian in a pedestrian detection component in a vehicle). However, the lack of requirements specifications on their behaviors makes further software engineering tasks challenging for such components. This is especially concerning for tasks such as safety assessment and assurance. In this position paper, we call for more attention from the requirements engineering community on supporting the specification of requirements for MLCs in safety critical domains. Towards that end, we propose an approach to improve the process of requirements specification in which an MLC is developed and operates by explicitly specifying domain-related concepts. Our approach extracts a universally accepted benchmark for hard-to-specify concepts (e.g., ""pedestrian"") and can be used to identify gaps in the associated dataset and the constructed machine-learned model.";https://ieeexplore.ieee.org/abstract/document/8933771/;dvAfxOaaj_wJ
Regalado, R. V. (2018, March). Building artificial intelligent (AI) products that make sense. In Proceedings of the 4th International Conference on Human-Computer Interaction and User Experience in Indonesia, CHIuXiD'18 (pp. 93-96).;1_ml_machine_data_learning;2018;Building artificial intelligent (AI) products that make sense;Ralph Vincent Regalado;Proceedings of the 4th International Conference on Human-Computer Interaction and User Experience in Indonesia, CHIuXiD'18, 93-96, 2018;"Developing AI products for the mass market has been an ongoing challenge considering product-market fit, limited technology know-how, and user adoption. This paper provides a list of challenges and explores how we can launch AI products from idea to market by applying design thinking and lean-startup strategies. Artificial Intelligence;";https://dl.acm.org/doi/abs/10.1145/3205946.3205960;QXoHhYiabX4J
Metelskaia, I., Ignatyeva, O., Denef, S., & Samsonowa, T. (2018, June). A business model template for AI solutions. In Proceedings of the International Conference on Intelligent Science and Technology (pp. 35-41).;1_ml_machine_data_learning;2018;A business model template for AI solutions;Iuliia Metelskaia, Olga Ignatyeva, Sebastian Denef, Tatjana Samsonowa;Proceedings of the International Conference on Intelligent Science and Technology, 35-41, 2018;In this paper, we present a canvas that describes the building blocks of business models for AI solutions. As startups, spin-offs and existing corporations increasingly transfer AI research and technology into commercial products and services, AI engineers can benefit from focusing and positioning their work within the overall strategy of such ventures. We designed the business model canvas for AI solutions based on existing research, nine cases from secondary sources, as well as five case studies that we conducted ourselves. Our research highlights the most frequently used elements in each block of the business model canvas, such as common characteristics in the value propositions, multi-sided platforms in company segments, automated service in customer relationship, social networks in channels, investors in key partners, R&D in key activities, human resources in key resources and Software-as-a-Service in revenue. We designed the canvas as a tool useful for anybody creating or analyzing AI solutions. On a larger perspective, our study contributes to the existing literature on business model and business model innovation by consolidating existing practices of AI sector and emphasizing on important patterns.;https://dl.acm.org/doi/abs/10.1145/3233740.3233750;pIgKje_Y7jkJ
Kumar, A., Boehm, M., & Yang, J. (2017, May). Data management in machine learning: Challenges, techniques, and systems. In Proceedings of the 2017 ACM International Conference on Management of Data (pp. 1717-1722).;1_ml_machine_data_learning;2017;Data management in machine learning: Challenges, techniques, and systems;Arun Kumar, Matthias Boehm, Jun Yang;Proceedings of the 2017 ACM International Conference on Management of Data, 1717-1722, 2017;Large-scale data analytics using statistical machine learning (ML), popularly called advanced analytics, underpins many modern data-driven applications. The data management community has been working for over a decade on tackling data management-related challenges that arise in ML workloads, and has built several systems for advanced analytics. This tutorial provides a comprehensive review of such systems and analyzes key data management challenges and techniques. We focus on three complementary lines of work: (1) integrating ML algorithms and languages with existing data systems such as RDBMSs, (2) adapting data management-inspired techniques such as query optimization, partitioning, and compression to new systems that target ML workloads, and (3) combining data management and ML ideas to build systems that improve ML lifecycle-related tasks. Finally, we identify key open data management challenges for future research in this important area.;https://dl.acm.org/doi/abs/10.1145/3035918.3054775;JEpIoJTSWdMJ
Holstein, K., Wortman Vaughan, J., Daumé III, H., Dudik, M., & Wallach, H. (2019, May). Improving fairness in machine learning systems: What do industry practitioners need?. In Proceedings of the 2019 CHI conference on human factors in computing systems (pp. 1-16).;6_fairness_discrimination_bias_decision;2019;Improving fairness in machine learning systems: What do industry practitioners need?;Kenneth Holstein, Jennifer Wortman Vaughan, Hal DaumÃ© III, Miro Dudik, Hanna Wallach;Proceedings of the 2019 CHI conference on human factors in computing systems, 1-16, 2019;The potential for machine learning (ML) systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness. If these tools are to have a positive impact on industry practice, however, it is crucial that their design be informed by an understanding of real-world needs. Through 35 semi-structured interviews and an anonymous survey of 267 ML practitioners, we conduct the first systematic investigation of commercial product teams' challenges and needs for support in developing fairer ML systems. We identify areas of alignment and disconnect between the challenges faced by teams in practice and the solutions proposed in the fair ML research literature. Based on these findings, we highlight directions for future ML and HCI research that will better address practitioners' needs.;https://dl.acm.org/doi/abs/10.1145/3290605.3300830;4M_UVrQydPoJ
De-Arteaga, M., Herlands, W., Neill, D. B., & Dubrawski, A. (2018). Machine learning for the developing world. ACM Transactions on Management Information Systems (TMIS), 9(2), 1-14.;1_ml_machine_data_learning;2018;Machine learning for the developing world;Maria De-Arteaga, William Herlands, Daniel B Neill, Artur Dubrawski;ACM Transactions on Management Information Systems (TMIS) 9 (2), 1-14, 2018;Researchers from across the social and computer sciences are increasingly using machine learning to study and address global development challenges. This article examines the burgeoning field of machine learning for the developing world (ML4D). First, we present a review of prominent literature. Next, we suggest best practices drawn from the literature for ensuring that ML4D projects are relevant to the advancement of development objectives. Finally, we discuss how developing world challenges can motivate the design of novel machine learning methodologies. This article provides insights into systematic differences between ML4D and more traditional machine learning applications. It also discusses how technical complications of ML4D can be treated as novel research questions, how ML4D can motivate new research directions, and where machine learning can be most useful.;https://dl.acm.org/doi/abs/10.1145/3210548;lIZdilMkXMwJ
Chaoji, V., Rastogi, R., & Roy, G. (2016). Machine learning in the real world. Proceedings of the VLDB Endowment, 9(13), 1597-1600.;1_ml_machine_data_learning;2016;Machine learning in the real world;Vineet Chaoji, Rajeev Rastogi, Gourav Roy;Proceedings of the VLDB Endowment 9 (13), 1597-1600, 2016;Machine Learning (ML) has become a mature technology that is being applied to a wide range of business problems such as web search, online advertising, product recommendations, object recognition, and so on. As a result, it has become imperative for researchers and practitioners to have a fundamental understanding of ML concepts and practical knowledge of end-to-end modeling. This tutorial takes a hands-on approach to introducing the audience to machine learning. The first part of the tutorial gives a broad overview and discusses some of the key concepts within machine learning. The second part of the tutorial takes the audience through the end-to-end modeling pipeline for a real-world income prediction problem.;https://dl.acm.org/doi/abs/10.14778/3007263.3007318;wPQgNdPygZ4J
Fard, A., Le, A., Larionov, G., Dhillon, W., & Bear, C. (2020, June). Vertica-ml: Distributed machine learning in vertica database. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data (pp. 755-768).;1_ml_machine_data_learning;2020;Vertica-ml: Distributed machine learning in vertica database;Arash Fard, Anh Le, George Larionov, Waqas Dhillon, Chuck Bear;Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data, 755-768, 2020;"A growing number of companies rely on machine learning as a key element for gaining a competitive edge from their collected Big Data. An in-database machine learning system can provide many advantages in this scenario, e.g., eliminating the overhead of data transfer, avoiding the maintenance costs of a separate analytical system, and addressing data security and provenance concerns. In this paper, we present our distributed machine learning subsystem within the Vertica database. This subsystem, Vertica-ML, includes machine learning functionalities with SQL API which cover a complete data science workflow as well as model management. We treat machine learning models in Vertica as first-class database objects like tables and views; therefore, they enjoy a similar mechanism for archiving and managing. We explain the architecture of the subsystem, and present a set of experiments to evaluate the performance of the machine learning algorithms implemented on top of it.";https://dl.acm.org/doi/abs/10.1145/3318464.3386137;lFm1yTVt4ZEJ
Nalchigar, S., Yu, E., & Keshavjee, K. (2021). Modeling machine learning requirements from three perspectives: a case report from the healthcare domain. Requirements Engineering, 26, 237-254.;1_ml_machine_data_learning;2021;Modeling machine learning requirements from three perspectives: a case report from the healthcare domain;Soroosh Nalchigar, Eric Yu, Karim Keshavjee;Requirements Engineering 26, 237-254, 2021;Implementing machine learning in an enterprise involves tackling a wide range of complexities with respect to requirements elicitation, design, development, and deployment of such solutions. Despite the necessity and relevance of requirements engineering approaches to the process, not much research has been done in this area. This paper employs a case study method to evaluate the expressiveness and usefulness of GR4ML,Â a conceptual modeling framework for requirements elicitation, design, and development of machine learning solutions. Our results confirm that the framework includes an adequate set of concepts for expressing machine learning requirements and solution design. The case study also demonstrates that the framework can be useful in machine learning projects by revealing new requirements that would have been missed without using the framework, as well as, by facilitating communication among project team members of different roles and backgrounds. Feedback from study participants and areas of improvement to the framework are also discussed.;https://link.springer.com/article/10.1007/s00766-020-00343-z;AhfHD5m_Wb0J
Mikkonen, T., Nurminen, J. K., Raatikainen, M., Fronza, I., Mäkitalo, N., & Männistö, T. (2021). Is machine learning software just software: A maintainability view. In Software Quality: Future Perspectives on Software Engineering Quality: 13th International Conference, SWQD 2021, Vienna, Austria, January 19–21, 2021, Proceedings 13 (pp. 94-105). Springer International Publishing.;1_ml_machine_data_learning;2021;Is machine learning software just software: A maintainability view;Tommi Mikkonen, Jukka K Nurminen, Mikko Raatikainen, Ilenia Fronza, Niko Mäkitalo, Tomi Männistö;Software Quality: Future Perspectives on Software Engineering Quality: 13th International Conference, SWQD 2021, Vienna, Austria, January 19–21, 2021, Proceedings 13, 94-105, 2021;Artificial intelligence (AI) and machine learning (ML) is becoming commonplace in numerous fields. As they are often embedded in the context of larger software systems, issues that are faced with software systems in general are also applicable to AI/ML. In this paper, we address ML systems and their characteristics in the light of software maintenance and its attributes, modularity, testability, reusability, analysability, and modifiability. To achieve this, we pinpoint similarities and differences between ML software and software as we traditionally understand it, and draw parallels as well as provide a programmer’s view to ML at a general level, using the established software design principles as the starting point.;https://link.springer.com/chapter/10.1007/978-3-030-65854-0_8;IOP1x93Y9G8J
Canhoto, A. I., & Clear, F. (2020). Artificial intelligence and machine learning as business tools: A framework for diagnosing value destruction potential. Business Horizons, 63(2), 183-193.;1_ml_machine_data_learning;2020;Artificial intelligence and machine learning as business tools: A framework for diagnosing value destruction potential;Ana Isabel Canhoto, Fintan Clear;Business Horizons 63 (2), 183-193, 2020;Artificial intelligence (AI) and machine learning (ML) may save money and improve the efficiency of business processes, but these technologies can also destroy business value, sometimes with grave consequences. The inability to identify and manage that risk can lead some managers to delay the adoption of these technologies and thus prevent them from realizing their potential. This article proposes a new framework by which to map the components of an AI solution and to identify and manage the value-destruction potential of AI and ML for businesses. We show how the defining characteristics of AI and ML can threaten the integrity of the AI systemâ€™s inputs, processes, and outcomes. We then draw from the concepts of value-creation content and value-creation process to show how these risks may hinder value creation or even result in value destruction. Finally, we illustrate the application of our framework with an example of the deployment of an AI-powered chatbot in customer service, and we discuss how to remedy the problems that arise.;https://www.sciencedirect.com/science/article/pii/S0007681319301570;QIDm35qtNAgJ
Lee, I., & Shin, Y. J. (2020). Machine learning for enterprises: Applications, algorithm selection, and challenges. Business Horizons, 63(2), 157-170.;1_ml_machine_data_learning;2020;Machine learning for enterprises: Applications, algorithm selection, and challenges;In Lee, Yong Jae Shin;Business Horizons 63 (2), 157-170, 2020;Machine learning holds great promise for lowering product and service costs, speeding up business processes, and serving customers better. It is recognized as one of the most important application areas in this era of unprecedented technological development, and its adoption is gaining momentum across almost all industries. In view of this, we offer a brief discussion of categories of machine learning and then present three types of machine-learning usage at enterprises. We then discuss the trade-off between the accuracy and interpretability of machine-learning algorithms, a crucial consideration in selecting the right algorithm for the task at hand. We next outline three cases of machine-learning development in financial services. Finally, we discuss challenges all managers must confront in deploying machine-learning applications.;https://www.sciencedirect.com/science/article/pii/S0007681319301521;_loFM5G3R3IJ
Marbán, O., Segovia, J., Menasalvas, E., & Fernández-Baizán, C. (2009). Toward data mining engineering: A software engineering approach. Information systems, 34(1), 87-107.;1_ml_machine_data_learning;2009;Toward data mining engineering: A software engineering approach;Oscar Marbán, Javier Segovia, Ernestina Menasalvas, Covadonga Fernández-Baizán;Information systems 34 (1), 87-107, 2009;The number, variety and complexity of projects involving data mining or knowledge discovery in databases activities have increased just lately at such a pace that aspects related to their development process need to be standardized for results to be integrated, reused and interchanged in the future. Data mining projects are quickly becoming engineering projects, and current standard processes, like CRISP-DM, need to be revisited to incorporate this engineering viewpoint. This is the central motivation of this paper that makes the point that …;https://www.sciencedirect.com/science/article/pii/S0306437908000355;bVTq7F0IV6oJ
Ferreira, H., Ruivo, P., & Reis, C. (2021). How do data scientists and managers influence machine learning value creation?. Procedia Computer Science, 181, 757-764.;1_ml_machine_data_learning;2021;How do data scientists and managers influence machine learning value creation?;Humberto Ferreira, Pedro Ruivo, Carolina Reis;Procedia Computer Science 181, 757-764, 2021;Corporations are leveraging machine learning (ML) to create business value (BV). So, it becomes relevant to not only ponder the antecedents that influence the ML BV process but also, the main actors that influence the creation of such value within organizations: data scientists and managers. Grounded in the dynamic-capabilities theory, a model is proposed and tested with 319 responses to a survey. While for both groups, platform maturity and data quality are equally important factors for financial performance, information intensity is an equally important factor for organizational performance. On one hand, data scientists care more about the catalytic effect of data quality on the relationship between platform maturity and financial performance, and the compatibility factor for organizational performance. On the other hand, managers care more about the feasibility factor for financial performance. The findings presented here offer insights on how data scientists and managers perceive the ML BV creation process.;https://www.sciencedirect.com/science/article/pii/S1877050921002714;adp0kS4An1sJ
Schröer, C., Kruse, F., & Gómez, J. M. (2021). A systematic literature review on applying CRISP-DM process model. Procedia Computer Science, 181, 526-534.;1_ml_machine_data_learning;2021;A systematic literature review on applying CRISP-DM process model;Christoph Schröer, Felix Kruse, Jorge Marx Gómez;Procedia Computer Science 181, 526-534, 2021;CRISP-DM is the de-facto standard and an industry-independent process model for applying data mining projects. Twenty years after its release in 2000, we would like to provide a systematic literature review of recent studies published in IEEE, ScienceDirect and ACM about data mining use cases applying CRISP-DM. We give an overview of the research focus, current methodologies, best practices and possible gaps in conducting the six phases of CRISP-DM. The main findings are that CRISP-DM is still a de-factor standard in data mining, but there are challenges since the most studies do not foresee a deployment phase. The contribution of our paper is to identify best practices and process phases in which data mining analysts can be better supported. Further contribution is a template for structuring and releasing CRISP-DM studies.;https://www.sciencedirect.com/science/article/pii/S1877050921002416;4BgTGPdXjBoJ
Martinez, I., Viles, E., & Olaizola, I. G. (2021). Data science methodologies: Current challenges and future approaches. Big Data Research, 24, 100183.;9_data_science_software_process;2021;Data science methodologies: Current challenges and future approaches;Iñigo Martinez, Elisabeth Viles, Igor G Olaizola;Big Data Research 24, 100183, 2021;Data science has employed great research efforts in developing advanced analytics, improving data models and cultivating new algorithms. However, not many authors have come across the organizational and socio-technical challenges that arise when executing a data science project: lack of vision and clear objectives, a biased emphasis on technical issues, a low level of maturity for ad-hoc projects and the ambiguity of roles in data science are among these challenges. Few methodologies have been proposed on the literature that tackle these …;https://www.sciencedirect.com/science/article/pii/S2214579620300514;Bdd2lNRY56cJ
Takeuchi, H., & Yamamoto, S. (2020). Business analysis method for constructing business–AI alignment model. Procedia Computer Science, 176, 1312-1321.;1_ml_machine_data_learning;2020;Business analysis method for constructing business–AI alignment model;Hironori Takeuchi, Shuichiro Yamamoto;Procedia Computer Science 176, 1312-1321, 2020;In this study, we consider the construction of a model for representing an artificial intelligence (AI) service system project. When developing a system using AI technologies to support a business task in a company, all project members from both business and IT divisions must have common understandings on the project before starting it. For this purpose, a business–IT alignment model for AI service systems is proposed as a business–AI alignment model. However, we need to substantiate this business–AI alignment model for each project, because it is a generic model. To address this problem, we propose a method for constructing the business– AI alignment model and apply it to a real project for developing an AI service system in a case study, and confirm that we can construct the project-specific business–AI alignment model–without support of data scientists.;https://www.sciencedirect.com/science/article/pii/S1877050920320408;IgkJsR5sgQwJ
Biesialska, K., Franch, X., & Muntés-Mulero, V. (2021). Big Data analytics in Agile software development: A systematic mapping study. Information and Software Technology, 132, 106448.;9_data_science_software_process;2021;Big Data analytics in Agile software development: A systematic mapping study;Katarzyna Biesialska, Xavier Franch, Victor MuntÃ©s-Mulero;Information and Software Technology 132, 106448, 2021;Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity.Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA).As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019.In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics.As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.;https://www.sciencedirect.com/science/article/pii/S0950584920301981;pYc283zw4EAJ
Stanula, P., Ziegenbein, A., & Metternich, J. (2018). Machine learning algorithms in production: A guideline for efficient data source selection. Procedia CIRP, 78, 261-266.;1_ml_machine_data_learning;2018;Machine learning algorithms in production: A guideline for efficient data source selection;Patrick Stanula, Amina Ziegenbein, Joachim Metternich;Procedia CIRP 78, 261-266, 2018;Data acquisition, storage and processing becomes increasingly affordable and the use of machine learning algorithms feasible in the field of manufacturing. Even though state of the art machine tools are packed with sensors, the dataâ€™s benefits are difficult to assess in advance. Thus, this paper presents a management approach to select the most promising data sources regarding a defined objective. Quality Function Deployment matches the process specific objectives with preselected data sources. The preselection prevents the necessity to examine all possibilities while not restricting innovative solutions. This allows a targeted approach to fully exploit the advantages of machine learning. The approach is validated by a use case based on machine tool data.;https://www.sciencedirect.com/science/article/pii/S2212827118310266;AVyBDa2zRd8J
Carreira, J., Fonseca, P., Tumanov, A., Zhang, A., & Katz, R. (2018). A case for serverless machine learning. In Workshop on Systems for ML and Open Source Software at NeurIPS (Vol. 2018, pp. 2-8).;8_serverless_cloud_cost_computing;2018;A case for serverless machine learning;Joao Carreira, Pedro Fonseca, Alexey Tumanov, Andrew Zhang, Randy Katz;Workshop on Systems for ML and Open Source Software at NeurIPS 2018, 2-8, 2018;The scale and complexity of ML workflows makes it hard to provision and manage resourcesâ€”a burden for ML practitioners that hinders both their productivity and effectiveness. Encouragingly, however, serverless computing has recently emerged as a compelling solution to address the general problem of data center resource management. This work analyzes the resource management problem in the specific context of ML workloads and explores a research direction that leverages serverless infrastructures to automate the management of resources for ML workflows. We make a case for a serverless machine learning framework, specializing both for serverless infrastructures and Machine Learning workflows, and argue that either of those in isolation is insufficient.;https://www.cs.purdue.edu/homes/pfonseca/papers/mlsys18-disaggregatedml.pdf;qwVNVz_zwwgJ
Feng, L., Kudva, P., Da Silva, D., & Hu, J. (2018, July). Exploring serverless computing for neural network training. In 2018 IEEE 11th international conference on cloud computing (CLOUD) (pp. 334-341). IEEE.;8_serverless_cloud_cost_computing;2018;Exploring serverless computing for neural network training;Lang Feng, Prabhakar Kudva, Dilma Da Silva, Jiang Hu;2018 IEEE 11th international conference on cloud computing (CLOUD), 334-341, 2018;Serverless or functions as a service runtimes have shown significant benefits to efficiency and cost for event-driven cloud applications. Although serverless runtimes are limited to applications requiring lightweight computation and memory, such as machine learning prediction and inference, they have shown improvements on these applications beyond other cloud runtimes. Training deep learning can be both compute and memory intensive. We investigate the use of serverless runtimes while leveraging data parallelism for large models, show the challenges and limitations due to the tightly coupled nature of such models, and propose modifications to the underlying runtime implementations that would mitigate them. For hyperparameter optimization of smaller deep learning models, we show that serverless runtimes can provide significant benefit.;https://ieeexplore.ieee.org/abstract/document/8457817/;xynWxhJUdQoJ
Deese, A. (2018, May). Implementation of unsupervised k-means clustering algorithm within amazon web services lambda. In 2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID) (pp. 626-632). IEEE.;8_serverless_cloud_cost_computing;2018;Implementation of unsupervised k-means clustering algorithm within amazon web services lambda;Anthony Deese;2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID), 626-632, 2018;This work demonstrates how an unsupervised learning algorithm based on k-Means Clustering with Kaufman Initialization may be implemented effectively as an Amazon Web Services Lambda Function, within their serverless cloud computing service. It emphasizes the need to employ a lean and modular design philosophy, transfer data efficiently between Lambda and DynamoDB, as well as employ Lambda Functions within mobile applications seamlessly and with negligible latency. This work presents a novel application of serverless cloud computing and provides specific examples that will allow readers to develop similar algorithms. The author provides compares the computation speed and cost of machine learning implementations on traditional PC and mobile hardware (running locally) as well as implementations that employ Lambda.;https://ieeexplore.ieee.org/abstract/document/8411080/;eHt8rywCbscJ
Tu, Z., Li, M., & Lin, J. (2018, June). Pay-per-request deployment of neural network models using serverless architectures. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations (pp. 6-10).;8_serverless_cloud_cost_computing;2018;Pay-per-request deployment of neural network models using serverless architectures;Zhucheng Tu, Mengping Li, Jimmy Lin;Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, 6-10, 2018;We demonstrate the serverless deployment of neural networks for model inferencing in NLP applications using Amazonâ€™s Lambda service for feedforward evaluation and DynamoDB for storing word embeddings. Our architecture realizes a pay-per-request pricing model, requiring zero ongoing costs for maintaining server instances. All virtual machine management is handled behind the scenes by the cloud provider without any direct developer intervention. We describe a number of techniques that allow efficient use of serverless resources, and evaluations confirm that our design is both scalable and inexpensive.;https://aclanthology.org/N18-5002/;jOlJfYaXARcJ
Ishakian, V., Muthusamy, V., & Slominski, A. (2018, April). Serving deep learning models in a serverless platform. In 2018 IEEE International conference on cloud engineering (IC2E) (pp. 257-262). IEEE.;8_serverless_cloud_cost_computing;2018;Serving deep learning models in a serverless platform;Vatche Ishakian, Vinod Muthusamy, Aleksander Slominski;2018 IEEE International conference on cloud engineering (IC2E), 257-262, 2018;Serverless computing has emerged as a compelling paradigm for the development and deployment of a wide range of event based cloud applications. At the same time, cloud providers and enterprise companies are heavily adopting machine learning and Artificial Intelligence to either differentiate themselves, or provide their customers with value added services. In this work we evaluate the suitability of a serverless computing environment for the inferencing of large neural network models. Our experimental evaluations are executed on the AWS Lambda environment using the MxNet deep learning framework. Our experimental results show that while the inferencing latency can be within an acceptable range, longer delays due to cold starts can skew the latency distribution and hence risk violating more stringent SLAs.;https://ieeexplore.ieee.org/abstract/document/8360337/;oLKqH1XEWhgJ
Bhattacharjee, A., Chhokra, A. D., Kang, Z., Sun, H., Gokhale, A., & Karsai, G. (2019, June). Barista: Efficient and scalable serverless serving system for deep learning prediction services. In 2019 IEEE International Conference on Cloud Engineering (IC2E) (pp. 23-33). IEEE.;8_serverless_cloud_cost_computing;2019;Barista: Efficient and scalable serverless serving system for deep learning prediction services;Anirban Bhattacharjee, Ajay Dev Chhokra, Zhuangwei Kang, Hongyang Sun, Aniruddha Gokhale, Gabor Karsai;2019 IEEE International Conference on Cloud Engineering (IC2E), 23-33, 2019;Pre-trained deep learning models are increasingly being used to offer a variety of compute-intensive predictive analytics services such as fitness tracking, speech, and image recognition. The stateless and highly parallelizable nature of deep learning models makes them well-suited for serverless computing paradigm. However, making effective resource management decisions for these services is a hard problem due to the dynamic workloads and diverse set of available resource configurations that have different deployment and management costs. To address these challenges, we present a distributed and scalable deep-learning prediction serving system called Barista and make the following contributions. First, we present a fast and effective methodology for forecasting workloads by identifying various trends. Second, we formulate an optimization problem to minimize the total cost incurred while ensuring bounded prediction latency with reasonable accuracy. Third, we propose an efficient heuristic to identify suitable compute resource configurations. Fourth, we propose an intelligent agent to allocate and manage the compute resources by horizontal and vertical scaling to maintain the required prediction latency. Finally, using representative real-world workloads for an urban transportation service, we demonstrate and validate the capabilities of Barista.;https://ieeexplore.ieee.org/abstract/document/8790088/;fJ5l9qizZ6EJ
Damkevala, D., Lunavara, R., Kosamkar, M., & Jayachandran, S. (2019, March). Behavior analysis using serverless machine learning. In 2019 6th International Conference on Computing for Sustainable Global Development (INDIACom) (pp. 1068-1072). IEEE.;1_ml_machine_data_learning;2019;Behavior analysis using serverless machine learning;Darezik Damkevala, Rohit Lunavara, Mansi Kosamkar, Suja Jayachandran;2019 6th International Conference on Computing for Sustainable Global Development (INDIACom), 1068-1072, 2019;This paper supplies a route for using the Watson Machine Learning API on IBM Cloud to carry out serverless data analytics using machine learning as a service. Transforming the large amount of data produced by an organization into intelligence can be done using advanced analytics methods such as using a modified Mahalanobis Distance algorithm for synthesis of correlation data under the purview of machine learning. Further refinement of correlation data is done using a Multivariate Reliability Classifier model. The consumption of this advanced analytics service can be done in a serverless manner where the developer only must be concerned with how the data is analyzed, i.e., scoring, batch or stream models with a continuous learning system without the outlay of hardware upon which to train those models. This paper examines the usage of such serverless AI systems in the scope of user behavior analysis over varied demographics.;https://ieeexplore.ieee.org/abstract/document/8991407/;NVjbbB_H4KwJ
Carreira, J., Fonseca, P., Tumanov, A., Zhang, A., & Katz, R. (2019, November). Cirrus: A serverless framework for end-to-end ml workflows. In Proceedings of the ACM Symposium on Cloud Computing (pp. 13-24).;8_serverless_cloud_cost_computing;2019;Cirrus: A serverless framework for end-to-end ml workflows;Joao Carreira, Pedro Fonseca, Alexey Tumanov, Andrew Zhang, Randy Katz;Proceedings of the ACM Symposium on Cloud Computing, 13-24, 2019;Machine learning (ML) workflows are extremely complex. The typical workflow consists of distinct stages of user interaction, such as preprocessing, training, and tuning, that are repeatedly executed by users but have heterogeneous computational requirements. This complexity makes it challenging for ML users to correctly provision and manage resources and, in practice, constitutes a significant burden that frequently causes over-provisioning and impairs user productivity. Serverless computing is a compelling model to address the resource management problem, in general, but there are numerous challenges to adopt it for existing ML frameworks due to significant restrictions on local resources.This work proposes Cirrus---an ML framework that automates the end-to-end management of datacenter resources for ML workflows by efficiently taking advantage of serverless infrastructures. Cirrus combines the simplicity of the serverless interface and the scalability of the serverless infrastructure (AWS Lambdas and S3) to minimize user effort. We show a design specialized for both serverless computation and iterative ML training is needed for robust and efficient ML training on serverless infrastructure. Our evaluation shows that Cirrus outperforms frameworks specialized along a single dimension: Cirrus is 100x faster than a general purpose serverless system [36] and 3.75x faster than specialized ML frameworks for traditional infrastructures [49].;https://dl.acm.org/doi/abs/10.1145/3357223.3362711;zZG2HMh2lDgJ
Wang, H., Niu, D., & Li, B. (2019, April). Distributed machine learning with a serverless architecture. In IEEE INFOCOM 2019-IEEE Conference on Computer Communications (pp. 1288-1296). IEEE.;8_serverless_cloud_cost_computing;2019;Distributed machine learning with a serverless architecture;Hao Wang, Di Niu, Baochun Li;IEEE INFOCOM 2019-IEEE Conference on Computer Communications, 1288-1296, 2019;The need to scale up machine learning, in the presence of a rapid growth of data both in volume and in variety, has sparked broad interests to develop distributed machine learning systems, typically based on parameter servers. However, since these systems are based on a dedicated cluster of physical or virtual machines, they have posed non-trivial cluster management overhead to machine learning practitioners and data scientists. In addition, there exists an inherent mismatch between the dynamically varying resource demands during a model training job and the inflexible resource provisioning model of current cluster-based systems.In this paper, we propose SIREN, an asynchronous distributed machine learning framework based on the emerging serverless architecture, with which stateless functions can be executed in the cloud without the complexity of building and maintaining virtual machine infrastructures. With SIREN, we are able to achieve a higher level of parallelism and elasticity by using a swarm of stateless functions, each working on a different batch of data, while greatly reducing system configuration overhead. Furthermore, we propose a scheduler based on Deep Reinforcement Learning to dynamically control the number and memory size of the stateless functions that should be used in each training epoch. The scheduler learns from the training process itself, in pursuit for the minimum possible training time given a cost. With our real-world prototype implementation on AWS Lambda, extensive experimental results have shown that SIREN can reduce model training time by up to 44%, as compared to traditional machine learning training benchmarks on AWS EC2 at the same cost.;https://ieeexplore.ieee.org/abstract/document/8737391/;4XyExjr311QJ
Fotouhi, M., Chen, D., & Lloyd, W. J. (2019, December). Function-as-a-service application service composition: Implications for a natural language processing application. In Proceedings of the 5th International Workshop on Serverless Computing (pp. 49-54).;8_serverless_cloud_cost_computing;2019;Function-as-a-service application service composition: Implications for a natural language processing application;Mohammadbagher Fotouhi, Derek Chen, Wes J Lloyd;Proceedings of the 5th International Workshop on Serverless Computing, 49-54, 2019;Serverless computing platforms provide Function-as-a-Service (FaaS) to end users for hosting individual functions known as microservices. In this paper, we describe the deployment of a Natural Language Processing (NLP) application using AWS Lambda. We investigate and study the performance and memory implications of two alternate service compositions. First, we evaluate a switchboard architecture, where a single Lambda deployment package aggregates all of the NLP application functions together into a single package. Second, we consider a service isolation architecture where each NLP function is deployed as a separate FaaS function decomposing the application to run across separate runtime containers. We compared the average runtime and processing throughput of these compositions using different pre-trained network weights to initialize our neural networks to perform inference. Additionally, we varied the workload dataset sizes to evaluate implications of inferencing throughput for our NLP application deployed to a FaaS platform. We found our switchboard composition, that shares FaaS runtime containers for all application tasks, produced a 14.75% runtime performance improvement, and also a 17.3% improvement in NLP processing throughput (samples/second). These results demonstrate the potential for careful application service compositions to provide notable performance improvements and ultimately cost savings for application deployments to serverless FaaS platforms.;https://dl.acm.org/doi/abs/10.1145/3366623.3368141;sOqvOMiW5qEJ
Zhang, C., Yu, M., Wang, W., & Yan, F. (2019). {MArk}: Exploiting Cloud Services for {Cost-Effective},{SLO-Aware} Machine Learning Inference Serving. In 2019 USENIX Annual Technical Conference (USENIX ATC 19) (pp. 1049-1062).;8_serverless_cloud_cost_computing;2019;{MArk}: Exploiting Cloud Services for {Cost-Effective},{SLO-Aware} Machine Learning Inference Serving;Chengliang Zhang, Minchen Yu, Wei Wang, Feng Yan;2019 USENIX Annual Technical Conference (USENIX ATC 19), 1049-1062, 2019;The advances of Machine Learning (ML) have sparked a growing demand of ML-as-a-Service: developers train ML models and publish them in the cloud as online services to provide low-latency inference at scale. The key challenge of ML model serving is to meet the response-time Service-Level Objectives (SLOs) of inference workloads while minimizing the serving cost. In this paper, we tackle the dual challenge of SLO compliance and cost effectiveness with MArk (Model Ark), a general-purpose inference serving system built in Amazon Web Services (AWS). MArk employs three design choices tailor-made for inference workload. First, MArk dynamically batches requests and opportunistically serves them using expensive hardware accelerators (eg, GPU) for improved performance-cost ratio. Second, instead of relying on feedback control scaling or over-provisioning to serve dynamic workload, which can be too slow or too expensive for inference serving, MArk employs predictive autoscaling to hide the provisioning latency at low cost. Third, given the stateless nature of inference serving, MArk exploits the flexible, yet costly serverless instances to cover the occasional load spikes that are hard to predict. We evaluated the performance of MArk using several state-of-the-art ML models trained in popular frameworks including TensorFlow, MXNet, and Keras. Compared with the premier industrial ML serving platform SageMaker, MArk reduces the serving cost up to 7.8Ã— while achieving even better latency performance.;https://www.usenix.org/conference/atc19/presentation/zhang-chengliang;a51k9aj6YowJ
Barcelona-Pons, D., Sánchez-Artigas, M., París, G., Sutra, P., & García-López, P. (2019, December). On the faas track: Building stateful distributed applications with serverless architectures. In Proceedings of the 20th international middleware conference (pp. 41-54).;8_serverless_cloud_cost_computing;2019;On the faas track: Building stateful distributed applications with serverless architectures;Daniel Barcelona-Pons, Marc Sánchez-Artigas, Gerard París, Pierre Sutra, Pedro García-López;Proceedings of the 20th international middleware conference, 41-54, 2019;Serverless computing is an emerging paradigm that greatly simplifies the usage of cloud resources and suits well to many tasks. Most notably, Function-as-a-Service (FaaS) enables programmers to develop cloud applications as individual functions that can run and scale independently. Yet, due to the disaggregation of storage and compute resources in FaaS, applications that require fine-grained support for mutable state and synchronization, such as machine learning and scientific computing, are hard to build.In this work, we present Crucial, a system to program highly-concurrent stateful applications with serverless architectures. Its programming model keeps the simplicity of FaaS and allows to port effortlessly multi-threaded algorithms to this new environment. Crucial is built upon the key insight that FaaS resembles to concurrent programming at the scale of a data center. As a consequence, a distributed shared memory layer is the right answer to the need for fine-grained state management and coordination in serverless. We validate our system with the help of micro-benchmarks and various applications. In particular, we implement two common machine learning algorithms: k-means clustering and logistic regression. For both cases, Crucial obtains superior or comparable performance to an equivalent Spark cluster.;https://dl.acm.org/doi/abs/10.1145/3361525.3361535;SEcweXwNUFUJ
Zhang, M., Krintz, C., Mock, M., & Wolski, R. (2019, July). Seneca: Fast and low cost hyperparameter search for machine learning models. In 2019 IEEE 12th international conference on cloud computing (CLOUD) (pp. 404-408). IEEE.;1_ml_machine_data_learning;2019;Seneca: Fast and low cost hyperparameter search for machine learning models;Michael Zhang, Chandra Krintz, Markus Mock, Rich Wolski;2019 IEEE 12th international conference on cloud computing (CLOUD), 404-408, 2019;The goal of our work is to simplify and expedite the construction and evaluation of machine learning models using autoscaled cloud computing resources. To enable this, we develop an open source system called Seneca, which leverages the serverless programming model and its implementation in Amazon Web Services (AWS) Lambda. Seneca takes a machine learning application, dataset, and a list of possible hyperparameter options as input and automatically constructs an AWS Lambda function. The function ingresses and splits the input dataset into training and testing subsets and constructs, tests, and evaluates (i.e. scores) a machine learning model for a given set of hyperparameter values. Seneca concurrently invokes functions for all combinations of the hyperparameters specified. It then returns the configuration (or model) that results in the best score to the user. In this paper, we overview the design and implementation of Seneca, and empirically evaluate its performance for a popular classification application.;https://ieeexplore.ieee.org/abstract/document/8814572/;bRQFnLgXaPMJ
Christidis, A., Davies, R., & Moschoyiannis, S. (2019, November). Serving machine learning workloads in resource constrained environments: A serverless deployment example. In 2019 IEEE 12th Conference on Service-Oriented Computing and Applications (SOCA) (pp. 55-63). IEEE.;8_serverless_cloud_cost_computing;2019;Serving machine learning workloads in resource constrained environments: A serverless deployment example;Angelos Christidis, Roy Davies, Sotiris Moschoyiannis;2019 IEEE 12th Conference on Service-Oriented Computing and Applications (SOCA), 55-63, 2019;"Deployed AI platforms typically ship with bulky system architectures which present bottlenecks and a high risk of failure. A serverless deployment can mitigate these factors and provide a cost-effective, automatically scalable (up or down) and elastic real-time on-demand AI solution. However, deploying high complexity production workloads into serverless environments is far from trivial, e.g., due to factors such as minimal allowance for physical codebase size, low amount of runtime memory, lack of GPU support and a maximum runtime before termination via timeout. In this paper we propose a set of optimization techniques and show how these transform a codebase which was previously incompatible with a serverless deployment into one that can be successfully deployed in a serverless environment; without compromising capability or performance. The techniques are illustrated via worked examples that have been deployed live on rail data and realtime predictions on train movements on the UK rail network. The similarities of a serverless environment to other resource constrained environments (IoT, Mobile) means the techniques can be applied to a range of use cases.";https://ieeexplore.ieee.org/abstract/document/8953024/;zv6Qn_Oc5gMJ
Bhattacharjee, A., Barve, Y., Khare, S., Bao, S., Gokhale, A., & Damiano, T. (2019). Stratum: A serverless framework for the lifecycle management of machine learning-based data analytics tasks. In 2019 USENIX Conference on Operational Machine Learning (OpML 19) (pp. 59-61).;1_ml_machine_data_learning;2019;Stratum: A serverless framework for the lifecycle management of machine learning-based data analytics tasks;Anirban Bhattacharjee, Yogesh Barve, Shweta Khare, Shunxing Bao, Aniruddha Gokhale, Thomas Damiano;2019 USENIX Conference on Operational Machine Learning (OpML 19), 59-61, 2019;With the proliferation of machine learning (ML) libraries and frameworks, and the programming languages that they use, along with operations of data loading, transformation, preparation and mining, ML model development is becoming a daunting task. Furthermore, with a plethora of cloud-based ML model development platforms, heterogeneity in hardware, increased focus on exploiting edge computing resources for low-latency prediction serving and often a lack of a complete understanding of resources required to execute ML workflows efficiently, ML model deployment demands expertise for managing the lifecycle of ML workflows efficiently and with minimal cost. To address these challenges, we propose an end-to-end data analytics, a serverless platform called Stratum. Stratum can deploy, schedule and dynamically manage data ingestion tools, live streaming apps, batch analytics tools, ML-as-a-service (for inference jobs), and visualization tools across the cloud-fog-edge spectrum. This paper describes the Stratum architecture highlighting the problems it resolves.;https://www.usenix.org/conference/opml19/presentation/bhattacharjee;LLoTLO_Et6EJ
Dakkak, A., Li, C., De Gonzalo, S. G., Xiong, J., & Hwu, W. M. (2019, July). Trims: Transparent and isolated model sharing for low latency deep learning inference in function-as-a-service. In 2019 IEEE 12th International Conference on Cloud Computing (CLOUD) (pp. 372-382). IEEE.;8_serverless_cloud_cost_computing;2019;Trims: Transparent and isolated model sharing for low latency deep learning inference in function-as-a-service;Abdul Dakkak, Cheng Li, Simon Garcia De Gonzalo, Jinjun Xiong, Wen-mei Hwu;2019 IEEE 12th International Conference on Cloud Computing (CLOUD), 372-382, 2019;Deep neural networks (DNNs) have become core computation components within low latency Function as a Service (FaaS) prediction pipelines. Cloud computing, as the defacto backbone of modern computing infrastructure, has to be able to handle user-defined FaaS pipelines containing diverse DNN inference workloads while maintaining isolation and latency guarantees with minimal resource waste. The current solution for guaranteeing isolation and latency within FaaS is inefficient. A major cause of the inefficiency is the need to move large amount of data within and across servers. We propose TrIMS as a novel solution to address this issue. TrIMSis a generic memory sharing technique that enables constant data to be shared across processes or containers while still maintaining isolation between users. TrIMS consists of a persistent model store across the GPU, CPU, local storage, and cloud storage hierarchy, an efficient resource management layer that provides isolation, and a succinct set of abstracts, applicationAPIs, and container technologies for easy and transparent integration with FaaS, Deep Learning (DL) frameworks, and user code. We demonstrate our solution by interfacing TrIMS with the Apache MXNet framework and demonstrate up to 24x speedup in latency for image classification models, up to 210x speedup for large models, and up to8Ã—system throughput improvement.;https://ieeexplore.ieee.org/abstract/document/8814494/;W73roq8owrYJ
Kaplunovich, A., & Yesha, Y. (2020, December). Automatic tuning of hyperparameters for neural networks in serverless cloud. In 2020 IEEE International Conference on Big Data (Big Data) (pp. 2751-2756). IEEE.;1_ml_machine_data_learning;2020;Automatic tuning of hyperparameters for neural networks in serverless cloud;Alex Kaplunovich, Yelena Yesha;2020 IEEE International Conference on Big Data (Big Data), 2751-2756, 2020;Deep Neural Networks are used to solve the most challenging world problems. In spite of the numerous advancements in the field, most of the models are being tuned manually. Experienced Data Scientists have to manually optimize hyperparameters, such as dropout rate, learning rate or number of neurons for Big Data applications. We have implemented a flexible automatic real-time hyperparameter tuning methodology. It works for arbitrary models written in Python and Keras. We also utilized state of the art Cloud services such as trigger based serverless computing (Lambda), and advanced GPU instances to implement automation, reliability and scalability.The existing tuning libraries, such as hyperopt, Scikit-Optimize or SageMaker, require developers to provide a list of hyperparameters and the range of their values manually. Our novel approach detects potential hyperparameters automatically from the source code, updates the original model to tune the parameters, runs the evaluation in the Cloud on spot instances, finds the optimal hyperparameters, and saves the results in the No-SQL database. The methodology can be applied to numerous Big Data Machine Learning systems.;https://ieeexplore.ieee.org/abstract/document/9378280/;Hd7VnczF7GYJ
Ali, A., Pinciroli, R., Yan, F., & Smirni, E. (2020, November). Batch: machine learning inference serving on serverless platforms with adaptive batching. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis (pp. 1-15). IEEE.;8_serverless_cloud_cost_computing;2020;Batch: machine learning inference serving on serverless platforms with adaptive batching;Ahsan Ali, Riccardo Pinciroli, Feng Yan, Evgenia Smirni;SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, 1-15, 2020;Serverless computing is a new pay-per-use cloud service paradigm that automates resource scaling for stateless functions and can potentially facilitate bursty machine learning serving. Batching is critical for latency performance and cost-effectiveness of machine learning inference, but unfortunately it is not supported by existing serverless platforms due to their stateless design. Our experiments show that without batching, machine learning serving cannot reap the benefits of serverless computing. In this paper, we present BATCH, a framework for supporting efficient machine learning serving on serverless platforms. BATCH uses an optimizer to provide inference tail latency guarantees and cost optimization and to enable adaptive batching support. We prototype BATCH atop of AWS Lambda and popular machine learning inference systems. The evaluation verifies the accuracy of the analytic optimizer and demonstrates performance and cost advantages over the state-of-the-art method MArk and the state-of-the-practice tool SageMaker.;https://ieeexplore.ieee.org/abstract/document/9355312/;wmIrRLVpf1cJ
Elordi, U., Unzueta, L., Goenetxea, J., Sanchez-Carballido, S., Arganda-Carreras, I., & Otaegui, O. (2020). Benchmarking deep neural network inference performance on serverless environments with MLPerf. IEEE Software, 38(1), 81-87.;8_serverless_cloud_cost_computing;2020;Benchmarking deep neural network inference performance on serverless environments with MLPerf;Unai Elordi, Luis Unzueta, Jon Goenetxea, Sergio Sanchez-Carballido, Ignacio Arganda-Carreras, Oihana Otaegui;IEEE Software 38 (1), 81-87, 2020;We provide a novel decomposition methodology from the current MLPerf benchmark to the serverless function execution model. We have tested our approach in Amazon Lambda to benchmark the processing capabilities of OpenCV and OpenVINO inference engines.;https://ieeexplore.ieee.org/abstract/document/9219119/;m8GoohbyR5sJ
Zhang, C., Yu, M., Wang, W., & Yan, F. (2020). Enabling cost-effective, slo-aware machine learning inference serving on public cloud. IEEE Transactions on Cloud Computing, 10(3), 1765-1779.;8_serverless_cloud_cost_computing;2020;Enabling cost-effective, slo-aware machine learning inference serving on public cloud;Chengliang Zhang, Minchen Yu, Wei Wang, Feng Yan;IEEE Transactions on Cloud Computing 10 (3), 1765-1779, 2020;The remarkable advances of Machine Learning (ML) have spurred an increasing demand for ML- as-a-Service on public cloud: developers train and publish ML models as online services to provide low-latency inference for dynamic queries. The primary challenge of ML model serving is to meet the response-time Service-Level Objectives (SLOs) of inference workloads while minimizing serving cost. In this article, we proposes MArk (Model Ark), a general-purpose inference serving system, to tackle the dual challenge of SLO compliance and cost effectiveness. MArk employs three design choices tailored to inference workload. First, MArk dynamically batches requests and opportunistically serves them using expensive hardware accelerators (e.g., GPU) for improved performance-cost ratio. Second, instead of relying on feedback control scaling or over-provisioning to serve dynamic workload, which can be too slow or too expensive, MArk employs predictive autoscaling to hide the provisioning latency at low cost. Third, given the stateless nature of inference serving, MArk exploits the flexible, yet costly serverless instances to cover occasional load spikes that are hard to predict. We evaluated the performance of MArk using several state-of-the-art ML models trained in TensorFlow, MXNet, and Keras. Compared with the premier industrial ML serving platform SageMaker, MArk reduces the serving cost up to while achieving even better latency performance.;https://ieeexplore.ieee.org/abstract/document/9132666/;xHiwp6E96OUJ
Shillaker, S., & Pietzuch, P. (2020). Faasm: Lightweight isolation for efficient stateful serverless computing. In 2020 USENIX Annual Technical Conference (USENIX ATC 20) (pp. 419-433).;8_serverless_cloud_cost_computing;2020;Faasm: Lightweight isolation for efficient stateful serverless computing;Simon Shillaker, Peter Pietzuch;2020 USENIX Annual Technical Conference (USENIX ATC 20), 419-433, 2020;Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms isolate functions in ephemeral, stateless containers, preventing them from directly sharing memory. This forces users to duplicate and serialise data repeatedly, adding unnecessary performance and resource costs. We believe that a new lightweight isolation approach is needed, which supports sharing memory directly between functions and reduces resource overheads.;https://www.usenix.org/conference/atc20/presentation/shillaker;GD1nGc7LV7YJ
Gunasekaran, J. R., Mishra, C. S., Thinakaran, P., Kandemir, M. T., & Das, C. R. (2020, December). Implications of public cloud resource heterogeneity for inference serving. In Proceedings of the 2020 Sixth International Workshop on Serverless Computing (pp. 7-12).;1_ml_machine_data_learning;2020;Implications of public cloud resource heterogeneity for inference serving;Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth Thinakaran, Mahmut Taylan Kandemir, Chita R Das;Proceedings of the 2020 Sixth International Workshop on Serverless Computing, 7-12, 2020;We are witnessing an increasing trend towards using Machine Learning (ML) based prediction systems, spanning across different application domains, including product recommendation systems, personal assistant devices, facial recognition, etc. These applications typically have diverse requirements in terms of accuracy and response latency, that can be satisfied by a myriad of ML models. However, the deployment cost of prediction serving primarily depends on the type of resources being procured, which by themselves are heterogeneous in terms of provisioning latencies and billing complexity. Thus, it is strenuous for an inference serving system to choose from this confounding array of resource types and model types to provide low-latency and cost-effective inferences. In this work we quantitatively characterize the cost, accuracy and latency implications of hosting ML inferences on different public cloud resource offerings. Our evaluation shows that, prior work does not solve the problem from both dimensions of model and resource heterogeneity. Hence, to holistically address this problem, we need to solve the issues that arise from combining both model and resource heterogeneity towards optimizing for application constraints. Towards this, we discuss the design implications of a self-managed inference serving system, which can optimize for application requirements based on public cloud resource characteristics.;https://dl.acm.org/doi/abs/10.1145/3429880.3430093;ZNu5PJJSl08J
Chahal, D., Ojha, R., Ramesh, M., & Singhal, R. (2020, October). Migrating large deep learning models to serverless architecture. In 2020 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW) (pp. 111-116). IEEE.;8_serverless_cloud_cost_computing;2020;Migrating large deep learning models to serverless architecture;Dheeraj Chahal, Ravi Ojha, Manju Ramesh, Rekha Singhal;2020 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW), 111-116, 2020;Serverless computing platform is emerging as a solution for event-driven artificial intelligence applications. Function-as-a-Service (FaaS) using serverless computing paradigm provides high performance and low cost solutions for deploying such applications on cloud while minimizing the operational logic. Using FaaS for efficient deployment of complex applications, such as natural language processing (NLP) and image processing, containing large deep learning models will be an advantage. However, constrained resources and stateless nature of FaaS offers numerous challenges while deploying such applications. In this work, we discuss the methodological suggestions and their implementation for deploying pre-trained large size machine learning and deep learning models on FaaS. We also evaluate the performance and deployment cost of an enterprise application, consisting of suite of deep vision preprocessing algorithms and models, on VM and FaaS platform. Our evaluation shows that migration from monolithic to FaaS platform significantly improves the performance of the application at a reduced cost.;https://ieeexplore.ieee.org/abstract/document/9307673/;M_gaYmOuBeUJ
Choi, J., Lee, J., & Cho, W. J. (2020, June). Prognostics by classifying degradation stage on Lambda architecture. In 2020 IEEE International Conference on Prognostics and Health Management (ICPHM) (pp. 1-9). IEEE.;1_ml_machine_data_learning;2020;Prognostics by classifying degradation stage on Lambda architecture;Jinhyuck Choi, Jinwoo Lee, Won Jeong Cho;2020 IEEE International Conference on Prognostics and Health Management (ICPHM), 1-9, 2020;To enhance the reliability and availability of an asset in its life, predicting the remaining useful life of an asset is strongly encouraged by assessing the extent of deviation or degradation of the asset's monitored parameters from its expected normal operating conditions. Although intelligent fault prognostic techniques such as machine learning and artificial neural networks have been applied in modern industries, application in actual industrial conditions requires that the forecasting process is revealed and more descriptive. To investigate the issue and increase the accuracy, this paper proposes an additional technique that can be further applied to any recent intelligent prognostic methods. The proposed method consists of two steps. First, the entire training set is divided into several degradation stages before regression using a heuristic approach and then the regression results are synthesized for each stage. The proposed method will increase the monotonicity of the predictive parameters, thus helping improve the predictive model's accuracy. To demonstrate the hypothesis, real condition monitoring data of high-pressure LNG pump and acceleration experimental data of a rotating machine is used for an experiment. Moreover, a system in which the proposed method can be appropriately executed is introduced with Lambda architecture. Finally, by demonstrating that the proposed method is capable of parallel computing, it is proven suitable for use in the proposed large-scale distributed processing system.;https://ieeexplore.ieee.org/abstract/document/9187061/;g7fmG-fE68oJ
Kaplunovich, A., & Yesha, Y. (2020, June). Refactoring of Neural Network Models for Hyperparameter Optimization in Serverless Cloud. In Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops (pp. 311-314).;1_ml_machine_data_learning;2020;Refactoring of Neural Network Models for Hyperparameter Optimization in Serverless Cloud;Alex Kaplunovich, Yelena Yesha;Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops, 311-314, 2020;Machine Learning and Neural Networks in particular have become hot topics in Computer Science. The recent 2019 Turing award to the forefathers of Deep Learning and AI - Yoshua Bengio, Geoffrey Hinton, and Yann LeCun proves the importance of the technology and its effect on science and industry. However, we have realized that even nowadays, the state of the art methods require several manual steps for neural network hyperparameter optimization. Our approach automates the model tuning by refactoring the original Python code using open-source libraries for processing. We were able to identify hyperparameters by parsing the original source and analyzing it. Given these parameters, we refactor the model, add the state of the art optimization library calls, and run the updated code in the Serverless Cloud. Our approach has proven to eliminate manual steps for an arbitrary TensorFlow and Keras tuning. We have created a tool called OptPar which automatically refactors an arbitrary Deep Neural Network optimizing its hyperparameters. Such a transformation can save hours of time for Data Scientists, giving them an opportunity to concentrate on designing their Machine Learning algorithms.;https://dl.acm.org/doi/abs/10.1145/3387940.3392268;KClItrg06QoJ
Zhang, M., Krintz, C., & Wolski, R. (2020, March). Stoic: Serverless teleoperable hybrid cloud for machine learning applications on edge device. In 2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops) (pp. 1-6). IEEE.;8_serverless_cloud_cost_computing;2020;Stoic: Serverless teleoperable hybrid cloud for machine learning applications on edge device;Michael Zhang, Chandra Krintz, Rich Wolski;2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops), 1-6, 2020;Serverless computing is a promising new event-driven programming model that was designed by cloud vendors to expedite the development and deployment of scalable web services on cloud computing systems. Using the model, developers write applications that consist of simple, independent, stateless functions that the cloud invokes on-demand (i.e. elastically), in response to system-wide events (data arrival, messages, web requests, etc.). In this work, we present STOIC (Serverless TeleOperable HybrId Cloud), an application scheduling and deployment system that extends the serverless model in two ways. First, it uses the model in a distributed setting and schedules application functions across multiple cloud systems. Second, STOIC supports serverless function execution using hardware acceleration (e.g. GPU resources) when available from the underlying cloud system. We overview the design and implementation of STOIC and empirically evaluate it using real-world machine learning applications and multi-tier (e.g. edge-cloud) deployments. We find that STOIC's combined use of edge and cloud resources is able to outperform using either cloud in isolation for the applications and datasets that we consider.;https://ieeexplore.ieee.org/abstract/document/9156239/;-4XZOcshV2YJ
Chadha, M., Jindal, A., & Gerndt, M. (2020, December). Towards federated learning using faas fabric. In Proceedings of the 2020 sixth international workshop on serverless computing (pp. 49-54).;0_federated_learning_data_privacy;2020;Towards federated learning using faas fabric;Mohak Chadha, Anshul Jindal, Michael Gerndt;Proceedings of the 2020 sixth international workshop on serverless computing, 49-54, 2020;Federated learning (FL) enables resource-constrained edge devices to learn a shared Machine Learning (ML) or Deep Neural Network (DNN) model, while keeping the training data local and providing privacy, security, and economic benefits. However, building a shared model for heterogeneous devices such as resource-constrained edge and cloud makes the efficient management of FL-clients challenging. Furthermore, with the rapid growth of FL-clients, the scaling of FL training process is also difficult.In this paper, we propose a possible solution to these challenges: federated learning over a combination of connected Function-as-a-Service platforms, i.e., FaaS fabric offering a seamless way of extending FL to heterogeneous devices. Towards this, we present FedKeeper, a tool for efficiently managing FL over FaaS fabric. We demonstrate the functionality of FedKeeper by using three FaaS platforms through an image classification task with a varying number of devices/clients, different stochastic optimizers, and local computations (local epochs).;https://dl.acm.org/doi/abs/10.1145/3429880.3430100;cFD0IabNjdQJ
Kanagaraj, K., & Geetha, S. (2021, February). A Hybrid Framework for Effective Prediction of Online Streaming Data. In Journal of Physics: Conference Series (Vol. 1767, No. 1, p. 012016). IOP Publishing.;8_serverless_cloud_cost_computing;2021;A Hybrid Framework for Effective Prediction of Online Streaming Data;K Kanagaraj, S Geetha;Journal of Physics: Conference Series 1767 (1), 012016, 2021;In this paper, we present a hybrid model to perform the training and testing of prediction model with online streaming data. Prediction of online streaming data is a time critical task. Huge volume of data that is being generated online need to be ingested to a prediction model and to be used to train and test the prediction model dynamically which improves the learning rate. The existing approaches for dynamic training and testing use the local infrastructure or virtual machines from the cloud infrastructure to increase the learning rate of the prediction model with streaming data. Recently many applications prefer serverless cloud infrastructure than virtual machines. However, using the serverless infrastructure for the entire prediction process will have time and space tradeoffs due to its autonomic feature. Hence in this paper we propose a hybrid approach that uses the three different environments such as the local infrastructure, virtual machine and serverless cloud for different stages. A novel approach to select the suitable environment to train and test the LSTM based air quality prediction model with stream data is proposed with increased learning rate and reduced resource utilization.;https://iopscience.iop.org/article/10.1088/1742-6596/1767/1/012016/meta;MFqpK1QZLVgJ
Naranjo, D. M., Risco, S., Moltó, G., & Blanquer, I. (2023). A serverless gateway for event‐driven machine learning inference in multiple clouds. Concurrency and Computation: Practice and Experience, 35(18), e6728.;8_serverless_cloud_cost_computing;2023;A serverless gateway for event‐driven machine learning inference in multiple clouds;Diana M Naranjo, Sebastián Risco, Germán Moltó, Ignacio Blanquer;Concurrency and Computation: Practice and Experience 35 (18), e6728, 2023;Serverless computing and, in particular, the functions as a service model has become a convincing paradigm for the development and implementation of highly scalable applications in the cloud. This is due to the transparent management of three key functionalities: triggering of functions due to events, automatic provisioning and scalability of resources, and fine‐grained pay‐per‐use. This article presents a serverless web‐based scientific gateway to execute the inference phase of previously trained machine learning and artificial intelligence models. The execution of the models is performed both in Amazon Web Services and in on‐premises clouds with the OSCAR framework for serverless scientific computing. In both cases, the computing infrastructure grows elastically according to the demand adopting scale‐to‐zero approaches to minimize costs. The web interface provides an improved user experience by simplifying the use of the models. The usage of machine learning in a computing platform that can use both on‐premises clouds and public clouds constitutes a step forward in the adoption of serverless computing for scientific applications.;https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6728;qyFhboXYZdoJ
Jarachanthan, J., Chen, L., Xu, F., & Li, B. (2021, August). Amps-inf: Automatic model partitioning for serverless inference with cost efficiency. In Proceedings of the 50th International Conference on Parallel Processing (pp. 1-12).;8_serverless_cloud_cost_computing;2021;Amps-inf: Automatic model partitioning for serverless inference with cost efficiency;Jananie Jarachanthan, Li Chen, Fei Xu, Bo Li;Proceedings of the 50th International Conference on Parallel Processing, 1-12, 2021;The salient pay-per-use nature of serverless computing has driven its continuous penetration as an alternative computing paradigm for various workloads. Yet, challenges arise and remain open when shifting machine learning workloads to the serverless environment. Specifically, the restriction on the deployment size over serverless platforms combining with the complexity of neural network models makes it difficult to deploy large models in a single serverless function. In this paper, we aim to fully exploit the advantages of the serverless computing paradigm for machine learning workloads targeting at mitigating management and overall cost while meeting the response-time Service Level Objective (SLO). We design and implement AMPS-Inf, an autonomous framework customized for model inferencing in serverless computing. Driven by the cost-efficiency and timely-response, our proposed AMPS-Inf automatically generates the optimal execution and resource provisioning plans for inference workloads. The core of AMPS-Inf relies on the formulation and solution of a Mixed-Integer Quadratic Programming problem for model partitioning and resource provisioning with the objective of minimizing cost without violating response time SLO. We deploy AMPS-Inf on the AWS Lambda platform, evaluate with the state-of-the-art pre-trained models in Keras including ResNet50, Inception-V3 and Xception, and compare with Amazon SageMaker and three baselines. Experimental results demonstrate that AMPS-Inf achieves up to 98% cost saving without degrading response time performance.;https://dl.acm.org/doi/abs/10.1145/3472456.3472501;O-auZiw4TxcJ
Kaplunovich, A., & Yesha, Y. (2021, May). Automatic Hyperparameter Optimization for Arbitrary Neural Networks in Serverless AWS Cloud. In 2021 12th International Conference on Information and Communication Systems (ICICS) (pp. 69-76). IEEE.;1_ml_machine_data_learning;2021;Automatic Hyperparameter Optimization for Arbitrary Neural Networks in Serverless AWS Cloud;Alex Kaplunovich, Yelena Yesha;2021 12th International Conference on Information and Communication Systems (ICICS), 69-76, 2021;Deep Neural Networks are the most efficient method to solve many challenging problems. The importance of the subject can be demonstrated by the fact that the 2019 Turing Award was given to the godfathers of AI (and Neural Networks) Yoshua Bengio, Geoffrey Hinton, and Yann LeCun. In spite of the numerous advancements in the field, most of the models are being tuned manually. Accurate models became especially important during the novel coronavirus pandemic.Many day-to-day decisions depend on the model predictions affecting billions of people. We implemented a flexible automatic real-time hyperparameter tuning approach for arbitrary DNN models written in Python and Keras without manual steps. All of the existing tuning libraries require manual steps (like hyperopt, Scikit-Optimize or SageMaker). We provide an innovative methodology to automate hyper-parameter tuning for an arbitrary Neural Network model source code, utilizing Serverless Cloud and implementing revolutionary microservices, security, interoperability and orchestration. Our methodology can be used in numerous applications, including Information and Communication Systems.;https://ieeexplore.ieee.org/abstract/document/9464618/;lM5R__3MHSgJ
Shahidi, N., Gunasekaran, J. R., & Kandemir, M. T. (2021, November). Cross-Platform Performance Evaluation of Stateful Serverless Workflows. In 2021 IEEE International Symposium on Workload Characterization (IISWC) (pp. 63-73). IEEE.;8_serverless_cloud_cost_computing;2021;Cross-Platform Performance Evaluation of Stateful Serverless Workflows;Narges Shahidi, Jashwant Raj Gunasekaran, Mahmut Taylan Kandemir;2021 IEEE International Symposium on Workload Characterization (IISWC), 63-73, 2021;Serverless computing, with its inherent event-driven design along with instantaneous scalability due to cloud-provider managed infrastructure, is starting to become a de-facto model for deploying latency critical user-interactive services. However, as much as they are suitable for event-driven services, their stateless nature is a major impediment for deploying long-running stateful applications. While commercial cloud providers offer a variety of solutions that club serverless functions along with intermediate storage to maintain application state, they are still far from optimized for deploying stateful applications at scale. More specifically, factors such as storage latency and scalability, network bandwidth, and deployment costs play a crucial role in determining whether current serverless applications are suitable for stateful workloads. In this paper, we evaluate the two widely-used stateful server-less offerings, Azure Durable functions and AWS Step functions, to quantify their effectiveness for implementing complex stateful workflows. We conduct a detailed measurement-driven characterization study with two real-world use cases, machine learning pipelines (inference and training) and video analytics, in order to characterize the different performance latency and cost tradeoffs. We observe from our experiments that AWS is suitable for workloads with higher degree of parallelism, while Azure durable entities offer a simplified framework that enables quicker application development. Overall, AWS is 89% more expensive than Azure for machine learning training application while Azure is 2Ã— faster than AWS for the machine learning inference application. Our results indicate that Azure durable is extremely inefficient in implementing parallel processing. Furthermore, we summarize the key findings from our characterization, which we believe to be insightful for any cloud tenant who has the problem of choosing an appropriate cloud vendor and offering, when deploving stateful workloads on serverless platforms,;https://ieeexplore.ieee.org/abstract/document/9668304/;br13pgnyhmwJ
Kurz, M. S. (2021, April). Distributed double machine learning with a serverless architecture. In Companion of the ACM/SPEC International Conference on Performance Engineering (pp. 27-33).;8_serverless_cloud_cost_computing;2021;Distributed double machine learning with a serverless architecture;Malte S Kurz;Companion of the ACM/SPEC International Conference on Performance Engineering, 27-33, 2021;This paper explores serverless cloud computing for double machine learning. Being based on repeated cross-fitting, double machine learning is particularly well suited to exploit the high level of parallelism achievable with serverless computing. It allows to get fast on-demand estimations without additional cloud maintenance effort. We provide a prototype Python implementation DoubleML-Serverless for the estimation of double machine learning models with the serverless computing platform AWS Lambda and demonstrate its utility with a case study analyzing estimation times and costs.;https://dl.acm.org/doi/abs/10.1145/3447545.3451181;HKwHZ4PFHKoJ
Thorpe, J., Qiao, Y., Eyolfson, J., Teng, S., Hu, G., Jia, Z., ... & Xu, G. H. (2021). Dorylus: Affordable, scalable, and accurate {GNN} training with distributed {CPU} servers and serverless threads. In 15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21) (pp. 495-514).;7_edge_computing_deep_learning;2021;Dorylus: Affordable, scalable, and accurate {GNN} training with distributed {CPU} servers and serverless threads;John Thorpe, Yifan Qiao, Jonathan Eyolfson, Shen Teng, Guanzhou Hu, Zhihao Jia, Jinliang Wei, Keval Vora, Ravi Netravali, Miryung Kim, Guoqing Harry Xu;15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21), 495-514, 2021;A graph neural network (GNN) enables deep learning on structured graph data. There are two major GNN training obstacles: 1) it relies on high-end servers with many GPUs which are expensive to purchase and maintain, and 2) limited memory on GPUs cannot scale to today's billion-edge graphs. This paper presents Dorylus: a distributed system for training GNNs. Uniquely, Dorylus can take advantage of serverless computing to increase scalability at a low cost.;https://www.usenix.org/conference/osdi21/presentation/thorpe;Ca7htR3EJv4J
Zhang, M., Krintz, C., & Wolski, R. (2021). Edge‐adaptable serverless acceleration for machine learning Internet of Things applications. Software: Practice and Experience, 51(9), 1852-1867.;8_serverless_cloud_cost_computing;2021;Edge‐adaptable serverless acceleration for machine learning Internet of Things applications;Michael Zhang, Chandra Krintz, Rich Wolski;Software: Practice and Experience 51 (9), 1852-1867, 2021;Serverless computing is an emerging event‐driven programming model that accelerates the development and deployment of scalable web services on cloud computing systems. Though widely integrated with the public cloud, serverless computing use is nascent for edge‐based, Internet of Things (IoT) deployments. In this work, we present STOIC (serverless teleoperable hybrid cloud), an IoT application deployment and offloading system that extends the serverless model in three ways. First, STOIC adopts a dynamic feedback control mechanism to precisely predict latency and dispatch workloads uniformly across edge and cloud systems using a distributed serverless framework. Second, STOIC leverages hardware acceleration (e.g., GPU resources) for serverless function execution when available from the underlying cloud system. Third, STOIC can be configured in multiple ways to overcome deployment variability associated with public cloud use. We overview the design and implementation of STOIC and empirically evaluate it using real‐world machine learning applications and multitier IoT deployments (edge and cloud). Specifically, we show that STOIC can be used for training image processing workloads (for object recognition)—once thought too resource‐intensive for edge deployments. We find that STOIC reduces overall execution time (response latency) and achieves placement accuracy that ranges from 92% to 97%.;https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2944;_nBaUDFYNDwJ
Sánchez-Artigas, M., & Sarroca, P. G. (2021, December). Experience paper: Towards enhancing cost efficiency in serverless machine learning training. In Proceedings of the 22nd international middleware conference (pp. 210-222).;8_serverless_cloud_cost_computing;2021;Experience paper: Towards enhancing cost efficiency in serverless machine learning training;Marc SÃ¡nchez-Artigas, Pablo Gimeno Sarroca;Proceedings of the 22nd international middleware conference, 210-222, 2021;"Function-as-a-Service (FaaS) has raised a growing interest in how to ""tame"" serverless to enable domain-specific use cases such as data-intensive applications and machine learning (ML), to name a few. Recently, several systems have been implemented for training ML models. Certainly, these research articles are significant steps in the correct direction. However, they do not completely answer the nagging question of when serverless ML training can be more cost-effective compared to traditional ""serverful"" computing. To help in this task, we propose MLLess, a FaaS-based ML training prototype built atop IBM Cloud Functions. To boost cost-efficiency, MLLess implements two key optimizations: a significance filter and a scale-in auto-tuner, and leverages them to specialize model training to the FaaS model. Our results certify that MLLess can be 15X faster than serverful ML systems [24] at a lower cost for ML models (such as sparse logistic regression and matrix factorization) that exhibit fast convergence.";https://dl.acm.org/doi/abs/10.1145/3464298.3494884;jxoG8825gu8J
Grafberger, A., Chadha, M., Jindal, A., Gu, J., & Gerndt, M. (2021, December). Fedless: Secure and scalable federated learning using serverless computing. In 2021 IEEE International Conference on Big Data (Big Data) (pp. 164-173). IEEE.;8_serverless_cloud_cost_computing;2021;Fedless: Secure and scalable federated learning using serverless computing;Andreas Grafberger, Mohak Chadha, Anshul Jindal, Jianfeng Gu, Michael Gerndt;2021 IEEE International Conference on Big Data (Big Data), 164-173, 2021;The traditional cloud-centric approach for Deep Learning (DL) requires training data to be collected and processed at a central server which is often challenging in privacy-sensitive domains like healthcare. Towards this, a new learning paradigm called Federated Learning (FL) has been proposed that brings the potential of DL to these domains while addressing privacy and data ownership issues. FL enables clients to learn a shared ML model while keeping the data local. However, conventional FL systems face challenges such as scalability, complex infrastructure management, and wasted compute and incurred costs due to idle clients. These challenges of FL systems closely align with the core problems that serverless computing and Function-as-a-Service (FaaS) platforms aim to solve. These include rapid scalability, no infrastructure management, automatic scaling to zero for idle clients, and a pay-per-use billing model. To this end, we present a novel system and framework for serverless FL, called FedLess. Our system supports multiple commercial and self-hosted FaaS providers and can be deployed in the cloud, on-premise in institutional data centers, and on edge devices. To the best of our knowledge, we are the first to enable FL across a large fabric of heterogeneous FaaS providers while providing important features like security and Differential Privacy. We demonstrate with comprehensive experiments that the successful training of DNNs for different tasks across up to 200 client functions and more is easily possible using our system. Furthermore, we demonstrate the practical viability of our methodology by comparing it against a traditional FL system and show that it can be cheaper and more resource-efficient.;https://ieeexplore.ieee.org/abstract/document/9672067/;G21gmieTsE0J
Yu, M., Jiang, Z., Ng, H. C., Wang, W., Chen, R., & Li, B. (2021, July). Gillis: Serving large neural networks in serverless functions with automatic model partitioning. In 2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS) (pp. 138-148). IEEE.;8_serverless_cloud_cost_computing;2021;Gillis: Serving large neural networks in serverless functions with automatic model partitioning;Minchen Yu, Zhifeng Jiang, Hok Chun Ng, Wei Wang, Ruichuan Chen, Bo Li;2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS), 138-148, 2021;The increased use of deep neural networks has stimulated the growing demand for cloud-based model serving platforms. Serverless computing offers a simplified solution: users deploy models as serverless functions and let the platform handle provisioning and scaling. However, serverless functions have constrained resources in CPU and memory, making them inefficient or infeasible to serve large neural networks-which have become increasingly popular. In this paper, we present Gillis, a serverless-based model serving system that automatically partitions a large model across multiple serverless functions for faster inference and reduced memory footprint per function. Gillis employs two novel model partitioning algorithms that respectively achieve latency-optimal serving and cost-optimal serving with SLO compliance. We have implemented Gillis on three serverless platforms-AWS Lambda, Google Cloud Functions, and KNIX-with MXNet as the serving backend. Experimental evaluations against popular models show that Gillis supports serving very large neural networks, reduces the inference latency substantially, and meets various SLOs with a low serving cost.;https://ieeexplore.ieee.org/abstract/document/9546452/;wyrREOgQLMUJ
Chahal, D., Ramesh, M., Ojha, R., & Singhal, R. (2021, May). High performance serverless architecture for deep learning workflows. In 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid) (pp. 790-796). IEEE.;8_serverless_cloud_cost_computing;2021;High performance serverless architecture for deep learning workflows;Dheeraj Chahal, Manju Ramesh, Ravi Ojha, Rekha Singhal;2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid), 790-796, 2021;Serverless architecture is a rapidly growing paradigm for deploying deep learning applications performing ephemeral computing and serving bursty workloads. Serverless architecture promises automatic scaling and cost efficiency for inferencing deep learning models while minimizing the operational logic. However, serverless computing is stateless with constraints on local resources. Hence, deploying complex deep learning applications containing large size models, frameworks, and libraries is a challenge.In this work, we discuss a methodology and architecture for migrating deep vision algorithms and model based applications to a serverless computing platform. We have tested our methodology using AWS infrastructure (AWS Lambda, Provisioned Concurrency, VPC endpoint, S3 and EFS) to mitigate the challenges in deploying composition of APIs containing large deep learning models and frameworks. We evaluate the performance and cost of our architecture for a real-life enterprise application used for document processing.;https://ieeexplore.ieee.org/abstract/document/9499397/;RN4Kg4evNUcJ
Paraskevoulakou, E., & Kyriazis, D. (2021, March). Leveraging the serverless paradigm for realizing machine learning pipelines across the edge-cloud continuum. In 2021 24th Conference on Innovation in Clouds, Internet and Networks and Workshops (ICIN) (pp. 110-117). IEEE.;1_ml_machine_data_learning;2021;Leveraging the serverless paradigm for realizing machine learning pipelines across the edge-cloud continuum;Efterpi Paraskevoulakou, Dimosthenis Kyriazis;2021 24th Conference on Innovation in Clouds, Internet and Networks and Workshops (ICIN), 110-117, 2021;"The exceedingly exponential-growing data rate highlighted numerous requirements and several approaches have been released to maximize the added-value of cloud and edge resources. Whereas data scientists utilize algorithmic models in order to transform datasets and extract actionable knowledge, a key challenge is oriented towards abstracting the underline layers: the ones enabling the management of infrastructure resources and the ones responsible to provide frameworks and components as services. In this sense, the serverless approach features as the novel paradigm of new cloud-related technology, enabling the agile implementation of applications and services. The concept of Function as a Service (FaaS) is introduced as a revolutionary model that offers the means to exploit serverless offerings. Developers have the potential to design their applications with the necessary scalability in the form of nanoservices without addressing themselves the way the infrastructure resources should be deployed and managed. By abstracting away the underlying hardware allocations, the data scientist concentrates on the business logic and critical problems of Machine Learning (ML) algorithms. This paper introduces an approach to realize the provision of ML Functions as a Service (i.e., ML-FaaS), by exploiting the Apache OpenWhisk event-driven, distributed serverless platform. The presented approach tackles also composite services that consist of single ones i.e., workflows of ML tasks including processes such as aggregation, cleaning, feature extraction, and analytics; thus, reflecting the complete data path. We also illustrate the operation of the approach mentioned above and assess its performance and effectiveness exploiting a holistic, end-toend anti-fraud detection machine learning pipeline.";https://ieeexplore.ieee.org/abstract/document/9385525/;pbhjSf50ITMJ
Chahal, D., Mishra, M., Palepu, S., & Singhal, R. (2021, April). Performance and cost comparison of cloud services for deep learning workload. In Companion of the ACM/SPEC International Conference on Performance Engineering (pp. 49-55).;8_serverless_cloud_cost_computing;2021;Performance and cost comparison of cloud services for deep learning workload;Dheeraj Chahal, Mayank Mishra, Surya Palepu, Rekha Singhal;Companion of the ACM/SPEC International Conference on Performance Engineering, 49-55, 2021;Many organizations are migrating their on-premise artificial intelligence workloads to the cloud due to the availability of cost-effective and highly scalable infrastructure, software and platform services. To ease the process of migration, many cloud vendors provide services, frameworks and tools that can be used for deployment of applications on cloud infrastructure. Finding the most appropriate service and infrastructure for a given application that results in a desired performance at minimal cost, is a challenge.In this work, we present a methodology to migrate a deep learning model based recommender system to ML platform and serverless architecture. Furthermore, we show our experimental evaluation of the AWS ML platform called SageMaker and the serverless platform service known as Lambda. In our study, we also discuss performance and cost trade-off while using cloud infrastructure.;https://dl.acm.org/doi/abs/10.1145/3447545.3451184;Uteu2oKY19oJ
Chahal, D., Palepu, S., Mishra, M., & Singhal, R. (2020, June). SLA-aware workload scheduling using hybrid cloud services. In Proceedings of the 1st Workshop on High Performance Serverless Computing (pp. 1-4).;8_serverless_cloud_cost_computing;2020;SLA-aware workload scheduling using hybrid cloud services;Dheeraj Chahal, Surya Palepu, Mayank Mishra, Rekha Singhal;Proceedings of the 1st Workshop on High Performance Serverless Computing, 1-4, 2020;Cloud services have an auto-scaling feature for load balancing to meet the performance requirements of an application. Existing auto-scaling techniques are based on upscaling and downscaling cloud resources to distribute the dynamically varying workloads. However, bursty workloads pose many challenges for auto-scaling and sometimes result in Service Level Agreement (SLA) violations. Furthermore, over-provisioning or under-provisioning cloud resources to address dynamically evolving workloads results in performance degradation and cost escalation.In this work, we present a workload characterization based approach for scheduling the bursty workload on a highly scalable serverless architecture in conjunction with a machine learning (ML) platform. We present the use of Amazon Web Services (AWS) ML platform SageMaker and serverless computing platform Lambda for load balancing the inference workload to avoid SLA violations. We evaluate our approach using a recommender system that is based on a deep learning model for inference.;https://dl.acm.org/doi/abs/10.1145/3452413.3464789;6PDCiMorQvQJ
Patros, P., Spillner, J., Papadopoulos, A. V., Varghese, B., Rana, O., & Dustdar, S. (2021). Toward sustainable serverless computing. IEEE Internet Computing, 25(6), 42-50.;8_serverless_cloud_cost_computing;2021;Toward sustainable serverless computing;Panos Patros, Josef Spillner, Alessandro V Papadopoulos, Blesson Varghese, Omer Rana, Schahram Dustdar;IEEE Internet Computing 25 (6), 42-50, 2021;"Although serverless computing generally involves executing short-lived “functions,” the increasing migration to this computing paradigm requires careful consideration of energy and power requirements. serverless computing is also viewed as an economically-driven computational approach, often influenced by the cost of computation, as users are charged for per-subsecond use of computational resources rather than the coarse-grained charging that is common with virtual machines and containers. To ensure that the startup times of serverless functions do not discourage their use, resource providers need to keep these functions hot, often by passing in synthetic data. We describe the real power consumption characteristics of serverless, based on execution traces reported in the literature, and describe potential strategies (some adopted from existing VM and container-based approaches) that can be used to reduce the energy overheads of serverless execution. Our analysis is, purposefully, biased toward the use of machine learning workloads because: (1) workloads are increasingly being used widely across different applications; (2) functions that implement machine learning algorithms can range in complexity from long-running (deep learning) versus short-running (inference only), enabling us to consider serverless across a variety of possible execution behaviors. The general findings are easily translatable to other domains.";https://ieeexplore.ieee.org/abstract/document/9646540/;1zQNDj3y_A0J
Jiang, J., Gan, S., Liu, Y., Wang, F., Alonso, G., Klimovic, A., ... & Zhang, C. (2021, June). Towards demystifying serverless machine learning training. In Proceedings of the 2021 International Conference on Management of Data (pp. 857-871).;8_serverless_cloud_cost_computing;2021;Towards demystifying serverless machine learning training;Jiawei Jiang, Shaoduo Gan, Yue Liu, Fanlin Wang, Gustavo Alonso, Ana Klimovic, Ankit Singla, Wentao Wu, Ce Zhang;Proceedings of the 2021 International Conference on Management of Data, 857-871, 2021;"The appeal of serverless (FaaS) has triggered a growing interest on how to use it in data-intensive applications such as ETL, query processing, or machine learning (ML). Several systems exist for training large-scale ML models on top of serverless infrastructures (e.g., AWS Lambda) but with inconclusive results in terms of their performance and relative advantage over ""serverful"" infrastructures (IaaS). In this paper we present a systematic, comparative study of distributed ML training over FaaS and IaaS. We present a design space covering design choices such as optimization algorithms and synchronization protocols, and implement a platform, LambdaML, that enables a fair comparison between FaaS and IaaS. We present experimental results using LambdaML, and further develop an analytic model to capture cost/performance tradeoffs that must be considered when opting for a serverless infrastructure. Our results indicate that ML training pays off in serverless only for models with efficient (i.e., reduced) communication and that quickly converge. In general, FaaS can be much faster but it is never significantly cheaper than IaaS.";https://dl.acm.org/doi/abs/10.1145/3448016.3459240;kdPAU8VN6ZQJ
Nesen, A., & Bhargava, B. (2021, June). Towards situational awareness with multimodal streaming data fusion: serverless computing approach. In Proceedings of the International Workshop on Big Data in Emergent Distributed Environments (pp. 1-6).;1_ml_machine_data_learning;2021;Towards situational awareness with multimodal streaming data fusion: serverless computing approach;Alina Nesen, Bharat Bhargava;Proceedings of the International Workshop on Big Data in Emergent Distributed Environments, 1-6, 2021;The availability of large quantities of data has given an impulse for methods and techniques to extract unseen useful knowledge and process it in a fast and scalable manner. In order to extract the most complete possible knowledge from the continuous data stream, it is necessary to use the heterogeneous data sources and process information from multiple modalities. The systems that utilize multimodal data must take advantage of the up-to-date approaches for data storage, usage, cleaning and storage. Neural networks and machine learning approaches are widely used for data-heavy software where pattern extractions and predictions need to be conducted while serverless computing frameworks are being increasingly used for machine learning solutions to optimize cost and speed of such systems. This work presents a framework for processing data from multimodal sources where the task of feature and pattern extraction is performed on a serverless computing platform. The use cases for public safety solutions to increase situational awareness are described and compared with other implementation approaches.;https://dl.acm.org/doi/abs/10.1145/3460866.3461769;kFM9h_1gdDkJ
Tagliabue, J. (2021, September). You do not need a bigger boat: Recommendations at reasonable scale in a (mostly) serverless and open stack. In Proceedings of the 15th ACM Conference on Recommender Systems (pp. 598-600).;1_ml_machine_data_learning;2021;You do not need a bigger boat: Recommendations at reasonable scale in a (mostly) serverless and open stack;Jacopo Tagliabue;Proceedings of the 15th ACM Conference on Recommender Systems, 598-600, 2021;We argue that immature data pipelines are preventing a large portion of industry practitioners from leveraging the latest research on recommender systems. We propose our template data stack for machine learning at “reasonable scale”, and show how many challenges are solved by embracing a serverless paradigm. Leveraging our experience, we detail how modern open source tools can provide a pipeline processing terabytes of data with minimal infrastructure work.;https://dl.acm.org/doi/abs/10.1145/3460231.3474604;Scc1P5EY1gAJ
Xu, F., Qin, Y., Chen, L., Zhou, Z., & Liu, F. (2021). λdnn: Achieving predictable distributed DNN training with serverless architectures. IEEE Transactions on Computers, 71(2), 450-463.;8_serverless_cloud_cost_computing;2021;Î»DNN: Achieving Predictable Distributed DNN Training With Serverless Architectures;Fei Xu, Yiling Qin, Li Chen, Zhi Zhou, Fangming Liu;IEEE Transactions on Computers 71 (2), 450-463, 2021;Serverless computing is becoming a promising paradigm for Distributed Deep Neural Network (DDNN) training in the cloud, as it allows users to decompose complex model training into a number of functions without managing virtual machines or servers. Though provided with a simpler resource interface (i.e., function number and memory size), inadequate function resource provisioning (either under-provisioning or over-provisioning) easily leads to unpredictable DDNN training performance in serverless platforms. Our empirical studies on AWS Lambda indicate that, such unpredictable performance of serverless DDNN training is mainly caused by the resource bottleneck of Parameter Servers (PS) and small local batch size. In this article, we design and implement DNN , a cost-efficient function resource provisioning framework to provide predictable performance for serverless DDNN training workloads, while saving the budget of provisioned functions. Leveraging the PS network bandwidth and function CPU utilization, we build a lightweight analytical DDNN training performance model to enable our design of DNN resource provisioning strategy, so as to guarantee DDNN training performance with serverless functions. Extensive prototype experiments on AWS Lambda and complementary trace-driven simulations demonstrate that, DNN can deliver predictable DDNN training performance and save the monetary cost of function resources by up to 66.7 percent, compared with the state-of-the-art resource provisioning strategies, yet with an acceptable runtime overhead.;https://ieeexplore.ieee.org/abstract/document/9336272/;W_NBurLI93sJ
Bac, T. P., Tran, M. N., & Kim, Y. (2022, January). Serverless computing approach for deploying machine learning applications in edge layer. In 2022 International Conference on Information Networking (ICOIN) (pp. 396-401). IEEE.;8_serverless_cloud_cost_computing;2022;Serverless computing approach for deploying machine learning applications in edge layer;Ta Phuong Bac, Minh Ngoc Tran, YoungHan Kim;2022 International Conference on Information Networking (ICOIN), 396-401, 2022;Serverless computing-a stateless cloud computing model, is an emerging solution that has shown significant benefits to efficiency and cost for event-driven applications in the cloud environment, including artificial intelligence (AI), machine learning applications. With serverless computing, the machine learning systemâ€™s complexity is minimized, flexible and straightforward in management. However, operating and managing serverless machine learning services on clouds faces many limitations such as latency and data privacy. Local distributed edge computing nodes which are closed to users can address these challenges of cloud-serverless AI applications. Based on this motivation, in this paper, we propose an architecture for deploying machine learning workload as serverless functions in the edge environment. We illustrate our proposed approach and evaluate its performance and effectiveness by exploiting a holistic end-to-end image classifier, a famous machine learning use case in the MNIST dataset. Our proof of concept provides comprehensive assessments that prove its effectiveness in latency reduction and distributed machine learning deployment.;https://ieeexplore.ieee.org/abstract/document/9687209/;OzoDcB_uBNkJ
Barcelona-Pons, D., Sutra, P., Sánchez-Artigas, M., París, G., & García-López, P. (2022). Stateful serverless computing with crucial. ACM Transactions on Software Engineering and Methodology (TOSEM), 31(3), 1-38.;8_serverless_cloud_cost_computing;2022;Stateful Serverless Computing with Crucial;Daniel Barcelona-Pons, Pierre Sutra, Marc Sánchez-Artigas, Gerard París, Pedro García-López;ACM Transactions on Software Engineering and Methodology (TOSEM) 31 (3), 1-38, 2022;Serverless computing greatly simplifies the use of cloud resources. In particular, Function-as-a-Service (FaaS) platforms enable programmers to develop applications as individual functions that can run and scale independently. Unfortunately, applications that require fine-grained support for mutable state and synchronization, such as machine learning (ML) and scientific computing, are notoriously hard to build with this new paradigm. In this work, we aim at bridging this gap. We present Crucial, a system to program highly-parallel stateful serverless applications. Crucial retains the simplicity of serverless computing. It is built upon the key insight that FaaS resembles to concurrent programming at the scale of a datacenter. Accordingly, a distributed shared memory layer is the natural answer to the needs for fine-grained state management and synchronization. Crucial allows to port effortlessly a multi-threaded code base to serverless, where it can benefit from the scalability and pay-per-use model of FaaS platforms. We validate Crucial with the help of micro-benchmarks and by considering various stateful applications. Beyond classical parallel tasks (e.g., a Monte Carlo simulation), these applications include representative ML algorithms such as k-means and logistic regression. Our evaluation shows that Crucial obtains superior or comparable performance to Apache Spark at similar cost (18%–40% faster). We also use Crucial to port (part of) a state-of-the-art multi-threaded ML library to serverless. The ported application is up to 30% faster than with a dedicated high-end server. Finally, we attest that Crucial can rival in performance with a single-machine, multi-threaded implementation of a complex coordination problem. Overall, Crucial delivers all these benefits with less than 6% of changes in the code bases of the evaluated applications.;https://dl.acm.org/doi/abs/10.1145/3490386;J1nYMQPZc2kJ
Yang, Y., Zhao, L., Li, Y., Zhang, H., Li, J., Zhao, M., ... & Li, K. (2022, February). INFless: a native serverless system for low-latency, high-throughput inference. In Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (pp. 768-781).;8_serverless_cloud_cost_computing;2022;INFless: a native serverless system for low-latency, high-throughput inference;Yanan Yang, Laiping Zhao, Yiming Li, Huanyu Zhang, Jie Li, Mingyang Zhao, Xingzhen Chen, Keqiu Li;Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 768-781, 2022;Modern websites increasingly rely on machine learning (ML) to improve their business efficiency. Developing and maintaining ML services incurs high costs for developers. Although serverless systems are a promising solution to reduce costs, we find that the current general purpose serverless systems cannot meet the low latency, high throughput demands of ML services.While simply ”patching” general serverless systems does not resolve the problem completely, we propose that such a system should natively combine the features of inference with a serverless paradigm. We present INFless, the first ML domain-specific serverless platform. It provides a unified, heterogeneous resource abstraction between CPU and accelerators, and achieves high throughput using built-in batching and non-uniform scaling mechanisms. It also supports low latency through coordinated management of batch queuing time, execution time and coldstart rate. We evaluate INFless using both a local cluster testbed and a large-scale simulation. Experimental results show that INFless outperforms state-of-the-art systems by 2×-5× on system throughput, meeting the latency goals of ML services.;https://dl.acm.org/doi/abs/10.1145/3503222.3507709;Qvk5QmpNQZsJ
Sarroca, P. G., & Sánchez-Artigas, M. (2024). Mlless: Achieving cost efficiency in serverless machine learning training. Journal of Parallel and Distributed Computing, 183, 104764.;8_serverless_cloud_cost_computing;2024;Mlless: Achieving cost efficiency in serverless machine learning training;Pablo Gimeno Sarroca, Marc Sánchez-Artigas;Journal of Parallel and Distributed Computing, 104764, 2023;Function-as-a-Service (FaaS) has raised a growing interest in how to “tame” serverless computing to enable domain-specific use cases such as data-intensive applications and machine learning (ML), to name a few. Recently, several systems have been implemented for training ML models. Certainly, these research articles are significant steps in the correct direction. However, they do not completely answer the nagging question of when serverless ML training can be more cost-effective compared to traditional “serverful” computing. To help in this …;https://www.sciencedirect.com/science/article/pii/S074373152300134X;1e47gT6hE38J
Tae, K. H., Roh, Y., Oh, Y. H., Kim, H., & Whang, S. E. (2019, June). Data cleaning for accurate, fair, and robust models: A big data-AI integration approach. In Proceedings of the 3rd International Workshop on Data Management for End-to-End Machine Learning (pp. 1-4).;1_ml_machine_data_learning;2019;Data cleaning for accurate, fair, and robust models: A big data-AI integration approach;Ki Hyun Tae, Yuji Roh, Young Hun Oh, Hyunsu Kim, Steven Euijong Whang;Proceedings of the 3rd International Workshop on Data Management for End-to-End Machine Learning, 1-4, 2019;The wide use of machine learning is fundamentally changing the software development paradigm (a.k.a. Software 2.0) where data becomes a first-class citizen, on par with code. As machine learning is used in sensitive applications, it becomes imperative that the trained model is accurate, fair, and robust to attacks. While many techniques have been proposed to improve the model training process (in-processing approach) or the trained model itself (post-processing), we argue that the most effective method is to clean the root cause of error: the data the model is trained on (pre-processing). Historically, there are at least three research communities that have been separately studying this problem: data management, machine learning (model fairness), and security. Although a significant amount of research has been done by each community, ultimately the same datasets must be preprocessed, and there is little understanding how the techniques relate to each other and can possibly be integrated. We contend that it is time to extend the notion of data cleaning for modern machine learning needs. We identify dependencies among the data preprocessing techniques and propose MLClean, a unified data cleaning framework that integrates the techniques and helps train accurate and fair models. This work is part of a broader trend of Big data -- Artificial Intelligence (AI) integration.;https://dl.acm.org/doi/abs/10.1145/3329486.3329493;vUB4SWVLUHsJ
Whang, S. E., & Lee, J. G. (2020). Data collection and quality challenges for deep learning. Proceedings of the VLDB Endowment, 13(12), 3429-3432.;1_ml_machine_data_learning;2020;Data collection and quality challenges for deep learning;Steven Euijong Whang, Jae-Gil Lee;Proceedings of the VLDB Endowment 13 (12), 3429-3432, 2020;Software 2.0 refers to the fundamental shift in software engineering where using machine learning becomes the new norm in software with the availability of big data and computing infrastructure. As a result, many software engineering practices need to be rethought from scratch where data becomes a first-class citizen, on par with code. It is well known that 80--90% of the time for machine learning development is spent on data preparation. Also, even the best machine learning algorithms cannot perform well without good data or at least handling biased and dirty data during model training. In this tutorial, we focus on data collection and quality challenges that frequently occur in deep learning applications. Compared to traditional machine learning, there is less need for feature engineering, but more need for significant amounts of data. We thus go through state-of-the-art data collection techniques for machine learning. Then, we cover data validation and cleaning techniques for improving data quality. Even if the data is still problematic, hope is not lost, and we cover fair and robust training techniques for handling data bias and errors. We believe that the data management community is well poised to lead the research in these directions. The presenters have extensive experience in developing machine learning platforms and publishing papers in top-tier database, data mining, and machine learning venues.;https://dl.acm.org/doi/abs/10.14778/3415478.3415562;_jlV4nHois4J
Toropov, E., Buitrago, P. A., & Moura, J. M. (2019). Shuffler: A large scale data management tool for machine learning in computer vision. In Proceedings of the Practice and Experience in Advanced Research Computing on Rise of the Machines (learning) (pp. 1-8).;1_ml_machine_data_learning;2019;Shuffler: A large scale data management tool for machine learning in computer vision;Evgeny Toropov, Paola A Buitrago, JosÃ© MF Moura;Proceedings of the Practice and Experience in Advanced Research Computing on Rise of the Machines (learning), 1-8, 2019;Datasets in the computer vision academic research community are primarily static. Once a dataset is accepted as a benchmark for a computer vision task, researchers working on this task will not alter it in order to make their results reproducible. At the same time, when exploring new tasks and new applications, datasets tend to be an ever changing entity. A practitioner may combine existing public datasets, filter images or objects in them, change annotations or add new ones to fit a task at hand, visualize sample images, or perhaps output statistics in the form of text or plots. In fact, datasets change as practitioners experiment with data as much as with algorithms, trying to make the most out of machine learning models. Given that ML and deep learning call for large volumes of data to produce satisfactory results, it is no surprise that the resulting data and software management associated to dealing with live datasets can be quite complex. As far as we know, there is no flexible, publicly available instrument to facilitate manipulating image data and their annotations throughout a ML pipeline. In this work, we present Shuffler, an open source tool that makes it easy to manage large computer vision datasets. It stores annotations in a relational, human-readable database. Shuffler defines over 40 data handling operations with annotations that are commonly useful in supervised learning applied to computer vision and supports some of the most well-known computer vision datasets. Finally, it is easily extensible, making the addition of new operations and datasets a task that is fast and easy to accomplish.;https://dl.acm.org/doi/abs/10.1145/3332186.3333046;SB2in_LUcAUJ
Zhang, Y., & Ives, Z. G. (2019). Juneau: data lake management for jupyter. Proceedings of the VLDB Endowment, 12(12).;9_data_science_software_process;2019;Juneau: data lake management for jupyter;Yi Zhang, Zachary G Ives;Proceedings of the VLDB Endowment 12 (12), 2019;In collaborative settings such as multi-investigator laboratories, data scientists need improved tools to manage not their data records but rather their data sets and data products, to facilitate both provenance tracking and data (and code) reuse within their data lakes and file systems. We demonstrate the Juneau System, which extends computational notebook software (Jupyter Notebook) as an instrumentation and data management point for overseeing and facilitating improved dataset usage, through capabilities for indexing, searching, and recommending “complementary” data sources, previously extracted machine learning features, and additional training data. This demonstration focuses on how we help the user find related datasets via search.;https://par.nsf.gov/servlets/purl/10195975;2NRolm_rN_kJ
Lyu, Y., Li, H., Sayagh, M., Jiang, Z. M., & Hassan, A. E. (2021). An empirical study of the impact of data splitting decisions on the performance of AIOps solutions. ACM Transactions on Software Engineering and Methodology (TOSEM), 30(4), 1-38.;1_ml_machine_data_learning;2021;An empirical study of the impact of data splitting decisions on the performance of AIOps solutions;Yingzhe Lyu, Heng Li, Mohammed Sayagh, Zhen Ming Jiang, Ahmed E Hassan;ACM Transactions on Software Engineering and Methodology (TOSEM) 30 (4), 1-38, 2021;AIOps (Artificial Intelligence for IT Operations) leverages machine learning models to help practitioners handle the massive data produced during the operations of large-scale systems. However, due to the nature of the operation data, AIOps modeling faces several data splitting-related challenges, such as imbalanced data, data leakage, and concept drift. In this work, we study the data leakage and concept drift challenges in the context of AIOps and evaluate the impact of different modeling decisions on such challenges. Specifically, we perform a case study on two commonly studied AIOps applications: (1) predicting job failures based on trace data from a large-scale cluster environment and (2) predicting disk failures based on disk monitoring data from a large-scale cloud storage environment. First, we observe that the data leakage issue exists in AIOps solutions. Using a time-based splitting of training and validation datasets can significantly reduce such data leakage, making it more appropriate than using a random splitting in the AIOps context. Second, we show that AIOps solutions suffer from concept drift. Periodically updating AIOps models can help mitigate the impact of such concept drift, while the performance benefit and the modeling cost of increasing the update frequency depend largely on the application data and the used models. Our findings encourage future studies and practices on developing AIOps solutions to pay attention to their data-splitting decisions to handle the data leakage and concept drift challenges.;https://dl.acm.org/doi/abs/10.1145/3447876;GBY8LPqLw1oJ
Karanikola, A., & Kotsiantis, S. (2019, November). A hybrid method for missing value imputation. In Proceedings of the 23rd Pan-Hellenic Conference on Informatics (pp. 74-79).;1_ml_machine_data_learning;2019;A hybrid method for missing value imputation;Aikaterini Karanikola, Sotiris Kotsiantis;Proceedings of the 23rd Pan-Hellenic Conference on Informatics, 2019;Missing values are a common incurrence in a great number of real-world datasets, emerging from diverse domains of interest. In research, missing data constitute a significant problem as it can affect the conclusions drawn from them. Considering this, the difficulty of data preprocessing is increasing as selecting an inappropriate way to handle missing information can lead to untrustworthy results. Unfortunately, like in most cases in Machine Learning, there is not a single solution that fits in every task related to the problem. For this reason, many strategies have been proposed to successfully deal with this issue. One of the most well-known, besides efficient, is imputation. Replacing a missing value with an estimation apparently eliminates the problem and provides complete datasets but the difficulty shifts in selecting the right method to impute missing values. A widely used imputation method that can be found in libraries of the most noted statistical and Machine Learning suites is IRMI. In this work, we propose a variant of IRMI in order to maintain the advantages of this famous imputation method, while outperforming its traditional variant used in many Machine Learning software tools. To achieve this, the benefits of boosting as well as decision tree theory are exploiting. To test the efficiency of our method, a series of experiments over 30 datasets was executed, measuring the classification accuracy of the proposed method to prove that outperforms its rivals, which include classic, as well as more sophisticated imputation strategies. Finally, the results of our study are provided, along with the conclusions that arise from them.;https://dl.acm.org/doi/abs/10.1145/3368640.3368653;Todo
Widanage, C., Perera, N., Abeykoon, V., Kamburugamuve, S., Kanewala, T. A., Maithree, H., ... & Fox, G. (2020, October). High performance data engineering everywhere. In 2020 IEEE International Conference on Smart Data Services (SMDS) (pp. 122-132). IEEE.;1_ml_machine_data_learning;2020;High performance data engineering everywhere;Chathura Widanage, Niranda Perera, Vibhatha Abeykoon, Supun Kamburugamuve, Thejaka Amila Kanewala, Hasara Maithree, Pulasthi Wickramasinghe, Ahmet Uyar, Gurhan Gunduz, Geoffrey Fox;2020 IEEE International Conference on Smart Data Services (SMDS), 122-132, 2020;The amazing advances being made in the fields of machine and deep learning are a highlight of the Big Data era for both enterprise and research communities. Modern applications require resources beyond a single node's ability to provide. However this is just a small part of the issues facing the overall data processing environment, which must also support a raft of data engineering for pre- and post-data processing, communication, and system integration. An important requirement of data analytics tools is to be able to easily integrate with existing frameworks in a multitude of languages, thereby increasing user productivity and efficiency. All this demands an efficient and highly distributed integrated approach for data processing, yet many of today's popular data analytics tools are unable to satisfy all these requirements at the same time. In this paper we present Cylon, an open-source high performance distributed data processing library that can be seamlessly integrated with existing Big Data and AI/ML frameworks. It is developed with a flexible C++ core on top of a compact data structure and exposes language bindings to C++, Java, and Python. We discuss Cylon's architecture in detail, and reveal how it can be imported as a library to existing applications or operate as a standalone framework. Initial experiments show that Cylon enhances popular tools such as Apache Spark and Dask with major performance improvements for key operations and better component linkages. Finally, we show how its design enables Cylon to be used cross-platform with minimum overhead, which includes popular AI tools such as PyTorch, Tensorflow, and Jupyter notebooks.;https://ieeexplore.ieee.org/abstract/document/9288485/;eLqR_6zT-RsJ
Heo, G., Roh, Y., Hwang, S., Lee, D., & Whang, S. E. (2020). Inspector gadget: a data programming-based labeling system for industrial images. arXiv preprint arXiv:2004.03264.;1_ml_machine_data_learning;2020;Inspector gadget: a data programming-based labeling system for industrial images;Geon Heo, Yuji Roh, Seonghyeon Hwang, Dayun Lee, Steven Euijong Whang;arXiv preprint arXiv:2004.03264, 2020;As machine learning for images becomes democratized in the Software 2.0 era, one of the serious bottlenecks is securing enough labeled data for training. This problem is especially critical in a manufacturing setting where smart factories rely on machine learning for product quality control by analyzing industrial images. Such images are typically large and may only need to be partially analyzed where only a small portion is problematic (e.g., identifying defects on a surface). Since manual labeling these images is expensive, weak supervision is an attractive alternative where the idea is to generate weak labels that are not perfect, but can be produced at scale. Data programming is a recent paradigm in this category where it uses human knowledge in the form of labeling functions and combines them into a generative model. Data programming has been successful in applications based on text or structured data and can also be applied to images usually if one can find a way to convert them into structured data. In this work, we expand the horizon of data programming by directly applying it to images without this conversion, which is a common scenario for industrial applications. We propose Inspector Gadget, an image labeling system that combines crowdsourcing, data augmentation, and data programming to produce weak labels at scale for image classification. We perform experiments on real industrial image datasets and show that Inspector Gadget obtains better performance than other weak-labeling techniques: Snuba, GOGGLES, and self-learning baselines using convolutional neural networks (CNNs) without pre-training.;https://arxiv.org/abs/2004.03264;LDFtMRdFe5gJ
Staples, M., Zhu, L., & Grundy, J. (2016, May). Continuous validation for data analytics systems. In Proceedings of the 38th International Conference on Software Engineering Companion (pp. 769-772).;1_ml_machine_data_learning;2016;Continuous validation for data analytics systems;Mark Staples, Liming Zhu, John Grundy;Proceedings of the 38th International Conference on Software Engineering Companion, 769-772, 2016;From a future history of 2025: Continuous development is common for build/test (continuous integration) and operations (devOps). This trend continues through the lifecycle, into what we call 'devUsage': continuous usage validation. In addition to ensuring systems meet user needs, organisations continuously validate their legal and ethical use. The rise of end-user programming and multi-sided platforms exacerbate validation challenges. A separate trend is the specialisation of software engineering for technical domains, including data analytics. This domain has specific validation challenges. We must validate the accuracy of statistical models, but also whether they have illegal or unethical biases. Usage needs addressed by machine learning are sometimes not specifiable in the traditional sense, and statistical models are often 'black boxes'. We describe future research to investigate solutions to these devUsage challenges for data analytics systems. We will adapt risk management and governance frameworks previously used for software product qualities, use social network communities for input from aligned stakeholder groups, and perform cross-validation using autonomic experimentation, cyber-physical data streams, and online discursive feedback.;https://dl.acm.org/doi/abs/10.1145/2889160.2889207;mCuhivobxe8J
Lwakatare, L. E., Rånge, E., Crnkovic, I., & Bosch, J. (2021, May). On the experiences of adopting automated data validation in an industrial machine learning project. In 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) (pp. 248-257). IEEE.;1_ml_machine_data_learning;2021;On the experiences of adopting automated data validation in an industrial machine learning project;Lucy Ellen Lwakatare, Ellinor RÃ¥nge, Ivica Crnkovic, Jan Bosch;2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), 248-257, 2021;"Background
Data errors are a common challenge in machine learning (ML) projects and generally cause significant performance degradation in ML-enabled software systems. To ensure early detection of erroneous data and avoid training MLmodels using bad data, research and industrial practice suggest incorporating a data validation process and tool in the ML system development process.
Aim
The study investigates the adoption of a data validation process and tool in industrial ML projects. The data validation process demands significant engineering resources for tool development and maintenance. Thus, it is important to identify the best practices for their adoption especially by development teams that are in the early phases of deploying ML-enabled software systems.
Method
Action research was conducted at a large-software intensive organization in telecommunications, specifically within the analytics R&D organization for an ML use case of classifying faults from returned hardware telecommunication devices.
Results
Based on the evaluation results and learning from our action research, we identified three best practices, three benefits, and two barriers to adopting the data validation process and tool in ML projects. We also propose a data validation framework(DVF) for systematizing the adoption of a data validation process.
Conclusions
The results show that adopting a data validation process and tool in ML projects is an effective approach to testing ML-enabled software systems. It requires having an overview of the level of data (feature, dataset, cross-dataset, data stream) at which certain data quality tests can be applied.";https://ieeexplore.ieee.org/abstract/document/9402048/;NYJXsJr0c4kJ
Barrak, A., Eghan, E. E., & Adams, B. (2021, March). On the co-evolution of ml pipelines and source code-empirical study of dvc projects. In 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER) (pp. 422-433). IEEE.;1_ml_machine_data_learning;2021;On the co-evolution of ml pipelines and source code-empirical study of dvc projects;Amine Barrak, Ellis E Eghan, Bram Adams;2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER), 422-433, 2021;The growing popularity of machine learning (ML) applications has led to the introduction of software engineering tools such as Data Versioning Control (DVC), MLFlow and Pachyderm that enable versioning ML data, models, pipelines and model evaluation metrics. Since these versioned ML artifacts need to be synchronized not only with each other, but also with the source and test code of the software applications into which the models are integrated, prior findings on co-evolution and coupling between software artifacts might need to be revisited. Hence, in order to understand the degree of coupling between ML-related and other software artifacts, as well as the adoption of ML versioning features, this paper empirically studies the usage of DVC in 391 Github projects, 25 of which in detail. Our results show that more than half of the DVC files in a project are changed at least once every one-tenth of the project's lifetime. Furthermore, we observe a tight coupling between DVC files and other artifacts, with 1/4 pull requests changing source code and 1/2 pull requests changing tests requiring a change to DVC files. As additional evidence of the observed complexity associated with adopting ML-related software engineering tools like DVC, an average of 78% of the studied projects showed a non-constant trend in pipeline complexity.;https://ieeexplore.ieee.org/abstract/document/9425888/;vtdwQ0DaCNMJ
Alaghbari, S., Mitschick, A., Blichmann, G., Voigt, M., & Dachselt, R. (2020). Achiever or explorer? gamifying the creation process of training data for machine learning. In Proceedings of Mensch und Computer 2020 (pp. 173-181).;1_ml_machine_data_learning;2020;Achiever or explorer? gamifying the creation process of training data for machine learning;Sarah Alaghbari, Annett Mitschick, Gregor Blichmann, Martin Voigt, Raimund Dachselt;Proceedings of Mensch und Computer 2020, 173-181, 2020;The development of artificial intelligence, e. g., for Computer Vision, through supervised learning requires the input of large amounts of annotated or labeled data objects as training data. The creation of high-quality training data is usually done manually which can be repetitive and tiring. Gamification, the use of game elements in a non-game context, is one method to make tedious tasks more interesting. This paper proposes a multi-step process for gamifying the manual creation of training data for machine learning purposes. We choose a user-adapted approach based on the results of a preceding user study with the target group (employees of an AI software development company) which helped us to identify annotation use cases and the users' player characteristics. The resulting concept includes levels of increasing difficulty, tutorials, progress indicators and a narrative built around a robot character which at the same time is a user assistant. The implemented prototype is an extension of the company's existing annotation tool and serves as a basis for further observations.;https://dl.acm.org/doi/abs/10.1145/3404983.3405519;27UOJvG4eJ8J
Yokoyama, H., Onoue, S., & Kikuchi, S. (2020, December). Towards building robust DNN applications: an industrial case study of evolutionary data augmentation. In Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering (pp. 1184-1188).;1_ml_machine_data_learning;2020;Towards building robust DNN applications: an industrial case study of evolutionary data augmentation;Haruki Yokoyama, Satoshi Onoue, Shinji Kikuchi;Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering, 1184-1188, 2020;"Data augmentation techniques that increase the amount of training data by adding realistic transformations are used in machine learning to improve the level of accuracy. Recent studies have demonstrated that data augmentation techniques improve the robustness of image classification models with open datasets; however, it has yet to be investigated whether these techniques are effective for industrial datasets. In this study, we investigate the feasibility of data augmentation techniques for industrial use. We evaluate data augmentation techniques in image classification and object detection tasks using an industrial in-house graphical user interface dataset. As the results indicate, the genetic algorithm-based data augmentation technique outperforms two random-based methods in terms of the robustness of the image classification model. In addition, through this evaluation and interviews with the developers, we learned following two lessons: data augmentation techniques should (1) maintain the training speed to avoid slowing the development and (2) include extensibility for a variety of tasks.";https://dl.acm.org/doi/abs/10.1145/3324884.3421841;7gahCvp6j0sJ
Vieira, D. M., Fernandes, C., Lucena, C., & Lifschitz, S. (2021). Driftage: a multi-agent system framework for concept drift detection. GigaScience, 10(6), giab030.;1_ml_machine_data_learning;2021;Driftage: a multi-agent system framework for concept drift detection;Diogo Munaro Vieira, Chrystinne Fernandes, Carlos Lucena, Sérgio Lifschitz;GigaScience 10 (6), giab030, 2021;The amount of data and behavior changes in society happens at a swift pace in this interconnected world. Consequently, machine learning algorithms lose accuracy because they do not know these new patterns. This change in the data pattern is known as concept drift. There exist many approaches for dealing with these drifts. Usually, these methods are costly to implement because they require (i) knowledge of drift detection algorithms, (ii) software engineering strategies, and (iii) continuous maintenance concerning new drifts.This article proposes to create Driftage: a new framework using multi-agent systems to simplify the implementation of concept drift detectors considerably and divide concept drift detection responsibilities between agents, enhancing explainability of each part of drift detection. As a case study, we illustrate our strategy using a muscle activity monitor of electromyography. We show a reduction in the number of false-positive drifts detected, improving detection interpretability, and enabling concept drift detectors’ interactivity with other knowledge bases.We conclude that using Driftage, arises a new paradigm to implement concept drift algorithms with multi-agent architecture that contributes to split drift detection responsability, algorithms interpretability and more dynamic algorithms adaptation.;https://academic.oup.com/gigascience/article-abstract/10/6/giab030/6290670;fRFLi9KBZEYJ
Chen, T. (2019, May). All versus one: an empirical comparison on retrained and incremental machine learning for modeling performance of adaptable software. In 2019 IEEE/ACM 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS) (pp. 157-168). IEEE.;1_ml_machine_data_learning;2019;All versus one: an empirical comparison on retrained and incremental machine learning for modeling performance of adaptable software;Tao Chen;2019 IEEE/ACM 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS), 157-168, 2019;"Given the ever-increasing complexity of adaptable software systems and their commonly hidden internal information (e.g., software runs in the public cloud), machine learning based performance modeling has gained momentum for evaluating, understanding and predicting software performance, which facilitates better informed self-adaptations. As performance data accumulates during the run of the software, updating the performance models becomes necessary. To this end, there are two conventional modeling methods: the retrained modeling that always discard the old model and retrain a new one using all available data; or the incremental modeling that retains the existing model and tunes it using one newly arrival data sample. Generally, literature on machine learning based performance modeling for adaptable software chooses either of those methods according to a general belief, but they provide insufficient evidences or references to justify their choice. This paper is the first to report on a comprehensive empirical study that examines both modeling methods under distinct domains of adaptable software, 5 performance indicators, 8 learning algorithms and settings, covering a total of 1,360 different conditions. Our findings challenge the general belief, which is shown to be only partially correct, and reveal some of the important, statistically significant factors that are often overlooked in existing work, providing evidence-based insights on the choice.";https://ieeexplore.ieee.org/abstract/document/8787029/;wm1CEK-kjloJ
Paleyes, A., Urma, R. G., & Lawrence, N. D. (2022). Challenges in deploying machine learning: a survey of case studies. ACM Computing Surveys, 55(6), 1-29.;1_ml_machine_data_learning;2022;Challenges in deploying machine learning: a survey of case studies;Andrei Paleyes, Raoul-Gabriel Urma, Neil D Lawrence;ACM Computing Surveys 55 (6), 1-29, 2022;In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries, and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow, we show that practitioners face issues at each stage of the deployment process. The goal of this article is to lay out a research agenda to explore approaches addressing these challenges.;https://dl.acm.org/doi/abs/10.1145/3533378;4yqSNR3CdvEJ
Lakhmiri, D., Digabel, S. L., & Tribes, C. (2021). HyperNOMAD: Hyperparameter optimization of deep neural networks using mesh adaptive direct search. ACM Transactions on Mathematical Software (TOMS), 47(3), 1-27.;1_ml_machine_data_learning;2021;HyperNOMAD: Hyperparameter optimization of deep neural networks using mesh adaptive direct search;Dounia Lakhmiri, Sébastien Le Digabel, Christophe Tribes;ACM Transactions on Mathematical Software (TOMS) 47 (3), 1-27, 2021;The performance of deep neural networks is highly sensitive to the choice of the hyperparameters that define the structure of the network and the learning process. When facing a new application, tuning a deep neural network is a tedious and time-consuming process that is often described as a “dark art.” This explains the necessity of automating the calibration of these hyperparameters. Derivative-free optimization is a field that develops methods designed to optimize time-consuming functions without relying on derivatives. This work introduces the HyperNOMAD package, an extension of the NOMAD software that applies the MADS algorithm [7] to simultaneously tune the hyperparameters responsible for both the architecture and the learning process of a deep neural network (DNN). This generic approach allows for an important flexibility in the exploration of the search space by taking advantage of categorical variables. HyperNOMAD is tested on the MNIST, Fashion-MNIST, and CIFAR-10 datasets and achieves results comparable to the current state of the art.;https://dl.acm.org/doi/abs/10.1145/3450975;-YuIOo596hwJ
Liu, J., Tripathi, S., Kurup, U., & Shah, M. (2019, December). Auptimizer-an extensible, open-source framework for hyperparameter tuning. In 2019 IEEE International Conference on Big Data (Big Data) (pp. 339-348). IEEE.;1_ml_machine_data_learning;2019;Auptimizer-an extensible, open-source framework for hyperparameter tuning;Jiayi Liu, Samarth Tripathi, Unmesh Kurup, Mohak Shah;2019 IEEE International Conference on Big Data (Big Data), 339-348, 2019;Tuning machine learning models at scale, especially finding the right hyperparameter values, can be difficult and time-consuming. In addition to the computational effort required, this process also requires some ancillary efforts including engineering tasks (e.g., job scheduling) as well as more mundane tasks (e.g., keeping track of the various parameters and associated results). We present Auptimizer, a general Hyperparameter Optimization (HPO) framework to help data scientists speed up model tuning and bookkeeping. With Auptimizer, users can use all available computing resources in distributed settings for model training. The user-friendly system design simplifies creating, controlling, and tracking of a typical machine learning project. The design also allows researchers to integrate new HPO algorithms. To demonstrate its flexibility, we show how Auptimizer integrates a few major HPO techniques (from random search to neural architecture search). The code is available at https://github.com/LGE-ARC-AdvancedAI/auptimizer.;https://ieeexplore.ieee.org/abstract/document/9006330/;JZhVao3JebMJ
Probst, P., Boulesteix, A. L., & Bischl, B. (2019). Tunability: Importance of hyperparameters of machine learning algorithms. The Journal of Machine Learning Research, 20(1), 1934-1965.;1_ml_machine_data_learning;2019;Tunability: Importance of hyperparameters of machine learning algorithms;Philipp Probst, Anne-Laure Boulesteix, Bernd Bischl;The Journal of Machine Learning Research 20 (1), 1934-1965, 2019;Modern supervised machine learning algorithms involve hyperparameters that have to be set before running them. Options for setting hyperparameters are default values from the software package, manual configuration by the user or configuring them for optimal predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly, we formalize the problem of tuning from a statistical point of view, define data-based defaults and suggest general measures quantifying the tunability of hyperparameters of algorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets from the OpenML platform and six common machine learning algorithms. We apply our measures to assess the tunability of their parameters. Our results yield default values for hyperparameters and enable users to decide whether it is worth conducting a possibly time consuming tuning strategy, to focus on the most important hyperparameters and to choose adequate hyperparameter spaces for tuning.;https://www.jmlr.org/papers/volume20/18-444/18-444.pdf;-i2sCJf7J24J
Narayan, S., Krishna, C. S., Mishra, V., Rai, A., Rai, H., Bharti, C., ... & Singh, N. (2020, December). Ultron-AutoML: an open-source, distributed, scalable framework for efficient hyper-parameter optimization. In 2020 IEEE International Conference on Big Data (Big Data) (pp. 1584-1593). IEEE.;1_ml_machine_data_learning;2020;Ultron-AutoML: an open-source, distributed, scalable framework for efficient hyper-parameter optimization;Swarnim Narayan, Chepuri Shri Krishna, Varun Mishra, Abhinav Rai, Himanshu Rai, Chandrakant Bharti, Gursirat Singh Sodhi, Ashish Gupta, Nitinbalaji Singh;2020 IEEE International Conference on Big Data (Big Data), 1584-1593, 2020;We present Ultron-AutoML, an open-source, distributed framework for efficient hyper-parameter optimization (HPO) of ML models. Considering that hyper-parameter optimization is compute intensive and time-consuming, the framework has been designed for reliability - the ability to successfully complete an HPO Job in a multi-tenant, failure prone environment, as well as efficiency - completing the job with minimum compute cost and wall-clock time. From a user's perspective, the framework emphasizes ease of use and customizability. The user can declaratively specify and execute an HPO Job, while ancillary tasks - containerizing and running the user's scripts, model checkpointing, monitoring progress, parallelization - are handled by the framework. At the same time, the user has complete flexibility in composing the code-base for specifying the ML model training algorithm as well as, optionally, any custom HPO algorithm. The framework supports the creation of data-pipelines to stream batches of shuffled and augmented data from a distributed file system. This comes in handy for training Deep Learning models based on self-supervised, semi-supervised or representation learning algorithms over large training datasets. We demonstrate the framework's reliability and efficiency by running a BERT pre-training job over a large training corpus using pre-emptible GPU compute targets. Despite the inherent unreliability of the underlying compute nodes, the framework is able to complete such long running jobs at 30% of the cost with a marginal increase in wall-clock time. The framework also comes with a service to monitor jobs and ensures reproducibility of any result.;https://ieeexplore.ieee.org/abstract/document/9378071/;xLc3dvTk0JMJ
Ré, C., Niu, F., Gudipati, P., & Srisuwananukorn, C. (2019). Overton: A data system for monitoring and improving machine-learned products. arXiv preprint arXiv:1909.05372.;1_ml_machine_data_learning;2019;Overton: A data system for monitoring and improving machine-learned products;Christopher RÃ©, Feng Niu, Pallavi Gudipati, Charles Srisuwananukorn;arXiv preprint arXiv:1909.05372, 2019;We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production machine learning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a set of novel high-level, declarative abstractions. Overton's vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks. In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year, Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing. In that time, Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors 1.7-2.9 times versus production systems.;https://arxiv.org/abs/1909.05372;u3K-PN1uCLEJ
Bosch, N., & Bosch, J. (2020, August). Software logs for machine learning in a DevOps environment. In 2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA) (pp. 29-33). IEEE.;1_ml_machine_data_learning;2020;Software logs for machine learning in a DevOps environment;Nathan Bosch, Jan Bosch;2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA), 29-33, 2020;System logs perform a critical function in software-intensive systems as logs record the state of the system and significant events in the system at important points in time. Unfortunately, log entries are typically created in an ad-hoc, unstructured and uncoordinated fashion, limiting their usefulness for analytics and machine learning. In a DevOps environment, especially, unmanaged evolution in log data structure causes frequent disruption of operations in automated data pipelines, dashboards and analytics. In this paper, we present the main challenges of contemporary approaches to generating, storing and managing the evolution of system logs data for large, complex, software-intensive systems based on an in-depth case study at a world-leading telecommunications company. Second, we present an approach for generating and managing the evolution of log data in a DevOps environment that does not suffer from the aforementioned challenges and is optimized for use in machine learning. Third, we provide validation of the approach based on expert interviews that confirm that the approach addresses the identified challenges and problems.;https://ieeexplore.ieee.org/abstract/document/9226340/;ZqOx3UoESpwJ
Haakman, M., Cruz, L., Huijgens, H., & van Deursen, A. (2021). AI lifecycle models need to be revised: An exploratory study in Fintech. Empirical Software Engineering, 26, 1-29.;1_ml_machine_data_learning;2021;AI lifecycle models need to be revised: An exploratory study in Fintech;Mark Haakman, Luís Cruz, Hennie Huijgens, Arie van Deursen;Empirical Software Engineering 26, 1-29, 2021;Tech-leading organizations are embracing the forthcoming artificial intelligence revolution. Intelligent systems are replacing and cooperating with traditional software components. Thus, the same development processes and standards in software engineering ought to be complied in artificial intelligence systems. This study aims to understand the processes by which artificial intelligence-based systems are developed and how state-of-the-art lifecycle models fit the current needs of the industry. We conducted an exploratory case study at ING, a global bank with a strong European base. We interviewed 17 people with different roles and from different departments within the organization. We have found that the following stages have been overlooked by previous lifecycle models: data collection, feasibility study, documentation, model monitoring, and model risk assessment. Our work shows that the real challenges of applying Machine Learning go much beyond sophisticated learning algorithms – more focus is needed on the entire lifecycle. In particular, regardless of the existing development tools for Machine Learning, we observe that they are still not meeting the particularities of this field.;https://link.springer.com/article/10.1007/s10664-021-09993-1;_mrxMfIs1BgJ
John, M. M., Olsson, H. H., & Bosch, J. (2021, September). Towards mlops: A framework and maturity model. In 2021 47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA) (pp. 1-8). IEEE.;1_ml_machine_data_learning;2021;Towards mlops: A framework and maturity model;Meenu Mary John, Helena HolmstrÃ¶m Olsson, Jan Bosch;2021 47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA), 1-8, 2021;The adoption of continuous software engineering practices such as DevOps (Development and Operations) in business operations has contributed to significantly shorter software development and deployment cycles. Recently, the term MLOps (Machine Learning Operations) has gained increasing interest as a practice that brings together data scientists and operations teams. However, the adoption of MLOps in practice is still in its infancy and there are few common guidelines on how to effectively integrate it into existing software development practices. In this paper, we conduct a systematic literature review and a grey literature review to derive a framework that identifies the activities involved in the adoption of MLOps and the stages in which companies evolve as they become more mature and advanced. We validate this framework in three case companies and show how they have managed to adopt and integrate MLOps in their large-scale software development companies. The contribution of this paper is threefold. First, we review contemporary literature to provide an overview of the state-of-the-art in MLOps. Based on this review, we derive an MLOps framework that details the activities involved in the continuous development of machine learning models. Second, we present a maturity model in which we outline the different stages that companies go through in evolving their MLOps practices. Third, we validate our framework in three embedded systems case companies and map the companies to the stages in the maturity model.;https://ieeexplore.ieee.org/abstract/document/9582569/;d9_qgXR_KcsJ
Chen, Z., Yao, H., Lou, Y., Cao, Y., Liu, Y., Wang, H., & Liu, X. (2021, May). An empirical study on deployment faults of deep learning based mobile applications. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE) (pp. 674-685). IEEE.;4_dl_testing_deep_network;2021;An empirical study on deployment faults of deep learning based mobile applications;Zhenpeng Chen, Huihan Yao, Yiling Lou, Yanbin Cao, Yuanqiang Liu, Haoyu Wang, Xuanzhe Liu;2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), 674-685, 2021;Deep learning (DL) is moving its step into a growing number of mobile software applications. These software applications, named as DL based mobile applications (abbreviated as mobile DL apps) integrate DL models trained using large-scale data with DL programs. A DL program encodes the structure of a desirable DL model and the process by which the model is trained using training data. Due to the increasing dependency of current mobile apps on DL, software engineering (SE) for mobile DL apps has become important. However, existing efforts in SE research community mainly focus on the development of DL models and extensively analyze faults in DL programs. In contrast, faults related to the deployment of DL models on mobile devices (named as deployment faults of mobile DL apps) have not been well studied. Since mobile DL apps have been used by billions of end users daily for various purposes including for safety-critical scenarios, characterizing their deployment faults is of enormous importance. To fill in the knowledge gap, this paper presents the first comprehensive study to date on the deployment faults of mobile DL apps. We identify 304 real deployment faults from Stack Overflow and GitHub, two commonly used data sources for studying software faults. Based on the identified faults, we construct a fine-granularity taxonomy consisting of 23 categories regarding to fault symptoms and distill common fix strategies for different fault symptoms. Furthermore, we suggest actionable implications and research avenues that can potentially facilitate the deployment of DL models on mobile devices.;https://ieeexplore.ieee.org/abstract/document/9401981/;tG8R_auOTPoJ
Njomou, A. T., Africa, A. J. B., Adams, B., & Fokaefs, M. (2021, March). MSR4ML: reconstructing artifact traceability in machine learning repositories. In 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER) (pp. 536-540). IEEE.;1_ml_machine_data_learning;2021;MSR4ML: reconstructing artifact traceability in machine learning repositories;Aquilas Tchanjou Njomou, Alexandra Johanne Bifona Africa, Bram Adams, Marios Fokaefs;2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER), 536-540, 2021;The increasing popularity of Machine Learning (ML) is generating challenges also for developers. The multitude of programming languages, libraries and available resources allow them to easily build their own models or algorithms. However, ML models are tightly connected to their data implying a different development process from other types of software. Software projects often rely on version control platforms, such as GitHub, but these platforms have not yet been extended to support ML projects. There is poor support for data versioning and no link between ML and software artifacts. Thus, traceability and model evolution can become challenging for developers. While some specific ML platforms exist, they still require considerable manual specification of ML artifacts and links between them. In this work, we propose a framework for automatic identification and traceability of links between data, code and ML model through Mining Software Repositories (MSR) techniques. Our tool combines static code analysis and mining commit data to identify ML, code and data artifacts, reconstruct links between them and retrieve commits that affect each end of the link. The objective is to increase productivity and the developers' awareness of their project through the recovered traceability.;https://ieeexplore.ieee.org/abstract/document/9426012/;-n4LLH3YN0wJ
Majumder, S., Balaji, N., Brey, K., Fu, W., & Menzies, T. (2018, May). 500+ times faster than deep learning: A case study exploring faster methods for text mining stackoverflow. In Proceedings of the 15th International Conference on Mining Software Repositories (pp. 554-563).;1_ml_machine_data_learning;2018;500+ times faster than deep learning: A case study exploring faster methods for text mining stackoverflow;Suvodeep Majumder, Nikhila Balaji, Katie Brey, Wei Fu, Tim Menzies;Proceedings of the 15th International Conference on Mining Software Repositories, 554-563, 2018;Deep learning methods are useful for high-dimensional data and are becoming widely used in many areas of software engineering. Deep learners utilizes extensive computational power and can take a long time to train- making it difficult to widely validate and repeat and improve their results. Further, they are not the best solution in all domains. For example, recent results show that for finding related Stack Overflow posts, a tuned SVM performs similarly to a deep learner, but is significantly faster to train.This paper extends that recent result by clustering the dataset, then tuning every learners within each cluster. This approach is over 500 times faster than deep learning (and over 900 times faster if we use all the cores on a standard laptop computer). Significantly, this faster approach generates classifiers nearly as good (within 2% F1 Score) as the much slower deep learning method. Hence we recommend this faster methods since it is much easier to reproduce and utilizes far fewer CPU resources.More generally, we recommend that before researchers release research results, that they compare their supposedly sophisticated methods against simpler alternatives (e.g applying simpler learners to build local models).;https://dl.acm.org/doi/abs/10.1145/3196398.3196424;vqQVyplTEbAJ
Wang, S., Shrestha, N., Subburaman, A. K., Wang, J., Wei, M., & Nagappan, N. (2021, May). Automatic unit test generation for machine learning libraries: How far are we?. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE) (pp. 1548-1560). IEEE.;10_testing_test_machine_metamorphic;2021;Automatic unit test generation for machine learning libraries: How far are we?;Song Wang, Nishtha Shrestha, Abarna Kucheri Subburaman, Junjie Wang, Moshi Wei, Nachiappan Nagappan;2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), 1548-1560, 2021;Automatic unit test generation that explores the input space and produces effective test cases for given programs have been studied for decades. Many unit test generation tools that can help generate unit test cases with high structural coverage over a program have been examined. However, the fact that existing test generation tools are mainly evaluated on general software programs calls into question about its practical effectiveness and usefulness for machine learning libraries, which are statistically orientated and have fundamentally different nature and construction from general software projects. In this paper, we set out to investigate the effectiveness of existing unit test generation techniques on machine learning libraries. To investigate this issue, we conducted an empirical study on five widely used machine learning libraries with two popular unit testcase generation tools, i.e., EVOSUITE and Randoop. We find that (1) most of the machine learning libraries do not maintain a high-quality unit test suite regarding commonly applied quality metrics such as code coverage (on average is 34.1%) and mutation score (on average is 21.3%), (2) unit test case generation tools, i.e., EVOSUITE and Randoop, lead to clear improvements in code coverage and mutation score, however, the improvement is limited, and (3) there exist common patterns in the uncovered code across the five machine learning libraries that can be used to improve unit test case generation tasks.;https://ieeexplore.ieee.org/abstract/document/9402041/;kdOG-3vLtDgJ
Zhu, J., Long, T., & Memon, A. (2021, May). Automatically authoring regression tests for machine-learning based systems. In 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) (pp. 374-383). IEEE.;10_testing_test_machine_metamorphic;2021;Automatically authoring regression tests for machine-learning based systems;Junjie Zhu, Teng Long, Atif Memon;2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), 374-383, 2021;"Two key design characteristics of machine learning (ML) systems-their ever-improving nature, and learning-based emergent functional behavior - create a moving target, posing new challenges for authoring/maintaining functional regression tests. We identify four specific challenges and address them by developing a new general methodology to automatically author and maintain tests. In particular, we use the volume of production data to periodically refresh our large corpus of test inputs and expected outputs; we use perturbation of the data to obtain coverage-adequate tests; and we use clustering to help identify patterns of failures that are indicative of software bugs. We demonstrate our methodology on an ML-based context-aware Speller. Our coverage-adequate, approx. 1 million regression test cases, automatically authored and maintained for Speller (1) are virtually maintenance free, (2) detect a higher number of Speller failures than previous manually-curated tests, (3) have better coverage of previously unknown functional boundaries of the ML component, and (4) lend themselves to automatic failure triaging by clustering and prioritizing subcategories of tests with over-represented failures. We identify several systematic failure patterns which were due to previously undetected bugs in the Speller, e.g., (1) when the user misses the first letter in a short word, and (2) when the user mistakenly inserts a character in the last token of an address; these have since been fixed.";https://ieeexplore.ieee.org/abstract/document/9402002/;ENiCgjRA1CIJ
Berend, D., Xie, X., Ma, L., Zhou, L., Liu, Y., Xu, C., & Zhao, J. (2020, December). Cats are not fish: Deep learning testing calls for out-of-distribution awareness. In Proceedings of the 35th IEEE/ACM international conference on automated software engineering (pp. 1041-1052).;4_dl_testing_deep_network;2020;Cats are not fish: Deep learning testing calls for out-of-distribution awareness;David Berend, Xiaofei Xie, Lei Ma, Lingjun Zhou, Yang Liu, Chi Xu, Jianjun Zhao;Proceedings of the 35th IEEE/ACM international conference on automated software engineering, 1041-1052, 2020;"As Deep Learning (DL) is continuously adopted in many industrial applications, its quality and reliability start to raise concerns. Similar to the traditional software development process, testing the DL software to uncover its defects at an early stage is an effective way to reduce risks after deployment. According to the fundamental assumption of deep learning, the DL software does not provide statistical guarantee and has limited capability in handling data that falls outside of its learned distribution, i.e., out-of-distribution (OOD) data. Although recent progress has been made in designing novel testing techniques for DL software, which can detect thousands of errors, the current state-of-the-art DL testing techniques usually do not take the distribution of generated test data into consideration. It is therefore hard to judge whether the ""identified errors"" are indeed meaningful errors to the DL application (i.e., due to quality issues of the model) or outliers that cannot be handled by the current model (i.e., due to the lack of training data). Tofill this gap, we take the first step and conduct a large scale empirical study, with a total of 451 experiment configurations, 42 deep neural networks (DNNs) and 1.2 million test data instances, to investigate and characterize the impact of OOD-awareness on DL testing. We further analyze the consequences when DL systems go into production by evaluating the effectiveness of adversarial retraining with distribution-aware errors. The results confirm that introducing data distribution awareness in both testing and enhancement phases outperforms distribution unaware retraining by up to 21.5%.";https://dl.acm.org/doi/abs/10.1145/3324884.3416609;9nxWuZM-6uQJ
Zhang, J. M., Harman, M., Ma, L., & Liu, Y. (2020). Machine learning testing: Survey, landscapes and horizons. IEEE Transactions on Software Engineering, 48(1), 1-36.;10_testing_test_machine_metamorphic;2020;Machine learning testing: Survey, landscapes and horizons;Jie M Zhang, Mark Harman, Lei Ma, Yang Liu;IEEE Transactions on Software Engineering 48 (1), 1-36, 2020;"This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.";https://ieeexplore.ieee.org/abstract/document/9000651/;nRkn144uGRwJ
Fischer, L., Ehrlinger, L., Geist, V., Ramler, R., Sobieczky, F., Zellinger, W., & Moser, B. (2020, August). Applying AI in practice: key challenges and lessons learned. In International Cross-Domain Conference for Machine Learning and Knowledge Extraction (pp. 451-471). Cham: Springer International Publishing.;1_ml_machine_data_learning;2020;Applying AI in practice: key challenges and lessons learned;Lukas Fischer, Lisa Ehrlinger, Verena Geist, Rudolf Ramler, Florian Sobieczky, Werner Zellinger, Bernhard Moser;International Cross-Domain Conference for Machine Learning and Knowledge Extraction, 451-471, 2020;The main challenges along with lessons learned from ongoing research in the application of machine learning systems in practice are discussed, taking into account aspects of theoretical foundations, systems engineering, and human-centered AI postulates. The analysis outlines a fundamental theory-practice gap which superimposes the challenges of AI system engineering at the level of data quality assurance, model building, software engineering and deployment.;https://link.springer.com/chapter/10.1007/978-3-030-57321-8_25;YvVulM-k_CEJ
Lewis, G. A., Ozkaya, I., & Xu, X. (2021, September). Software architecture challenges for ml systems. In 2021 IEEE International Conference on Software Maintenance and Evolution (ICSME) (pp. 634-638). IEEE.;1_ml_machine_data_learning;2021;Software architecture challenges for ml systems;Grace A Lewis, Ipek Ozkaya, Xiwei Xu;2021 IEEE International Conference on Software Maintenance and Evolution (ICSME), 634-638, 2021;Developing machine learning (ML) systems, just like any other system, requires architecture thinking. However, there are characteristics of ML components that create challenges and unique quality attribute (QA) concerns for software architecture and design activities, such as data-dependent behavior, detecting and responding to drift over time, and timely capture of ground truth to inform retraining. This paper presents four categories of software architecture challenges that need to be addressed to support ML system development, maintenance and evolution: software architecture practices for ML systems, architecture patterns and tactics for ML-important QAs, monitorability as a driving QA, and co-architecting and co-versioning. These challenges were collected from targeted workshops, practitioner interviews, and industry engagements. The goal of our work is to encourage further research in these areas and use the information presented in this paper to guide the development of empirically-validated practices for architecting ML systems.;https://ieeexplore.ieee.org/abstract/document/9609199/;LFDWbanXMLQJ
Shrivastava, S., Patel, D., Gifford, W. M., Siegel, S., & Kalagnanam, J. (2019). Thunderml: A toolkit for enabling ai/ml models on cloud for industry 4.0. In Web Services–ICWS 2019: 26th International Conference, Held as Part of the Services Conference Federation, SCF 2019, San Diego, CA, USA, June 25–30, 2019, Proceedings 26 (pp. 163-180). Springer International Publishing.;1_ml_machine_data_learning;2019;Thunderml: A toolkit for enabling ai/ml models on cloud for industry 4.0;Shrey Shrivastava, Dhaval Patel, Wesley M Gifford, Stuart Siegel, Jayant Kalagnanam;Web Services–ICWS 2019: 26th International Conference, Held as Part of the Services Conference Federation, SCF 2019, San Diego, CA, USA, June 25–30, 2019, Proceedings 26, 163-180, 2019;AI, machine learning, and deep learning tools have now become easily accessible on the cloud. However, the adoption of these cloud-based services for heavy industries has been limited due to the gap between general purpose AI tools and operational requirements for production industries. There are three fundamentals gaps. The first is the lack of purpose built solution pipelines designed for common industrial problem types, the second is the lack of tools for automating the learning from noisy sensor data and the third is the lack of platforms which help practitioners leverage cloud-based environment for building and deploying custom modeling pipelines. In this paper, we present ThunderML, a toolkit that addresses these gaps by providing powerful programming model that allows rapid authoring, training and deployment for Industry 4.0 applications. Importantly, the system also facilitates cloud-based deployments by providing a vendor agnostic pipeline execution and deployment layer.;https://link.springer.com/chapter/10.1007/978-3-030-23499-7_11;bgJLDyi12AwJ
Shahoud, S., Khalloof, H., Winter, M., Duepmeier, C., & Hagenmeyer, V. (2020, November). A meta learning approach for automating model selection in big data environments using microservice and container virtualization technologies. In Proceedings of the 12th International Conference on Management of Digital EcoSystems (pp. 84-91).;1_ml_machine_data_learning;2020;A meta learning approach for automating model selection in big data environments using microservice and container virtualization technologies;Shadi Shahoud, Hatem Khalloof, Moritz Winter, Clemens Duepmeier, Veit Hagenmeyer;Proceedings of the 12th International Conference on Management of Digital EcoSystems, 84-91, 2020;For a given specific machine learning task, very often several machine learning algorithms and their right configurations are tested in a trial-and-error approach, until an adequate solution is found. This wastes human resources for constructing multiple models, requires a data analytics expert and is time-consuming, since a variety of learning algorithms are proposed in literature and the non-expert users do not know which one to use in order to obtain good performance results. Meta learning addresses these problems and supports non-expert users by recommending a promising learning algorithm based on meta features computed from a given dataset. In the present paper, a new generic microservice-based framework for realizing the concept of meta learning in Big Data environments is introduced. This framework makes use of a powerful Big Data software stack, container visualization, modern web technologies and a microservice architecture for a fully manageable and highly scalable solution. In this demonstration and for evaluation purpose, time series model selection is taken into account. The performance and usability of the new framework is evaluated on state-of-the-art machine learning algorithms for time series forecasting: it is shown that the proposed microservice-based meta learning framework introduces an excellent performance in assigning the adequate forecasting model for the chosen time series datasets. Moreover, the recommendation of the most appropriate forecasting model results in a well acceptable low overhead demonstrating that the framework can provide an efficient approach to solve the problem of model selection in context of Big Data.;https://dl.acm.org/doi/abs/10.1145/3415958.3433072;9AhZ6qVo728J
Smith, M. J., Sala, C., Kanter, J. M., & Veeramachaneni, K. (2020, June). The machine learning bazaar: Harnessing the ml ecosystem for effective system development. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data (pp. 785-800).;1_ml_machine_data_learning;2020;The machine learning bazaar: Harnessing the ml ecosystem for effective system development;Micah J Smith, Carles Sala, James Max Kanter, Kalyan Veeramachaneni;Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data, 785-800, 2020;"As machine learning is applied more widely, data scientists often struggle to find or create end-to-end machine learning systems for specific tasks. The proliferation of libraries and frameworks and the complexity of the tasks have led to the emergence of ""pipeline jungles"" - brittle, ad hoc ML systems. To address these problems, we introduce the Machine Learning Bazaar, a new framework for developing machine learning and automated machine learning software systems. First, we introduce ML primitives, a unified API and specification for data processing and ML components from different software libraries. Next, we compose primitives into usable ML pipelines, abstracting away glue code, data flow, and data storage. We further pair these pipelines with a hierarchy of AutoML strategies - Bayesian optimization and bandit learning. We use these components to create a general-purpose, multi-task, end-to-end AutoML system that provides solutions to a variety of data modalities (image, text, graph, tabular, relational, etc.) and problem types (classification, regression, anomaly detection, graph matching, etc.). We demonstrate 5 real-world use cases and 2 case studies of our approach. Finally, we present an evaluation suite of 456 real-world ML tasks and describe the characteristics of 2.5 million pipelines searched over this task suite.";https://dl.acm.org/doi/abs/10.1145/3318464.3386146;b9r-0BsBy5kJ
Jin, H., Song, Q., & Hu, X. (2019, July). Auto-keras: An efficient neural architecture search system. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining (pp. 1946-1956).;1_ml_machine_data_learning;2019;Auto-keras: An efficient neural architecture search system;Haifeng Jin, Qingquan Song, Xia Hu;Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, 1946-1956, 2019;Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Extensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The code and documentation are available at https://autokeras.com. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.;https://dl.acm.org/doi/abs/10.1145/3292500.3330648;z_Md7qY8rZkJ
Lee, K. M., Yoo, J., Kim, S. W., Lee, J. H., & Hong, J. (2019). Autonomic machine learning platform. International Journal of Information Management, 49, 491-501.;1_ml_machine_data_learning;2019;Autonomic machine learning platform;Keon Myung Lee, Jaesoo Yoo, Sang-Wook Kim, Jee-Hyong Lee, Jiman Hong;International Journal of Information Management 49, 491-501, 2019;Acquiring information properly through machine learning requires familiarity with the available algorithms and understanding how they work and how to address the given problem in the best possible way. However, even for machine-learning experts in specific industrial fields, in order to predict and acquire information properly in different industrial fields, it is necessary to attempt several instances of trial and error to succeed with the application of machine learning. For non-experts, it is much more difficult to make accurate predictions through machine learning.In this paper, we propose an autonomic machine learning platform which provides the decision factors to be made during the developing of machine learning applications. In the proposed autonomic machine learning platform, machine learning processes are automated based on the specification of autonomic levels. This autonomic machine learning platform can be used to derive a high-quality learning result by minimizing expertsâ€™ interventions and reducing the number of design selections that require expert knowledge and intuition. We also demonstrate that the proposed autonomic machine learning platform is suitable for smart cities which typically require considerable amounts of security sensitive information.;https://www.sciencedirect.com/science/article/pii/S026840121831154X;XI_hu_48xXMJ
Buchgeher, G., Czech, G., Ribeiro, A. S., Kloihofer, W., Meloni, P., Busia, P., ... & Portela, M. (2021). Task-Specific Automation in Deep Learning Processes. In Database and Expert Systems Applications-DEXA 2021 Workshops: BIOKDD, IWCFS, MLKgraphs, AI-CARES, ProTime, AISys 2021, Virtual Event, September 27–30, 2021, Proceedings 32 (pp. 159-169). Springer International Publishing.;1_ml_machine_data_learning;2021;Task-Specific Automation in Deep Learning Processes;Georg Buchgeher, Gerald Czech, Adriano Souza Ribeiro, Werner Kloihofer, Paolo Meloni, Paola Busia, Gianfranco Deriu, Maura Pintor, Battista Biggio, Cristina Chesta, Luca Rinelli, David Solans, Manuel Portela;Database and Expert Systems Applications-DEXA 2021 Workshops: BIOKDD, IWCFS, MLKgraphs, AI-CARES, ProTime, AISys 2021, Virtual Event, September 27â€“30, 2021, Proceedings 32, 159-169, 2021;Recent advances in deep learning facilitate the training, testing, and deployment of models through so-called pipelines. Those pipelines are typically orchestrated with general-purpose machine learning frameworks (e.g., Tensorflow Extended), where developers manually call the single steps for each task-specific application. The diversity of task- and technology-specific requirements in deep learning projects increases the orchestration effort. There are recent advances to automate the orchestration with machine learning, which are however, still immature and do not support task-specific applications. Hence, we claim that partial automation of pipeline orchestration with respect to specific tasks and technologies decreases the overall development effort. We verify this claim with the ALOHA tool flow, where task-specific glue code is automated. The gains of the ALOHA tool flow pipeline are evaluated with respect to human effort, computing performance, and security.;https://link.springer.com/chapter/10.1007/978-3-030-87101-7_16;S4Mr0AkobWAJ
Yi, S., Zou, J., Ren, W., & Luo, H. (2020, June). AutoTrain: An Efficient Auto-training System for Small-scale Image Classification. In 2020 International Wireless Communications and Mobile Computing (IWCMC) (pp. 2039-2044). IEEE.;1_ml_machine_data_learning;2020;AutoTrain: An Efficient Auto-training System for Small-scale Image Classification;Shaoxiao Yi, Junwei Zou, Wei Ren, Hong Luo;2020 International Wireless Communications and Mobile Computing (IWCMC), 2039-2044, 2020;Machine learning has become the most promising research field. However, the involved models usually require complex, tedious and expensive manual intervention. The automated machine learning technology plays a significant role in mitigating this issue. However, the current studies ignore the importance of automation in data preprocessing. In this paper, we propose an efficient automatic training system, AutoTrain, to solve small-scale image classification problems. First, we design sample equalization in data augmentation to improve the performance of training on uneven data. Then, a Bayesian optimization-based strategy controller is introduced to rapidly find the strategy applied in data augmentation. Additionally, we present a dynamic adjustment model to fit tasks with different scales and complexity. Finally, experimental results show that the AutoTrain's training speed is about 3 times faster on average than the conventional methods. And the avergae accuracy of AutoTrain has 2% improved to the conventional methods.;https://ieeexplore.ieee.org/abstract/document/9148450/;zc0vJgxWQ90J
Santhanam, P. (2020). Quality management of machine learning systems. In Engineering Dependable and Secure Machine Learning Systems: Third International Workshop, EDSMLS 2020, New York City, NY, USA, February 7, 2020, Revised Selected Papers 3 (pp. 1-13). Springer International Publishing.;1_ml_machine_data_learning;2020;Quality Management of Machine Learning Systems;Peter Santhanam;Engineering Dependable and Secure Machine Learning Systems: Third International Workshop, EDSMLS 2020, New York City, NY, USA, February 7, 2020, Revised Selected Papers 3 (pp. 1-13). Springer International Publishing.;In the past decade, Artificial Intelligence (AI) has become a part of our daily lives due to major advances in Machine Learning (ML) techniques. In spite of an explosive growth in the raw AI technology and in consumer facing applications on the internet, its adoption in business applications has conspicuously lagged behind. For business/mission-critical systems, serious concerns about reliability and maintainability of AI applications remain. Due to the statistical nature of the output, software ‘defects’ are not well defined. Consequently, many traditional quality management techniques such as program debugging, static code analysis, functional testing, etc. have to be reevaluated. Beyond the correctness of an AI model, many other new quality attributes, such as fairness, robustness, explainability, transparency, etc. become important in delivering an AI system. The purpose of this paper is to present a view of a holistic quality management framework for ML applications based on the current advances and identify new areas of software engineering research to achieve a more trustworthy AI.;https://link.springer.com/chapter/10.1007/978-3-030-62144-5_1;TODO
Almahmoud, J., DeLine, R., & Drucker, S. M. (2021). How Teams Communicate about the Quality of ML Models: A Case Study at an International Technology Company. Proceedings of the ACM on Human-Computer Interaction, 5(GROUP), 1-24.;1_ml_machine_data_learning;2021;How Teams Communicate about the Quality of ML Models: A Case Study at an International Technology Company;Jumana Almahmoud, Robert DeLine, Steven M Drucker;Proceedings of the ACM on Human-Computer Interaction 5 (GROUP), 1-24, 2021;Machine learning (ML) has become a crucial component in software products, either as part of the user experience or used internally by software teams. Prior studies have explored how ML is affecting development team roles beyond data scientists, including user experience designers, program managers, developers and operations engineers. However, there has been little investigation of how team members in different roles on the team communicate about ML, in particular about the quality of models. We use the general term quality to look beyond technical issues of model evaluation, such as accuracy and overfitting, to any issue affecting whether a model is suitable for use, including ethical, engineering, operations, and legal considerations. What challenges do teams face in discussing the quality of ML models? What work practices mitigate those challenges? To address these questions, we conducted a mixed-methods study at a large software company, first interviewing15 employees in a variety of roles, then surveying 168 employees to broaden our understanding. We found several challenges, including a mismatch between user-focused and model-focused notions of performance, misunderstandings about the capabilities and limitations of evolving ML technology, and difficulties in understanding concerns beyond one's own role. We found several mitigation strategies, including the use of demos during discussions to keep the team customer-focused.;https://dl.acm.org/doi/abs/10.1145/3463934;fAridGkqpHYJ
Felderer, M., & Ramler, R. (2021). Quality assurance for AI-based systems: Overview and challenges (introduction to interactive session). In Software Quality: Future Perspectives on Software Engineering Quality: 13th International Conference, SWQD 2021, Vienna, Austria, January 19–21, 2021, Proceedings 13 (pp. 33-42). Springer International Publishing.;1_ml_machine_data_learning;2021;Quality assurance for AI-based systems: Overview and challenges (introduction to interactive session);Michael Felderer, Rudolf Ramler;Software Quality: Future Perspectives on Software Engineering Quality: 13th International Conference, SWQD 2021, Vienna, Austria, January 19â€“21, 2021, Proceedings 13, 33-42, 2021;The number and importance of AI-based systems in all domains is growing. With the pervasive use and the dependence on AI-based systems, the quality of these systems becomes essential for their practical usage. However, quality assurance for AI-based systems is an emerging area that has not been well explored and requires collaboration between the SE and AI research communities. This paper discusses terminology and challenges on quality assurance for AI-based systems to set a baseline for that purpose. Therefore, we define basic concepts and characterize AI-based systems along the three dimensions of artifact type, process, and quality characteristics. Furthermore, we elaborate on the key challenges of (1) understandability and interpretability of AI models, (2) lack of specifications and defined requirements, (3) need for validation data and test input generation, (4) defining expected outcomes as test oracles, (5) accuracy and correctness measures, (6) non-functional properties of AI-based systems, (7) self-adaptive and self-learning characteristics, and (8) dynamic and frequently changing environments.;https://link.springer.com/chapter/10.1007/978-3-030-65854-0_3;UCXc06z05OsJ
Myllyaho, L., Raatikainen, M., Männistö, T., Nurminen, J. K., & Mikkonen, T. (2022). On misbehaviour and fault tolerance in machine learning systems. Journal of Systems and Software, 183, 111096.;1_ml_machine_data_learning;2022;On misbehaviour and fault tolerance in machine learning systems;Lalli Myllyaho, Mikko Raatikainen, Tomi Männistö, Jukka K Nurminen, Tommi Mikkonen;Journal of Systems and Software 183, 111096, 2022;Machine learning (ML) provides us with numerous opportunities, allowing ML systems to adapt to new situations and contexts. At the same time, this adaptability raises uncertainties concerning the run-time product quality or dependability, such as reliability and security, of these systems. Systems can be tested and monitored, but this does not provide protection against faults and failures in adapted ML systems themselves. We studied software designs that aim at introducing fault tolerance in ML systems so that possible problems in ML components of the systems can be avoided. The research was conducted as a case study, and its data was collected through five semi-structured interviews with experienced software architects. We present a conceptualisation of the misbehaviour of ML systems, the perceived role of fault tolerance, and the designs used. Common patterns to incorporating ML components in design in a fault tolerant fashion have started to emerge. ML models are, for example, guarded by monitoring the inputs and their distribution, and enforcing business rules on acceptable outputs. Multiple, specialised ML models are used to adapt to the variations and changes in the surrounding world, and simpler fall-over techniques like default outputs are put in place to have systems up and running in the face of problems. However, the general role of these patterns is not widely acknowledged. This is mainly due to the relative immaturity of using ML as part of a complete software system: the field still lacks established frameworks and practices beyond training to implement, operate, and maintain the software that utilises ML. ML software engineering needs further analysis and development on all fronts.;https://www.sciencedirect.com/science/article/pii/S016412122100193X;leeFJnk-bv0J
Nurminen, J. K., Halvari, T., Harviainen, J., Mylläri, J., Röyskö, A., Silvennoinen, J., & Mikkonen, T. (2019, October). Software framework for data fault injection to test machine learning systems. In 2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW) (pp. 294-299). IEEE.;1_ml_machine_data_learning;2019;Software framework for data fault injection to test machine learning systems;Jukka K Nurminen, Tuomas Halvari, Juha Harviainen, Juha Mylläri, Antti Röyskö, Juuso Silvennoinen, Tommi Mikkonen;2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW), 294-299, 2019;Data-intensive systems are sensitive to the quality of data. Data often has problems due to faulty sensors or network problems, for instance. In this work, we develop a software framework to emulate faults in data and use it to study how machine learning (ML) systems work when the data has problems. We aim for flexibility: users can use predefined or their own dedicated fault models. Likewise, different kind of data (e.g. text, time series, video) can be used and the system under test can vary from a single ML model to a complicated software system. Our goal is to show how data faults can be emulated and how that can be used in the study and development of ML solutions.;https://ieeexplore.ieee.org/abstract/document/8990189/;YBGDTIUcjhsJ
Li, G., Pattabiraman, K., & DeBardeleben, N. (2018, October). Tensorfi: A configurable fault injector for tensorflow applications. In 2018 IEEE International symposium on software reliability engineering workshops (ISSREW) (pp. 313-320). IEEE.;1_ml_machine_data_learning;2018;Tensorfi: A configurable fault injector for tensorflow applications;Guanpeng Li, Karthik Pattabiraman, Nathan DeBardeleben;2018 IEEE International symposium on software reliability engineering workshops (ISSREW), 313-320, 2018;Machine Learning (ML) applications have emerged as the killer applications for next generation hardware and software platforms, and there is a lot of interest in software frameworks to build such applications. TensorFlow is a high-level dataflow framework for building ML applications and has become the most popular one in the recent past. ML applications are also being increasingly used in safety-critical systems such as self-driving cars and home robotics. Therefore, there is a compelling need to evaluate the resilience of ML applications built using frameworks such as TensorFlow. In this paper, we build a high-level fault injection framework for TensorFlow called TensorFI for evaluating the resilience of ML applications. TensorFI is flexible, easy to use, and portable. It also allows ML application programmers to explore the effects of different parameters and algorithms on error resilience.;https://ieeexplore.ieee.org/abstract/document/8539213/;sSEQubdWYZ4J
Soro, S. (2021). TinyML for ubiquitous edge AI. arXiv preprint arXiv:2102.01255.;7_edge_computing_deep_learning;2021;TinyML for ubiquitous edge AI;Stanislava Soro;arXiv preprint arXiv:2102.01255, 2021;TinyML is a fast-growing multidisciplinary field at the intersection of machine learning, hardware, and software, that focuses on enabling deep learning algorithms on embedded (microcontroller powered) devices operating at extremely low power range (mW range and below). TinyML addresses the challenges in designing power-efficient, compact deep neural network models, supporting software framework, and embedded hardware that will enable a wide range of customized, ubiquitous inference applications on battery-operated, resource-constrained devices. In this report, we discuss the major challenges and technological enablers that direct this field's expansion. TinyML will open the door to the new types of edge services and applications that do not rely on cloud processing but thrive on distributed edge inference and autonomous reasoning.;https://arxiv.org/abs/2102.01255;mwn-mn3I5x0J
Banbury, C., Reddi, V. J., Torelli, P., Holleman, J., Jeffries, N., Kiraly, C., ... & Xuesong, X. (2021). Mlperf tiny benchmark. arXiv preprint arXiv:2106.07597.;7_edge_computing_deep_learning;2021;Mlperf tiny benchmark;Colby Banbury, Vijay Janapa Reddi, Peter Torelli, Jeremy Holleman, Nat Jeffries, Csaba Kiraly, Pietro Montino, David Kanter, Sebastian Ahmed, Danilo Pau, Urmish Thakker, Antonio Torrini, Peter Warden, Jay Cordaro, Giuseppe Di Guglielmo, Javier Duarte, Stephen Gibellini, Videet Parekh, Honson Tran, Nhan Tran, Niu Wenxu, Xu Xuesong;arXiv preprint arXiv:2106.07597, 2021;Advancements in ultra-low-power tiny machine learning (TinyML) systems promise to unlock an entirely new class of smart applications. However, continued progress is limited by the lack of a widely accepted and easily reproducible benchmark for these systems. To meet this need, we present MLPerf Tiny, the first industry-standard benchmark suite for ultra-low-power tiny machine learning systems. The benchmark suite is the collaborative effort of more than 50 organizations from industry and academia and reflects the needs of the community. MLPerf Tiny measures the accuracy, latency, and energy of machine learning inference to properly evaluate the tradeoffs between systems. Additionally, MLPerf Tiny implements a modular design that enables benchmark submitters to show the benefits of their product, regardless of where it falls on the ML deployment stack, in a fair and reproducible manner. The suite features four benchmarks: keyword spotting, visual wake words, image classification, and anomaly detection.;https://arxiv.org/abs/2106.07597;rCpJRGjCm2MJ
Banbury, C., Zhou, C., Fedorov, I., Matas, R., Thakker, U., Gope, D., ... & Whatmough, P. (2021). Micronets: Neural network architectures for deploying tinyml applications on commodity microcontrollers. Proceedings of Machine Learning and Systems, 3, 517-532.;7_edge_computing_deep_learning;2021;Micronets: Neural network architectures for deploying tinyml applications on commodity microcontrollers;Colby Banbury, Chuteng Zhou, Igor Fedorov, Ramon Matas, Urmish Thakker, Dibakar Gope, Vijay Janapa Reddi, Matthew Mattina, Paul Whatmough;Proceedings of Machine Learning and Systems 3, 517-532, 2021;Executing machine learning workloads locally on resource constrained microcontrollers (MCUs) promises to drastically expand the application space of IoT. However, so-called TinyML presents severe technical challenges, as deep neural network inference demands a large compute and memory budget. To address this challenge, neural architecture search (NAS) promises to help design accurate ML models that meet the tight MCU memory, latency, and energy constraints. A key component of NAS algorithms is their latency/energy model, ie, the mapping from a given neural network architecture to its inference latency/energy on an MCU. In this paper, we observe an intriguing property of NAS search spaces for MCU model design: on average, model latency varies linearly with model operation (op) count under a uniform prior over models in the search space. Exploiting this insight, we employ differentiable NAS (DNAS) to search for models with low memory usage and low op count, where op count is treated as a viable proxy to latency. Experimental results validate our methodology, yielding our MicroNet models, which we deploy on MCUs using Tensorflow Lite Micro, a standard open-source neural network (NN) inference runtime widely used in the TinyML community. MicroNets demonstrate state-of-the-art results for all three TinyMLperf industry-standard benchmark tasks: visual wake words, audio keyword spotting, and anomaly detection. Models and training scripts can be found at https://github. com/ARM-software/ML-zoo.;https://proceedings.mlsys.org/paper_files/paper/2021/hash/c4d41d9619462c534b7b61d1f772385e-Abstract.html;NlbuAk-tlUsJ
Banbury, C. R., Reddi, V. J., Lam, M., Fu, W., Fazel, A., Holleman, J., ... & Yadav, P. (2020). Benchmarking tinyml systems: Challenges and direction. arXiv preprint arXiv:2003.04821.;7_edge_computing_deep_learning;2020;Benchmarking tinyml systems: Challenges and direction;Colby R Banbury, Vijay Janapa Reddi, Max Lam, William Fu, Amin Fazel, Jeremy Holleman, Xinyuan Huang, Robert Hurtado, David Kanter, Anton Lokhmotov, David Patterson, Danilo Pau, Jae-sun Seo, Jeff Sieracki, Urmish Thakker, Marian Verhelst, Poonam Yadav;arXiv preprint arXiv:2003.04821, 2020;Recent advancements in ultra-low-power machine learning (TinyML) hardware promises to unlock an entirely new class of smart applications. However, continued progress is limited by the lack of a widely accepted benchmark for these systems. Benchmarking allows us to measure and thereby systematically compare, evaluate, and improve the performance of systems and is therefore fundamental to a field reaching maturity. In this position paper, we present the current landscape of TinyML and discuss the challenges and direction towards developing a fair and useful hardware benchmark for TinyML workloads. Furthermore, we present our four benchmarks and discuss our selection methodology. Our viewpoints reflect the collective thoughts of the TinyMLPerf working group that is comprised of over 30 organizations.;https://arxiv.org/abs/2003.04821;i6w40rpbz3IJ
Cai, H., Gan, C., Zhu, L., & Han, S. (2020). Tinytl: Reduce activations, not trainable parameters for efficient on-device learning. arXiv preprint arXiv:2007.11622.;7_edge_computing_deep_learning;2020;Tinytl: Reduce activations, not trainable parameters for efficient on-device learning;Han Cai, Chuang Gan, Ligeng Zhu, Song Han;arXiv preprint arXiv:2007.11622, 2020;On-device learning enables edge devices to continually adapt the AI models to new data, which requires a small memory footprint to fit the tight memory constraint of edge devices. Existing work solves this problem by reducing the number of trainable parameters. However, this doesn't directly translate to memory saving since the major bottleneck is the activations, not parameters. In this work, we present Tiny-Transfer-Learning (TinyTL) for memory-efficient on-device learning. TinyTL freezes the weights while only learns the bias modules, thus no need to store the intermediate activations. To maintain the adaptation capacity, we introduce a new memory-efficient bias module, the lite residual module, to refine the feature extractor by learning small residual feature maps adding only 3.8% memory overhead. Extensive experiments show that TinyTL significantly saves the memory (up to 6.5x) with little accuracy loss compared to fine-tuning the full network. Compared to fine-tuning the last layer, TinyTL provides significant accuracy improvements (up to 34.1%) with little memory overhead. Furthermore, combined with feature extractor adaptation, TinyTL provides 7.3-12.9x memory saving without sacrificing accuracy compared to fine-tuning the full Inception-V3.;https://arxiv.org/abs/2007.11622;tBzLiTS_iV0J
David, R., Duke, J., Jain, A., Janapa Reddi, V., Jeffries, N., Li, J., ... & Rhodes, R. (2021). Tensorflow lite micro: Embedded machine learning for tinyml systems. Proceedings of Machine Learning and Systems, 3, 800-811.;7_edge_computing_deep_learning;2021;Tensorflow lite micro: Embedded machine learning for tinyml systems;Robert David, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, Ian Nappier, Meghna Natraj, Tiezhen Wang, Pete Warden, Rocky Rhodes;Proceedings of Machine Learning and Systems 3, 800-811, 2021;We introduce TensorFlow (TF) Micro, an open-source machine learning inference framework for running deep-learning models on embedded systems. TF Micro tackles the efficiency requirements imposed by embedded system resource constraints and the fragmentation challenges that make cross-platform interoperability nearly impossible. The framework adopts a unique interpreter-based approach that provides flexibility while overcoming the challenges. This paper explains the design decisions behind TF Micro and describes its implementation. We present an evaluation to demonstrate its low resource requirement and minimal run-time performance overhead.;https://proceedings.mlsys.org/paper_files/paper/2021/hash/6c44dc73014d66ba49b28d483a8f8b0d-Abstract.html;1IM3Q3o0o5EJ
Fedorov, I., Adams, R. P., Mattina, M., & Whatmough, P. (2019). Sparse: Sparse architecture search for cnns on resource-constrained microcontrollers. Advances in Neural Information Processing Systems, 32.;7_edge_computing_deep_learning;2019;Sparse: Sparse architecture search for cnns on resource-constrained microcontrollers;Igor Fedorov, Ryan P Adams, Matthew Mattina, Paul Whatmough;Advances in Neural Information Processing Systems 32, 2019;The vast majority of processors in the world are actually microcontroller units (MCUs), which find widespread use performing simple control tasks in applications ranging from automobiles to medical devices and office equipment. The Internet of Things (IoT) promises to inject machine learning into many of these every-day objects via tiny, cheap MCUs. However, these resource-impoverished hardware platforms severely limit the complexity of machine learning models that can be deployed. For example, although convolutional neural networks (CNNs) achieve state-of-the-art results on many visual recognition tasks, CNN inference on MCUs is challenging due to severe memory limitations. To circumvent the memory challenge associated with CNNs, various alternatives have been proposed that do fit within the memory budget of an MCU, albeit at the cost of prediction accuracy. This paper challenges the idea that CNNs are not suitable for deployment on MCUs. We demonstrate that it is possible to automatically design CNNs which generalize well, while also being small enough to fit onto memory-limited MCUs. Our Sparse Architecture Search method combines neural architecture search with pruning in a single, unified approach, which learns superior models on four popular IoT datasets. The CNNs we find are more accurate and up to 7.4Ã— smaller than previous approaches, while meeting the strict MCU working memory constraint.;https://proceedings.neurips.cc/paper_files/paper/2019/hash/044a23cadb567653eb51d4eb40acaa88-Abstract.html;O5elfKWxOT0J
Goel, A., Tung, C., Lu, Y. H., & Thiruvathukal, G. K. (2020, June). A survey of methods for low-power deep learning and computer vision. In 2020 IEEE 6th World Forum on Internet of Things (WF-IoT) (pp. 1-6). IEEE.;11_deep_network_model_layer;2020;A survey of methods for low-power deep learning and computer vision;Abhinav Goel, Caleb Tung, Yung-Hsiang Lu, George K Thiruvathukal;2020 IEEE 6th World Forum on Internet of Things (WF-IoT), 1-6, 2020;Deep neural networks (DNNs) are successful in many computer vision tasks. However, the most accurate DNNs require millions of parameters and operations, making them energy, computation and memory intensive. This impedes the deployment of large DNNs in low-power devices with limited compute resources. Recent research improves DNN models by reducing the memory requirement, energy consumption, and number of operations without significantly decreasing the accuracy. This paper surveys the progress of low-power deep learning and computer vision, specifically in regards to inference, and discusses the methods for compacting and accelerating DNN models. The techniques can be divided into four major categories: (1) parameter quantization and pruning, (2) compressed convolutional filters and matrix factorization, (3) network architecture search, and (4) knowledge distillation. We analyze the accuracy, advantages, disadvantages, and potential solutions to the problems with the techniques in each category. We also discuss new evaluation metrics as a guideline for future research.;https://ieeexplore.ieee.org/abstract/document/9221198/;DmLNkSY5v60J
Islam, M. J., Nguyen, G., Pan, R., & Rajan, H. (2019, August). A comprehensive study on deep learning bug characteristics. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 510-520).;4_dl_testing_deep_network;2019;A comprehensive study on deep learning bug characteristics;Md Johirul Islam, Giang Nguyen, Rangeet Pan, Hridesh Rajan;Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 510-520, 2019;Deep learning has gained substantial popularity in recent years. Developers mainly rely on libraries and tools to add deep learning capabilities to their software. What kinds of bugs are frequently found in such software? What are the root causes of such bugs? What impacts do such bugs have? Which stages of deep learning pipeline are more bug prone? Are there any antipatterns? Understanding such characteristics of bugs in deep learning software has the potential to foster the development of better deep learning platforms, debugging mechanisms, development practices, and encourage the development of analysis and verification frameworks. Therefore, we study 2716 high-quality posts from Stack Overflow and 500 bug fix commits from Github about five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand the types of bugs, root causes of bugs, impacts of bugs, bug-prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy software. The key findings of our study include: data bug and logic bug are the most severe bug types in deep learning software appearing more than 48% of the times, major root causes of these bugs are Incorrect Model Parameter (IPS) and Structural Inefficiency (SI) showing up more than 43% of the times.We have also found that the bugs in the usage of deep learning libraries have some common antipatterns.;https://dl.acm.org/doi/abs/10.1145/3338906.3338955;8eoH8WXEdBAJ
Lai, L., Suda, N., & Chandra, V. (2018). Cmsis-nn: Efficient neural network kernels for arm cortex-m cpus. arXiv preprint arXiv:1801.06601.;7_edge_computing_deep_learning;2018;Cmsis-nn: Efficient neural network kernels for arm cortex-m cpus;Liangzhen Lai, Naveen Suda, Vikas Chandra;arXiv preprint arXiv:1801.06601, 2018;Deep Neural Networks are becoming increasingly popular in always-on IoT edge devices performing data analytics right at the source, reducing latency as well as energy consumption for data communication. This paper presents CMSIS-NN, efficient kernels developed to maximize the performance and minimize the memory footprint of neural network (NN) applications on Arm Cortex-M processors targeted for intelligent IoT edge devices. Neural network inference based on CMSIS-NN kernels achieves 4.6X improvement in runtime/throughput and 4.9X improvement in energy efficiency.;https://arxiv.org/abs/1801.06601;TstGKsDjyUIJ
LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. nature, 521(7553), 436-444.;11_deep_network_model_layer;2015;Deep learning;Yann LeCun, Yoshua Bengio, Geoffrey Hinton;nature 521 (7553), 436-444, 2015;Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.;https://www.nature.com/articles/nature14539;0qfs6zbVakoJ
Loukides, M. (2019). TinyML: The challenges and opportunities of low-power ML applications.;1_ml_machine_data_learning;2019;TinyML: The challenges and opportunities of low-power ML applications;only citation on google scholar;only citation on google scholar;only citation on google scholar;https://www. oreilly. com/radar/tinyml-the-challenges-and-opportunities-of-low-power-ml-applications;zKYhvpqTGo8J
Makhshari, A., & Mesbah, A. (2021, May). IoT bugs and development challenges. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE) (pp. 460-472). IEEE.;7_edge_computing_deep_learning;2021;IoT bugs and development challenges;Amir Makhshari, Ali Mesbah;2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), 460-472, 2021;IoT systems are rapidly adopted in various domains, from embedded systems to smart homes. Despite their growing adoption and popularity, there has been no thorough study to understand IoT development challenges from the practitioners' point of view. We provide the first systematic study of bugs and challenges that IoT developers face in practice, through a large-scale empirical investigation. We collected 5,565 bug reports from 91 representative IoT project repositories and categorized a random sample of 323 based on the observed failures, root causes, and the locations of the faulty components. In addition, we conducted nine interviews with IoT experts to uncover more details about IoT bugs and to gain insight into IoT developers' challenges. Lastly, we surveyed 194 IoT developers to validate our findings and gain further insights. We propose the first bug taxonomy for IoT systems based on our results. We highlight frequent bug categories and their root causes, correlations between them, and common pitfalls and challenges that IoT developers face. We recommend future directions for IoT areas that require research and development attention.;https://ieeexplore.ieee.org/abstract/document/9402092/;aFnyIU2fzKYJ
Oshana, R., & Kraeling, M. (Eds.). (2019). Software engineering for embedded systems: Methods, practical techniques, and applications. Newnes.;1_ml_machine_data_learning;2019;Software engineering for embedded systems: Methods, practical techniques, and applications;Robert Oshana, Mark Kraeling;Newnes, 2019;Software Engineering for Embedded Systems: Methods, Practical Techniques, and Applications, Second Edition provides the techniques and technologies in software engineering to optimally design and implement an embedded system. Written by experts with a solution focus, this encyclopedic reference gives an indispensable aid on how to tackle the day-to-day problems encountered when using software engineering methods to develop embedded systems. New sections cover peripheral programming, Internet of things, security and cryptography, networking and packet processing, and hands on labs. Users will learn about the principles of good architecture for an embedded system, design practices, details on principles, and much more. Provides a roadmap of key problems/issues and references to their solution in the text Reviews core methods and how to apply them Contains examples that demonstrate timeless implementation details Users case studies to show how key ideas can be implemented, the rationale for choices made, and design guidelines and trade-offs;https://books.google.com/books?hl=en&lr=&id=g6aeDwAAQBAJ&oi=fnd&pg=PP1&dq=Oshana,+R.,+%26+Kraeling,+M.+(Eds.).+(2019).+Software+engineering+for+embedded+systems:+Methods,+practical+techniques,+and+applications.+Newnes.&ots=8GWA5nTsNp&sig=Ia1wS6V90XHIIBBKoTx_2I6YI08;RQMqMQqp2TsJ
Paissan, F., Ancilotto, A., & Farella, E. (2022). PhiNets: a scalable backbone for low-power AI at the edge. ACM Transactions on Embedded Computing Systems, 21(5), 1-18.;4_dl_testing_deep_network;2022;PhiNets: a scalable backbone for low-power AI at the edge;Francesco Paissan, Alberto Ancilotto, Elisabetta Farella;ACM Transactions on Embedded Computing Systems 21 (5), 1-18, 2022;In the Internet of Things era, where we see many interconnected and heterogeneous mobile and fixed smart devices, distributing the intelligence from the cloud to the edge has become a necessity. Due to limited computational and communication capabilities, low memory and limited energy budget, bringing artificial intelligence algorithms to peripheral devices, such as end-nodes of a sensor network, is a challenging task and requires the design of innovative solutions. In this work, we present PhiNets, a new scalable backbone optimized for deep-learning-based image processing on resource-constrained platforms. PhiNets are based on inverted residual blocks specifically designed to decouple the computational cost, working memory, and parameter memory, thus exploiting all available resources for a given platform. With a YoloV2 detection head and Simple Online and Realtime Tracking (SORT), the proposed architecture achieves state-of-the-art results in (i) detection on the COCO and VOC2012 benchmarks, and (ii) tracking on the MOT15 benchmark. PhiNets obtain a reduction in parameter count of around 90% with respect to previous state-of-the-art models (EfficientNetv1, MobileNetv2) and achieve better performance with lower computational cost. Moreover, we demonstrate our approach on a prototype node based on an STM32H743 microcontroller (MCU) with 2 MB of internal Flash and 1MB of RAM and achieve power requirements in the order of 10 mW. The code for the PhiNets is publicly available on GitHub.;https://dl.acm.org/doi/abs/10.1145/3510832;5xWolcg2iaoJ
Reddi, V. J., Plancher, B., Kennedy, S., Moroney, L., Warden, P., Agarwal, A., ... & Tingley, D. (2021). Widening access to applied machine learning with tinyml. arXiv preprint arXiv:2106.04008.;7_edge_computing_deep_learning;2021;Widening access to applied machine learning with tinyml;Vijay Janapa Reddi, Brian Plancher, Susan Kennedy, Laurence Moroney, Pete Warden, Anant Agarwal, Colby Banbury, Massimo Banzi, Matthew Bennett, Benjamin Brown, Sharad Chitlangia, Radhika Ghosal, Sarah Grafman, Rupert Jaeger, Srivatsan Krishnan, Maximilian Lam, Daniel Leiker, Cara Mann, Mark Mazumder, Dominic Pajak, Dhilan Ramaprasad, J Evan Smith, Matthew Stewart, Dustin Tingley;arXiv preprint arXiv:2106.04008, 2021;Broadening access to both computational and educational resources is critical to diffusing machine-learning (ML) innovation. However, today, most ML resources and experts are siloed in a few countries and organizations. In this paper, we describe our pedagogical approach to increasing access to applied ML through a massive open online course (MOOC) on Tiny Machine Learning (TinyML). We suggest that TinyML, ML on resource-constrained embedded devices, is an attractive means to widen access because TinyML both leverages low-cost and globally accessible hardware, and encourages the development of complete, self-contained applications, from data collection to deployment. To this end, a collaboration between academia (Harvard University) and industry (Google) produced a four-part MOOC that provides application-oriented instruction on how to develop solutions using TinyML. The series is openly available on the edX MOOC platform, has no prerequisites beyond basic programming, and is designed for learners from a global variety of backgrounds. It introduces pupils to real-world applications, ML algorithms, data-set engineering, and the ethical considerations of these technologies via hands-on programming and deployment of TinyML applications in both the cloud and their own microcontrollers. To facilitate continued learning, community building, and collaboration beyond the courses, we launched a standalone website, a forum, a chat, and an optional course-project competition. We also released the course materials publicly, hoping they will inspire the next generation of ML practitioners and educators and further broaden access to cutting-edge ML technologies.;https://arxiv.org/abs/2106.04008;2IVJenFVqp4J
Shafique, M., Naseer, M., Theocharides, T., Kyrkou, C., Mutlu, O., Orosa, L., & Choi, J. (2020). Robust machine learning systems: Challenges, current trends, perspectives, and the road ahead. IEEE Design & Test, 37(2), 30-57.;5_adversarial_attack_example_model;2020;Robust machine learning systems: Challenges, current trends, perspectives, and the road ahead;Muhammad Shafique, Mahum Naseer, Theocharis Theocharides, Christos Kyrkou, Onur Mutlu, Lois Orosa, Jungwook Choi;IEEE Design & Test 37 (2), 30-57, 2020;Currently, machine learning (ML) techniques are at the heart of smart cyber-physical systems (CPSs) and Internet-of-Things (loT). This article discusses various challenges and probable solutions for security attacks on these ML-inspired hardware and software techniques.;https://ieeexplore.ieee.org/abstract/document/8979377/;4uDjBhUUP9gJ
Shafique, M., Theocharides, T., Reddy, V. J., & Murmann, B. (2021, December). TinyML: current progress, research challenges, and future roadmap. In 2021 58th ACM/IEEE Design Automation Conference (DAC) (pp. 1303-1306). IEEE.;7_edge_computing_deep_learning;2021;TinyML: current progress, research challenges, and future roadmap;Muhammad Shafique, Theocharis Theocharides, Vijay Janapa Reddy, Boris Murmann;2021 58th ACM/IEEE Design Automation Conference (DAC), 1303-1306, 2021;TinyML: tiny in size, BIG in impact!This paper highlights the current progress, challenges and open research opportunities in the domain of tinyML, benchmarking, and emerging applications for Edge-AI.;https://ieeexplore.ieee.org/abstract/document/9586232/;0VWwwLqeItEJ
Shi, W., Cao, J., Zhang, Q., Li, Y., & Xu, L. (2016). Edge computing: Vision and challenges. IEEE internet of things journal, 3(5), 637-646.;7_edge_computing_deep_learning;2016;Edge computing: Vision and challenges;Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, Lanyu Xu;IEEE internet of things journal 3 (5), 637-646, 2016;The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.;https://ieeexplore.ieee.org/abstract/document/7488250/;Dbfii--B_SgJ
Sze, V., Chen, Y. H., Emer, J., Suleiman, A., & Zhang, Z. (2017, April). Hardware for machine learning: Challenges and opportunities. In 2017 IEEE Custom Integrated Circuits Conference (CICC) (pp. 1-8). IEEE.;7_edge_computing_deep_learning;2017;Hardware for machine learning: Challenges and opportunities;Vivienne Sze, Yu-Hsin Chen, Joel Emer, Amr Suleiman, Zhengdong Zhang;2017 IEEE Custom Integrated Circuits Conference (CICC), 1-8, 2017;"Machine learning plays a critical role in extracting meaningful information out of the zetabytes of sensor data collected every day. For some applications, the goal is to analyze and understand the data to identify trends (e.g., surveillance, portable/wearable electronics); in other applications, the goal is to take immediate action based the data (e.g., robotics/drones, self-driving cars, smart Internet of Things). For many of these applications, local embedded processing near the sensor is preferred over the cloud due to privacy or latency concerns, or limitations in the communication bandwidth. However, at the sensor there are often stringent constraints on energy consumption and cost in addition to throughput and accuracy requirements. Furthermore, flexibility is often required such that the processing can be adapted for different applications or environments (e.g., update the weights and model in the classifier). In many applications, machine learning often involves transforming the input data into a higher dimensional space, which, along with programmable weights, increases data movement and consequently energy consumption. In this paper, we will discuss how these challenges can be addressed at various levels of hardware design ranging from architecture, hardware-friendly algorithms, mixed-signal circuits, and advanced technologies (including memories and sensors).";https://ieeexplore.ieee.org/abstract/document/7993626/;C0Z4UUonFYgJ
Warden, P., & Situnayake, D. (2019). Tinyml: Machine learning with tensorflow lite on arduino and ultra-low-power microcontrollers. O'Reilly Media.;7_edge_computing_deep_learning;2019;Tinyml: Machine learning with tensorflow lite on arduino and ultra-low-power microcontrollers;Pete Warden, Daniel Situnayake;O'Reilly Media, 2019;Deep learning networks are getting smaller. Much smaller. The Google Assistant team can detect words with a model just 14 kilobytes in size—small enough to run on a microcontroller. With this practical book you’ll enter the field of TinyML, where deep learning and embedded systems combine to make astounding things possible with tiny devices. Pete Warden and Daniel Situnayake explain how you can train models small enough to fit into any environment. Ideal for software and hardware developers who want to build embedded systems using machine learning, this guide walks you through creating a series of TinyML projects, step-by-step. No machine learning or microcontroller experience is necessary. Build a speech recognizer, a camera that detects people, and a magic wand that responds to gestures Work with Arduino and ultra-low-power microcontrollers Learn the essentials of ML and how to train your own models Train models to understand audio, image, and accelerometer data Explore TensorFlow Lite for Microcontrollers, Google’s toolkit for TinyML Debug applications and provide safeguards for privacy and security Optimize latency, energy usage, and model and binary size;https://books.google.com/books?hl=en&lr=&id=tn3EDwAAQBAJ&oi=fnd&pg=PP1&dq=Warden,+P.,+%26+Situnayake,+D.+(2019).+Tinyml:+Machine+learning+with+tensorflow+lite+on+arduino+and+ultra-low-power+microcontrollers.+O%27Reilly+Media.&ots=jpunbp35xZ&sig=tYJyM_Ren1-9s267SoI8Uw7iEOs;yKCG61_uQ_8J
Xu, D., Li, T., Li, Y., Su, X., Tarkoma, S., Jiang, T., ... & Hui, P. (2020). Edge intelligence: Architectures, challenges, and applications. arXiv preprint arXiv:2003.12172.;7_edge_computing_deep_learning;2020;Edge intelligence: Architectures, challenges, and applications;Dianlei Xu, Tong Li, Yong Li, Xiang Su, Sasu Tarkoma, Tao Jiang, Jon Crowcroft, Pan Hui;arXiv preprint arXiv:2003.12172, 2020;Edge intelligence refers to a set of connected systems and devices for data collection, caching, processing, and analysis in locations close to where data is captured based on artificial intelligence. The aim of edge intelligence is to enhance the quality and speed of data processing and protect the privacy and security of the data. Although recently emerged, spanning the period from 2011 to now, this field of research has shown explosive growth over the past five years. In this paper, we present a thorough and comprehensive survey on the literature surrounding edge intelligence. We first identify four fundamental components of edge intelligence, namely edge caching, edge training, edge inference, and edge offloading, based on theoretical and practical results pertaining to proposed and deployed systems. We then aim for a systematic classification of the state of the solutions by examining research results and observations for each of the four components and present a taxonomy that includes practical problems, adopted techniques, and application goals. For each category, we elaborate, compare and analyse the literature from the perspectives of adopted techniques, objectives, performance, advantages and drawbacks, etc. This survey article provides a comprehensive introduction to edge intelligence and its application areas. In addition, we summarise the development of the emerging research field and the current state-of-the-art and discuss the important open issues and possible theoretical and technical solutions.;https://arxiv.org/abs/2003.12172;0lAmGW0FyioJ
Vázquez, M. Á., Pallois, J. P., Debbah, M., Masouros, C., Kenyon, T., Deng, Y., ... & Erfanian, J. (2019, September). Deploying artificial intelligence in the wireless infrastructure: the challenges ahead. In 2019 IEEE 2nd 5G World Forum (5GWF) (pp. 458-459). IEEE.;1_ml_machine_data_learning;2019;Deploying artificial intelligence in the wireless infrastructure: the challenges ahead;Miguel Ángel Vázquez, Jean Paul Pallois, Merouane Debbah, Christos Masouros, Tony Kenyon, Yansha Deng, Fisseha Mekuria, Ana Pérez-Neira, Javan Erfanian;2019 IEEE 2nd 5G World Forum (5GWF), 458-459, 2019;The adoption of artificial intelligence (AI) techniques entails a substantial change in the wireless ecosystem where data as well as their owners become crucial. As a result, the roll out of AI techniques in wireless systems raises a plethora of questions. In this context, we describe the challenges observed by the wireless stakeholders when deploying AI. Furthermore, we introduce the recent discussion in field of ethics that appear when managing wireless communications data.;https://ieeexplore.ieee.org/abstract/document/8911693/;U_ZTNNfkPtIJ
Ozlati, S., & Yampolskiy, R. (2017, March). The formalization of AI risk management and safety standards. In Workshops at the Thirty-First AAAI Conference on Artificial Intelligence.;2_safety_system_autonomous_vehicle;2017;The formalization of AI risk management and safety standards;Shabnam Ozlati, Roman Yampolskiy;Workshops at the Thirty-First AAAI Conference on Artificial Intelligence, 2017;Researchers have identified a number of possible risks posed to humanity by anticipated advancements in artificial intelligence (AI), but the extant literature on the topic is largely academic or theoretical in nature. Despite the likelihood that much of AI’s future development will occur in industry settings, the insights generated by the AI safety research community have yet to be translated into a set of practical guidelines for working developers, project managers, and other industrial stakeholders. There are no currently established standards in place to guide the safe development of AI technologies, but the risk management approach employed in mature industries such as aerospace and medical manufacturing offers a promising model that may be adapted to AI related safety concerns. Within these industries, the safety guidelines and best practices derived from the risk management approach are developed, evaluated, formalized, and disseminated by industry specific Standards Developing Organizations (SDOs). This paper proposes a project to spur the development and adoption of formal AI risk management practices by demonstrating the approach’s viability through the completion of an AI risk assessment process. The results of the proposed activities are intended to lay the initial groundwork necessary for the eventual creation of an AI SDO.;https://cdn.aaai.org/ocs/ws/ws0371/15175-68337-1-PB.pdf;FitbVMzJ0iUJ
Gundersen, O. E., & Kjensmo, S. (2018, April). State of the art: Reproducibility in artificial intelligence. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 32, No. 1).;3_explanation_model_machine_learning;2018;State of the art: Reproducibility in artificial intelligence;Odd Erik Gundersen, SigbjÃ¸rn Kjensmo;Proceedings of the AAAI Conference on Artificial Intelligence 32 (1), 2018;Background Research results in artificial intelligence (AI) are criticized for not being reproducible. Objective To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings None of the papers document all of the variables. The metrics show that between 20% and 30% of the variables for each factor are documented. One of the metrics show statistically significant increase over time while the others show no change. Interpretation The reproducibility scores decrease with in-creased documentation requirements. Improvement over time is found. Conclusion Both hypotheses are supported.;https://ojs.aaai.org/index.php/AAAI/article/view/11503;lgQREDtGOC4J
Sankaran, A., Panwar, N., Khare, S., Mani, S., Sethi, A., Aralikatte, R., & Gantayat, N. (2018, April). Democratization of deep learning using DARVIZ. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 32, No. 1).;4_dl_testing_deep_network;2018;Democratization of Deep Learning Using DARVIZ;Anush Sankaran, Naveen Panwar, Shreya Khare, Senthil Mani, Akshay Sethi, Rahul Aralikatte, Neelamadhav Gantayat;Proceedings of the AAAI Conference on Artificial Intelligence, 2018;"With an abundance of research papers in deep learning, adoption and reproducibility of existing works becomes a challenge. To make a DL developer life easy, we propose a novel system, DARVIZ, to visually design a DL model using a drag-and-drop framework in an platform agnostic manner. The code could be automatically generated in both Caffe and Keras. DARVIZ could import (i) any existing Caffe code, or (ii) a research paper containing a DL design; extract the design, and present it in visual editor.";https://ojs.aaai.org/index.php/AAAI/article/view/11376;TODO
Rudzicz, F., Paprica, P. A., & Janczarski, M. (2019). Towards international standards for evaluating machine learning. In SafeAI@ AAAI.;2_safety_system_autonomous_vehicle;2019;Towards international standards for evaluating machine learning.;Frank Rudzicz, P Alison Paprica, Marta Janczarski;SafeAI@ AAAI, 2019;Various international efforts to standardize artificial intelligence have begun, and many of these efforts involve issues related to privacy, trustworthiness, safety, and public wellbeing, which are topics that donâ€™t necessarily have international consensus, and may not for the foreseeable future. Meanwhile, the pursuit of achieving state-of-the-art accuracy in machine learning has resulted in a somewhat ad hoc application of empirical methodology that may limit the correctness of the computation of those accuracies, resulting in unpredictable applicability of those models. Trusting the objective quantitative performance of our systems is itself a safety concern and should inform the earliest standards towards safety in AI.;https://ceur-ws.org/Vol-2301/paper_10.pdf;ihHANAReHQkJ
Elkholy, A., Yang, F., & Gustafson, S. (2019). Interpretable automated machine learning in maana (tm) knowledge platform. arXiv preprint arXiv:1905.02168.;1_ml_machine_data_learning;2019;Interpretable automated machine learning in maana (tm) knowledge platform;Alexander Elkholy, Fangkai Yang, Steven Gustafson;arXiv preprint arXiv:1905.02168, 2019;Machine learning is becoming an essential part of developing solutions for many industrial applications, but the lack of interpretability hinders wide industry adoption to rapidly build, test, deploy and validate machine learning models, in the sense that the insight of developing machine learning solutions are not structurally encoded, justified and transferred. In this paper we describe Maana Meta-learning Service, an interpretable and interactive automated machine learning service residing in Maana Knowledge Platform that performs machine-guided, user assisted pipeline search and hyper-parameter tuning and generates structured knowledge about decisions for pipeline profiling and selection. The service is shipped with Maana Knowledge Platform and is validated using benchmark dataset. Furthermore, its capability of deriving knowledge from pipeline search facilitates various inference tasks and transferring to similar data science projects.;https://arxiv.org/abs/1905.02168;gE0EyWq7gyAJ
Poerner, N., Roth, B., & Schütze, H. (2018). Evaluating neural network explanation methods using hybrid documents and morphological agreement. arXiv preprint arXiv:1801.06422.;11_deep_network_model_layer;2018;Evaluating neural network explanation methods using hybrid documents and morphological agreement;Nina Poerner, Benjamin Roth, Hinrich SchÃ¼tze;arXiv preprint arXiv:1801.06422, 2018;The behavior of deep neural networks (DNNs) is hard to understand. This makes it necessary to explore post hoc explanation methods. We conduct the first comprehensive evaluation of explanation methods for NLP. To this end, we design two novel evaluation paradigms that cover two important classes of NLP problems: small context and large context problems. Both paradigms require no manual annotation and are therefore broadly applicable. We also introduce LIMSSE, an explanation method inspired by LIME that is designed for NLP. We show empirically that LIMSSE, LRP and DeepLIFT are the most effective explanation methods and recommend them for explaining DNNs in NLP.;https://arxiv.org/abs/1801.06422;cns6UyaW2NwJ
Bilal, M., & Oyedele, L. O. (2020). Guidelines for applied machine learning in construction industry—A case of profit margins estimation. Advanced engineering informatics, 43, 101013.;1_ml_machine_data_learning;2020;Guidelines for applied machine learning in construction industryâ€”A case of profit margins estimation;Muhammad Bilal, Lukumon O Oyedele;Advanced engineering informatics 43, 101013, 2020;"The progress in the field of Machine Learning (ML) has enabled the automation of tasks that were considered impossible to program until recently. These advancements today have incited firms to seek intelligent solutions as part of their enterprise software stack. Even governments across the globe are motivating firms through policies to tape into ML arena as it promises opportunities for growth, productivity and efficiency. In reflex, many firms embark on ML without knowing what it entails. The outcomes so far are not as expected because the ML, as hyped by tech firms, is not the silver bullet. However, whatever ML offers, firms urge to capitalise it for their competitive advantage. Applying ML to real-life construction industry problems goes beyond just prototyping predictive models. It entails intensive activities which, in addition to training robust ML models, provides a comprehensive framework for answering questions asked by construction folks when intelligent solutions are getting deployed at their premises to substitute or facilitate their decision-making tasks. Existing ML guidelines used in the IT industry are vastly restricted to training ML models. This paper presents guidelines for Applied Machine Learning (AML) in the construction industry from training to operationalising models, which are drawn from our experience of working with construction folks to deliver Construction Simulation Tool (CST). The unique aspect of these guidelines lies not only in providing a novel framework for training models but also answering critical questions related to model confidence, trust, interpretability, bias, feature importance and model extrapolation capabilities. Generally, ML models are presumed black boxes; hence argued that nobody knows what a model learns and how it generates predictions. Even very few ML folks barely know approaches to answer questions asked by the end users. Without explaining the competence of ML, the broader adoption of intelligent solutions in the construction industry cannot be attained. This paper proposed a detailed process for AML to develop intelligent solutions in the construction industry. Most discussions in the study are elaborated in the context of profit margin estimation for new projects.";https://www.sciencedirect.com/science/article/pii/S1474034619305865;yRSsoN9dEJ0J
Blacker, P., Bridges, C. P., & Hadfield, S. (2019, July). Rapid prototyping of deep learning models on radiation hardened cpus. In 2019 NASA/ESA Conference on Adaptive Hardware and Systems (AHS) (pp. 25-32). IEEE.;7_edge_computing_deep_learning;2019;Rapid prototyping of deep learning models on radiation hardened cpus;Peter Blacker, Christopher Paul Bridges, Simon Hadfield;2019 NASA/ESA Conference on Adaptive Hardware and Systems (AHS), 25-32, 2019;Interest is increasing in the use of neural networks and deep-learning for on-board processing tasks in the space industry [1]. However development has lagged behind terrestrial applications for several reasons: space qualified computers have significantly less processing power than their terrestrial equivalents, reliability requirements are more stringent than the majority of applications deep-learning is being used for. The long requirements, design and qualification cycles in much of the space industry slows adoption of recent developments. GPUs are the first hardware choice for implementing neural networks on terrestrial computers, however no radiation hardened equivalent parts are currently available. Field Programmable Gate Array devices are capable of efficiently implementing neural networks and radiation hardened parts are available, however the process to deploy and validate an inference network is non-trivial and robust tools that automate the process are not available. We present an open source tool chain that can automatically deploy a trained inference network from the TensorFlow framework directly to the LEON 3, and an industrial case study of the design process used to train and optimise a deep-learning model for this processor. This does not directly change the three challenges described above however it greatly accelerates prototyping and analysis of neural network solutions, allowing these options to be more easily considered than is currently possible. Future improvements to the tools are identified along with a summary of some of the obstacles to using neural networks and potential solutions to these in the future.;https://ieeexplore.ieee.org/abstract/document/8792934/;IkFvxiAH-QwJ
Goodman, B., & Flaxman, S. (2017). European Union regulations on algorithmic decision-making and a “right to explanation”. AI magazine, 38(3), 50-57.;6_fairness_discrimination_bias_decision;2017;European Union regulations on algorithmic decision-making and a “right to explanation”;Bryce Goodman, Seth Flaxman;AI magazine 38 (3), 50-57, 2017;We summarize the potential impact that the European Union’s new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which “significantly affect” users. The law will also effectively create a “right to explanation,” whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation.;https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2741;UhP8DuHVFn0J
Perrault, A., Fang, F., Sinha, A., & Tambe, M. (2020). Artificial intelligence for social impact: Learning and planning in the data-to-deployment pipeline. AI Magazine, 41(4), 3-16.;12_ai_ethical_ethic_intelligence;2020;Artificial intelligence for social impact: Learning and planning in the data-to-deployment pipeline;Andrew Perrault, Fei Fang, Arunesh Sinha, Milind Tambe;AI Magazine 41 (4), 3-16, 2020;"With the maturing of artificial intelligence (AI) and multiagent systems research, we have a tremendous opportunity to direct these advances toward addressing complex societal problems. In pursuit of this goal of AI for social impact, we as AI researchers must go beyond improvements in computational methodology; it is important to step out in the field to demonstrate social impact. To this end, we focus on the problems of public safety and security, wildlife conservation, and public health in low-resource communities, and present research advances in multiagent systems to address one key cross-cutting challenge: how to effectively deploy our limited intervention resources in these problem domains. We present case studies from our deployments around the world as well as lessons learned that we hope are of use to researchers who are interested in AI for social impact. In pushing this research agenda, we believe AI can indeed play an important role in fighting social injustice and improving society.";https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/5296;pirJ7ZtRxPoJ
Berscheid, J., & Roewer-Despres, F. (2019). Beyond transparency: a proposed framework for accountability in decision-making AI systems. AI Matters, 5(2), 13-22.;12_ai_ethical_ethic_intelligence;2019;Beyond transparency: a proposed framework for accountability in decision-making AI systems;Janelle Berscheid, Francois Roewer-Despres;AI Matters 5 (2), 13-22, 2019;Transparency in decision-making AI systems can only become actionable in practice when all stakeholders share responsibility for validating outcomes. We propose a three-party regulatory framework that incentivizes collaborative development in the AI ecosystem and guarantees fairness and accountability are not merely afterthoughts in high-impact domains.;https://dl.acm.org/doi/abs/10.1145/3340470.3340476;3d-_Bf0x6XAJ
Muthusamy, V., Slominski, A., & Ishakian, V. (2018, September). Towards enterprise-ready AI deployments minimizing the risk of consuming AI models in business applications. In 2018 First International Conference on Artificial Intelligence for Industries (AI4I) (pp. 108-109). IEEE.;1_ml_machine_data_learning;2018;Towards enterprise-ready AI deployments minimizing the risk of consuming AI models in business applications;Vinod Muthusamy, Aleksander Slominski, Vatche Ishakian;2018 First International Conference on Artificial Intelligence for Industries (AI4I), 108-109, 2018;The stochastic nature of artificial intelligence (AI) models introduces risk to business applications that use AI models without careful consideration. This paper offers an approach to use AI techniques to gain insights on the usage of the AI models and control how they are deployed to a production application.;https://ieeexplore.ieee.org/abstract/document/8665685/;PnxqOuVn0P8J
Chen, T. C., Wang, W. T., Kao, K., Yu, C. L., Lin, C., Chang, S. H., & Tsung, P. K. (2019, March). NeuroPilot: A cross-platform framework for edge-AI. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 167-170). IEEE.;7_edge_computing_deep_learning;2019;NeuroPilot: A cross-platform framework for edge-AI;Tung-Chien Chen, Wei-Ting Wang, Kloze Kao, Chia-Lin Yu, Code Lin, Shu-Hsin Chang, Pei-Kuei Tsung;2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS), 167-170, 2019;Artificial intelligence (AI) has been applied from cloud servers to edge devices because of its rapid response, privacy, robustness, and the efficient use of network bandwidth. However, it is challengeable to deploy the computation and memory-bandwidth intensive AI to edge devices for the power and hardware resource are limited. The various needs of applications, diverse devices and the fragmented supporting tools make the integration a tough work. In this paper, the NeuroPilot, a cross-platform framework for edge AI, is introduced. Technologies on software, hardware and integration levels are proposed to achieve the high performance and preserve the flexibility meanwhile. The NeuroPilot solution provides the superior edge AI ability for a wide range of applications.;https://ieeexplore.ieee.org/abstract/document/8771536/;PR80iYxfjewJ
Sajan, K. K., Ramachandran, G. S., & Krishnamachari, B. (2019, November). Enhancing support for machine learning and edge computing on an iot data marketplace. In Proceedings of the First International Workshop on Challenges in Artificial Intelligence and Machine Learning for Internet of Things (pp. 19-24).;7_edge_computing_deep_learning;2019;Enhancing support for machine learning and edge computing on an iot data marketplace;Kurian Karyakulam Sajan, Gowri Sankar Ramachandran, Bhaskar Krishnamachari;Proceedings of the First International Workshop on Challenges in Artificial Intelligence and Machine Learning for Internet of Things, 19-24, 2019;IoT applications are increasingly employing machine learning (ML) algorithms to manage and control the operational environment autonomously while predicting future actions. To leverage these emerging technologies, the application developers require an enormous amount of data to build models. Data marketplaces enable the IoT application developers to buy data from IoT device owners to train machine learning models. Contemporary data marketplaces only focus on connecting the IoT infrastructure owner (seller) with application developers (buyer) while lacking integrated support for data analytics. Application developers are required to manually create and manage machine learning pipelines by combining edge computing resources with data sources. In this paper, we present an architectural framework to build machine learning pipelines for data marketplaces automatically. Our framework enables application developers (buyers) to leverage the edge computing resources provided by the sellers and compose low-latency IoT applications that incorporate ML-based processing. We present a proof-of-concept implementation on the I3 data marketplace and outline open challenges in combining machine-learning, AI, and edge computing technologies with data marketplaces.;https://dl.acm.org/doi/abs/10.1145/3363347.3363364;LDyCSSQxWQoJ
Zhang, B. H., Lemoine, B., & Mitchell, M. (2018, December). Mitigating unwanted biases with adversarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society (pp. 335-340).;6_fairness_discrimination_bias_decision;2018;Mitigating unwanted biases with adversarial learning;Brian Hu Zhang, Blake Lemoine, Margaret Mitchell;Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, 335-340, 2018;Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.;https://dl.acm.org/doi/abs/10.1145/3278721.3278779;v4k0PGBZBJgJ
Tan, S., Caruana, R., Hooker, G., & Lou, Y. (2018, December). Distill-and-compare: Auditing black-box models using transparent model distillation. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society (pp. 303-310).;3_explanation_model_machine_learning;2018;Distill-and-compare: Auditing black-box models using transparent model distillation;Sarah Tan, Rich Caruana, Giles Hooker, Yin Lou;Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, 303-310, 2018;Black-box risk scoring models permeate our lives, yet are typically proprietary or opaque. We propose Distill-and-Compare, an approach to audit such models without probing the black-box model API or pre-defining features to audit. To gain insight into black-box models, we treat them as teachers, training transparent student models to mimic the risk scores assigned by the black-box models. We compare the mimic model trained with distillation to a second, un-distilled transparent model trained on ground truth outcomes, and use differences between the two models to gain insight into the black-box model. We demonstrate the approach on four data sets: COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club. We also propose a statistical test to determine if a data set is missing key features used to train the black-box model. Our test finds that the ProPublica data is likely missing key feature(s) used in COMPAS.;https://dl.acm.org/doi/abs/10.1145/3278721.3278725;HwmirnxSA1MJ
Shaw, N. P., Stöckel, A., Orr, R. W., Lidbetter, T. F., & Cohen, R. (2018, December). Towards provably moral AI agents in bottom-up learning frameworks. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society (pp. 271-277).;12_ai_ethical_ethic_intelligence;2018;Towards provably moral AI agents in bottom-up learning frameworks;Nolan P Shaw, Andreas StÃ¶ckel, Ryan W Orr, Thomas F Lidbetter, Robin Cohen;Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, 271-277, 2018;We examine moral machine decision making as inspired by a central question posed by Rossi with respect to moral preferences: can AI systems based on statistical machine learning (which do not provide a natural way to explain or justify their decisions) be used for embedding morality into a machine in a way that allows us to prove that nothing morally wrong will happen? We argue for an evaluation which is held to the same standards as a human agent, removing the demand that ethical behaviour is always achieved. We introduce four key meta-qualities desired for our moral standards, and then proceed to clarify how we can prove that an agent will correctly learn to perform moral actions given a set of samples within certain error bounds. Our group-dynamic approach enables us to demonstrate that the learned models converge to a common function to achieve stability. We further explain a valuable intrinsic consistency check made possible through the derivation of logical statements from the machine learning model. In all, this work proposes an approach for building ethical AI systems, coming from the perspective of artificial intelligence research, and sheds important light on understanding how much learning is required in order for an intelligent agent to behave morally with negligible error.;https://dl.acm.org/doi/abs/10.1145/3278721.3278728;ETfMkO3CNtgJ
Zhao, J., Mortier, R., Crowcroft, J., & Wang, L. (2018, December). Privacy-preserving machine learning based data analytics on edge devices. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society (pp. 341-346).;7_edge_computing_deep_learning;2018;Privacy-preserving machine learning based data analytics on edge devices;Jianxin Zhao, Richard Mortier, Jon Crowcroft, Liang Wang;Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, 341-346, 2018;Emerging Machine Learning (ML) techniques, such as Deep Neural Network, are widely used in today's applications and services. However, with social awareness of privacy and personal data rapidly rising, it becomes a pressing and challenging societal issue to both keep personal data private and benefit from the data analytics power of ML techniques at the same time. In this paper, we argue that to avoid those costs, reduce latency in data processing, and minimise the raw data revealed to service providers, many future AI and ML services could be deployed on users' devices at the Internet edge rather than putting everything on the cloud. Moving ML-based data analytics from cloud to edge devices brings a series of challenges. We make three contributions in this paper. First, besides the widely discussed resource limitation on edge devices, we further identify two other challenges that are not yet recognised in existing literature: lack of suitable models for users, and difficulties in deploying services for users. Second, we present preliminary work of the first systematic solution, i.e. Zoo, to fully support the construction, composing, and deployment of ML models on edge and local devices. Third, in the deployment example, ML service are proved to be easy to compose and deploy with Zoo. Evaluation shows its superior performance compared with state-of-art deep learning platforms and Google ML services.;https://dl.acm.org/doi/abs/10.1145/3278721.3278778;BF7WUzh4jLEJ
Maas, M. M. (2018, December). Regulating for'Normal AI Accidents' Operational Lessons for the Responsible Governance of Artificial Intelligence Deployment. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society (pp. 223-228).;2_safety_system_autonomous_vehicle;2018;Regulating for'Normal AI Accidents' Operational Lessons for the Responsible Governance of Artificial Intelligence Deployment;Matthijs M Maas;Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, 223-228, 2018;New technologies, particularly those which are deployed rapidly across sectors, or which have to operate in competitive conditions, can disrupt previously stable technology governance regimes. This leads to a precarious need to balance caution against performance while exploring the resulting 'safe operating space'. This paper will argue that Artificial Intelligence is one such critical technology, the responsible deployment of which is likely to prove especially complex, because even narrow AI applications often involve networked (tightly coupled, opaque) systems operating in complex or competitive environments. This ensures such systems are prone to 'normal accident'-type failures which can cascade rapidly, and are hard to contain or even detect in time. Legal and governance approaches to the deployment of AI will have to reckon with the specific causes and features of such 'normal accidents'. While this suggests that large-scale, cascading errors in AI systems are inevitable, an examination of the operational features that lead technologies to exhibit 'normal accidents' enables us to derive both tentative principles for precautionary policymaking, and practical recommendations for the safe(r) deployment of AI systems. This may help enhance the safety and security of these systems in the public sphere, both in the short- and in the long term.;https://dl.acm.org/doi/abs/10.1145/3278721.3278766;mrGjSCwkiiIJ
Vasconcelos, M., Cardonha, C., & Gonçalves, B. (2018, December). Modeling epistemological principles for bias mitigation in AI systems: an illustration in hiring decisions. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society (pp. 323-329).;6_fairness_discrimination_bias_decision;2018;Modeling epistemological principles for bias mitigation in AI systems: an illustration in hiring decisions;Marisa Vasconcelos, Carlos Cardonha, Bernardo GonÃ§alves;Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, 323-329, 2018;Artificial Intelligence (AI) has been used extensively in automatic decision making in a broad variety of scenarios, ranging from credit ratings for loans to recommendations of movies. Traditional design guidelines for AI models focus essentially on accuracy maximization, but recent work has shown that economically irrational and socially unacceptable scenarios of discrimination and unfairness are likely to arise unless these issues are explicitly addressed. This undesirable behavior has several possible sources, such as biased datasets used for training that may not be detected in black-box models. After pointing out connections between such bias of AI and the problem of induction, we focus on Popper's contributions after Hume's, which offer a logical theory of preferences. An AI model can be preferred over others on purely rational grounds after one or more attempts at refutation based on accuracy and fairness. Inspired by such epistemological principles, this paper proposes a structured approach to mitigate discrimination and unfairness caused by bias in AI systems. In the proposed computational framework, models are selected and enhanced after attempts at refutation. To illustrate our discussion, we focus on hiring decision scenarios where an AI system filters in which job applicants should go to the interview phase.;https://dl.acm.org/doi/abs/10.1145/3278721.3278751;zOsi6Ugry7wJ
Sharma, S., Henderson, J., & Ghosh, J. (2019). Certifai: Counterfactual explanations for robustness, transparency, interpretability, and fairness of artificial intelligence models. arXiv preprint arXiv:1905.07857.;6_fairness_discrimination_bias_decision;2019;Certifai: Counterfactual explanations for robustness, transparency, interpretability, and fairness of artificial intelligence models;Shubham Sharma, Jette Henderson, Joydeep Ghosh;arXiv preprint arXiv:1905.07857, 2019;As artificial intelligence plays an increasingly important role in our society, there are ethical and moral obligations for both businesses and researchers to ensure that their machine learning models are designed, deployed, and maintained responsibly. These models need to be rigorously audited for fairness, robustness, transparency, and interpretability. A variety of methods have been developed that focus on these issues in isolation, however, managing these methods in conjunction with model development can be cumbersome and timeconsuming. In this paper, we introduce a unified and model-agnostic approach to address these issues: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models (CERTIFAI). Unlike previous methods in this domain, CERTIFAI is a general tool that can be applied to any black-box model and any type of input data. Given a model and an input instance, CERTIFAI uses a custom genetic algorithm to generate counterfactuals: instances close to the input that change the prediction of the model. We demonstrate how these counterfactuals can be used to examine issues of robustness, interpretability, transparency, and fairness. Additionally, we introduce CERScore, the first black-box model robustness score that performs comparably to methods that have access to model internals.;https://arxiv.org/abs/1905.07857;UBAd0SV2rlgJ
Hind, M., Wei, D., Campbell, M., Codella, N. C., Dhurandhar, A., Mojsilović, A., ... & Varshney, K. R. (2019, January). TED: Teaching AI to explain its decisions. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 123-129).;3_explanation_model_machine_learning;2019;TED: Teaching AI to explain its decisions;Michael Hind, Dennis Wei, Murray Campbell, Noel CF Codella, Amit Dhurandhar, Aleksandra MojsiloviÄ‡, Karthikeyan Natesan Ramamurthy, Kush R Varshney;Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, 123-129, 2019;Artificial intelligence systems are being increasingly deployed due to their potential to increase the efficiency, scale, consistency, fairness, and accuracy of decisions. However, as many of these systems are opaque in their operation, there is a growing demand for such systems to provide explanations for their decisions. Conventional approaches to this problem attempt to expose or discover the inner workings of a machine learning model with the hope that the resulting explanations will be meaningful to the consumer. In contrast, this paper suggests a new approach to this problem. It introduces a simple, practical framework, called Teaching Explanations for Decisions (TED), that provides meaningful explanations that match the mental model of the consumer. We illustrate the generality and effectiveness of this approach with two different examples, resulting in highly accurate explanations with no loss of prediction accuracy for these two examples.;https://dl.acm.org/doi/abs/10.1145/3306618.3314273;hoNo_PJXeWAJ
Zhang, Y., Bellamy, R., & Varshney, K. (2020, February). Joint optimization of AI fairness and utility: a human-centered approach. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (pp. 400-406).;6_fairness_discrimination_bias_decision;2020;Joint optimization of AI fairness and utility: a human-centered approach;Yunfeng Zhang, Rachel Bellamy, Kush Varshney;Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 400-406, 2020;Today, AI is increasingly being used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. The AI research community has proposed many methods to measure and mitigate unwanted biases, but few of them involve inputs from human policy makers. We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers' preferences on how to make the tradeoff among these objectives. In this paper, we propose a framework and some exemplar methods for eliciting such preferences and for optimizing an AI model according to these preferences.;https://dl.acm.org/doi/abs/10.1145/3375627.3375862;AIQpDPZlkKcJ
Leben, D. (2020, February). Normative principles for evaluating fairness in machine learning. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (pp. 86-92).;6_fairness_discrimination_bias_decision;2020;Normative principles for evaluating fairness in machine learning;Derek Leben;Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 86-92, 2020;There are many incompatible ways to measure fair outcomes for machine learning algorithms. The goal of this paper is to characterize rates of success and error across protected groups (race, gender, sexual orientation) as a distribution problem, and describe the possible solutions to this problem according to different normative principles from moral and political philosophy. These normative principles are based on various competing attributes within a distribution problem: intentions, compensation, desert, consent, and consequences. Each principle will be applied to a sample risk-assessment classifier to demonstrate the philosophical arguments underlying different sets of fairness metrics.;https://dl.acm.org/doi/abs/10.1145/3375627.3375808;42JeTG0Z8W8J
Ahmad, M. A., Eckert, C., & Teredesai, A. (2019). The challenge of imputation in explainable artificial intelligence models. arXiv preprint arXiv:1907.12669.;3_explanation_model_machine_learning;2019;The challenge of imputation in explainable artificial intelligence models;Muhammad Aurangzeb Ahmad, Carly Eckert, Ankur Teredesai;arXiv preprint arXiv:1907.12669, 2019;Explainable models in Artificial Intelligence are often employed to ensure transparency and accountability of AI systems. The fidelity of the explanations are dependent upon the algorithms used as well as on the fidelity of the data. Many real world datasets have missing values that can greatly influence explanation fidelity. The standard way to deal with such scenarios is imputation. This can, however, lead to situations where the imputed values may correspond to a setting which refer to counterfactuals. Acting on explanations from AI models with imputed values may lead to unsafe outcomes. In this paper, we explore different settings where AI models with imputation can be problematic and describe ways to address such scenarios.;https://arxiv.org/abs/1907.12669;2gn7UHAiKWkJ
Muñoz-González, L., Biggio, B., Demontis, A., Paudice, A., Wongrassamee, V., Lupu, E. C., & Roli, F. (2017, November). Towards poisoning of deep learning algorithms with back-gradient optimization. In Proceedings of the 10th ACM workshop on artificial intelligence and security (pp. 27-38).;5_adversarial_attack_example_model;2017;Towards poisoning of deep learning algorithms with back-gradient optimization;Luis Muñoz-González, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, Emil C Lupu, Fabio Roli;Proceedings of the 10th ACM workshop on artificial intelligence and security, 27-38, 2017;A number of online services nowadays rely upon machine learning to extract valuable information from data collected in the wild. This exposes learning algorithms to the threat of data poisoning, i.e., a coordinate attack in which a fraction of the training data is controlled by the attacker and manipulated to subvert the learning process. To date, these attacks have been devised only against a limited class of binary learning algorithms, due to the inherent complexity of the gradient-based procedure used to optimize the poisoning points (a.k.a. adversarial training examples). In this work, we first extend the definition of poisoning attacks to multiclass problems. We then propose a novel poisoning algorithm based on the idea of back-gradient optimization, i.e., to compute the gradient of interest through automatic differentiation, while also reversing the learning procedure to drastically reduce the attack complexity. Compared to current poisoning strategies, our approach is able to target a wider class of learning algorithms, trained with gradient-based procedures, including neural networks and deep learning architectures. We empirically evaluate its effectiveness on several application examples, including spam filtering, malware detection, and handwritten digit recognition. We finally show that, similarly to adversarial test examples, adversarial training examples can also be transferred across different learning algorithms.;https://dl.acm.org/doi/abs/10.1145/3128572.3140451;TA491WeeJLIJ
Carlini, N., & Wagner, D. (2017, November). Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM workshop on artificial intelligence and security (pp. 3-14).;5_adversarial_attack_example_model;2017;Adversarial examples are not easily detected: Bypassing ten detection methods;Nicholas Carlini, David Wagner;Proceedings of the 10th ACM workshop on artificial intelligence and security, 3-14, 2017;Neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. In order to better understand the space of adversarial examples, we survey ten recent proposals that are designed for detection and compare their efficacy. We show that all can be defeated by constructing new loss functions. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and the properties believed to be intrinsic to adversarial examples are in fact not. Finally, we propose several simple guidelines for evaluating future proposed defenses.;https://dl.acm.org/doi/abs/10.1145/3128572.3140444;YZ8RfzTGulwJ
Rajkomar, A., Hardt, M., Howell, M. D., Corrado, G., & Chin, M. H. (2018). Ensuring fairness in machine learning to advance health equity. Annals of internal medicine, 169(12), 866-872.;3_explanation_model_machine_learning;2018;Ensuring fairness in machine learning to advance health equity;Alvin Rajkomar, Michaela Hardt, Michael D Howell, Greg Corrado, Marshall H Chin;Annals of internal medicine 169 (12), 866-872, 2018;Machine learning is used increasingly in clinical care to improve diagnosis, treatment selection, and health system efficiency. Because machine-learning models learn from historically collected data, populations that have experienced human and structural biases in the past—called protected groups—are vulnerable to harm by incorrect predictions or withholding of resources. This article describes how model design, biases in data, and the interactions of model predictions with clinicians and patients may exacerbate health care disparities. Rather than simply guarding against these harms passively, machine-learning systems should be used proactively to advance health equity. For that goal to be achieved, principles of distributive justice must be incorporated into model design, deployment, and evaluation. The article describes several technical implementations of distributive justice—specifically those that ensure equality in patient outcomes, performance, and resource allocation—and guides clinicians as to when they should prioritize each principle. Machine learning is providing increasingly sophisticated decision support and population-level monitoring, and it should encode principles of justice to ensure that models benefit all patients.;https://www.acpjournals.org/doi/abs/10.7326/M18-1990;8jXRT6hsP3IJ
Hu, Q., Ma, L., & Zhao, J. (2018, December). DeepGraph: A PyCharm tool for visualizing and understanding deep learning models. In 2018 25th Asia-Pacific Software Engineering Conference (APSEC) (pp. 628-632). IEEE.;4_dl_testing_deep_network;2018;DeepGraph: A PyCharm tool for visualizing and understanding deep learning models;Qiang Hu, Lei Ma, Jianjun Zhao;2018 25th Asia-Pacific Software Engineering Conference (APSEC), 628-632, 2018;As more and more domain specific big data become available, there comes a strong need on the fast development and deployment of deep learning (DL) systems with high quality for domain specific applications, including many safety-critical scenarios. In traditional software engineering, software visualization plays an important role to enhance developers' performance with many tools available. However, there are limited visualization supports existing for DL systems, especially in integrated development environments (IDEs) that allow a developer to visualize the source code of a deep neural network (DNN) and its graph architecture. In this paper, we propose DeepGraph, a visualization tool for visualizing and understanding a deep neural network. DeepGraph analyzes the training program to construct the graph representation of a DNN, and establishes and maintains the linkage (mapping) between the source code of the training program and its corresponding neural network architecture. We implemented DeepGraph as a PyCharm plugin and performed preliminary empirical study to demonstrate its usefulness for understanding deep neural networks.;https://ieeexplore.ieee.org/abstract/document/8719435/;8uSHbc5NRoMJ
Tomsett, R., Chan, K., & Chakraborty, S. (2019, May). Model poisoning attacks against distributed machine learning systems. In Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications (Vol. 11006, pp. 481-489). SPIE.;5_adversarial_attack_example_model;2019;Model poisoning attacks against distributed machine learning systems;Richard Tomsett, Kevin Chan, Supriyo Chakraborty;Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications 11006, 481-489, 2019;Future military coalition operations will increasingly rely on machine learning (ML) methods to improve situational awareness. The coalition context presents unique challenges for ML: the tactical environment creates significant computing and communications limitations while also having to deal with an adversarial presence. Further, coalition operations must operate in a distributed manner, while coping with the constraints posed by the operational environment. Envisioned ML deployments in military assets must be resilient to these challenges. Here, we focus on the susceptibility of ML models to be poisoned (during training) or fooled (after training) by adversarial inputs. We review recent work on distributed adversarial ML, and present new results from our own investigations into model poisoning attacks on distributed learning systems without a central parameter aggregation node.;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11006/110061D/Model-poisoning-attacks-against-distributed-machine-learning-systems/10.1117/12.2520275.short;xyVN-CoImhAJ
Huang, Y. L., Sun, W. L., & Yeh, K. W. (2019, June). MLoC: A Cloud Framework adopting Machine Learning for Industrial Automation. In 2019 12th Asian Control Conference (ASCC) (pp. 1413-1418). IEEE.;1_ml_machine_data_learning;2019;MLoC: A Cloud Framework adopting Machine Learning for Industrial Automation;Yu-Lun Huang, Wen-Lin Sun, Kai-Wei Yeh;2019 12th Asian Control Conference (ASCC), 1413-1418, 2019;By leveraging the modern machine learning algorithms, we can build up more Artificial Intelligence (AI) systems, like self-driving cars, smart factories and financial analysis systems, to improve our daily life. In addition to building up an AI system, several prerequisites are required to drive the system, including data collection, data storage, machine learning models, training dataset, parameters tuning, and so on. To obtain the benefit of scalability and flexibility, most AI systems are built on a cloud platform, which shares resources with others in the same infrastructure. Though the above concept is trivial, the implementation faces big challenges when realizing it. In this paper, an easy-to-use cloud framework for machine learning as well as its implementation guideline is presented for building up a cloud-based development platform. We conduct several experiments on analyzing and monitoring the health condition of bearings of motors. We compare and analyze the feasibility of the proposed framework.;https://ieeexplore.ieee.org/abstract/document/8764986/;JgcWL1fvUHIJ
Hu, Q., Ma, L., Xie, X., Yu, B., Liu, Y., & Zhao, J. (2019, November). Deepmutation++: A mutation testing framework for deep learning systems. In 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE) (pp. 1158-1161). IEEE.;4_dl_testing_deep_network;2019;Deepmutation++: A mutation testing framework for deep learning systems;Qiang Hu, Lei Ma, Xiaofei Xie, Bing Yu, Yang Liu, Jianjun Zhao;2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE), 1158-1161, 2019;Deep neural networks (DNNs) are increasingly expanding their real-world applications across domains, e.g., image processing, speech recognition and natural language processing. However, there is still limited tool support for DNN testing in terms of test data quality and model robustness. In this paper, we introduce a mutation testing-based tool for DNNs, DeepMutation++, which facilitates the DNN quality evaluation, supporting both feed-forward neural networks (FNNs) and stateful recurrent neural networks (RNNs). It not only enables to statically analyze the robustness of a DNN model against the input as a whole, but also allows to identify the vulnerable segments of a sequential input (e.g. audio input) by runtime analysis. It is worth noting that DeepMutation++ specially features the support of RNNs mutation testing. The tool demo video can be found on the project website https://sites.google.com/view/deepmutationpp.;https://ieeexplore.ieee.org/abstract/document/8952248/;2Rro-blZwHYJ
Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B., & Swami, A. (2017, April). Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia conference on computer and communications security (pp. 506-519).;5_adversarial_attack_example_model;2017;Practical black-box attacks against machine learning;Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, Ananthram Swami;Proceedings of the 2017 ACM on Asia conference on computer and communications security, 506-519, 2017;Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.;https://dl.acm.org/doi/abs/10.1145/3052973.3053009;Zva6CISMDcwJ
Zhang, J., Gu, Z., Jang, J., Wu, H., Stoecklin, M. P., Huang, H., & Molloy, I. (2018, May). Protecting intellectual property of deep neural networks with watermarking. In Proceedings of the 2018 on Asia conference on computer and communications security (pp. 159-172).;4_dl_testing_deep_network;2018;Protecting intellectual property of deep neural networks with watermarking;Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph Stoecklin, Heqing Huang, Ian Molloy;Proceedings of the 2018 on Asia conference on computer and communications security, 159-172, 2018;"Deep learning technologies, which are the key components of state-of-the-art Artificial Intelligence (AI) services, have shown great success in providing human-level capabilities for a variety of tasks, such as visual analysis, speech recognition, and natural language processing and etc. Building a production-level deep learning model is a non-trivial task, which requires a large amount of training data, powerful computing resources, and human expertises. Therefore, illegitimate reproducing, distribution, and the derivation of proprietary deep learning models can lead to copyright infringement and economic harm to model creators. Therefore, it is essential to devise a technique to protect the intellectual property of deep learning models and enable external verification of the model ownership.In this paper, we generalize the ""digital watermarking'' concept from multimedia ownership verification to deep neural network (DNNs) models. We investigate three DNN-applicable watermark generation algorithms, propose a watermark implanting approach to infuse watermark into deep learning models, and design a remote verification mechanism to determine the model ownership. By extending the intrinsic generalization and memorization capabilities of deep neural networks, we enable the models to learn specially crafted watermarks at training and activate with pre-specified predictions when observing the watermark patterns at inference. We evaluate our approach with two image recognition benchmark datasets. Our framework accurately (100%) and quickly verifies the ownership of all the remotely deployed deep learning models without affecting the model accuracy for normal input data. In addition, the embedded watermarks in DNN models are robust and resilient to different counter-watermark mechanisms, such as fine-tuning, parameter pruning, and model inversion attacks.";https://dl.acm.org/doi/abs/10.1145/3196494.3196550;gqVsw_Gw55EJ
Tan, S., Sim, K. C., & Gales, M. (2015, December). Improving the interpretability of deep neural networks with stimulated learning. In 2015 ieee workshop on automatic speech recognition and understanding (asru) (pp. 617-623). IEEE.;11_deep_network_model_layer;2015;Improving the interpretability of deep neural networks with stimulated learning;Shawn Tan, Khe Chai Sim, Mark Gales;2015 ieee workshop on automatic speech recognition and understanding (asru), 617-623, 2015;Deep Neural Networks (DNNs) have demonstrated improvements in acoustic modelling for automatic speech recognition. However, they are often used as a black box, and not much is understood about what each of the hidden layers does. We seek to understand how the activations in the hidden layers change with different input, and how we can leverage such knowledge to modify the behaviour of the model. To this end, we propose stimulated deep learning where stimuli are introduced during the DNN training process to influence the behaviour of the hidden units. Specifically, constraints are applied so that the hidden units of each layer will exhibit phone-dependent regional activities when arranged in a 2-dimensional grid. We demonstrate that such constraints are able to yield visible activation regions without compromising the classification of the network and suppressing the activations for a region affects the classification accuracy of the corresponding phone more than the others.;https://ieeexplore.ieee.org/abstract/document/7404853/;OiIXnEQeMEUJ
Varshney, K. R., & Alemzadeh, H. (2017). On the safety of machine learning: Cyber-physical systems, decision sciences, and data products. Big data, 5(3), 246-255.;2_safety_system_autonomous_vehicle;2017;On the safety of machine learning: Cyber-physical systems, decision sciences, and data products;Kush R Varshney, Homa Alemzadeh;Big data 5 (3), 246-255, 2017;Machine learning algorithms increasingly influence our decisions and interact with us in all parts of our daily lives. Therefore, just as we consider the safety of power plants, highways, and a variety of other engineered socio-technical systems, we must also take into account the safety of systems involving machine learning. Heretofore, the definition of safety has not been formalized in a machine learning context. In this article, we do so by defining machine learning safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. We then use this definition to examine safety in all sorts of applications in cyber-physical systems, decision sciences, and data products. We find that the foundational principle of modern statistical machine learning, empirical risk minimization, is not always a sufficient objective. We discuss how four different categories of strategies for achieving safety in engineering, including inherently safe design, safety reserves, safe fail, and procedural safeguards can be mapped to a machine learning context. We then discuss example techniques that can be adopted in each category, such as considering interpretability and causality of predictive models, objective functions beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software and open data.;https://www.liebertpub.com/doi/abs/10.1089/big.2016.0051;ppRbU4485qYJ
d'Alessandro, B., O'Neil, C., & LaGatta, T. (2017). Conscientious classification: A data scientist's guide to discrimination-aware classification. Big data, 5(2), 120-134.;6_fairness_discrimination_bias_decision;2017;Conscientious classification: A data scientist's guide to discrimination-aware classification;Brian d'Alessandro, Cathy O'Neil, Tom LaGatta;Big data 5 (2), 120-134, 2017;Recent research has helped to cultivate growing awareness that machine-learning systems fueled by big data can create or exacerbate troubling disparities in society. Much of this research comes from outside of the practicing data science community, leaving its members with little concrete guidance to proactively address these concerns. This article introduces issues of discrimination to the data science community on its own terms. In it, we tour the familiar data-mining process while providing a taxonomy of common practices that have the potential to produce unintended discrimination. We also survey how discrimination is commonly measured, and suggest how familiar development processes can be augmented to mitigate systems' discriminatory potential. We advocate that data scientists should be intentional about modeling and reducing discriminatory outcomes. Without doing so, their efforts will result in perpetuating any systemic discrimination that may exist, but under a misleading veil of data-driven objectivity.;https://www.liebertpub.com/doi/abs/10.1089/big.2016.0048;xTzOAPLf66YJ
Farroha, J. (2019, May). Security analysis and recommendations for AI/ML enabled automated cyber medical systems. In Big Data: Learning, Analytics, and Applications (Vol. 10989, pp. 201-216). SPIE.;1_ml_machine_data_learning;2019;Security analysis and recommendations for AI/ML enabled automated cyber medical systems;J Farroha;Big Data: Learning, Analytics, and Applications 10989, 201-216, 2019;Artificial Intelligence (AI) and remote surgery go together in defining the future of intelligent medicine. The advancement of robotic surgery became a possibility by leveraging intelligent sensors, Machine Learning (ML), and reliable wireless connectivity in addition to low-latency response to surgical commands and automated responses to patient's status resulting in an accurate automated system. The trend is to develop cyber-medical systems through the integration of medical knowledge and engineering applications in order to create customizable intelligent healthcare procedures. Security plays a critical role while the systems navigate the dynamics of human-controlled connectivity versus autonomy, and remote commands versus automated responses to sensors. The systems need to “learn” from experience and transferred knowledge from other systems while protecting against learning from false data. This paper addresses the Cyber Security risks introduced by adapting the emerging technologies as well as providing potential solutions that are based on best practices. The ML model used in cyber medicine should be as simple as possible providing the required accuracy to produce the desired outcome, understanding that more complex models have higher chances of suffering the degradation effects of overfitting.;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10989/109890R/Security-analysis-and-recommendations-for-AI-ML-enabled-automated-cyber/10.1117/12.2518791.short;VqvwrTJhUsgJ
Veale, M., & Binns, R. (2017). Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data. Big Data & Society, 4(2), 2053951717743530.;6_fairness_discrimination_bias_decision;2017;Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data;Michael Veale, Reuben Binns;Big Data & Society 4 (2), 2053951717743530, 2017;Decisions based on algorithmic, machine learning models can be unfair, reproducing biases in historical data used to train them. While computational techniques are emerging to address aspects of these concerns through communities such as discrimination-aware data mining (DADM) and fairness, accountability and transparency machine learning (FATML), their practical implementation faces real-world challenges. For legal, institutional or commercial reasons, organisations might not hold the data on sensitive attributes such as gender, ethnicity, sexuality or disability needed to diagnose and mitigate emergent indirect discrimination-by-proxy, such as redlining. Such organisations might also lack the knowledge and capacity to identify and manage fairness issues that are emergent properties of complex sociotechnical systems. This paper presents and discusses three potential approaches to deal with such knowledge and information deficits in the context of fairer machine learning. Trusted third parties could selectively store data necessary for performing discrimination discovery and incorporating fairness constraints into model-building in a privacy-preserving manner. Collaborative online platforms would allow diverse organisations to record, share and access contextual and experiential knowledge to promote fairness in machine learning systems. Finally, unsupervised learning and pedagogically interpretable algorithms might allow fairness hypotheses to be built for further selective testing and exploration. Real-world fairness challenges in machine learning are not abstract, constrained optimisation problems, but are institutionally and contextually grounded. Computational fairness tools are useful, but must be researched and developed in and with the messy contexts that will shape their deployment, rather than just for imagined situations. Not doing so risks real, near-term algorithmic harm.;https://journals.sagepub.com/doi/abs/10.1177/2053951717743530;307dj2y_VgUJ
Zhang, Y., Xu, F., Frise, E., Wu, S., Yu, B., & Xu, W. (2016, May). DataLab: a version data management and analytics system. In Proceedings of the 2nd International Workshop on BIG Data Software Engineering (pp. 12-18).;9_data_science_software_process;2016;DataLab: a version data management and analytics system;Yang Zhang, Fangzhou Xu, Erwin Frise, Siqi Wu, Bin Yu, Wei Xu;Proceedings of the 2nd International Workshop on BIG Data Software Engineering, 12-18, 2016;"One challenge in big data analytics is the lack of tools to manage the complex interactions among code, data and parameters, especially in the common situation where all these factors can change a lot. We present our preliminary experience with DataLab, a system we build to manage the big data workflow. DataLab improves big data analytical workflow in several novel ways. 1) DataLab manages the revision of both code and data in a coherent system, and includes a distributed code execution engine to run users' code; 2) DataLab keeps track of all the data analytics results in a data work flow graph, and is able to compare the code / results between any two versions, making it easier for users to intuitively see the results of their code change; 3) DataLab provides an efficient data management system to separate data from their metadata, allowing efficient preprocessing filters; and 4) DataLab provides a common API so people can build different applications on top of it. We also present our experience of applying a DataLab prototype in a real bioinformatics application.";https://dl.acm.org/doi/abs/10.1145/2896825.2896830;gdx9OVnQ-qwJ
Feldman, K., Faust, L., Wu, X., Huang, C., & Chawla, N. V. (2017). Beyond volume: The impact of complex healthcare data on the machine learning pipeline. In Towards Integrative Machine Learning and Knowledge Extraction: BIRS Workshop, Banff, AB, Canada, July 24-26, 2015, Revised Selected Papers (pp. 150-169). Springer International Publishing.;1_ml_machine_data_learning;2017;Beyond volume: The impact of complex healthcare data on the machine learning pipeline;Keith Feldman, Louis Faust, Xian Wu, Chao Huang, Nitesh V Chawla;Towards Integrative Machine Learning and Knowledge Extraction: BIRS Workshop, Banff, AB, Canada, July 24-26, 2015, Revised Selected Papers, 150-169, 2017;From medical charts to national census, healthcare has traditionally operated under a paper-based paradigm. However, the past decade has marked a long and arduous transformation bringing healthcare into the digital age. Ranging from electronic health records, to digitized imaging and laboratory reports, to public health datasets, today, healthcare now generates an incredible amount of digital information. Such a wealth of data presents an exciting opportunity for integrated machine learning solutions to address problems across multiple facets of healthcare practice and administration. Unfortunately, the ability to derive accurate and informative insights requires more than the ability to execute machine learning models. Rather, a deeper understanding of the data on which the models are run is imperative for their success. While a significant effort has been undertaken to develop models able to process the volume of data obtained during the analysis of millions of digitalized patient records, it is important to remember that volume represents only one aspect of the data. In fact, drawing on data from an increasingly diverse set of sources, healthcare data presents an incredibly complex set of attributes that must be accounted for throughout the machine learning pipeline. This chapter focuses on highlighting such challenges, and is broken down into three distinct components, each representing a phase of the pipeline. We begin with attributes of the data accounted for during preprocessing, then move to considerations during model building, and end with challenges to the interpretation of model output. For each component, we present a discussion around data as it relates to the healthcare domain and offer insight into the challenges each may impose on the efficiency of machine learning techniques.;https://link.springer.com/chapter/10.1007/978-3-319-69775-8_9;y9kgqdjl2i4J
Ladia, A. (2019, June). Privacy centric collaborative machine learning model training via blockchain. In International Congress on Blockchain and Applications (pp. 62-70). Cham: Springer International Publishing.;0_federated_learning_data_privacy;2019;Privacy centric collaborative machine learning model training via blockchain;Aman Ladia;International Congress on Blockchain and Applications, 62-70, 2019;This paper tackles the issue of data siloing, where organisations are unable to share data with each other because of privacy concerns. Machine Learning models, which could benefit greatly from larger data sets shared between organisations, suffer in this era of data isolation. To solve this problem, a blockchain based implementation is proposed that allows training of machine learning models in a privacy compliant way. Instead of using blockchain in a typical database-style manner, the proposed solution uses blockchain as a means to handle joint ownership and joint control over a computer system known as the Training Machine. The Training Machine, set-up jointly by consortium members, serves as a secure, independent container that accepts data sets and an untrained model as inputs from different entities, trains the model internally, and outputs the trained model without revealing any data to other entities. Data is then deleted automatically. Blockchain ensures that this machine is not under the control of any one entity but is rather controlled transparently by all data-sharing parties. By placing sensitive information in an isolated system, and establishing blockchain based access control, the solution ensures that data is not accessible to any party other than the owner. The paper also shares use cases of this technology, along with a risk analysis and proof of concept.;https://link.springer.com/chapter/10.1007/978-3-030-23813-1_8;h5jd0kF4mwsJ
Huang, X., Kwiatkowska, M., Wang, S., & Wu, M. (2017). Safety verification of deep neural networks. In Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30 (pp. 3-29). Springer International Publishing.;4_dl_testing_deep_network;2017;Safety Verification of Deep Neural Networks;Xiaowei Huang, Marta Kwiatkowska, Sen Wang, Min Wu ;Lecture Notes in Computer Science book series (LNTCS,volume 10426);Deep neural networks have achieved impressive experimental results in image classification, but can surprisingly be unstable with respect to adversarial perturbations, that is, minimal changes to the input image that cause the network to misclassify it. With potential applications including perception modules and end-to-end controllers for self-driving cars, this raises concerns about their safety. We develop a novel automated verification framework for feed-forward multi-layer neural networks based on Satisfiability Modulo Theory (SMT). We focus on safety of image classification decisions with respect to image manipulations, such as scratches or changes to camera angle or lighting conditions that would result in the same class being assigned by a human, and define safety for an individual decision in terms of invariance of the classification within a small neighbourhood of the original image. We enable exhaustive search of the region by employing discretisation, and propagate the analysis layer by layer. Our method works directly with the network code and, in contrast to existing methods, can guarantee that adversarial examples, if they exist, are found for the given region and family of manipulations. If found, adversarial examples can be shown to human testers and/or used to fine-tune the network. We implement the techniques using Z3 and evaluate them on state-of-the-art networks, including regularised and deep learning networks. We also compare against existing techniques to search for adversarial examples and estimate network robustness.;https://link.springer.com/chapter/10.1007/978-3-319-63387-9_1;Todo
Liu, Y., Ma, S., Aafer, Y., Lee, W. C., Zhai, J., Wang, W., & Zhang, X. (2018, January). Trojaning attack on neural networks. In 25th Annual Network And Distributed System Security Symposium (NDSS 2018). Internet Soc.;9_data_science_software_process;2018;Trojaning attack on neural networks;Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, Xiangyu Zhang;25th Annual Network And Distributed System Security Symposium (NDSS 2018), 2018;Computer Science, Information Systems Computer Science, Theory & Methods Science & Technology Computer Science Technology;https://scholarship.libraries.rutgers.edu/esploro/outputs/991031794682704646?institution=01RUT_INST&skipUsageReporting=true&recordUsage=false;oQ2G-ddENO4J
Borgli, R. J., Stensland, H. K., Halvorsen, P., & Riegler, M. A. (2019, September). Saga: An Open Source Platform for Training Machine Learning Models and Community-driven Sharing of Techniques. In 2019 International Conference on Content-Based Multimedia Indexing (CBMI) (pp. 1-4). IEEE.;1_ml_machine_data_learning;2019;Saga: An Open Source Platform for Training Machine Learning Models and Community-driven Sharing of Techniques;Rune Johan Borgli, Håkon Kvale Stensland, Pål Halvorsen, Michael Alexander Riegler;2019 International Conference on Content-Based Multimedia Indexing (CBMI), 1-4, 2019;With the increasing popularity of machine learning in areas such as multimedia indexing and social media analysis, comes an increasing number of tools for developing and training the machine learning models. These tools assist in the selection and optimization of hyperparameters, but the user becomes locked into the platform's built-in solutions. Therefore, this demo presents an open-source, community-driven platform for sharing machine learning models, training techniques, and datasets. The platform, called Saga, provides a machine learning training pipeline such as those found in machine learning services provided by cloud providers. However, Saga also provides a store where users can upload and share their machine learning training methods. Additionally, the store allows users to rate and comment on other users' uploaded methods, as well as download and run them without any additional setup. In our demo, we will run a scenario where a user wants to train an image classifier using Saga. The demonstration will involve downloading methods from the store and displaying the pipeline provided by the Saga platform.;https://ieeexplore.ieee.org/abstract/document/8877455/;Vx2I9Y5nU4YJ
ElShawi, R., Sherif, Y., Al‐Mallah, M., & Sakr, S. (2021). Interpretability in healthcare: A comparative study of local machine learning interpretability techniques. Computational Intelligence, 37(4), 1633-1650.;3_explanation_model_machine_learning;2021;Interpretability in healthcare: A comparative study of local machine learning interpretability techniques;Radwa ElShawi, Youssef Sherif, Mouaz Al‐Mallah, Sherif Sakr;Computational Intelligence 37 (4), 1633-1650, 2021;Although complex machine learning models (eg, random forest, neural networks) are commonly outperforming the traditional and simple interpretable models (eg, linear regression, decision tree), in the healthcare domain, clinicians find it hard to understand and trust these complex models due to the lack of intuition and explanation of their predictions. With the new general data protection regulation (GDPR), the importance for plausibility and verifiability of the predictions made by machine learning models has become essential. Hence, interpretability techniques for machine learning models are an area focus of research. In general, the main aim of these interpretability techniques is to shed light and provide insights into the prediction process of the machine learning models and to be able to explain how the results from the prediction was generated. A major problem in this context is that both the quality of the interpretability techniques and trust of the machine learning model predictions are challenging to measure. In this article, we propose four fundamental quantitative measures for assessing the quality of interpretability techniques—similarity, bias detection, execution time, and trust. We present a comprehensive experimental evaluation of six recent and popular local model agnostic interpretability techniques, namely, LIME, SHAP, Anchors, LORE, ILIME“ and MAPLE on different types of real‐world healthcare data. Building on previous work, our experimental evaluation covers different aspects for its comparison including identity, stability, separability, similarity, execution time, bias detection, and trust. The results of our experiments show that MAPLE achieves the highest performance for the identity across all data sets included in this study, while LIME achieves the lowest performance for the identity metric. LIME achieves the highest performance for the separability metric across all data sets. On average, SHAP has the smallest average time to output explanation across all data sets included in this study. For detecting the bias, SHAP and MAPLE enable the participants to better detect the bias. For the trust metric, Anchors achieves the highest performance on all data sets included in this work.;https://onlinelibrary.wiley.com/doi/abs/10.1111/coin.12410;GvKGhCJh0lYJ
Rao, S. S., Pradyumna, S., Kalambur, S., & Sitaram, D. (2018, November). Bodhisattva-Rapid Deployment of AI on Containers. In 2018 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM) (pp. 100-104). IEEE.;1_ml_machine_data_learning;2018;Bodhisattva-Rapid Deployment of AI on Containers;Shreyas S Rao, S Pradyumna, Subramaniam Kalambur, Dinkar Sitaram;2018 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM), 100-104, 2018;Cloud-based machine learning is becoming increasingly important in all verticals of the industry as all organizations want to leverage ML and AI to solve real-world problems of emerging markets. But, incorporating these services into business solutions is a goliath task, mainly due to the sheer effort necessary to go from development to deployment. We present a novel idea that enables users to easily specify, create, train and rapidly deploy machine learning models through a scalable Machine-Learning-as-a-Service (MLaaS) offering. The MLaaS is provided as an end-to-end microservice suite in a container-based PaaS environment for web applications on the cloud. Our implementation provides an intuitive web-based GUI for tenants to consume these services in a few quick steps. The utility of our service is demonstrated by training ML models for various use cases and comparing them on factors like time-to-deploy, resource usage and training metrics.;https://ieeexplore.ieee.org/abstract/document/8648632/;J5gbGp0nlZgJ
Reith, R. N., Schneider, T., & Tkachenko, O. (2019, November). Efficiently stealing your machine learning models. In Proceedings of the 18th ACM Workshop on Privacy in the Electronic Society (pp. 198-210).;5_adversarial_attack_example_model;2019;Efficiently stealing your machine learning models;Robert Nikolai Reith, Thomas Schneider, Oleksandr Tkachenko;Proceedings of the 18th ACM Workshop on Privacy in the Electronic Society, 198-210, 2019;Machine Learning as a Service (MLaaS) is a growing paradigm in the Machine Learning (ML) landscape. More and more ML models are being uploaded to the cloud and made accessible from all over the world. Creating good ML models, however, can be expensive and the used data is often sensitive. Recently, Secure Multi-Party Computation (SMPC) protocols for MLaaS have been proposed, which protect sensitive user data and ML models at the expense of substantially higher computation and communication than plaintext evaluation. In this paper, we show that for a subset of ML models used in MLaaS, namely Support Vector Machines (SVMs) and Support Vector Regression Machines (SVRs) which have found many applications to classifying multimedia data such as texts and images, it is possible for adversaries to passively extract the private models even if they are protected by SMPC, using known and newly devised model extraction attacks. We show that our attacks are not only theoretically possible but also practically feasible and cheap, which makes them lucrative to financially motivated attackers such as competitors or customers. We perform model extraction attacks on the homomorphic encryption-based protocol for privacy-preserving SVR-based indoor localization by Zhang et al. (International Workshop on Security 2016). We show that it is possible to extract a highly accurate model using only 854 queries with the estimated cost of $0.09 on the Amazon ML platform, and our attack would take only 7 minutes over the Internet. Also, we perform our model extraction attacks on SVM and SVR models trained on publicly available state-of-the-art ML datasets.;https://dl.acm.org/doi/abs/10.1145/3338498.3358646;OnVTBiFkL78J
Semerikov, S. O., Teplytskyi, I. O., Yechkalo, Y. V., & Kiv, A. E. (2018). Computer simulation of neural networks using spreadsheets: The dawn of the age of Camelot. arXiv preprint arXiv:1807.00018.;4_dl_testing_deep_network;2018;Computer simulation of neural networks using spreadsheets: The dawn of the age of Camelot;Serhiy O Semerikov, Illia O Teplytskyi, Yuliia V Yechkalo, Arnold E Kiv;arXiv preprint arXiv:1807.00018, 2018;"The article substantiates the necessity to develop training methods of computer simulation of neural networks in the spreadsheet environment. The systematic review of their application to simulating artificial neural networks is performed. The authors distinguish basic approaches to solving the problem of network computer simulation training in the spreadsheet environment, joint application of spreadsheets and tools of neural network simulation, application of third-party add-ins to spreadsheets, development of macros using the embedded languages of spreadsheets; use of standard spreadsheet add-ins for non-linear optimization, creation of neural networks in the spreadsheet environment without add-ins and macros. After analyzing a collection of writings of 1890-1950, the research determines the role of the scientific journal ""Bulletin of Mathematical Biophysics"", its founder Nicolas Rashevsky and the scientific community around the journal in creating and developing models and methods of computational neuroscience. There are identified psychophysical basics of creating neural networks, mathematical foundations of neural computing and methods of neuroengineering (image recognition, in particular). The role of Walter Pitts in combining the descriptive and quantitative theories of training is discussed. It is shown that to acquire neural simulation competences in the spreadsheet environment, one should master the models based on the historical and genetic approach. It is indicated that there are three groups of models, which are promising in terms of developing corresponding methods - the continuous two-factor model of Rashevsky, the discrete model of McCulloch and Pitts, and the discrete-continuous models of Householder and Landahl.";https://arxiv.org/abs/1807.00018;pts1xb2jgxgJ
Llewellynn, T., Fernández-Carrobles, M. M., Deniz, O., Fricker, S., Storkey, A., Pazos, N., ... & Tutschku, K. (2017, May). BONSEYES: platform for open development of systems of artificial intelligence. In Proceedings of the computing frontiers conference (pp. 299-304).;1_ml_machine_data_learning;2017;BONSEYES: platform for open development of systems of artificial intelligence;Tim Llewellynn, M Milagro FernÃ¡ndez-Carrobles, Oscar Deniz, Samuel Fricker, Amos Storkey, Nuria Pazos, Gordana Velikic, Kirsten Leufgen, Rozenn Dahyot, Sebastian Koller, Georgios Goumas, Peter Leitner, Ganesh Dasika, Lei Wang, Kurt Tutschku;Proceedings of the computing frontiers conference, 299-304, 2017;"The Bonseyes EU H2020 collaborative project aims to develop a platform consisting of a Data Marketplace, a Deep Learning Toolbox, and Developer Reference Platforms for organizations wanting to adopt Artificial Intelligence. The project will be focused on using artificial intelligence in low power Internet of Things (IoT) devices (""edge computing""), embedded computing systems, and data center servers (""cloud computing""). It will bring about orders of magnitude improvements in efficiency, performance, reliability, security, and productivity in the design and programming of systems of artificial intelligence that incorporate Smart Cyber-Physical Systems (CPS). In addition, it will solve a causality problem for organizations who lack access to Data and Models. Its open software architecture will facilitate adoption of the whole concept on a wider scale. To evaluate the effectiveness, technical feasibility, and to quantify the real-world improvements in efficiency, security, performance, effort and cost of adding AI to products and services using the Bonseyes platform, four complementary demonstrators will be built. Bonseyes platform capabilities are aimed at being aligned with the European FI-PPP activities and take advantage of its flagship project FIWARE. This paper provides a description of the project motivation, goals and preliminary work.";https://dl.acm.org/doi/abs/10.1145/3075564.3076259;E7TMN8nE9r8J
Murugesan, S., Malik, S., Du, F., Koh, E., & Lai, T. M. (2019). Deepcompare: Visual and interactive comparison of deep learning model performance. IEEE computer graphics and applications, 39(5), 47-59.;11_deep_network_model_layer;2019;Deepcompare: Visual and interactive comparison of deep learning model performance;Sugeerth Murugesan, Sana Malik, Fan Du, Eunyee Koh, Tuan Manh Lai;IEEE computer graphics and applications 39 (5), 47-59, 2019;"Deep learning models have become the state-of-the-art for many tasks, from text sentiment analysis to facial image recognition. However, understanding why certain models perform better than others or how one model learns differently than another is often difficult yet critical for increasing their effectiveness, improving prediction accuracy, and enabling fairness. Traditional methods for comparing modelsâ€™ efficacy, such as accuracy, precision, and recall provide a quantitative view of performance; however, the qualitative intricacies of why one model performs better than another are hidden. In this paper, we interview machine learning practitioners to understand their evaluation and comparison workflow. From there, we iteratively design a visual analytic approach, DeepCompare, to systematically compare the results of deep learning models, in order to provide insight into the model behavior and interactively assess tradeoffs between two such models. The tool allows users to evaluate model results, identify and compare activation patterns for misclassifications, and link the test results back to specific neurons. We conduct a preliminary evaluation through two real-world case studies to show that experts can make more informed decisions about the effectiveness of different types of models, understand in more detail the strengths and weaknesses of the models, and holistically evaluate the behavior of the models.";https://ieeexplore.ieee.org/abstract/document/8723177/;w6uIYYmGb4UJ
Castro-Lopez, O., & Vega-Lopez, I. F. (2019, February). Multi-target compiler for the deployment of machine learning models. In 2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO) (pp. 280-281). IEEE.;1_ml_machine_data_learning;2019;Multi-target compiler for the deployment of machine learning models;Oscar Castro-Lopez, Ines F Vega-Lopez;2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO), 280-281, 2019;The deployment of machine learning models into production environments is a crucial task. Its seamless integration with the operational system can be quite challenging as it must adhere to the same software requirements such as memory management, latency, and scalability as the rest of the system. Unfortunately, none of these requirements are taken into consideration when inferring new models from data. A straightforward approach for deployment consists of building a pipeline connecting the modeling tools to the operational system. This approach follows a client-server architecture and it may not address the design requirements of the software in production, especially the ones related to efficiency. An alternative is to manually generate the source code implementing the model in the programming language that was originally used to develop the software in production. However, this approach is usually avoided because it is a time-consuming and error-prone task. To circumvent the aforementioned problems, we propose to automate the process of machine learning model deployment. For this, we have developed a special-purpose compiler. Machine learning models can be formally defined using a standard language. We use this formal description as an input for our compiler, which translates it into the source code that implements the model. Our proposed compiler generates code for different programming languages. Furthermore, with this compiler we can generate source code that exploits specific characteristics of the systems hardware architecture such as multi-core CPUs and graphic processing cards. We have conducted experiments that indicate that automated code generation for deploying machine learning models is, not only feasible but also efficient.;https://ieeexplore.ieee.org/abstract/document/8661199/;dee0QQ7_a1YJ
Amershi, S., Chickering, M., Drucker, S. M., Lee, B., Simard, P., & Suh, J. (2015, April). Modeltracker: Redesigning performance analysis tools for machine learning. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (pp. 337-346).;1_ml_machine_data_learning;2015;Modeltracker: Redesigning performance analysis tools for machine learning;Saleema Amershi, Max Chickering, Steven M Drucker, Bongshin Lee, Patrice Simard, Jina Suh;Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, 337-346, 2015;Model building in machine learning is an iterative process. The performance analysis and debugging step typically involves a disruptive cognitive switch from model building to error analysis, discouraging an informed approach to model building. We present ModelTracker, an interactive visualization that subsumes information contained in numerous traditional summary statistics and graphs while displaying example-level performance and enabling direct error examination and debugging. Usage analysis from machine learning practitioners building real models with ModelTracker over six months shows ModelTracker is used often and throughout model building. A controlled experiment focusing on ModelTracker's debugging capabilities shows participants prefer ModelTracker over traditional tools without a loss in model performance.;https://dl.acm.org/doi/abs/10.1145/2702123.2702509;rCGPCoIl_wwJ
Chang, J. C., Amershi, S., & Kamar, E. (2017, May). Revolt: Collaborative crowdsourcing for labeling machine learning datasets. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (pp. 2334-2346).;1_ml_machine_data_learning;2017;Revolt: Collaborative crowdsourcing for labeling machine learning datasets;Joseph Chee Chang, Saleema Amershi, Ece Kamar;Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, 2334-2346, 2017;Crowdsourcing provides a scalable and efficient way to construct labeled datasets for training machine learning systems. However, creating comprehensive label guidelines for crowdworkers is often prohibitive even for seemingly simple concepts. Incomplete or ambiguous label guidelines can then result in differing interpretations of concepts and inconsistent labels. Existing approaches for improving label quality, such as worker screening or detection of poor work, are ineffective for this problem and can lead to rejection of honest work and a missed opportunity to capture rich interpretations about data. We introduce Revolt, a collaborative approach that brings ideas from expert annotation workflows to crowd-based labeling. Revolt eliminates the burden of creating detailed label guidelines by harnessing crowd disagreements to identify ambiguous concepts and create rich structures (groups of semantically related items) for post-hoc label decisions. Experiments comparing Revolt to traditional crowdsourced labeling show that Revolt produces high quality labels without requiring label guidelines in turn for an increase in monetary cost. This up front cost, however, is mitigated by Revolt's ability to produce reusable structures that can accommodate a variety of label boundaries without requiring new data to be collected. Further comparisons of Revolt's collaborative and non-collaborative variants show that collaboration reaches higher label accuracy with lower monetary cost.;https://dl.acm.org/doi/abs/10.1145/3025453.3026044;8qLFJtsUMyAJ
Veale, M., Van Kleek, M., & Binns, R. (2018, April). Fairness and accountability design needs for algorithmic support in high-stakes public sector decision-making. In Proceedings of the 2018 chi conference on human factors in computing systems (pp. 1-14).;6_fairness_discrimination_bias_decision;2018;Fairness and accountability design needs for algorithmic support in high-stakes public sector decision-making;Michael Veale, Max Van Kleek, Reuben Binns;Proceedings of the 2018 chi conference on human factors in computing systems, 1-14, 2018;Calls for heightened consideration of fairness and accountability in algorithmically-informed public decisions-like taxation, justice, and child protection-are now commonplace. How might designers support such human values? We interviewed 27 public sector machine learning practitioners across 5 OECD countries regarding challenges understanding and imbuing public values into their work. The results suggest a disconnect between organisational and institutional realities, constraints and needs, and those addressed by current research into usable, transparent and 'discrimination-aware' machine learning-absences likely to undermine practical initiatives unless addressed. We see design opportunities in this disconnect, such as in supporting the tracking of concept drift in secondary data sources, and in building usable transparency tools to identify risks and incorporate domain knowledge, aimed both at managers and at the 'street-level bureaucrats' on the frontlines of public service. We conclude by outlining ethical challenges and future directions for collaboration in these high-stakes applications.;https://dl.acm.org/doi/abs/10.1145/3173574.3174014;h_013lxTWCMJ
Hind, M., Houde, S., Martino, J., Mojsilovic, A., Piorkowski, D., Richards, J., & Varshney, K. R. (2020, April). Experiences with improving the transparency of AI models and services. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems (pp. 1-8).;3_explanation_model_machine_learning;2020;Developing a preliminary causal loop diagram for understanding the wicked complexity of the COVID-19 pandemic;Michael Hind, Stephanie Houde, Jacquelyn Martino, Aleksandra Mojsilovic, David Piorkowski, John Richards, Kush R. Varshney;CHI EA '20: Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems;AI models and services are used in a growing number of high-stakes areas, resulting in a need for increased transparency. Consistent with this, several proposals for higher quality and more consistent documentation of AI data, models, and systems have emerged. Little is known, however, about the needs of those who would produce or consume these new forms of documentation. Through semi-structured developer interviews, and two document-creation exercises, we have assembled a clearer picture of these needs and the various challenges faced in creating accurate and useful AI documentation. Based on the observations from this work, supplemented by feedback received during multiple design explorations and stakeholder conversations, we make recommendations for easing the collection and flexible presentation of AI facts to promote transparency.;https://dl.acm.org/doi/abs/10.1145/3334480.3383051;Todo
Yin, M., Wortman Vaughan, J., & Wallach, H. (2019, May). Understanding the effect of accuracy on trust in machine learning models. In Proceedings of the 2019 chi conference on human factors in computing systems (pp. 1-12).;3_explanation_model_machine_learning;2019;Understanding the effect of accuracy on trust in machine learning models;Ming Yin, Jennifer Wortman Vaughan, Hanna Wallach;Proceedings of the 2019 chi conference on human factors in computing systems, 1-12, 2019;We address a relatively under-explored aspect of human-computer interaction: people's abilities to understand the relationship between a machine learning model's stated performance on held-out data and its expected performance post deployment. We conduct large-scale, randomized human-subject experiments to examine whether laypeople's trust in a model, measured in terms of both the frequency with which they revise their predictions to match those of the model and their self-reported levels of trust in the model, varies depending on the model's stated accuracy on held-out data and on its observed accuracy in practice. We find that people's trust in a model is affected by both its stated accuracy and its observed accuracy, and that the effect of stated accuracy can change depending on the observed accuracy. Our work relates to recent research on interpretable machine learning, but moves beyond the typical focus on model internals, exploring a different component of the machine learning pipeline.;https://dl.acm.org/doi/abs/10.1145/3290605.3300509;ETts8NmGtmYJ
Hohman, F., Hodas, N., & Chau, D. H. (2017, May). Shapeshop: Towards understanding deep learning representations via interactive experimentation. In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems (pp. 1694-1699).;11_deep_network_model_layer;2017;Shapeshop: Towards understanding deep learning representations via interactive experimentation;Fred Hohman, Nathan Hodas, Duen Horng Chau;Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems, 1694-1699, 2017;"Deep learning is the driving force behind many recent technologies; however, deep neural networks are often viewed as ""black-boxes"" due to their internal complexity that is hard to understand. Little research focuses on helping people explore and understand the relationship between a user's data and the learned representations in deep learning models. We present our ongoing work, ShapeShop, an interactive system for visualizing and understanding what semantics a neural network model has learned. Built using standard web technologies, ShapeShop allows users to experiment with and compare deep learning models to help explore the robustness of image classifiers.";https://dl.acm.org/doi/abs/10.1145/3027063.3053103;xxWrojA8ujoJ
Boehm, M., Antonov, I., Baunsgaard, S., Dokter, M., Ginthör, R., Innerebner, K., ... & Wrede, S. B. (2019). SystemDS: A declarative machine learning system for the end-to-end data science lifecycle. arXiv preprint arXiv:1909.02976.;1_ml_machine_data_learning;2019;SystemDS: A declarative machine learning system for the end-to-end data science lifecycle;Matthias Boehm, Iulian Antonov, Sebastian Baunsgaard, Mark Dokter, Robert GinthÃ¶r, Kevin Innerebner, Florijan Klezin, Stefanie Lindstaedt, Arnab Phani, Benjamin Rath, Berthold Reinwald, Shafaq Siddiqi, Sebastian Benjamin Wrede;arXiv preprint arXiv:1909.02976, 2019;Machine learning (ML) applications become increasingly common in many domains. ML systems to execute these workloads include numerical computing frameworks and libraries, ML algorithm libraries, and specialized systems for deep neural networks and distributed ML. These systems focus primarily on efficient model training and scoring. However, the data science process is exploratory, and deals with underspecified objectives and a wide variety of heterogeneous data sources. Therefore, additional tools are employed for data engineering and debugging, which requires boundary crossing, unnecessary manual effort, and lacks optimization across the lifecycle. In this paper, we introduce SystemDS, an open source ML system for the end-to-end data science lifecycle from data integration, cleaning, and preparation, over local, distributed, and federated ML model training, to debugging and serving. To this end, we aim to provide a stack of declarative language abstractions for the different lifecycle tasks, and users with different expertise. We describe the overall system architecture, explain major design decisions (motivated by lessons learned from Apache SystemML), and discuss key features and research directions. Finally, we provide preliminary results that show the potential of end-to-end lifecycle optimization.;https://arxiv.org/abs/1909.02976;haY5QwfVI0EJ
Zhu, J., Liapis, A., Risi, S., Bidarra, R., & Youngblood, G. M. (2018, August). Explainable AI for designers: A human-centered perspective on mixed-initiative co-creation. In 2018 IEEE conference on computational intelligence and games (CIG) (pp. 1-8). IEEE.;3_explanation_model_machine_learning;2018;Explainable AI for designers: A human-centered perspective on mixed-initiative co-creation;Jichen Zhu, Antonios Liapis, Sebastian Risi, Rafael Bidarra, G Michael Youngblood;2018 IEEE conference on computational intelligence and games (CIG), 1-8, 2018;Growing interest in eXplainable Artificial Intelligence (XAI) aims to make AI and machine learning more understandable to human users. However, most existing work focuses on new algorithms, and not on usability, practical interpretability and efficacy on real users. In this vision paper, we propose a new research area of eXplainable AI for Designers (XAID), specifically for game designers. By focusing on a specific user group, their needs and tasks, we propose a human-centered approach for facilitating game designers to co-create with AI/ML techniques through XAID. We illustrate our initial XAID framework through three use cases, which require an understanding both of the innate properties of the AI techniques and users' needs, and we identify key open challenges.;https://ieeexplore.ieee.org/abstract/document/8490433/;UIhLquiIPkkJ
Bozarth, A., Dwyer, B., Hu, F., Jalova, D., Muthuraman, K., Pentreath, N., ... & Bommireddipalli, V. (2019, November). Model Asset eXchange: Path to Ubiquitous Deep Learning Deployment. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management (pp. 2953-2956).;1_ml_machine_data_learning;2019;Model Asset eXchange: Path to Ubiquitous Deep Learning Deployment;Alex Bozarth, Brendan Dwyer, Fei Hu, Daniel Jalova, Karthik Muthuraman, Nick Pentreath, Simon Plovyt, Gabriela de Queiroz, Saishruthi Swaminathan, Patrick Titzler, Xin Wu, Hong Xu, Frederick R Reiss, Vijay Bommireddipalli;Proceedings of the 28th ACM International Conference on Information and Knowledge Management, 2953-2956, 2019;A recent trend observed in traditionally challenging fields such as computer vision and natural language processing has been the significant performance gains shown by deep learning (DL). In many different research fields, DL models have been evolving rapidly and become ubiquitous. Despite researchers' excitement, unfortunately, most software developers are not DL experts and oftentimes have a difficult time following the booming DL research outputs. As a result, it usually takes a significant amount of time for the latest superior DL models to prevail in industry. This issue is further exacerbated by the common use of sundry incompatible DL programming frameworks, such as Tensorflow, PyTorch, Theano, etc. To address this issue, we propose a system, called Model Asset Exchange (MAX), that avails developers of easy access to state-of-the-art DL models. Regardless of the underlying DL programming frameworks, it provides an open source Python library (called the MAX framework) that wraps DL models and unifies programming interfaces with our standardized RESTful APIs. These RESTful APIs enable developers to exploit the wrapped DL models for inference tasks without the need to fully understand different DL programming frameworks. Using MAX, we have wrapped and open-sourced more than 30 state-of-the-art DL models from various research fields, including computer vision, natural language processing and signal processing, etc. In the end, we selectively demonstrate two web applications that are built on top of MAX, as well as the process of adding a DL model to MAX.;https://dl.acm.org/doi/abs/10.1145/3357384.3357860;m9joVsiXGjcJ
Chouldechova, A., & Roth, A. (2018). The frontiers of fairness in machine learning. arXiv preprint arXiv:1810.08810.;6_fairness_discrimination_bias_decision;2018;The frontiers of fairness in machine learning;Alexandra Chouldechova, Aaron Roth;arXiv preprint arXiv:1810.08810, 2018;The last few years have seen an explosion of academic and popular interest in algorithmic fairness. Despite this interest and the volume and velocity of work that has been produced recently, the fundamental science of fairness in machine learning is still in a nascent state. In March 2018, we convened a group of experts as part of a CCC visioning workshop to assess the state of the field, and distill the most promising research directions going forward. This report summarizes the findings of that workshop. Along the way, it surveys recent theoretical work in the field and points towards promising directions for research.;https://arxiv.org/abs/1810.08810;HIDXv2JWe10J
Casalicchio, G., Bossek, J., Lang, M., Kirchhoff, D., Kerschke, P., Hofner, B., ... & Bischl, B. (2019). OpenML: An R package to connect to the machine learning platform OpenML. Computational Statistics, 34, 977-991.;1_ml_machine_data_learning;2019;OpenML: An R package to connect to the machine learning platform OpenML;Giuseppe Casalicchio, Jakob Bossek, Michel Lang, Dominik Kirchhoff, Pascal Kerschke, Benjamin Hofner, Heidi Seibold, Joaquin Vanschoren, Bernd Bischl;Computational Statistics 34, 977-991, 2019;OpenML is an online machine learning platform where researchers can easily share data, machine learning tasks and experiments as well as organize them online to work and collaborate more efficiently. In this paper, we present an R package to interface with the OpenML platform and illustrate its usage in combination with the machine learning R package mlr (Bischl et al. J Mach Learn Res 17(170):1–5, 2016). We show how the OpenML package allows R users to easily search, download and upload data sets and machine learning tasks. Furthermore, we also show how to upload results of experiments, share them with others and download results from other users. Beyond ensuring reproducibility of results, the OpenML platform automates much of the drudge work, speeds up research, facilitates collaboration and increases the users’ visibility online.;https://link.springer.com/article/10.1007/s00180-017-0742-2;RfY9WsA_EX8J
Onoufriou, G., Bickerton, R., Pearson, S., & Leontidis, G. (2019). Nemesyst: A hybrid parallelism deep learning-based framework applied for internet of things enabled food retailing refrigeration systems. Computers in Industry, 113, 103133.;7_edge_computing_deep_learning;2019;Nemesyst: A hybrid parallelism deep learning-based framework applied for internet of things enabled food retailing refrigeration systems;George Onoufriou, Ronald Bickerton, Simon Pearson, Georgios Leontidis;Computers in Industry 113, 103133, 2019;Deep learning has attracted considerable attention across multiple application domains, including computer vision, signal processing and natural language processing. Although quite a few single node deep learning frameworks exist, such as tensorflow, pytorch and keras, we still lack a complete processing structure that can accommodate large scale data processing, version control, and deployment, all while staying agnostic of any specific single node framework. To bridge this gap, this paper proposes a new, higher level framework, i.e. Nemesyst …;https://www.sciencedirect.com/science/article/pii/S0166361519304026;iq3Q96WkO3wJ
Adebayo, J. A. (2016). FairML: ToolBox for diagnosing bias in predictive modeling (Doctoral dissertation, Massachusetts Institute of Technology).;6_fairness_discrimination_bias_decision;2016;FairML: ToolBox for diagnosing bias in predictive modeling;Julius A Adebayo;Massachusetts Institute of Technology, 2016;Predictive models are increasingly deployed for the purpose of determining access to services such as credit, insurance, and employment. Despite societal gains in efficiency and productivity through deployment of these models, potential systemic flaws have not been fully addressed, particularly the potential for unintentional discrimination. This discrimination could be on the basis of race, gender, religion, sexual orientation, or other characteristics. This thesis addresses the question: how can an analyst determine the relative significance of the inputs to a black-box predictive model in order to assess the model's fairness (or discriminatory extent)? We present FairML, an end-to- end toolbox for auditing predictive models by quantifying the relative significance of the model's inputs. FairML leverages model compression and four input ranking algorithms to quantify a model's relative predictive dependence on its inputs. The relative significance of the inputs to a predictive model can then be used to assess the fairness (or discriminatory extent) of such a model. With FairML, analysts can more easily audit cumbersome predictive models that are difficult to interpret.;https://dspace.mit.edu/handle/1721.1/108212;zR43wGmFvsgJ
Yu, Y., Liu, X., & Chen, Z. (2018, October). Attacks and defenses towards machine learning based systems. In Proceedings of the 2nd International Conference on Computer Science and Application Engineering (pp. 1-7).;5_adversarial_attack_example_model;2018;Attacks and defenses towards machine learning based systems;Yingchao Yu, Xueyong Liu, Zuoning Chen;Proceedings of the 2nd International Conference on Computer Science and Application Engineering, 1-7, 2018;Recent research1 has shown that machine learning models are venerable to attacks by adversaries almost at all phases of machine learning pipeline, such as positioning attacks on training data, attacks on the learning algorithm, input attacks based on carefully crafted adversarial samples, model steal and model inversion attack etc. Input samples that are maliciously created can affect the learning process of a ML system by either slowing the learning process, or affecting the performance of the learned model or causing the system make error. So, understanding the security of machine learning algorithms and systems is emerging as an important research area among computer security and machine learning researchers and practitioners. We present a survey on this emerging area: firstly, we define the processing pipeline of a generic machine learning system, then, we identify the attacks in different points of the pipeline and its potential defense solution. Finally, the research work of this paper is summarized and the further research directions are proposed.;https://dl.acm.org/doi/abs/10.1145/3207677.3277988;ypFnxw2D_RwJ
Arun, A., & Arnaldo, I. (2019). Shooting the moving target: machine learning in cybersecurity. In 2019 USENIX Conference on Operational Machine Learning (OpML 19) (pp. 13-14).;1_ml_machine_data_learning;2019;Shooting the moving target: machine learning in cybersecurity;Ankit Arun, Ignacio Arnaldo;2019 USENIX Conference on Operational Machine Learning (OpML 19), 13-14, 2019;We introduce a platform used to productionize machine learning models for detecting cyberthreats. To keep up with a diverse and ever-evolving threat landscape, it is of paramount importance to seamlessly iterate over the two pillars of machine learning: data and models. To satisfy this requirement, the introduced platform is modular, extensible, and automates the continuous improvement of the detection models. The platform counts more than 1000 successful model deployments at over 30 production environments.;https://www.usenix.org/conference/opml19/presentation/arun;JdxwciMki9AJ
Agarwal, A., Beygelzimer, A., Dudík, M., Langford, J., & Wallach, H. (2018, July). A reductions approach to fair classification. In International conference on machine learning (pp. 60-69). PMLR.;6_fairness_discrimination_bias_decision;2018;A reductions approach to fair classification;Alekh Agarwal, Alina Beygelzimer, Miroslav DudÃ­k, John Langford, Hanna Wallach;International conference on machine learning, 60-69, 2018;We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.;https://proceedings.mlr.press/v80/agarwal18a.html;Cig7sEeuIOoJ
Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., & Pedreschi, D. (2018). A survey of methods for explaining black box models. ACM computing surveys (CSUR), 51(5), 1-42.;3_explanation_model_machine_learning;2018;A survey of methods for explaining black box models;Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, Dino Pedreschi;ACM computing surveys (CSUR) 51 (5), 1-42, 2018;In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.;https://dl.acm.org/doi/abs/10.1145/3236009;nXSS2E7-M48J
Lapuschkin, S., Binder, A., Montavon, G., Muller, K. R., & Samek, W. (2016). Analyzing classifiers: Fisher vectors and deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2912-2920).;11_deep_network_model_layer;2016;Analyzing classifiers: Fisher vectors and deep neural networks;Sebastian Lapuschkin, Alexander Binder, GrÃ©goire Montavon, Klaus-Robert Muller, Wojciech Samek;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2912-2920, 2016;Fisher vector (FV) classifiers and Deep Neural Networks (DNNs) are popular and successful algorithms for solving image classification problems. However, both are generally consideredblack box'predictors as the non-linear transformations involved have so far prevented transparent and interpretable reasoning. Recently, a principled technique, Layer-wise Relevance Propagation (LRP), has been developed in order to better comprehend the inherent structured reasoning of complex nonlinear classification models such as Bag of Feature models or DNNs. In this paper we (1) extend the LRP framework also for Fisher vector classifiers and then use it as analysis tool to (2) quantify the importance of context for classification,(3) qualitatively compare DNNs against FV classifiers in terms of important image regions and (4) detect potential flaws and biases in data. All experiments are performed on the PASCAL VOC 2007 and ILSVRC 2012 data sets.;http://openaccess.thecvf.com/content_cvpr_2016/html/Bach_Analyzing_Classifiers_Fisher_CVPR_2016_paper.html;i-YyjNRN958J
Moosavi-Dezfooli, S. M., Fawzi, A., & Frossard, P. (2016). Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2574-2582).;5_adversarial_attack_example_model;2016;Deepfool: a simple and accurate method to fool deep neural networks;Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard;Proceedings of the IEEE conference on computer vision and pattern recognition, 2574-2582, 2016;State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust.;http://openaccess.thecvf.com/content_cvpr_2016/html/Moosavi-Dezfooli_DeepFool_A_Simple_CVPR_2016_paper.html;m-Ich6jmKswJ
Bau, D., Zhou, B., Khosla, A., Oliva, A., & Torralba, A. (2017). Network dissection: Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 6541-6549).;11_deep_network_model_layer;2017;Network dissection: Quantifying interpretability of deep visual representations;David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba;Proceedings of the IEEE conference on computer vision and pattern recognition, 6541-6549, 2017;We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a data set of concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are labeled across a broad range of visual concepts including objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability is an axis-independent property of the representation space, then we apply the method to compare the latent representations of various networks when trained to solve different classification problems. We further analyze the effect of training iterations, compare networks trained with different initializations, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power;http://openaccess.thecvf.com/content_cvpr_2017/html/Bau_Network_Dissection_Quantifying_CVPR_2017_paper.html;7yDej1RrxPoJ
Zhang, Q., Wu, Y. N., & Zhu, S. C. (2018). Interpretable convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 8827-8836).;11_deep_network_model_layer;2018;Interpretable convolutional neural networks;Quanshi Zhang, Ying Nian Wu, Song-Chun Zhu;Proceedings of the IEEE conference on computer vision and pattern recognition, 8827-8836, 2018;This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable CNN, in order to clarify knowledge representations in high conv-layers of the CNN. In an interpretable CNN, each filter in a high conv-layer represents a specific object part. Our interpretable CNNs use the same training data as ordinary CNNs without a need for any annotations of object parts or textures for supervision. The interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. We can apply our method to different types of CNNs with various structures. The explicit knowledge representation in an interpretable CNN can help people understand the logic inside a CNN, ie, what patterns are memorized by the CNN for prediction. Experiments have shown that filters in an interpretable CNN are more semantically meaningful than those in a traditional CNN. The code is available at https://github. com/zqs1022/interpretableCNN.;http://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Interpretable_Convolutional_Neural_CVPR_2018_paper.html;sVVpR4yaZuwJ
Eykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A., Xiao, C., ... & Song, D. (2018). Robust physical-world attacks on deep learning visual classification. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1625-1634).;5_adversarial_attack_example_model;2018;Robust physical-world attacks on deep learning visual classification;Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, Dawn Song;Proceedings of the IEEE conference on computer vision and pattern recognition, 1625-1634, 2018;Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations. Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm, Robust Physical Perturbations (RP 2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP 2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. With a perturbation in the form of only black and white stickers, we attack a real stop sign, causing targeted misclassification in 100% of the images obtained in lab settings, and in 84.8% of the captured video frames obtained on a moving vehicle (field test) for the target classifier.;http://openaccess.thecvf.com/content_cvpr_2018/html/Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper;LSVmof2KT5sJ
Wagner, J., Kohler, J. M., Gindele, T., Hetzel, L., Wiedemer, J. T., & Behnke, S. (2019). Interpretable and fine-grained visual explanations for convolutional neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 9097-9107).;11_deep_network_model_layer;2019;Interpretable and fine-grained visual explanations for convolutional neural networks;Jorg Wagner, Jan Mathias Kohler, Tobias Gindele, Leon Hetzel, Jakob Thaddaus Wiedemer, Sven Behnke;Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 9097-9107, 2019;To verify and validate networks, it is essential to gain insight into their decisions, limitations as well as possible shortcomings of training data. In this work, we propose a post-hoc, optimization based visual explanation method, which highlights the evidence in the input image for a specific prediction. Our approach is based on a novel technique to defend against adversarial evidence (ie faulty evidence due to artefacts) by filtering gradients during optimization. The defense does not depend on human-tuned parameters. It enables explanations which are both fine-grained and preserve the characteristics of images, such as edges and colors. The explanations are interpretable, suited for visualizing detailed evidence and can be tested as they are valid model inputs. We qualitatively and quantitatively evaluate our approach on a multitude of models and datasets.;http://openaccess.thecvf.com/content_CVPR_2019/html/Wagner_Interpretable_and_Fine-Grained_Visual_Explanations_for_Convolutional_Neural_Networks_CVPR_2019_paper.html;w0YMYH18P24J
Gunning, D. (2017). Explainable artificial intelligence (xai). Defense advanced research projects agency (DARPA), nd Web, 2(2), 1.;3_explanation_model_machine_learning;2017;Explainable artificial intelligence (xai);David Gunning;Defense advanced research projects agency (DARPA), nd Web 2 (2), 1, 2017;"… • The current generation of AI systems offer tremendous benefits, but their effectiveness will be 
limited by the machine’s inability to explain its decisions and actions to users • Explainable AI will 
be essential if users are to understand, appropriately trust, and effectively manage this incoming 
generation of artificially intelligent partners … • XAI will create a suite of machine learning 
techniques that • Produce more explainable models, while maintaining a high level of learning 
performance (eg, prediction accuracy) • Enable human users to understand, appropriately …";https://nsarchive.gwu.edu/sites/default/files/documents/5794867/National-Security-Archive-David-Gunning-DARPA.pdf;hT6Q8cvYflEJ
García, M., Domínguez, C., Heras, J., Mata, E., & Pascual, V. (2018, June). An on-going framework for easily experimenting with deep learning models for bioimaging analysis. In International Symposium on Distributed Computing and Artificial Intelligence (pp. 330-333). Cham: Springer International Publishing.;11_deep_network_model_layer;2018;An on-going framework for easily experimenting with deep learning models for bioimaging analysis;Manuel García, César Domínguez, Jónathan Heras, Eloy Mata, Vico Pascual;International Symposium on Distributed Computing and Artificial Intelligence, 330-333, 2018;Due to the broad use of deep learning methods in Bioimaging, it seems convenient to create a framework that facilitates the task of analysing different models and selecting the best one to solve each particular problem. In this work-in-progress, we are developing a Python framework to deal with such a task in the case of bioimage classification. Namely, the purpose of the framework is to automate and facilitate the process of choosing the best combination of feature extractors (obtained from transfer learning and other techniques), and classification models. The features and models to test are fixed by a simple configuration file to facilitate the use of the framework by non-expert users. The best model is automatically selected through a statistical study, and then it can be employed to predict the category of new images.;https://link.springer.com/chapter/10.1007/978-3-319-99608-0_39;SecsAn8xa9QJ
Shah, V., & Kumar, A. (2019, June). The ML data prep zoo: Towards semi-automatic data preparation for ML. In Proceedings of the 3rd International Workshop on Data Management for End-to-End Machine Learning (pp. 1-4).;1_ml_machine_data_learning;2019;The ML data prep zoo: Towards semi-automatic data preparation for ML;Vraj Shah, Arun Kumar;Proceedings of the 3rd International Workshop on Data Management for End-to-End Machine Learning, 1-4, 2019;"Data preparation (prep) time is a major bottleneck for many ML applications. It is often painful grunt work that is handled manually by data scientists, reducing their productivity and raising costs. It is also a roadblock for emerging AutoML platforms. We envision a new line of community-driven research to tackle this bottleneck based on a simple philosophy: use ML to semi-automate data prep for ML. For impactful research on this problem, we believe the major impediment is not new algorithms or theory but rather common task definitions and benchmark labeled datasets. To this end, we formalize a few major data prep tasks for ML over structured data as applied ML tasks. We discuss research challenges in scaling up data labeling, defining accuracy metrics, and creating practical tool support. We present a case study of our progress on a key data prep task: ML schema inference. Finally, we propose a public ""zoo"" of labeled datasets and pre-trained ML models for data prep tasks to act as a community-led repository for further research on this problem.";https://dl.acm.org/doi/abs/10.1145/3329486.3329499;E-8ohsQ3tbEJ
Kossak, F., & Zwick, M. (2019). ML-PipeDebugger: A Debugging Tool for Data Processing Pipelines. In Database and Expert Systems Applications: 30th International Conference, DEXA 2019, Linz, Austria, August 26–29, 2019, Proceedings, Part II 30 (pp. 263-272). Springer International Publishing.;9_data_science_software_process;2019;ML-PipeDebugger: A Debugging Tool for Data Processing Pipelines;Felix Kossak, Michael Zwick;Database and Expert Systems Applications: 30th International Conference, DEXA 2019, Linz, Austria, August 26–29, 2019, Proceedings, Part II 30, 263-272, 2019;Data pre-processing for data analysis usually requires a considerable number of interdependent steps, many of which are liable to errors or to introduce unwanted biases. Such errors can lead to cases where predictions for similar data instances differ unexpectedly much. An important question is then to find out where in the data processing pipeline the deviation was caused. We present a tool that can help identify critical data processing steps, allowing to “debug” or improve data pre-processing and model generation. More generally, the tool gives a view of how different data instances behave in relation to each other throughout a pipeline. The task to identify critical steps turns out to be rather complex, mostly because features of different types and ranges have to be compared, because required statistical measures must be obtained from often small samples, and because time series can be involved.;https://link.springer.com/chapter/10.1007/978-3-030-27618-8_20;7A29ZKokk-YJ
Ring, D., Barbier, J., Gales, G., Kent, B., & Lutz, S. (2019, July). Jumping in at the deep end: how to experiment with machine learning in post-production software. In Proceedings of the 2019 Digital Production Symposium (pp. 1-5).;1_ml_machine_data_learning;2019;Jumping in at the deep end: how to experiment with machine learning in post-production software;Dan Ring, Johanna Barbier, Guillaume Gales, Ben Kent, Sebastian Lutz;Proceedings of the 2019 Digital Production Symposium, 1-5, 2019;"Recent years has seen an explosion in Machine Learning (ML) research. The challenge is now to transfer these new algorithms into the hands of artists and TD's in visual effects and animation studios, so that they can start experimenting with ML within their existing pipelines. This paper presents some of the current challenges to experimentation and deployment of ML frameworks in the post-production industry. It introduces our open-source ""ML-Server"" client / server system as an answer to enabling rapid prototyping, experimentation and development of ML models in post-production software. Data, code and examples for the system can be found on the GitHub repository page:https://github.com/TheFoundryVisionmongers/nuke-ML-server";https://dl.acm.org/doi/abs/10.1145/3329715.3338880;ZfSx70qstSMJ
Montavon, G., Samek, W., & Müller, K. R. (2018). Methods for interpreting and understanding deep neural networks. Digital signal processing, 73, 1-15.;11_deep_network_model_layer;2018;Methods for interpreting and understanding deep neural networks;Grégoire Montavon, Wojciech Samek, Klaus-Robert Müller;Digital signal processing 73, 1-15, 2018;This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to …;https://www.sciencedirect.com/science/article/pii/S1051200417302385;m__erzt4Dm4J
Yin, J., Gahlot, S., Laanait, N., Maheshwari, K., Morrison, J., Dash, S., & Shankar, M. (2019, November). Strategies to deploy and scale deep learning on the summit supercomputer. In 2019 IEEE/ACM Third Workshop on Deep Learning on Supercomputers (DLS) (pp. 84-94). IEEE.;1_ml_machine_data_learning;2019;Strategies to deploy and scale deep learning on the summit supercomputer;Junqi Yin, Shubhankar Gahlot, Nouamane Laanait, Ketan Maheshwari, Jack Morrison, Sajal Dash, Mallikarjun Shankar;2019 IEEE/ACM Third Workshop on Deep Learning on Supercomputers (DLS), 84-94, 2019;The rapid growth and wide applicability of Deep Learning (DL) frameworks poses challenges to computing centers which need to deploy and support the software, and also to domain scientists who have to keep up with the system environment and scale up scientific exploration through DL. We offer recommendations for deploying and scaling DL frameworks on the Summit supercomputer, currently atop the Top500 list, at the Oak Ridge National Laboratory Leadership Computing Facility (OLCF). We discuss DL software deployment in the form of containers, and compare performance of native-built frameworks and containerized deployment. Software containers show no noticeable negative performance impact and exhibit faster Python loading times and promise easier maintenance. To explore strategies for scaling up DL model training campaigns, we assess DL compute kernel performance, discuss and recommend I/O data formats and staging, and identify communication needs for scalable message exchange for DL runs at scale. We recommend that users take a step-wise tuning approach beginning with algorithmic kernel choice, node I/O configuration, and communications tuning as best-practice. We present baseline examples of scaling efficiency 87% for a DL run of ResNet50 running on 1024 nodes (6144 V100 GPUs).;https://ieeexplore.ieee.org/abstract/document/8945109/;3pFWD4vY6H4J
Berrar, D., & Dubitzky, W. (2017, October). On the Jeffreys-Lindley paradox and the looming reproducibility crisis in machine learning. In 2017 IEEE international conference on data science and advanced analytics (DSAA) (pp. 334-340). IEEE.;3_explanation_model_machine_learning;2017;On the Jeffreys-Lindley paradox and the looming reproducibility crisis in machine learning;Daniel Berrar, Werner Dubitzky;2017 IEEE international conference on data science and advanced analytics (DSAA), 334-340, 2017;"Null hypothesis significance testing has become a mainstay in machine learning, with the p-value being firmly embedded in the current research practice. Significance testing is widely believed to lend scientific rigor to the interpretation of empirical findings; however, its serious problems have received scant attention in the machine learning literature so far. Here, we investigate one particular problem: the Jeffreys-Lindley paradox. This paradox describes a statistical conundrum where the frequentist and Bayesian interpretation are diametrically opposed. In four experiments using synthetic data sets and a subsequent thought experiment, we demonstrate that this paradox has severe, real consequences for the current research practice. We caution that this practice might lead to a situation that is similar to the current reproducibility crisis in other fields of science. We offer for debate four avenues that might avert the looming crisis.";https://ieeexplore.ieee.org/abstract/document/8259793/;Zji1o59hebMJ
Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., & Kagal, L. (2018, October). Explaining explanations: An overview of interpretability of machine learning. In 2018 IEEE 5th International Conference on data science and advanced analytics (DSAA) (pp. 80-89). IEEE.;3_explanation_model_machine_learning;2018;Explaining explanations: An overview of interpretability of machine learning;Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, Lalana Kagal;2018 IEEE 5th International Conference on data science and advanced analytics (DSAA), 80-89, 2018;There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.;https://ieeexplore.ieee.org/abstract/document/8631448.;f3wbGKu_mkwJ
Antunes, N., Balby, L., Figueiredo, F., Lourenco, N., Meira, W., & Santos, W. (2018, June). Fairness and transparency of machine learning for trustworthy cloud services. In 2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W) (pp. 188-193). IEEE.;6_fairness_discrimination_bias_decision;2018;Fairness and transparency of machine learning for trustworthy cloud services;Nuno Antunes, Leandro Balby, Flavio Figueiredo, Nuno Lourenco, Wagner Meira, Walter Santos;2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W), 188-193, 2018;Machine learning is nowadays ubiquitous, providing mechanisms for supporting decision making that leverages big data analytics. However, this recent rise in importance of machine learning also raises societal concerns about the dependability and trustworthiness of systems which depend on such automated predictions. Within this context, the new general data protection regulation (GDPR) demands that organizations take the appropriate measures to protect individuals' data, and use it in a privacy-preserving, fair and transparent fashion. In this paper we present how fairness and transparency are supported in the ATMOSPHERE ecosystem for trustworthy clouds. For this, we present the scope of fairness and transparency concerns in the project and then discuss the techniques that are being developed to address each of these concerns. Furthermore, we discuss how fairness and transparency are used with other quality attributes to characterize the trustworthiness of cloud systems.;https://ieeexplore.ieee.org/abstract/document/8416248/;DMDOQ9aOfpgJ
Benjamins, R. (2020). Towards organizational guidelines for the responsible use of AI. arXiv preprint arXiv:2001.09758.;12_ai_ethical_ethic_intelligence;2020;Towards organizational guidelines for the responsible use of AI;Richard Benjamins;arXiv preprint arXiv:2001.09758, 2020;In the past few years, several large companies have published ethical principles of Artificial Intelligence (AI). National governments, the European Commission, and inter-governmental organizations have come up with requirements to ensure the good use of AI. However, individual organizations that want to join this effort, are faced with many unsolved questions. This paper proposes guidelines for organizations committed to the responsible use of AI, but lack the required knowledge and experience. The guidelines consist of two parts: i) helping organizations to decide what principles to adopt, and ii) a methodology for implementing the principles in organizational processes. In case of future AI regulation, organizations following this approach will be well-prepared.;https://arxiv.org/abs/2001.09758;KrX0amDfuoAJ
Zeiler, M. D., & Fergus, R. (2014). Visualizing and understanding convolutional networks. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13 (pp. 818-833). Springer International Publishing.;11_deep_network_model_layer;2014;Visualizing and understanding convolutional networks;Matthew D Zeiler, Rob Fergus;Computer Visionâ€“ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13, 818-833, 2014;Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.;https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53;nTTNO0SEi2gJ
Baier, L., Jöhren, F., & Seebacher, S. (2019, June). Challenges in the Deployment and Operation of Machine Learning in Practice. In ECIS (Vol. 1).;1_ml_machine_data_learning;2019;Challenges in the Deployment and Operation of Machine Learning in Practice.;Lucas Baier, Fabian Jöhren, Stefan Seebacher;ECIS, 2019;Machine learning has recently emerged as a powerful technique to increase operational efficiency or to develop new value propositions. However, the translation of a prediction algorithm into an operationally usable machine learning model is a time-consuming and in various ways challenging task. In this work, we target to systematically elicit the challenges in deployment and operation to enable broader practical dissemination of machine learning applications. To this end, we first identify relevant challenges with a structured literature analysis. Subsequently, we conduct an interview study with machine learning practitioners across various industries, perform a qualitative content analysis, and identify challenges organized along three distinct categories as well as six overarching clusters. Eventually, results from both literature and interviews are evaluated with a comparative analysis. Key issues identified include automated strategies for data drift detection and handling, standardization of machine learning infrastructure, and appropriate communication and expectation management.;https://www.researchgate.net/profile/Lucas-Baier/publication/332996647_CHALLENGES_IN_THE_DEPLOYMENT_AND_OPERATION_OF_MACHINE_LEARNING_IN_PRACTICE/links/5cd57a7c92851c4eab924c03/CHALLENGES-IN-THE-DEPLOYMENT-AND-OPERATION-OF-MACHINE-LEARNING-IN-PRACTICE.pdf;todo
Van Rijn, J. N., Bischl, B., Torgo, L., Gao, B., Umaashankar, V., Fischer, S., ... & Vanschoren, J. (2013). OpenML: A collaborative science platform. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part III 13 (pp. 645-649). Springer Berlin Heidelberg.;1_ml_machine_data_learning;2013;OpenML: A collaborative science platform;Jan N Van Rijn, Bernd Bischl, Luis Torgo, Bo Gao, Venkatesh Umaashankar, Simon Fischer, Patrick Winter, Bernd Wiswedel, Michael R Berthold, Joaquin Vanschoren;Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part III 13, 645-649, 2013;We present OpenML, a novel open science platform that provides easy access to machine learning data, software and results to encourage further study and application. It organizes all submitted results online so they can be easily found and reused, and features a web API which is being integrated in popular machine learning tools such as Weka, KNIME, RapidMiner and R packages, so that experiments can be shared easily.;https://link.springer.com/chapter/10.1007/978-3-642-40994-3_46;uEp2uiMjEXoJ
Biggio, B., Corona, I., Maiorca, D., Nelson, B., Šrndić, N., Laskov, P., ... & Roli, F. (2013). Evasion attacks against machine learning at test time. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part III 13 (pp. 387-402). Springer Berlin Heidelberg.;5_adversarial_attack_example_model;2013;Evasion attacks against machine learning at test time;Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić, Pavel Laskov, Giorgio Giacinto, Fabio Roli;Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part III 13, 387-402, 2013;In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker’s knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.;https://link.springer.com/chapter/10.1007/978-3-642-40994-3_25;5rjswQiJysgJ
Casalicchio, G., Molnar, C., & Bischl, B. (2019). Visualizing the feature importance for black box models. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10–14, 2018, Proceedings, Part I 18 (pp. 655-670). Springer International Publishing.;3_explanation_model_machine_learning;2019;Visualizing the feature importance for black box models;Giuseppe Casalicchio, Christoph Molnar, Bernd Bischl;Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10â€“14, 2018, Proceedings, Part I 18, 655-670, 2019;In recent years, a large amount of model-agnostic methods to improve the transparency, trustability, and interpretability of machine learning models have been developed. Based on a recent method for model-agnostic global feature importance, we introduce a local feature importance measure for individual observations and propose two visual tools: partial importance (PI) and individual conditional importance (ICI) plots which visualize how changes in a feature affect the model performance on average, as well as for individual observations. Our proposed methods are related to partial dependence (PD) and individual conditional expectation (ICE) plots, but visualize the expected (conditional) feature importance instead of the expected (conditional) prediction. Furthermore, we show that averaging ICI curves across observations yields a PI curve, and integrating the PI curve with respect to the distribution of the considered feature results in the global feature importance. Another contribution of our paper is the Shapley feature importance, which fairly distributes the overall performance of a model among the features according to the marginal contributions and which can be used to compare the feature importance across different models. Code related to this paper is available at: https://github.com/giuseppec/featureImportance .;https://link.springer.com/chapter/10.1007/978-3-030-10925-7_40;mmGt2WkviRgJ
Vanschoren, J., & Blockeel, H. (2009). A community-based platform for machine learning experimentation. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2009, Bled, Slovenia, September 7-11, 2009, Proceedings, Part II 20 (pp. 750-754). Springer Berlin Heidelberg.;1_ml_machine_data_learning;2009;A community-based platform for machine learning experimentation;Joaquin Vanschoren, Hendrik Blockeel;Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2009, Bled, Slovenia, September 7-11, 2009, Proceedings, Part II 20, 750-754, 2009;We demonstrate the practical uses of a community-based platform for the sharing and in-depth investigation of the thousands of machine learning experiments executed every day. It is aimed at researchers and practitioners of data mining techniques, and is publicly available at http://expdb.cs.kuleuven.be. The system offers standards and APIâ€™s for sharing experimental results, extensive querying capabilities of the gathered results and allows easy integration in existing data mining toolboxes. We believe such a system may speed up scientific discovery and enhance the scientific rigor of machine learning research.;https://link.springer.com/chapter/10.1007/978-3-642-04174-7_56;O2F0gueW7N4J
Patel, S., McGinnis, R. S., Silva, I., DiCristofaro, S., Mahadevan, N., Jortberg, E., ... & Ghaffari, R. (2016, August). A wearable computing platform for developing cloud-based machine learning models for health monitoring applications. In 2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (pp. 5997-6001). IEEE.;7_edge_computing_deep_learning;2016;A wearable computing platform for developing cloud-based machine learning models for health monitoring applications;Shyamal Patel, Ryan S McGinnis, Ikaro Silva, Steve DiCristofaro, Nikhil Mahadevan, Elise Jortberg, Jaime Franco, Albert Martin, Joseph Lust, Milan Raj, Bryan McGrane, Paolo DePetrillo, AJ Aranyosi, Melissa Ceruolo, Jesus Pindado, Roozbeh Ghaffari;2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 5997-6001, 2016;Wearable sensors have the potential to enable clinical-grade ambulatory health monitoring outside the clinic. Technological advances have enabled development of devices that can measure vital signs with great precision and significant progress has been made towards extracting clinically meaningful information from these devices in research studies. However, translating measurement accuracies achieved in the controlled settings such as the lab and clinic to unconstrained environments such as the home remains a challenge. In this paper, we present a novel wearable computing platform for unobtrusive collection of labeled datasets and a new paradigm for continuous development, deployment and evaluation of machine learning models to ensure robust model performance as we transition from the lab to home. Using this system, we train activity classification models across two studies and track changes in model performance as we go from constrained to unconstrained settings.;https://ieeexplore.ieee.org/abstract/document/7592095/;Je_-v_vDiFEJ
Ellis, C. A., Gu, P., Sendi, M. S., Huddleston, D., Sharma, A., & Mahmoudi, B. (2019, July). A Cloud-based Framework for Implementing Portable Machine Learning Pipelines for Neural Data Analysis. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (pp. 4466-4469). IEEE.;1_ml_machine_data_learning;2019;A Cloud-based Framework for Implementing Portable Machine Learning Pipelines for Neural Data Analysis;Charles A Ellis, Ping Gu, Mohammad SE Sendi, Daniel Huddleston, Ashish Sharma, Babak Mahmoudi;2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 4466-4469, 2019;Cloud-based computing has created new avenues for innovative research. In recent years, numerous cloud-based, data analysis projects within the biomedical domain have been implemented. As this field is likely to grow, there is a need for a unified platform for the developing and testing of advanced analytic and modeling tools that enables those tools to be easily reused for biomedical data analysis by a broad set of users with diverse technical skills. A cloud-based platform of this nature could greatly assist future research endeavors. In this paper, we take the first step towards building such a platform. We define an approach by which containerized analytic pipelines can be distributed for use on cloud-based or on-premise computing platforms. We demonstrate our approach by implementing a portable biomarker identification pipeline using a logistic regression model with elastic net regularization (LR-ENR) and running it on Google Cloud. We used this pipeline for the diagnosis of Parkinson's disease based on a combination of clinical, demographic, and MRI-based features and for the identification of the most predictive biomarkers.;https://ieeexplore.ieee.org/abstract/document/8856929/;dfeOz7_XwwEJ
Shin, M., Kim, J., Mohaisen, A., Park, J., & Lee, K. H. (2018, June). Neural network syntax analyzer for embedded standardized deep learning. In Proceedings of the 2nd International Workshop on Embedded and Mobile Deep Learning (pp. 37-41).;7_edge_computing_deep_learning;2018;Neural network syntax analyzer for embedded standardized deep learning;MyungJae Shin, Joongheon Kim, Aziz Mohaisen, Jaebok Park, Kyung Hee Lee;Proceedings of the 2nd International Workshop on Embedded and Mobile Deep Learning, 37-41, 2018;Deep learning frameworks based on the neural network model have attracted a lot of attention recently for their potential in various applications. Accordingly, recent developments in the fields of deep learning configuration platforms have led to renewed interests in neural network unified format (NNUF) for standardized deep learning computation. The attempt of making NNUF becomes quite challenging because primarily used platforms change over time and the structures of deep learning computation models are continuously evolving. This paper presents the design and implementation of a parser of NNUF for standardized deep learning computation. We call the platform implemented with the neural network exchange framework (NNEF) standard as the NNUF. This framework provides platform-independent processes for configuring and training deep learning neural networks, where the independence is offered by the NNUF model. This model allows us to configure all components of neural network graphs. Our framework also allows the resulting graph to be easily shared with other platform-dependent descriptions which configure various neural network architectures in their own ways. This paper presents the details of the parser design, JavaCC-based implementation, and initial results.;https://dl.acm.org/doi/abs/10.1145/3212725.3212727;Rj-MVszbGIMJ
Damiani, E., & Frati, F. (2018). Towards conceptual models for machine learning computations. In Conceptual Modeling: 37th International Conference, ER 2018, Xi'an, China, October 22–25, 2018, Proceedings 37 (pp. 3-9). Springer International Publishing.;1_ml_machine_data_learning;2018;Towards Conceptual Models for Machine Learning Computations;Ernesto Damiani, Fulvio Frati; Lecture Notes in Computer Science book series (LNISA,volume 11157);We make the case for conceptual models that give the human designer full visibility and control over key aspects of ML applications, including input data preparation, training and inference of the ML models. Our models aim to: (i) achieve better documentation of ML analytics (ii) provide a foundation for a chain of trust in the ML analytics outcome (iii) provide a lever to enforce ethical and legal constraints within the ML pipeline. Representational models can dramatically increase reusability of large-scale ML analytics, while decreasing their roll-out time and cost. Also, they will support novel solutions to time-honored issues of analytics like non-uniform data veracity, privacy and latency profiles.;https://link.springer.com/chapter/10.1007/978-3-030-00847-5_1;Todo
Bacciu, D., Biggio, B., Lisboa, P. J., Martin Guerrero, J. D., Oneto, L., & Vellido Alcacena, A. (2019). Societal issues in machine learning: When learning from data is not enough. In ESANN 2019, 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning: Bruges, April 24-25-26, 2019: proceedings (pp. 455-464). European Symposium on Artificial Neural Networks (ESANN).;12_ai_ethical_ethic_intelligence;2019;Societal issues in machine learning: When learning from data is not enough;Davide Bacciu, Battista Biggio, Paulo JG Lisboa, Jose David Martin Guerrero, Luca Oneto, Alfredo Vellido Alcacena;ESANN 2019, 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning: Bruges, April 24-25-26, 2019: proceedings, 455-464, 2019;It has been argued that Artificial Intelligence (AI) is experiencing a fast process of commodification. Such characterization is on the interest of big IT companies, but it correctly reflects the current industrialization of AI. This phenomenon means that AI systems and products are reaching the society at large and, therefore, that societal issues related to the use of AI and Machine Learning (ML) cannot be ignored any longer. Designing ML models from this human-centered perspective means incorporating human-relevant requirements such as safety, fairness, privacy, and interpretability, but also considering broad societal issues such as ethics and legislation. These are essential aspects to foster the acceptance of ML-based technologies, as well as to ensure compliance with an evolving legislation concerning the impact of digital technologies on ethically and privacy sensitive matters. The ESANN special session for which this tutorial acts as an introduction aims to showcase the state of the art on these increasingly relevant topics among ML theoreticians and practitioners. For this purpose, we welcomed both solid contributions and preliminary relevant results showing the potential, the limitations and the challenges of new ideas, as well as refinements, or hybridizations among the different fields of research, ML and related approaches in facing real-world problems involving societal issues.;https://upcommons.upc.edu/handle/2117/179507;2dB8XHthGKUJ
Ducuing, C., Oneto, L., & Petralli, S. (2019). Fairness and Accountability of machine learning Models in Railway Market: are Applicable Railway Laws Up to Regulate Them?. https://www. elen. ucl. ac. be/esann/index. php? pg= welcome.;6_fairness_discrimination_bias_decision;2019;Fairness and Accountability of machine learning Models in Railway Market: are Applicable Railway Laws Up to Regulate Them?;Charlotte Ducuing, Luca Oneto, Simone Petralli;https://www. elen. ucl. ac. be/esann/index. php? pg= welcome, 2019;In this work we discuss whether the law is up to regulate the use of machine learning model in the context of the railway public transportation system. In particular, we deal with the problems of fairness and accountability of these models when exploited in the context of train traffic management. Railway sector-specific regulation, in their quality as network industry, hereby serves as a pilot. We show that, even where technological solutions are available, the law needs to keep up to support and accurately regulate the use of the technological solutions and we identify stumble points in this regard.;https://lirias.kuleuven.be/retrieve/526441;iH9HZGIsEmIJ
Galhotra, S., Brun, Y., & Meliou, A. (2017, August). Fairness testing: testing software for discrimination. In Proceedings of the 2017 11th Joint meeting on foundations of software engineering (pp. 498-510).;6_fairness_discrimination_bias_decision;2017;Fairness testing: testing software for discrimination;Sainyam Galhotra, Yuriy Brun, Alexandra Meliou;Proceedings of the 2017 11th Joint meeting on foundations of software engineering, 498-510, 2017;This paper defines software fairness and discrimination and develops a testing-based method for measuring if and how much software discriminates, focusing on causality in discriminatory behavior. Evidence of software discrimination has been found in modern software systems that recommend criminal sentences, grant access to financial products, and determine who is allowed to participate in promotions. Our approach, Themis, generates efficient test suites to measure discrimination. Given a schema describing valid system inputs, Themis generates discrimination tests automatically and does not require an oracle. We evaluate Themis on 20 software systems, 12 of which come from prior work with explicit focus on avoiding discrimination. We find that (1) Themis is effective at discovering software discrimination, (2) state-of-the-art techniques for removing discrimination from algorithms fail in many situations, at times discriminating against as much as 98% of an input subdomain, (3) Themis optimizations are effective at producing efficient test suites for measuring discrimination, and (4) Themis is more efficient on systems that exhibit more discrimination. We thus demonstrate that fairness testing is a critical aspect of the software development cycle in domains with possible discrimination and provide initial tools for measuring software discrimination.;https://dl.acm.org/doi/abs/10.1145/3106237.3106277;0vd3BmHCWaAJ
Du, X., Xie, X., Li, Y., Ma, L., Liu, Y., & Zhao, J. (2019, August). Deepstellar: Model-based quantitative analysis of stateful deep learning systems. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 477-487).;4_dl_testing_deep_network;2019;Deepstellar: Model-based quantitative analysis of stateful deep learning systems;Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, Jianjun Zhao;Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 477-487, 2019;Deep Learning (DL) has achieved tremendous success in many cutting-edge applications. However, the state-of-the-art DL systems still suffer from quality issues. While some recent progress has been made on the analysis of feed-forward DL systems, little study has been done on the Recurrent Neural Network (RNN)-based stateful DL systems, which are widely used in audio, natural languages and video processing, etc. In this paper, we initiate the very first step towards the quantitative analysis of RNN-based DL systems. We model RNN as an abstract state transition system to characterize its internal behaviors. Based on the abstract model, we design two trace similarity metrics and five coverage criteria which enable the quantitative analysis of RNNs. We further propose two algorithms powered by the quantitative measures for adversarial sample detection and coverage-guided test generation. We evaluate DeepStellar on four RNN-based systems covering image classification and automated speech recognition. The results demonstrate that the abstract model is useful in capturing the internal behaviors of RNNs, and confirm that (1) the similarity metrics could effectively capture the differences between samples even with very small perturbations (achieving 97% accuracy for detecting adversarial samples) and (2) the coverage criteria are useful in revealing erroneous behaviors (generating three times more adversarial samples than random testing and hundreds times more than the unrolling approach).;https://dl.acm.org/doi/abs/10.1145/3338906.3338954;R1r-ZW6SAJ0J
Zhao, H., Liang, J., Yin, X., Yang, L., Yang, P., & Wang, Y. (2018, October). Domain-specific ModelWare: to make the machine learning model reusable and reproducible. In Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (pp. 1-2).;1_ml_machine_data_learning;2018;Domain-specific modelware to make the machine learning model reusable and reproducible;Hui Zhao, Jimin Liang, Xuezhen Yin, Lingfeng Yang, Peili Yang, Yuhang Wang;Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement, 1-2, 2018;Machine learning task is a routine process including data collection, feature engineering, model training, hyper-parameters tuning, model evaluation and model deployment. The process is usually complex, iterated and time-consuming. Commonly, researchers seldom start building the machine model from scratch. They may select some well-known and well-trained models in similar task domains as the reference models. Then they try to tune the hyper-parameters and accelerate the iteration. Thus, some models are often reused and need to be reproduced by using new training dataset. Moreover, understanding the model and the iteration is more necessary. This scenario is very similar to that of software reuse. In this poster, we propose Modelware and argue the need of Modelware to make the machine learning model reusable and reproducible. We define the Modelware which is the reused object and develop a model repository to provide the model lineage management and model visit tool. The big data for building model is managed collaboratively so that the model can be reproduced. The iteration process to obtain the final optimized model is abstracted and implemented using a lightweight workflow. Finally, we take two different classification tasks as the demonstration.;https://dl.acm.org/doi/abs/10.1145/3239235.3267439;6HGX7aeB4GIJ
Papernot, N., McDaniel, P., Sinha, A., & Wellman, M. P. (2018, April). Sok: Security and privacy in machine learning. In 2018 IEEE European Symposium on Security and Privacy (EuroS&P) (pp. 399-414). IEEE.;5_adversarial_attack_example_model;2018;Sok: Security and privacy in machine learning;Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, Michael P. Wellman;2018 IEEE European Symposium on Security and Privacy (EuroS&P);Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive-new systems and models are being deployed in every domain imaginable, leading to widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date.We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. In particular, it is apparent that constructing a theoretical understanding of the sensitivity of modern ML algorithms to the data they analyze, à la PAC theory, will foster a science of security and privacy in ML.;https://ieeexplore.ieee.org/abstract/document/8406613;Todo
Arras, L., Arjona-Medina, J., Widrich, M., Montavon, G., Gillhofer, M., Müller, K. R., ... & Samek, W. (2019). Explaining and interpreting LSTMs. Explainable ai: Interpreting, explaining and visualizing deep learning, 211-238.;11_deep_network_model_layer;2019;Explaining and interpreting LSTMs;Leila Arras, José Arjona-Medina, Michael Widrich, Grégoire Montavon, Michael Gillhofer, Klaus-Robert Müller, Sepp Hochreiter, Wojciech Samek;Explainable ai: Interpreting, explaining and visualizing deep learning, 211-238, 2019;While neural networks have acted as a strong unifying force in the design of modern AI systems, the neural network architectures themselves remain highly heterogeneous due to the variety of tasks to be solved. In this chapter, we explore how to adapt the Layer-wise Relevance Propagation (LRP) technique used for explaining the predictions of feed-forward networks to the LSTM architecture used for sequential data modeling and forecasting. The special accumulators and gated interactions present in the LSTM require both a new propagation scheme and an extension of the underlying theoretical framework to deliver faithful explanations.;https://link.springer.com/chapter/10.1007/978-3-030-28954-6_11;7zimCioTCdgJ
Samek, W., & Müller, K. R. (2019). Towards explainable artificial intelligence. Explainable AI: interpreting, explaining and visualizing deep learning, 5-22.;3_explanation_model_machine_learning;2019;Towards explainable artificial intelligence;Wojciech Samek, Klaus-Robert Müller;Explainable AI: interpreting, explaining and visualizing deep learning, 5-22, 2019;In recent years, machine learning (ML) has become a key enabling technology for the sciences and industry. Especially through improvements in methodology, the availability of large databases and increased computational power, today’s ML algorithms are able to achieve excellent performance (at times even exceeding the human level) on an increasing number of complex tasks. Deep learning models are at the forefront of this development. However, due to their nested non-linear structure, these powerful models have been generally considered “black boxes”,Black-Box AI not providing any information about what exactly makes them arrive at their predictions. Since in many applications, e.g., in the medical domain, such lack of transparency may be not acceptable, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This introductory paper presents recent developments and applications in this field and makes a plea for a wider use of explainable learning algorithms in practice.;https://link.springer.com/chapter/10.1007/978-3-030-28954-6_1;3am4zxoEBvEJ
Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., ... & Gebru, T. (2019, January). Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency (pp. 220-229).;3_explanation_model_machine_learning;2019;Model cards for model reporting;Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru;Proceedings of the conference on fairness, accountability, and transparency, 220-229, 2019;Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.;https://dl.acm.org/doi/abs/10.1145/3287560.3287596;rW8B7kegIYAJ
Selbst, A. D., Boyd, D., Friedler, S. A., Venkatasubramanian, S., & Vertesi, J. (2019, January). Fairness and abstraction in sociotechnical systems. In Proceedings of the conference on fairness, accountability, and transparency (pp. 59-68).;6_fairness_discrimination_bias_decision;2019;Fairness and abstraction in sociotechnical systems;Andrew D Selbst, Danah Boyd, Sorelle A Friedler, Suresh Venkatasubramanian, Janet Vertesi;Proceedings of the conference on fairness, accountability, and transparency, 59-68, 2019;"A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce ""fair"" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five ""traps"" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.";https://dl.acm.org/doi/abs/10.1145/3287560.3287598;wmI-iBOrx7kJ
Bhatt, U., Xiang, A., Sharma, S., Weller, A., Taly, A., Jia, Y., ... & Eckersley, P. (2020, January). Explainable machine learning in deployment. In Proceedings of the 2020 conference on fairness, accountability, and transparency (pp. 648-657).;3_explanation_model_machine_learning;2020;Explainable machine learning in deployment;Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, JosÃ© MF Moura, Peter Eckersley;Proceedings of the 2020 conference on fairness, accountability, and transparency, 648-657, 2020;Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.;https://dl.acm.org/doi/abs/10.1145/3351095.3375624;FtzdTrwBM6UJ
Friedler, S. A., Scheidegger, C., Venkatasubramanian, S., Choudhary, S., Hamilton, E. P., & Roth, D. (2019, January). A comparative study of fairness-enhancing interventions in machine learning. In Proceedings of the conference on fairness, accountability, and transparency (pp. 329-338).;6_fairness_discrimination_bias_decision;2019;A comparative study of fairness-enhancing interventions in machine learning;Sorelle A Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam Choudhary, Evan P Hamilton, Derek Roth;Proceedings of the conference on fairness, accountability, and transparency, 329-338, 2019;Computers are increasingly used to make decisions that have significant impact on people's lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions that require investigation for these algorithms to receive broad adoption.We present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures and existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits) and to different forms of preprocessing, indicating that fairness interventions might be more brittle than previously thought.;https://dl.acm.org/doi/abs/10.1145/3287560.3287589;7wKXqWXrSAQJ
Sangroya, A., Anantaram, C., Rawat, M., & Rastogi, M. (2019, August). Using Formal Concept Analysis to Explain Black Box Deep Learning Classification Models. In FCA4AI@ IJCAI (pp. 19-26).;3_explanation_model_machine_learning;2019;Using Formal Concept Analysis to Explain Black Box Deep Learning Classification Models.;Amit Sangroya, C Anantaram, Mrinal Rawat, Mouli Rastogi;FCA4AI@ IJCAI, 19-26, 2019;Recently many machine learning based AI systems have been designed as black boxes. These are the systems that hide the internal logic from the users. Lack of transparency in decision making limits their use in various real world applications. In this paper, we propose a framework that utilizes formal concept analysis to explain AI models. We use classification analysis to study abnormalities in the data which is further used to explain the outcome of machine learning model. The ML method used to demonstrate the ideas is two class classification problem. We validate the proposed framework using a real world machine learning task: diabetes prediction. Our results show that using a formal concept analysis approach can result in better explanations.;https://inria.hal.science/hal-02431335/file/proceedings_fca4ai_2019.pdf#page=20;FsRdioa0R8sJ
Gutzen, R., Von Papen, M., Trensch, G., Quaglio, P., Grün, S., & Denker, M. (2018). Reproducible neural network simulations: statistical methods for model validation on the level of network activity data. Frontiers in neuroinformatics, 12, 90.;4_dl_testing_deep_network;2018;Reproducible neural network simulations: statistical methods for model validation on the level of network activity data;Robin Gutzen, Michael Von Papen, Guido Trensch, Pietro Quaglio, Sonja GrÃ¼n, Michael Denker;Frontiers in neuroinformatics 12, 90, 2018;Computational neuroscience relies on simulations of neural network models to bridge the gap between the theory of neural networks and the experimentally observed activity dynamics in the brain. The rigorous validation of simulation results against reference data is thus an indispensable part of any simulation workflow. Moreover, the availability of different simulation environments and levels of model description require also validation of model implementations against each other to evaluate their equivalence. Despite rapid advances in the formalized description of models, data, and analysis workflows, there is no accepted consensus regarding the terminology and practical implementation of validation workflows in the context of neural simulations. This situation prevents the generic, unbiased comparison between published models, which is a key element of enhancing reproducibility of computational research in neuroscience. In this study, we argue for the establishment of standardized statistical test metrics that enable the quantitative validation of network models on the level of the population dynamics. Despite the importance of validating the elementary components of a simulation, such as single cell dynamics, building networks from validated building blocks does not entail the validity of the simulation on the network scale. Therefore, we introduce a corresponding set of validation tests and present an example workflow that practically demonstrates the iterative model validation of a spiking neural network model against its reproduction on the SpiNNaker neuromorphic hardware system. We formally implement the workflow using a generic Python library that we introduce for validation tests on neural network activity data. Together with the companion study (Trensch et al., ), the work presents a consistent definition, formalization, and implementation of the verification and validation process for neural network simulations.;https://www.frontiersin.org/articles/10.3389/fninf.2018.00090/full;P1cnF7hQWfsJ
de Oliveira Werneck, R., de Almeida, W. R., Stein, B. V., Pazinato, D. V., Júnior, P. R. M., Penatti, O. A. B., ... & da Silva Torres, R. (2018). Kuaa: A unified framework for design, deployment, execution, and recommendation of machine learning experiments. Future Generation Computer Systems, 78, 59-76.;1_ml_machine_data_learning;2018;Kuaa: A unified framework for design, deployment, execution, and recommendation of machine learning experiments;Rafael de Oliveira Werneck, Waldir Rodrigues de Almeida, Bernardo Vecchia Stein, Daniel Vatanabe Pazinato, Pedro Ribeiro Mendes Júnior, Otávio Augusto Bizetto Penatti, Anderson Rocha, Ricardo da Silva Torres;Future Generation Computer Systems 78, 59-76, 2018;In this work, we propose Kuaa, a workflow-based framework that can be used for designing, deploying, and executing machine learning experiments in an automated fashion. This framework is able to provide a standardized environment for exploratory analysis of machine learning solutions, as it supports the evaluation of feature descriptors, normalizers, classifiers, and fusion approaches in a wide range of tasks involving machine learning. Kuaa also is capable of providing users with the recommendation of machine-learning workflows. The use of recommendations allows users to identify, evaluate, and possibly reuse previously defined successful solutions. We propose the use of similarity measures (e.g., Jaccard, Sørensen, and Jaro–Winkler) and learning-to-rank methods (LRAR) in the implementation of the recommendation service. Experimental results show that Jaro–Winkler yields the highest effectiveness performance with comparable results to those observed for LRAR, presenting the best alternative machine learning experiments to the user. In both cases, the recommendations performed are very promising and the developed framework might help users in different daily exploratory machine learning tasks.;https://www.sciencedirect.com/science/article/pii/S0167739X17301565;Ask5t0YNIjYJ
Adhikari, A., Tax, D. M., Satta, R., & Faeth, M. (2019, June). LEAFAGE: Example-based and Feature importance-based Explanations for Black-box ML models. In 2019 IEEE international conference on fuzzy systems (FUZZ-IEEE) (pp. 1-7). IEEE.;3_explanation_model_machine_learning;2019;LEAFAGE: Example-based and Feature importance-based Explanations for Black-box ML models;Ajaya Adhikari, David MJ Tax, Riccardo Satta, Matthias Faeth;2019 IEEE international conference on fuzzy systems (FUZZ-IEEE), 1-7, 2019;"Explainable Artificial Intelligence (XAI) is an emergent research field which tries to cope with the lack of transparency of AI systems, by providing human understandable explanations for the underlying Machine Learning models. This work presents a new explanation extraction method called LEAFAGE. Explanations are provided both in terms of feature importance and of similar classification examples. The latter is a well known strategy for problem solving and justification in social science. LEAFAGE leverages on the fact that the reasoning behind a single decision/prediction for a single data point is generally simpler to understand than the complete model; it produces explanations by generating simpler yet locally accurate approximations of the original model. LEAFAGE performs overall better than the current state of the art in terms of fidelity of the model approximation, in particular when Machine Learning models with non-linear decision boundaries are analysed. LEAFAGE was also tested in terms of usefulness for the user, an aspect still largely overlooked in the scientific literature. Results show interesting and partly counter-intuitive findings, such as the fact that providing no explanation is sometimes better than providing certain kinds of explanation.";https://ieeexplore.ieee.org/abstract/document/8858846/;lkhb9HO1WtYJ
Zhou, S. M., & Gan, J. Q. (2008). Low-level interpretability and high-level interpretability: a unified view of data-driven interpretable fuzzy system modelling. Fuzzy sets and systems, 159(23), 3091-3131.;3_explanation_model_machine_learning;2008;Low-level interpretability and high-level interpretability: a unified view of data-driven interpretable fuzzy system modelling;Shang-Ming Zhou, John Q Gan;Fuzzy sets and systems 159 (23), 3091-3131, 2008;This paper aims at providing an in-depth overview of designing interpretable fuzzy inference models from data within a unified framework. The objective of complex system modelling is to develop reliable and understandable models for human being to get insights into complex real-world systems whose first-principle models are unknown. Because system behaviour can be described naturally as a series of linguistic rules, data-driven fuzzy modelling becomes an attractive and widely used paradigm for this purpose. However, fuzzy models …;https://www.sciencedirect.com/science/article/pii/S0165011408002765;iVRQm3tfh5UJ
Fallon, C. K., & Blaha, L. M. (2018). Improving automation transparency: Addressing some of machine learning’s unique challenges. In Augmented Cognition: Intelligent Technologies: 12th International Conference, AC 2018, Held as Part of HCI International 2018, Las Vegas, NV, USA, July 15-20, 2018, Proceedings, Part I (pp. 245-254). Springer International Publishing.;3_explanation_model_machine_learning;2018;Improving automation transparency: Addressing some of machine learning’s unique challenges;Corey K Fallon, Leslie M Blaha;Augmented Cognition: Intelligent Technologies: 12th International Conference, AC 2018, Held as Part of HCI International 2018, Las Vegas, NV, USA, July 15-20, 2018, Proceedings …, 2018;A variety of factors can affect one’s reliance on an automated aid. Some of these factors include one’s perception of the system’s trustworthiness, such as perceived reliability of the system or one’s ability to understand the system’s underlying reasoning. A mismatch between the operator’s perception and the true capabilities and characteristics of the system can lead to inappropriate reliance on the tool. This improper use of the system can manifest as either underutilization of the technology or complacency resulting from over-trusting the system. Increasing an automated tool’s transparency is one approach that enables the operator to more appropriately rely on the technology. Transparent automated systems provide additional information that allows the user to see the system’s intent and understand its underlying processes and capabilities. Several researchers have developed frameworks to support the design of more transparent automation. However, these frameworks may not fully consider the particular challenges to transparency design introduced by automation that leverages machine learning. Like all automation, these systems can benefit from transparency. However, artificial intelligence poses new challenges that must be considered when designing for transparency. Unique considerations must be made in terms of the type, and amount or level of transparency information conveyed to the user.;https://link.springer.com/chapter/10.1007/978-3-319-91470-1_21;lcwmFXOoJQ8J
Abdollahi, B., & Nasraoui, O. (2018). Transparency in fair machine learning: the case of explainable recommender systems. Human and machine learning: visible, explainable, trustworthy and transparent, 21-35.;3_explanation_model_machine_learning;2018;Transparency in fair machine learning: the case of explainable recommender systems;Behnoush Abdollahi, Olfa Nasraoui;Human and machine learning: visible, explainable, trustworthy and transparent, 21-35, 2018;Machine Learning (ML) models are increasingly being used in many sectors, ranging from health and education to justice and criminal investigation. Therefore, building a fair and transparent model which conveys the reasoning behind its predictions is of great importance. This chapter discusses the role of explanation mechanisms in building fair machine learning models and explainable ML technique. We focus on the special case of recommender systems because they are a prominent example of a ML model that interacts directly with humans. This is in contrast to many other traditional decision making systems that interact with experts (e.g. in the health-care domain). In addition, we discuss the main sources of bias that can lead to biased and unfair models. We then review the taxonomy of explanation styles for recommender systems and review models that can provide explanations for their recommendations. We conclude by reviewing evaluation metrics for assessing the power ofExplainability explainability in recommender systems.;https://link.springer.com/chapter/10.1007/978-3-319-90403-0_2;VwXbCIENMKIJ
Isakov, M., Gadepally, V., Gettings, K. M., & Kinsy, M. A. (2019, September). Survey of attacks and defenses on edge-deployed neural networks. In 2019 IEEE High Performance Extreme Computing Conference (HPEC) (pp. 1-8). IEEE.;0_federated_learning_data_privacy;2019;Survey of attacks and defenses on edge-deployed neural networks;Mihailo Isakov, Vijay Gadepally, Karen M Gettings, Michel A Kinsy;2019 IEEE High Performance Extreme Computing Conference (HPEC), 1-8, 2019;Deep Neural Network (DNN) workloads are quickly moving from datacenters onto edge devices, for latency, privacy, or energy reasons. While datacenter networks can be protected using conventional cybersecurity measures, edge neural networks bring a host of new security challenges. Unlike classic IoT applications, edge neural networks are typically very compute and memory intensive, their execution is data-independent, and they are robust to noise and faults. Neural network models may be very expensive to develop, and can potentially reveal information about the private data they were trained on, requiring special care in distribution. The hidden states and outputs of the network can also be used in reconstructing user inputs, potentially violating users' privacy. Furthermore, neural networks are vulnerable to adversarial attacks, which may cause misclassifications and violate the integrity of the output. These properties add challenges when securing edge-deployed DNNs, requiring new considerations, threat models, priorities, and approaches in securely and privately deploying DNNs to the edge. In this work, we cover the landscape of attacks on, and defenses, of neural networks deployed in edge devices and provide a taxonomy of attacks and defenses targeting edge DNNs.;https://ieeexplore.ieee.org/abstract/document/8916519/;smEMcYGVQTYJ
Zhou, J., & Chen, F. (2018). 2D transparency space—bring domain users and machine learning experts together. Human and machine learning: Visible, explainable, trustworthy and transparent, 3-19.;3_explanation_model_machine_learning;2018;2D transparency space—bring domain users and machine learning experts together;Jianlong Zhou, Fang Chen;Human and machine learning: Visible, explainable, trustworthy and transparent, 3-19, 2018;Machine Learning (ML) is currently facing prolonged challenges with the user acceptance of delivered solutions as well as seeing system misuse, disuse, or even failure. These fundamental challenges can be attributed to the nature of the “black-box” of ML methods for domain users when offering ML-based solutions. That is, transparency of ML is essential for domain users to trust and use ML confidently in their practices. This chapter argues for a change in how we view the relationship between human and machine learning to translate ML results into impact. We present a two-dimensional transparency space which integrates domain users and ML experts together to make ML transparent. We identify typical Transparent ML (TML) challenges and discuss key obstacles to TML, which aim to inspire active discussions of making ML transparent with a systematic view in this timely field.;https://link.springer.com/chapter/10.1007/978-3-319-90403-0_1;wK7sR1TQH10J
Bellamy, R. K., Dey, K., Hind, M., Hoffman, S. C., Houde, S., Kannan, K., ... & Zhang, Y. (2018). AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. arXiv preprint arXiv:1810.01943.;6_fairness_discrimination_bias_decision;2018;AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias;Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy, John Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush R Varshney, Yunfeng Zhang;arXiv preprint arXiv:1810.01943, 2018;Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license {https://github.com/ibm/aif360). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (https://aif360.mybluemix.net) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.;https://arxiv.org/abs/1810.01943;-TTUanoLTjoJ
Bellamy, R. K., Dey, K., Hind, M., Hoffman, S. C., Houde, S., Kannan, K., ... & Zhang, Y. (2019). AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. IBM Journal of Research and Development, 63(4/5), 4-1.;6_fairness_discrimination_bias_decision;2019;AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias;Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra MojsiloviÄ‡, Seema Nagar, K Natesan Ramamurthy, John Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush R Varshney, Yunfeng Zhang;IBM Journal of Research and Development 63 (4/5), 4: 1-4: 15, 2019;Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This article introduces a new open-source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license ( https://github.com/ibm/aif360 ). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms for use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience that provides a gentle introduction to the concepts and capabilities for line-of-business users, researchers, and developers to extend the toolkit with their new algorithms and improvements and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.;https://ieeexplore.ieee.org/abstract/document/8843908/;_6NzzbmAT1EJ
Tolan, S., Miron, M., Gómez, E., & Castillo, C. (2019, June). Why machine learning may lead to unfairness: Evidence from risk assessment for juvenile justice in catalonia. In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law (pp. 83-92).;6_fairness_discrimination_bias_decision;2019;Why machine learning may lead to unfairness: Evidence from risk assessment for juvenile justice in catalonia;Songül Tolan, Marius Miron, Emilia Gómez, Carlos Castillo;Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law, 83-92, 2019;"In this paper we study the limitations of Machine Learning (ML) algorithms for predicting juvenile recidivism. Particularly, we are interested in analyzing the trade-off between predictive performance and fairness. To that extent, we evaluate fairness of ML models in conjunction with SAVRY, a structured professional risk assessment framework, on a novel dataset originated in Catalonia. In terms of accuracy on the prediction of recidivism, the ML models slightly outperform SAVRY; the results improve with more data or more features available for training (AUCROC of 0.64 with SAVRY vs. AUCROC of 0.71 with ML models). However, across three fairness metrics used in other studies, we find that SAVRY is in general fair, while the ML models tend to discriminate against male defendants, foreigners, or people of specific national groups. For instance, foreigners who did not recidivate are almost twice as likely to be wrongly classified as high risk by ML models than Spanish nationals. Finally, we discuss potential sources of this unfairness and provide explanations for them, by combining ML interpretability techniques with a thorough data analysis. Our findings provide an explanation for why ML techniques lead to unfairness in data-driven risk assessment, even when protected attributes are not used in training.";https://dl.acm.org/doi/abs/10.1145/3322640.3326705;_1OySblWU2EJ
Wang, G., Chen, X., & Xu, C. (2019, May). Adversarial watermarking to attack deep neural networks. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1962-1966). IEEE.;5_adversarial_attack_example_model;2019;Adversarial watermarking to attack deep neural networks;Gengxing Wang, Xinyuan Chen, Chang Xu;ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1962-1966, 2019;Watermark is one of the most fundamental approaches for avoiding potential copyright infringement activities. However, whether its introduction would effect the understanding of deep learning models remains unstudied. In this work, we propose a visible adversarial attack method that transforms and places a provided watermark on the target image to interfere the classification result from an Inception V3 model, which is pretrained on ImageNet. Specifically, the watermark is adjusted iteratively on location, transparency, color, angle and size which are determined by only 9 parameters. We define two types of attack to better simulate the watermark approaches in reality, respectively the watermark is constrained in either transparency or size. Experiments show that the generated adversarial samples are not only capable of fooling the Inception V3 model with high success rates, but also transferable to other models with high confidence, such as the Rekognition developed by Amazon.;https://ieeexplore.ieee.org/abstract/document/8682351/;5NB9z-bUOjIJ
Bore, N. K., Raman, R. K., Markus, I. M., Remy, S. L., Bent, O., Hind, M., ... & Weldemariam, K. (2019, May). Promoting distributed trust in machine learning and computational simulation. In 2019 IEEE International Conference on Blockchain and Cryptocurrency (ICBC) (pp. 311-319). IEEE.;0_federated_learning_data_privacy;2019;Promoting distributed trust in machine learning and computational simulation;Nelson Kibichii Bore, Ravi Kiran Raman, Isaac M Markus, Sekou L Remy, Oliver Bent, Michael Hind, Eleftheria K Pissadaki, Biplav Srivastava, Roman Vaculin, Kush R Varshney, Komminist Weldemariam;2019 IEEE International Conference on Blockchain and Cryptocurrency (ICBC), 311-319, 2019;Policy decisions are increasingly dependent on the outcomes of simulations and/or machine learning models. The ability to share and interact with these outcomes is relevant across multiple fields and is especially critical in the disease modeling community where models are often only accessible and workable to the researchers that generate them. This work presents a blockchain-enabled system that establishes a decentralized trust between parties involved in a modeling process. Utilizing the OpenMalaria framework, we demonstrate the ability to store, share and maintain auditable logs and records of each step in the simulation process, showing how to validate results generated by computational workers. We also show how the system monitors worker outputs to rank and identify faulty workers via comparison to nearest neighbors or historical reward spaces as a means of ensuring model quality.;https://ieeexplore.ieee.org/abstract/document/8751423/;OnIlK8q7LhcJ
Cammarota, R., Banerjee, I., & Rosenberg, O. (2018, November). Machine learning IP protection. In 2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD) (pp. 1-3). IEEE.;5_adversarial_attack_example_model;2018;Machine learning IP protection;Rosario Cammarota, Indranil Banerjee, Ofer Rosenberg;2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), 1-3, 2018;Machine learning, specifically deep learning is becoming a key technology component in application domains such as identity management, finance, automotive, and healthcare, to name a few. Proprietary machine learning models - Machine Learning IP - are developed and deployed at the network edge, end devices and in the cloud, to maximize user experience. With the proliferation of applications embedding Machine Learning IPs, machine learning models and hyper-parameters become attractive to attackers, and require protection. Major players in the semiconductor industry provide mechanisms on device to protect the IP at rest and during execution from being copied, altered, reverse engineered, and abused by attackers. In this work we explore system security architecture mechanisms and their applications to Machine Learning IP protection.;https://ieeexplore.ieee.org/abstract/document/8587658/;LYvjUnEe16wJ
Narasimhamurthy, M., Kushner, T., Dutta, S., & Sankaranarayanan, S. (2019, November). Verifying conformance of neural network models. In 2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD) (pp. 1-8). IEEE.;2_safety_system_autonomous_vehicle;2019;Verifying conformance of neural network models;Monal Narasimhamurthy, Taisa Kushner, Souradeep Dutta, Sriram Sankaranarayanan;2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), 1-8, 2019;Neural networks are increasingly used as data-driven models for a wide variety of physical systems such as ground vehicles, airplanes, human physiology and automobile engines. These models are in-turn used for designing and verifying autonomous systems. The advantages of using neural networks include the ability to capture characteristics of particular systems using the available data. This is particularly advantageous for medical systems, wherein the data collected from individuals can be used to design devices that are well-adapted to a particular individual's unique physiological characteristics. At the same time, neural network models remain opaque: their structure makes them hard to understand and interpret by human developers. One key challenge lies in checking that neural network models of processes are “conformant” to the well established scientific (physical, chemical and biological) laws that underlie these models. In this paper, we will show how conformance often fails in models that are otherwise accurate and trained using the best practices in machine learning, with potentially serious consequences. We motivate the need for learning and verifying key conformance properties in data-driven models of the human insulin-glucose system and data-driven automobile models. We survey verification approaches for neural networks that can hold the key to learning and verifying conformance.;https://ieeexplore.ieee.org/abstract/document/8942151/;NFJdhTNtcS4J
Grueneberg, K., Ko, B., Wood, D., Wang, X., Steuer, D., & Lim, Y. (2019, July). IoT Data Management System for Rapid Development of Machine Learning Models. In 2019 IEEE International Conference on Cognitive Computing (ICCC) (pp. 59-63). IEEE.;7_edge_computing_deep_learning;2019;IoT Data Management System for Rapid Development of Machine Learning Models;Keith Grueneberg, Bongjun Ko, David Wood, Xiping Wang, Dean Steuer, Yeonsup Lim;2019 IEEE International Conference on Cognitive Computing (ICCC), 59-63, 2019;Capturing and managing the data needed to build effective machine learning models for custom IoT environments requires a great deal of effort. The amount of data generated from IoT devices is abundant, but tools to find datasets appropriate for the desired models are lacking. This paper presents a data capture system and data management catalog with solutions addressing the challenges of curating IoT data applied to purpose-built machine learning deployments.;https://ieeexplore.ieee.org/abstract/document/8816926/;FZ0d2SZK8SUJ
Ács, D., & Coleşa, A. (2019, September). Securely exposing machine learning models to web clients using intel sgx. In 2019 IEEE 15th International Conference on Intelligent Computer Communication and Processing (ICCP) (pp. 161-168). IEEE.;5_adversarial_attack_example_model;2019;Securely exposing machine learning models to web clients using intel sgx;Dávid Ács, Adrian Coleşa;2019 IEEE 15th International Conference on Intelligent Computer Communication and Processing (ICCP), 161-168, 2019;Machine Learning (ML) methods are applied frequently to predict outcomes or features, that would otherwise require tedious manual work. ML models are usually deployed on Web servers, where end user can query them providing the input data. Server side deployment's shortcoming is that users' data must be sent to a server on each query, increasing network usage and leading to privacy/legal issues.In this paper we present a system which aims to ease the deployment of ML models on the client side of Web applications, while protecting the Intellectual Property (IP) of the model owner. Protection of the ML model is realized with Intel SGX which assures that a loaded model cannot be inspected by the end-user.;https://ieeexplore.ieee.org/abstract/document/8959635/;6I3IA9yaTlkJ
Fong, R. C., & Vedaldi, A. (2017). Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the IEEE international conference on computer vision (pp. 3429-3437).;11_deep_network_model_layer;2017;Interpretable explanations of black boxes by meaningful perturbation;Ruth C Fong, Andrea Vedaldi;Proceedings of the IEEE international conference on computer vision, 3429-3437, 2017;"As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks"" look"" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.";http://openaccess.thecvf.com/content_iccv_2017/html/Fong_Interpretable_Explanations_of_ICCV_2017_paper.html;c4FWzCfkGr8J
Scherzinger, S., Seifert, C., & Wiese, L. (2019, July). The best of both worlds: Challenges in linking provenance and explainability in distributed machine learning. In 2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS) (pp. 1620-1629). IEEE.;1_ml_machine_data_learning;2019;The best of both worlds: Challenges in linking provenance and explainability in distributed machine learning;Stefanie Scherzinger, Christin Seifert, Lena Wiese;2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS), 1620-1629, 2019;Machine learning experts prefer to think of their input as a single, homogeneous, and consistent data set. However, when analyzing large volumes of data, the entire data set may not be manageable on a single server, but must be stored on a distributed file system instead. Moreover, with the pressing demand to deliver explainable models, the experts may no longer focus on the machine learning algorithms in isolation, but must take into account the distributed nature of the data stored, as well as the impact of any data pre-processing steps upstream in their data analysis pipeline. In this paper, we make the point that even basic transformations during data preparation can impact the model learned, and that this is exacerbated in a distributed setting. We then sketch our vision of end-to-end explainability of the model learned, taking the pre-processing into account. In particular, we point out the potentials of linking the contributions of research on data provenance with the efforts on explainability in machine learning. In doing so, we highlight pitfalls we may experience in a distributed system on the way to generating more holistic explanations for our machine learning models.;https://ieeexplore.ieee.org/abstract/document/8885193/;mvpq2xGqQHAJ
Volkovs, M., Chiang, F., Szlichta, J., & Miller, R. J. (2014, March). Continuous data cleaning. In 2014 IEEE 30th international conference on data engineering (pp. 244-255). IEEE.;1_ml_machine_data_learning;2014;Continuous data cleaning;Maksims Volkovs, Fei Chiang, Jaroslaw Szlichta, RenÃ©e J Miller;2014 IEEE 30th international conference on data engineering, 244-255, 2014;In declarative data cleaning, data semantics are encoded as constraints and errors arise when the data violates the constraints. Various forms of statistical and logical inference can be used to reason about and repair inconsistencies (errors) in data. Recently, unified approaches that repair both errors in data and errors in semantics (the constraints) have been proposed. However, both data-only approaches and unified approaches are by and large static in that they apply cleaning to a single snapshot of the data and constraints. We introduce a continuous data cleaning framework that can be applied to dynamic data and constraint environments. Our approach permits both the data and its semantics to evolve and suggests repairs based on the accumulated evidence to date. Importantly, our approach uses not only the data and constraints as evidence, but also considers the past repairs chosen and applied by a user (user repair preferences). We introduce a repair classifier that predicts the type of repair needed to resolve an inconsistency, and that learns from past user repair preferences to recommend more accurate repairs in the future. Our evaluation shows that our techniques achieve high prediction accuracy and generate high quality repairs. Of independent interest, our work makes use of a set of data statistics that are shown to be sensitive to predicting particular repair types.;https://ieeexplore.ieee.org/abstract/document/6816655/;ilWZMb80f-wJ
Sigl, M. B. (2019, April). Don't Fear the REAPER: A Framework for Materializing and Reusing Deep-Learning Models. In 2019 IEEE 35th International Conference on Data Engineering (ICDE) (pp. 2091-2095). IEEE.;1_ml_machine_data_learning;2019;Don't Fear the REAPER: A Framework for Materializing and Reusing Deep-Learning Models;Melanie B Sigl;2019 IEEE 35th International Conference on Data Engineering (ICDE), 2091-2095, 2019;Training of Deep-Learning models represents a computationally intensive task, especially with high-dimensional and high-volume data, where training may take days or weeks. Current research mainly focuses on model materialization, workflow optimization, and lifecylce management. However, searching for a similar dataset in a dataset repository to reuse a suitable pre-built model has received little attention by the data-management community. A remarkable feature of reusing an already built model is that the iterative process of finding a suitable model is shortened, and additionally, only a small set of training data is required. Thus, transferring such knowledge has become an important issue in the Deep-Learning community. However, the relationship between the source and target task is not well understood yet. The aim of this research is to reduce training time of machine learning from a data-management perspective through model reuse, and shed some light on the above relationship in the case when reusing a model is appropriate. We propose a framework to aid data scientists in searching for a similar dataset given a new dataset and selecting a suitable model. We discuss a novel approach on how to determine similarity among datasets and how to chose an appropriate model. We are confident that with recent advances of model materialization and dataset repositories our research improves the knowledge on this topic.;https://ieeexplore.ieee.org/abstract/document/8731492/;CZhnbn9W5C4J
Frost, R., Paul, D., & Li, F. (2019, April). AI pro: Data processing framework for AI models. In 2019 IEEE 35th International Conference on Data Engineering (ICDE) (pp. 1980-1983). IEEE.;7_edge_computing_deep_learning;2019;AI pro: Data processing framework for AI models;Richie Frost, Debjyoti Paul, Feifei Li;2019 IEEE 35th International Conference on Data Engineering (ICDE), 1980-1983, 2019;We present AI Pro, an open-source framework for data processing with Artificial Intelligence (AI) models. Our framework empowers its users with immense capability to transform raw data into meaningful information with a simple configuration file. AI Pro's configuration file generates a data pipeline from start to finish with as many data transformations as desired. AI Pro supports major deep learning frameworks and Open Neural Network Exchange (ONNX), which allows users to choose models from any AI frameworks supported by ONNX. Its wide range of features and user friendly web interface grants everyone the opportunity to broaden their AI application horizons, irrespective of the user's technical expertise. AI Pro has all the quintessential features to perform end-to-end data processing, which we demonstrate using two real world scenarios.;https://ieeexplore.ieee.org/abstract/document/8731594/;zUx8julQeAcJ
Hu, H., Liu, Y., Wang, Z., & Lan, C. (2019, November). A distributed fair machine learning framework with private demographic data protection. In 2019 IEEE International Conference on Data Mining (ICDM) (pp. 1102-1107). IEEE.;6_fairness_discrimination_bias_decision;2019;A distributed fair machine learning framework with private demographic data protection;Hui Hu, Yijun Liu, Zhen Wang, Chao Lan;2019 IEEE International Conference on Data Mining (ICDM), 1102-1107, 2019;Fair machine learning has become a significant research topic with broad societal impact. However, most fair learning methods require direct access to personal demographic data, which is increasingly restricted to use for protecting user privacy (e.g. by the EU General Data Protection Regulation). In this paper, we propose a distributed fair learning framework for protecting the privacy of demographic data. We assume this data is privately held by a third party, which can communicate with the data center (responsible for model development) without revealing the demographic information. We propose a principled approach to design fair learning methods under this framework, exemplify four methods and show they consistently outperform their existing counterparts in both fairness and accuracy across two real-world data sets. We theoretically analyze the framework, and prove it can learn models with high fairness or high accuracy, with their trade-offs balanced by a threshold variable.;https://ieeexplore.ieee.org/abstract/document/8970908/;YWBy1fzag40J
Weber, C., Hirmer, P., Reimann, P., & Schwarz, H. (2019, May). A New Process Model for the Comprehensive Management of Machine Learning Models. In ICEIS (1) (pp. 415-422).;1_ml_machine_data_learning;2019;A New Process Model for the Comprehensive Management of Machine Learning Models.;Christian Weber, Pascal Hirmer, Peter Reimann, Holger Schwarz;ICEIS (1), 415-422, 2019;The management of machine learning models is an extremely challenging task. Hundreds of prototypical models are being built and just a few are mature enough to be deployed into operational enterprise information systems. The lifecycle of a model includes an experimental phase in which a model is planned, built and tested. After that, the model enters the operational phase that includes deploying, using, and retiring it. The experimental phase is well known through established process models like CRISP-DM or KDD. However, these models do not detail on the interaction between the experimental and the operational phase of machine learning models. In this paper, we provide a new process model to show the interaction points of the experimental and operational phase of a machine learning model. For each step of our process, we discuss according functions which are relevant to managing machine learning models.;https://www.scitepress.org/Papers/2019/77253/77253.pdf;BpQZAGxRRKUJ
Ahmad, M. A., Eckert, C., & Teredesai, A. (2018, August). Interpretable machine learning in healthcare. In Proceedings of the 2018 ACM international conference on bioinformatics, computational biology, and health informatics (pp. 559-560).;3_explanation_model_machine_learning;2018;Interpretable machine learning in healthcare;Muhammad Aurangzeb Ahmad, Carly Eckert, Ankur Teredesai;Proceedings of the 2018 ACM international conference on bioinformatics, computational biology, and health informatics, 559-560, 2018;This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.;https://dl.acm.org/doi/abs/10.1145/3233547.3233667;0-gwNu8O3TQJ
Goodfellow, I. J., Shlens, J., & Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.;5_adversarial_attack_example_model;2014;Explaining and harnessing adversarial examples;Ian J Goodfellow, Jonathon Shlens, Christian Szegedy;arXiv preprint arXiv:1412.6572, 2014;Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.;https://arxiv.org/abs/1412.6572;yt-_NiU95M4J
Zintgraf, L. M., Cohen, T. S., Adel, T., & Welling, M. (2017). Visualizing deep neural network decisions: Prediction difference analysis. arXiv preprint arXiv:1702.04595.;11_deep_network_model_layer;2017;Visualizing deep neural network decisions: Prediction difference analysis;Luisa M Zintgraf, Taco S Cohen, Tameem Adel, Max Welling;arXiv preprint arXiv:1702.04595, 2017;This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).;https://arxiv.org/abs/1702.04595;NMrsvFI43rgJ
Kindermans, P. J., Schütt, K. T., Alber, M., Müller, K. R., Erhan, D., Kim, B., & Dähne, S. (2017). Learning how to explain neural networks: Patternnet and patternattribution. arXiv preprint arXiv:1705.05598.;11_deep_network_model_layer;2017;Learning how to explain neural networks: Patternnet and patternattribution;Pieter-Jan Kindermans, Kristof T Schütt, Maximilian Alber, Klaus-Robert Müller, Dumitru Erhan, Been Kim, Sven Dähne;arXiv preprint arXiv:1705.05598, 2017;DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.;https://arxiv.org/abs/1705.05598;hAxX8SFGqCMJ
Song, Y., Kim, T., Nowozin, S., Ermon, S., & Kushman, N. (2017). Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. arXiv preprint arXiv:1710.10766.;5_adversarial_attack_example_model;2017;Pixeldefend: Leveraging generative models to understand and defend against adversarial examples;Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, Nate Kushman;arXiv preprint arXiv:1710.10766, 2017;Adversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. What makes them so special in the eyes of image classifiers? In this paper, we show empirically that adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models. Using statistical hypothesis testing, we find that modern neural density models are surprisingly good at detecting imperceptible image perturbations. Based on this discovery, we devised PixelDefend, a new approach that purifies a maliciously perturbed image by moving it back towards the distribution seen in the training data. The purified image is then run through an unmodified classifier, making our method agnostic to both the classifier and the attacking method. As a result, PixelDefend can be used to protect already deployed models and be combined with other model-specific defenses. Experiments show that our method greatly improves resilience across a wide variety of state-of-the-art attacking methods, increasing accuracy on the strongest attack from 63% to 84% for Fashion MNIST and from 32% to 70% for CIFAR-10.;https://arxiv.org/abs/1710.10766;l95gfWyvpIAJ
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., & Vladu, A. (2017). Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.;5_adversarial_attack_example_model;2017;Towards deep learning models resistant to adversarial attacks;Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu;arXiv preprint arXiv:1706.06083, 2017;Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.;https://arxiv.org/abs/1706.06083;4SLudL17lMQJ
Brendel, W., Rauber, J., & Bethge, M. (2017). Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. arXiv preprint arXiv:1712.04248.;5_adversarial_attack_example_model;2017;Decision-based adversarial attacks: Reliable attacks against black-box machine learning models;Wieland Brendel, Jonas Rauber, Matthias Bethge;arXiv preprint arXiv:1712.04248, 2017;Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at https://github.com/bethgelab/foolbox .;https://arxiv.org/abs/1712.04248;JRl1-z9B9xAJ
Liu, Y., Zhang, W., & Yu, N. (2019, July). Query-free embedding attack against deep learning. In 2019 IEEE International Conference on Multimedia and Expo (ICME) (pp. 380-386). IEEE.;5_adversarial_attack_example_model;2019;Query-free embedding attack against deep learning;Yujia Liu, Weiming Zhang, Nenghai Yu;2019 IEEE International Conference on Multimedia and Expo (ICME), 380-386, 2019;Deep neural networks are vulnerable to adversarial examples, subtly perturbed images which can fool networks to output incorrect classification results. To deceive deep learning models, in this paper, instead of utilizing the weakness of networks themselves, we present Embedding Attack, which is to attack the common image resizing operation in the deep learning preprocessing pipeline. By this attack, adversaries can embed a small target image into a benign image to produce adversarial examples without querying the target network. When the adversarial example is resized to the required shape, the embedded target image will be recovered. We design embedding attacks for three common image resizing methods and prove that our algorithms are optimal when the target image can be fully recovered. Furthermore, we design a universal embedding attack that enables adversarial examples to work under different resizing methods.;https://ieeexplore.ieee.org/abstract/document/8784813/;f4USKfbPXPsJ
Zemel, R., Wu, Y., Swersky, K., Pitassi, T., & Dwork, C. (2013, May). Learning fair representations. In International conference on machine learning (pp. 325-333). PMLR.;6_fairness_discrimination_bias_decision;2013;Learning fair representations;Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, Cynthia Dwork;International conference on machine learning, 325-333, 2013;"We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a whole), and individual fairness (similar individuals should be treated similarly). We formulate fairness as an optimization problem of finding a good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group. We show positive results of our algorithm relative to other known techniques, on three datasets. Moreover, we demonstrate several advantages to our approach. First, our intermediate representation can be used for other classification tasks (ie, transfer learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.";http://proceedings.mlr.press/v28/zemel13;V4-zAmi-ElUJ
Polyzotis, N., Roy, S., Whang, S. E., & Zinkevich, M. (2017, May). Data management challenges in production machine learning. In Proceedings of the 2017 ACM International Conference on Management of Data (pp. 1723-1726).;1_ml_machine_data_learning;2017;Data Management Challenges in Production Machine Learning;"Neoklis Polyzotis, Sudip Roy, Steven Euijong Whang, Martin Zinkevich
";SIGMOD '17: Proceedings of the 2017 ACM International Conference on Management of Data;The tutorial discusses data-management issues that arise in the context of machine learning pipelines deployed in production. Informed by our own experience with such largescale pipelines, we focus on issues related to understanding, validating, cleaning, and enriching training data. The goal of the tutorial is to bring forth these issues, draw connections to prior work in the database literature, and outline the open research questions that are not addressed by prior art.;https://dl.acm.org/doi/abs/10.1145/3035918.3054782;Todo
Koh, P. W., & Liang, P. (2017, July). Understanding black-box predictions via influence functions. In International conference on machine learning (pp. 1885-1894). PMLR.;3_explanation_model_machine_learning;2017;Understanding black-box predictions via influence functions;Pang Wei Koh, Percy Liang;International conference on machine learning, 1885-1894, 2017;How can we explain the predictions of a black-box model? In this paper, we use influence functions—a classic technique from robust statistics—to trace a model’s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.;http://proceedings.mlr.press/v70/koh17a?ref=https://githubhelp.com;_74zezE0AjAJ
Flaounas, I. (2017). Beyond the technical challenges for deploying machine learning solutions in a software company. arXiv preprint arXiv:1708.02363.;1_ml_machine_data_learning;2017;Beyond the technical challenges for deploying machine learning solutions in a software company;Ilias Flaounas;arXiv preprint arXiv:1708.02363, 2017;"Recently software development companies started to embrace Machine Learning (ML) techniques for introducing a series of advanced functionality in their products such as personalisation of the user experience, improved search, content recommendation and automation. The technical challenges for tackling these problems are heavily researched in literature. A less studied area is a pragmatic approach to the role of humans in a complex modern industrial environment where ML based systems are developed. Key stakeholders affect the system from inception and up to operation and maintenance. Product managers want to embed ""smart"" experiences for their users and drive the decisions on what should be built next; software engineers are challenged to build or utilise ML software tools that require skills that are well outside of their comfort zone; legal and risk departments may influence design choices and data access; operations teams are requested to maintain ML systems which are non-stationary in their nature and change behaviour over time; and finally ML practitioners should communicate with all these stakeholders to successfully build a reliable system. This paper discusses some of the challenges we faced in Atlassian as we started investing more in the ML space.";https://arxiv.org/abs/1708.02363;DUKnAW1JGPYJ
Jagielski, M., Kearns, M., Mao, J., Oprea, A., Roth, A., Sharifi-Malvajerdi, S., & Ullman, J. (2019, May). Differentially private fair learning. In International Conference on Machine Learning (pp. 3000-3008). PMLR.;6_fairness_discrimination_bias_decision;2019;Differentially private fair learning;Matthew Jagielski, Michael Kearns, Jieming Mao, Alina Oprea, Aaron Roth, Saeed Sharifi-Malvajerdi, Jonathan Ullman;International Conference on Machine Learning, 3000-3008, 2019;Motivated by settings in which predictive models may be required to be non-discriminatory with respect to certain attributes (such as race), but even collecting the sensitive attribute may be forbidden or restricted, we initiate the study of fair learning under the constraint of differential privacy. Our first algorithm is a private implementation of the equalized odds post-processing approach of (Hardt et al., 2016). This algorithm is appealingly simple, but must be able to use protected group membership explicitly at test time, which can be viewed as a form of “disparate treatment”. Our second algorithm is a differentially private version of the oracle-efficient in-processing approach of (Agarwal et al., 2018) which is more complex but need not have access to protected group membership at test time. We identify new tradeoffs between fairness, accuracy, and privacy that emerge only when requiring all three properties, and show that these tradeoffs can be milder if group membership may be used at test time. We conclude with a brief experimental evaluation.;https://proceedings.mlr.press/v97/jagielski19a.html;9n9bYnaHoCIJ
Odena, A., Olsson, C., Andersen, D., & Goodfellow, I. (2019, May). Tensorfuzz: Debugging neural networks with coverage-guided fuzzing. In International Conference on Machine Learning (pp. 4901-4911). PMLR.;4_dl_testing_deep_network;2019;Tensorfuzz: Debugging neural networks with coverage-guided fuzzing;Augustus Odena, Catherine Olsson, David Andersen, Ian Goodfellow;International Conference on Machine Learning, 4901-4911, 2019;Neural networks are difficult to interpret and debug. We introduce testing techniques for neural networks that can discover errors occurring only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how approximate nearest neighbor (ANN) algorithms can provide this coverage metric for neural networks. We then combine these methods with techniques for property-based testing (PBT). In PBT, one asserts properties that a function should satisfy and the system automatically generates tests exercising those properties. We then apply this system to practical goals including (but not limited to) surfacing broken loss functions in popular GitHub repositories and making performance improvements to TensorFlow. Finally, we release an open source library called TensorFuzz that implements the described techniques.;https://proceedings.mlr.press/v97/odena19a.html;6G--1TWLqrgJ
Zhao, S., Talasila, M., Jacobson, G., Borcea, C., Aftab, S. A., & Murray, J. F. (2018, December). Packaging and sharing machine learning models via the acumos ai open platform. In 2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA) (pp. 841-846). IEEE.;1_ml_machine_data_learning;2018;Packaging and sharing machine learning models via the acumos ai open platform;Shuai Zhao, Manoop Talasila, Guy Jacobson, Cristian Borcea, Syed Anwar Aftab, John F Murray;2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA), 841-846, 2018;Applying Machine Learning (ML) to business applications for automation usually faces difficulties when integrating diverse ML dependencies and services, mainly because of the lack of a common ML framework. In most cases, the ML models are developed for applications which are targeted for specific business domain use cases, leading to duplicated effort, and making reuse impossible. This paper presents Acumos, an open platform capable of packaging ML models into portable containerized microservices which can be easily shared via the platform's catalog, and can be integrated into various business applications. We present a case study of packaging sentiment analysis and classification ML models via the Acumos platform, permitting easy sharing with others. We demonstrate that the Acumos platform reduces the technical burden on application developers when applying machine learning models to their business applications. Furthermore, the platform allows the reuse of readily available ML microservices in various business domains.;https://ieeexplore.ieee.org/abstract/document/8614160/;b9Q8LQWefIwJ
Ahamed, F., & Farid, F. (2018, December). Applying internet of things and machine-learning for personalized healthcare: Issues and challenges. In 2018 International Conference on Machine Learning and Data Engineering (iCMLDE) (pp. 19-21). IEEE.;1_ml_machine_data_learning;2018;Applying internet of things and machine-learning for personalized healthcare: Issues and challenges;Farhad Ahamed, Farnaz Farid;2018 International Conference on Machine Learning and Data Engineering (iCMLDE), 19-21, 2018;Personalized Healthcare (PH) is a new patientoriented healthcare approach which expects to improve the traditional healthcare system. The focus of this new advancement is the patient data collected from patient Electronic health records (EHR), Internet of Things (IoT) sensor devices, wearables and mobile devices, web-based information and social media. PH applies Artificial Intelligence (AI) techniques to the collected dataset to improve disease progression technique, disease prediction, patient selfmanagement and clinical intervention. Machine learning techniques are widely used in this regard to develop analytic models. These models are integrated into different healthcare service applications and clinical decision support systems. These models mainly analyse the collected data from sensor devices and other sources to identify behavioral patterns and clinical conditions of the patient. For example, these models analyse the collected data to identify the patient's improvements, habits and anomaly in daily routine, changes in sleeping and mobility, eating, drinking and digestive pattern. Based on those patterns the healthcare applications and the clinical decision support systems recommend lifestyle advice, special treatment and care plans for the patient. The doctors and caregivers can also be engaged in the care plan process to validate lifestyle advice. However, there are many uncertainties and a grey area when it comes to applying machine learning in this context. Clinical, behaviour and lifestyle data in nature are very sensitive. There could be different types of biased involved in the process of data collection and interpretation. The training data model could have an older version of the dataset. All these could lead to an incorrect decision from the system without the user's knowledge. In this paper, some of the standards of the ML models reported in the recent research trends, identify the reliability issues and propose improvements.;https://ieeexplore.ieee.org/abstract/document/8613997/;zFYl9yz0mogJ
Li, J., Wang, G., Zhang, C., & Zhang, B. (2020). Deep Learning Training Management Platform Based on Distributed Technologies in Resource-Constrained Scenarios. In Advances in Natural Computation, Fuzzy Systems and Knowledge Discovery: Volume 1 (pp. 54-62). Springer International Publishing.;7_edge_computing_deep_learning;2020;Deep Learning Training Management Platform Based on Distributed Technologies in Resource-Constrained Scenarios;Jie Li, Guoteng Wang, Changsheng Zhang, Bin Zhang;Advances in Natural Computation, Fuzzy Systems and Knowledge Discovery: Volume 1, 54-62, 2020;Deep learning has attracted a lot of research attention in the past few years for its efficiency and accuracy. However, there exist two problems of their study. Firstly, the computing power of one single machine is limited and not suitable for handling with training deep learning models with massive cells. Secondly, it costs much to train models on different deep learning frameworks. Motivated by these problems, this paper proposed a deep learning training management platform based on distributed technologies, which integrates different kinds of deep learning frameworks through virtualization technologies and coordinates machines through distributed technologies. Specially, specific algorithms are proposed to solve the multi-task scheduling problem, the computing resources allocation problem and the fault tolerance problem in resource limited scenarios. It turns out that the platform can be widely used in small and medium-sized research teams.;https://link.springer.com/chapter/10.1007/978-3-030-32456-8_6;6bkwo-PYoPIJ
Xie, C., Qi, H., Ma, L., & Zhao, J. (2019, May). DeepVisual: a visual programming tool for deep learning systems. In 2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC) (pp. 130-134). IEEE.;4_dl_testing_deep_network;2019;DeepVisual: a visual programming tool for deep learning systems;Chao Xie, Hua Qi, Lei Ma, Jianjun Zhao;2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC), 130-134, 2019;As deep learning (DL) opens the way to many technological innovations in a wild range of fields, more and more researchers and developers from diverse domains start to take advantage of DLs. In many circumstances, a developer leverages a DL framework and programs the training software in the form of source code (e.g., Python, Java). However, not all of the developers across domains are skilled at programming. It is highly desirable to provide a way so that a developer could focus on how to design and optimize their DL systems instead of spending too much time on programming. To simplify the programming process towards saving time and effort especially for beginners, we propose and implement DeepVisual, a visual programming tool for the design and development of DL systems. DeepVisual represents each layer of a neural network as a component. A user can drag-and-drop components to design and build a DL model, after which the training code is automatically generated. Moreover, DeepVisual supports to extract the neural network architecture on the given source code as input. We implement DeepVisual as a PyCharm plugin and demonstrate its usefulness on two typical use cases.;https://ieeexplore.ieee.org/abstract/document/8813295/;fFZ9-fuOgvYJ
Pei, K., Zhu, L., Cao, Y., Yang, J., Vondrick, C., & Jana, S. (2017). Towards practical verification of machine learning: The case of computer vision systems. arXiv preprint arXiv:1712.01785.;4_dl_testing_deep_network;2017;Towards practical verification of machine learning: The case of computer vision systems;Kexin Pei, Linjie Zhu, Yinzhi Cao, Junfeng Yang, Carl Vondrick, Suman Jana;arXiv preprint arXiv:1712.01785, 2017;Due to the increasing usage of machine learning (ML) techniques in security- and safety-critical domains, such as autonomous systems and medical diagnosis, ensuring correct behavior of ML systems, especially for different corner cases, is of growing importance. In this paper, we propose a generic framework for evaluating security and robustness of ML systems using different real-world safety properties. We further design, implement and evaluate VeriVis, a scalable methodology that can verify a diverse set of safety properties for state-of-the-art computer vision systems with only blackbox access. VeriVis leverage different input space reduction techniques for efficient verification of different safety properties. VeriVis is able to find thousands of safety violations in fifteen state-of-the-art computer vision systems including ten Deep Neural Networks (DNNs) such as Inception-v3 and Nvidia's Dave self-driving system with thousands of neurons as well as five commercial third-party vision APIs including Google vision and Clarifai for twelve different safety properties. Furthermore, VeriVis can successfully verify local safety properties, on average, for around 31.7% of the test images. VeriVis finds up to 64.8x more violations than existing gradient-based methods that, unlike VeriVis, cannot ensure non-existence of any violations. Finally, we show that retraining using the safety violations detected by VeriVis can reduce the average number of violations up to 60.2%.;https://arxiv.org/abs/1712.01785;YaB46zy8MWIJ
Wang, J., Dong, G., Sun, J., Wang, X., & Zhang, P. (2019, May). Adversarial sample detection for deep neural network through model mutation testing. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) (pp. 1245-1256). IEEE.;5_adversarial_attack_example_model;2019;Adversarial sample detection for deep neural network through model mutation testing;Jingyi Wang, Guoliang Dong, Jun Sun, Xinyu Wang, Peixin Zhang;2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 1245-1256, 2019;Deep neural networks (DNN) have been shown to be useful in a wide range of applications. However, they are also known to be vulnerable to adversarial samples. By transforming a normal sample with some carefully crafted human imperceptible perturbations, even highly accurate DNN make wrong decisions. Multiple defense mechanisms have been proposed which aim to hinder the generation of such adversarial samples. However, a recent work show that most of them are ineffective. In this work, we propose an alternative approach to detect adversarial samples at runtime. Our main observation is that adversarial samples are much more sensitive than normal samples if we impose random mutations on the DNN. We thus first propose a measure of 'sensitivity' and show empirically that normal samples and adversarial samples have distinguishable sensitivity. We then integrate statistical hypothesis testing and model mutation testing to check whether an input sample is likely to be normal or adversarial at runtime by measuring its sensitivity. We evaluated our approach on the MNIST and CIFAR10 datasets. The results show that our approach detects adversarial samples generated by state-of-the-art attacking methods efficiently and accurately.;https://ieeexplore.ieee.org/abstract/document/8812047/;fiKnXVGWUU0J
Braiek, H. B., & Khomh, F. (2019, September). Deepevolution: A search-based testing approach for deep neural networks. In 2019 IEEE International Conference on Software Maintenance and Evolution (ICSME) (pp. 454-458). IEEE.;4_dl_testing_deep_network;2019;Deepevolution: A search-based testing approach for deep neural networks;Houssem Ben Braiek, Foutse Khomh;2019 IEEE International Conference on Software Maintenance and Evolution (ICSME), 454-458, 2019;The increasing inclusion of Deep Learning (DL) models in safety-critical systems such as autonomous vehicles have led to the development of multiple model-based DL testing techniques. One common denominator of these testing techniques is the automated generation of test cases, e.g., new inputs transformed from the original training data with the aim to optimize some test adequacy criteria. So far, the effectiveness of these approaches has been hindered by their reliance on random fuzzing or transformations that do not always produce test cases with a good diversity. To overcome these limitations, we propose, DeepEvolution, a novel search-based approach for testing DL models that relies on metaheuristics to ensure a maximum diversity in generated test cases. We assess the effectiveness of DeepEvolution in testing computer-vision DL models and found that it significantly increases the neuronal coverage of generated test cases. Moreover, using DeepEvolution, we could successfully find several corner-case behaviors. Finally, DeepEvolution outperformed Tensorfuzz (a coverage-guided fuzzing tool developed at Google Brain) in detecting latent defects introduced during the quantization of the models. These results suggest that search-based approaches can help build effective testing tools for DL systems.;https://ieeexplore.ieee.org/abstract/document/8919189/;0EocY-VoGrEJ
Amouzgar, F., Beheshti, A., Ghodratnama, S., Benatallah, B., Yang, J., & Sheng, Q. Z. (2019). isheets: A spreadsheet-based machine learning development platform for data-driven process analytics. In Service-Oriented Computing–ICSOC 2018 Workshops: ADMS, ASOCA, ISYyCC, CloTS, DDBS, and NLS4IoT, Hangzhou, China, November 12–15, 2018, Revised Selected Papers 16 (pp. 453-457). Springer International Publishing.;1_ml_machine_data_learning;2019;isheets: A spreadsheet-based machine learning development platform for data-driven process analytics;Farhad Amouzgar, Amin Beheshti, Samira Ghodratnama, Boualem Benatallah, Jian Yang, Quan Z Sheng;Service-Oriented Computing–ICSOC 2018 Workshops: ADMS, ASOCA, ISYyCC, CloTS, DDBS, and NLS4IoT, Hangzhou, China, November 12–15, 2018, Revised Selected Papers 16, 453-457, 2019;In the era of big data, the quality of services any organization provides largely depends on the quality of their data-driven processes. In this context, the goal of process data science, is to enable innovative forms of information processing that enable enhanced insight and decision making. For example, consider the data-driven and knowledge-intensive processes in Australian government’s office of the e-Safety commissioner, where the goal is to empowering all citizens to have safer, more positive experiences online. An example process, is to analyze the large amount of data generated every second on social networks to understand patterns of suicidal thoughts, online bullying and criminal/exterimist behaviour. Current processes leverage machine learning systems, e.g., to perform automatic mental-health-disorders detection from social networks. This approach is challenging for knowledge workers (end-user analysts) who have little knowledge of computer science to use machine learning solutions in their data-driven processes. In this paper, we present a novel platform, namely iSheets, that makes it easy for knowledge workers of all skill levels to use machine learning technology, the way people use spreadsheet. We present and develop a Machine Learning (ML) as a service framework and a spreadsheet-based ML development platform to enable knowledge workers in data-driven processes engage with ML tasks and uncover hidden insights through learning in an easy way.;https://link.springer.com/chapter/10.1007/978-3-030-17642-6_43;jdDBZfBLZ2sJ
Verma, D., White, G., & de Mel, G. (2019, July). Federated AI for the enterprise: a web services based implementation. In 2019 IEEE international conference on web services (ICWS) (pp. 20-27). IEEE.;0_federated_learning_data_privacy;2019;Federated AI for the enterprise: a web services based implementation;Dinesh Verma, Graham White, Geeth de Mel;2019 IEEE international conference on web services (ICWS), 20-27, 2019;Many enterprise solutions can greatly benefit from Machine Learning (ML) models that are created from cross-domain enterprise data. However, many enterprises cannot share data freely across different locations due to regulatory restrictions, performance issues in moving large data volumes, or requirements to maintain autonomy. In such situations, the enterprise can benefit from the concept of federated learning in which ML models are created at multiple different geographic sites. These are combined together at a federation server without the need to share data. Motivated by the fact that web-services based architectures provide a means for robust integration of cross-domain information, in this paper, we describe a solution to the federated learning problem using such an architecture. We specifically focus on the problems enterprises encounter in using distributed data and discuss how we solved those problems through the solution architecture.;https://ieeexplore.ieee.org/abstract/document/8818446/;FuybQldWty0J
Ferguson, M., Jeong, S., Law, K. H., Levitan, S., Narayanan, A., Burkhardt, R., ... & Lee, Y. T. T. (2019, August). A standardized representation of convolutional neural networks for reliable deployment of machine learning models in the manufacturing industry. In International Design Engineering Technical Conferences and Computers and Information in Engineering Conference (Vol. 59179, p. V001T02A005). American Society of Mechanical Engineers.;4_dl_testing_deep_network;2019;A standardized representation of convolutional neural networks for reliable deployment of machine learning models in the manufacturing industry;Max Ferguson, Seongwoon Jeong, Kincho H Law, Svetlana Levitan, Anantha Narayanan, Rainer Burkhardt, Tridivesh Jena, Yung-Tsun Tina Lee;International Design Engineering Technical Conferences and Computers and Information in Engineering Conference 59179, V001T02A005, 2019;The use of deep convolutional neural networks is becoming increasingly popular in the engineering and manufacturing sectors. However, managing the distribution of trained models is still a difficult task, partially due to the limitations of standardized methods for neural network representation. This paper seeks to address this issue by proposing a standardized format for convolutional neural networks, based on the Predictive Model Markup Language (PMML). A number of pre-trained ImageNet models are converted to the proposed PMML format to demonstrate the flexibility and utility of this format. These models are then fine-tuned to detect casting defects in Xray images. Finally, a scoring engine is developed to evaluate new input images against models in the proposed format. The utility of the proposed format and scoring engine is demonstrated by benchmarking the performance of the defect-detection models on a range of different computation platforms. The scoring engine and trained models are made available at https://github.com/maxkferg/python-pmml.;https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings-abstract/IDETC-CIE2019/V001T02A005/1069687;ZH3uKAi4UVEJ
Wachter, S., Mittelstadt, B., & Floridi, L. (2017). Why a right to explanation of automated decision-making does not exist in the general data protection regulation. International Data Privacy Law, 7(2), 76-99.;3_explanation_model_machine_learning;2017;Why a right to explanation of automated decision-making does not exist in the general data protection regulation;Sandra Wachter, Brent Mittelstadt, Luciano Floridi;International Data Privacy Law 7 (2), 76-99, 2017;In recent months, researchers, 1 government bodies, 2 and the media3 have claimed that a ‘right to explanation’of decisions made by automated and artificially intelligent algorithmic systems is legally mandated;https://academic.oup.com/idpl/article-pdf/doi/10.1093/idpl/ipx005/17932196/ipx005.pdf;zpFliP9Pq6oJ
Benrimoh, D., Israel, S., Perlman, K., Fratila, R., & Krause, M. (2018). Meticulous transparency—an evaluation process for an agile AI regulatory scheme. In Recent Trends and Future Technology in Applied Intelligence: 31st International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2018, Montreal, QC, Canada, June 25-28, 2018, Proceedings 31 (pp. 869-880). Springer International Publishing.;12_ai_ethical_ethic_intelligence;2018;Meticulous transparency—an evaluation process for an agile AI regulatory scheme;David Benrimoh, Sonia Israel, Kelly Perlman, Robert Fratila, Matthew Krause;Recent Trends and Future Technology in Applied Intelligence: 31st International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems, IEA …, 2018;Artificial intelligence (AI) poses both great potential and risk, as a rapidly developing and generally applicable technology. To ensure the ethical development and responsible use of AI, we outline a new ethical evaluation framework for usage by future regulators: Meticulous Transparency (MT). MT allows regulators to keep pace with technological progress by evaluating AI applications for their capabilities and the intentionality of developers, rather than evaluating conformity to static regulations or ethical codes of the underlying technologies themselves. MT shifts the focus of ethical evaluation from the technology itself to instead why it is being built, and potential consequences. MT assessment is reminiscent of a Research Ethics Board submission in medical research, with required explanation depending on the potential impact of the AI system. We propose the use of MT to transform AI-specific ethical quandaries into more familiar ethical questions, which society must then address.;https://link.springer.com/chapter/10.1007/978-3-319-92058-0_83;3mwrtIpSdF4J
Adadi, A., & Berrada, M. (2018). Peeking inside the black-box: a survey on explainable artificial intelligence (XAI). IEEE access, 6, 52138-52160.;12_ai_ethical_ethic_intelligence;2018;Peeking inside the black-box: a survey on explainable artificial intelligence (XAI);Amina Adadi, Mohammed Berrada;IEEE access 6, 52138-52160, 2018;At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.;https://ieeexplore.ieee.org/abstract/document/8466590/;xwIUjztrSQUJ
Akhtar, N., & Mian, A. (2018). Threat of adversarial attacks on deep learning in computer vision: A survey. Ieee Access, 6, 14410-14430.;5_adversarial_attack_example_model;2018;Threat of adversarial attacks on deep learning in computer vision: A survey;Naveed Akhtar, Ajmal Mian;Ieee Access 6, 14410-14430, 2018;Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.;https://ieeexplore.ieee.org/abstract/document/8294186/;RMVLSFgjBj0J
Hassan, A., Hamza, R., Yan, H., & Li, P. (2019). An efficient outsourced privacy preserving machine learning scheme with public verifiability. IEEE Access, 7, 146322-146330.;0_federated_learning_data_privacy;2019;An efficient outsourced privacy preserving machine learning scheme with public verifiability;Alzubair Hassan, Rafik Hamza, Hongyang Yan, Ping Li;IEEE Access 7, 146322-146330, 2019;Cloud computing has been widely applied in numerous applications for storage and data analytics tasks. However, cloud servers engaged through a third party cannot be fully trusted by multiple data users. Thus, security and privacy concerns become the main obstructions to use machine learning services, especially with multiple data providers. Additionally, some recent outsourcing machine learning schemes have been proposed in order to preserve the privacy of data providers. Yet, these schemes cannot satisfy the property of public verifiability. In this paper, we present an efficient privacy-preserving machine learning scheme for multiple data providers. The proposed scheme allows all participants in the system model to publicly verify the correctness of the encrypted data. Furthermore, a unidirectional proxy re-encryption (UPRE) scheme is employed to reduce the high computational costs along with multiple data providers. The cloud server embeds noise in the encrypted data, allowing the analytics to apply machine learning techniques and preserve the privacy of data providersâ€™ information. The results and experiments tests demonstrate that the proposed scheme has the ability to reduce computational costs and communication overheads.;https://ieeexplore.ieee.org/abstract/document/8862813/;iRGpuiZsOYUJ
Alves, J. M., Honório, L. M., & Capretz, M. A. (2019). Ml4iot: A framework to orchestrate machine learning workflows on internet of things data. IEEE Access, 7, 152953-152967.;1_ml_machine_data_learning;2019;Ml4iot: A framework to orchestrate machine learning workflows on internet of things data;José M Alves, Leonardo M Honório, Miriam AM Capretz;IEEE Access 7, 152953-152967, 2019;Internet of Things (IoT) applications generate vast amounts of real-time data. Temporal analysis of these data series to discover behavioural patterns may lead to qualified knowledge affecting a broad range of industries. Hence, the use of machine learning (ML) algorithms over IoT data has the potential to improve safety, economy, and performance in critical processes. However, creating ML workflows at scale is a challenging task that depends upon both production and specialized skills. Such tasks require investigation, understanding, selection, and implementation of specific ML workflows, which often lead to bottlenecks, production issues, and code management complexity and even then may not have a final desirable outcome. This paper proposes the Machine Learning Framework for IoT data (ML4IoT), which is designed to orchestrate ML workflows, particularly on large volumes of data series. The ML4IoT framework enables the implementation of several types of ML models, each one with a different workflow. These models can be easily configured and used through a simple pipeline. ML4IoT has been designed to use container-based components to enable training and deployment of various ML models in parallel. The results obtained suggest that the proposed framework can manage real-world IoT heterogeneous data by providing elasticity, robustness, and performance.;https://ieeexplore.ieee.org/abstract/document/8876834/;8FcAhJoiqi8J
Loyola-Gonzalez, O. (2019). Black-box vs. white-box: Understanding their advantages and weaknesses from a practical point of view. IEEE access, 7, 154096-154113.;3_explanation_model_machine_learning;2019;Black-box vs. white-box: Understanding their advantages and weaknesses from a practical point of view;Octavio Loyola-Gonzalez;IEEE access 7, 154096-154113, 2019;"Nowadays, in the international scientific community of machine learning, there exists an enormous discussion about the use of black-box models or explainable models; especially in practical problems. On the one hand, a part of the community defends that black-box models are more accurate than explainable models in some contexts, like image preprocessing. On the other hand, there exist another part of the community alleging that explainable models are better than black-box models because they can obtain comparable results and also they can explain these results in a language close to a human expert by using patterns. In this paper, advantages and weaknesses for each approach are shown; taking into account a state-of-the-art review for both approaches, their practical applications, trends, and future challenges. This paper shows that both approaches are suitable for solving practical problems, but experts in machine learning need to understand the input data, the problem to solve, and the best way for showing the output data before applying a machine learning model. Also, we propose some ideas for fusing both, explainable and black-box, approaches to provide better solutions to experts in real-world domains. Additionally, we show one way to measure the effectiveness of the applied machine learning model by using expert opinions jointly with statistical methods. Throughout this paper, we show the impact of using explainable and black-box models on the security and medical applications.";https://ieeexplore.ieee.org/abstract/document/8882211/;FMi_rAT1SRsJ
Yan, M., Wang, L., & Fei, A. (2019). ARTDL: Adaptive random testing for deep learning systems. IEEE Access, 8, 3055-3064.;4_dl_testing_deep_network;2019;ARTDL: Adaptive random testing for deep learning systems;Min Yan, Li Wang, Aiguo Fei;IEEE Access 8, 3055-3064, 2019;With recent breakthroughs in Deep Learning (DL), DL systems are increasingly deployed in safety-critical fields. Hence, some software testing methods are required to ensure the reliability and safety of DL systems. Since the rules of DL systems are inferred from training data, it is difficult to know the implementation rules about each behavior of DL systems. At the same time, Random Testing (RT) is a popular testing method and the knowledge about software implementation is not needed when we use RT. Therefore, RT is very suitable for the testing of DL systems. And the existing mechanisms for testing DL systems also depend heavily on RT by the labeled test data. In order to increase the effectiveness of RT for DL systems, we design, implement and evaluate the Adaptive Random Testing for DL systems (ARTDL), which is the first Adaptive Random Testing (ART) method to improve the effectiveness of RT for DL systems. ARTDL refers to the idea of ART. That is, fewer test cases are needed to detect failures by selecting the test case with the furthest distance from non-failure-causing test cases. Firstly, we propose the Feature-based Euclidean Distance (FED) as the distance metric that can be used to measure the difference between failure-causing inputs and non-failure-causing inputs. Secondly, we verify the availability of FED by presenting the failure pattern of DL models. Finally, we design ARTDL algorithm to generate the test cases that are more likely to cause failures based on the FED. We implement ARTDL to test top performing DL models in the field of image classification and automatic driving. The results show that, on average, the number of test cases used to find the first bug is reduced by 62.74% through ARTDL, compared with RT.;https://ieeexplore.ieee.org/abstract/document/8944083/;yieZGz1Hz68J
Spell, D. C., Wang, L. Y., Shomer, R. T., Nooraei, B., Waggoner, J., Zeng, X. H. T., ... & Kirsche, D. (2016, December). QED: Groupon's ETL management and curated feature catalog system for machine learning. In 2016 IEEE International Conference on Big Data (Big Data) (pp. 1639-1646). IEEE.;1_ml_machine_data_learning;2016;QED: Groupon's ETL management and curated feature catalog system for machine learning;Derrick C Spell, Ling-Yong Wang, Richard T Shomer, Bahador Nooraei, Jarrell Waggoner, Xiao-Han T Zeng, Jae Young Chung, Kai-Chen Cheng, Daniel Kirsche;2016 IEEE International Conference on Big Data (Big Data), 1639-1646, 2016;In today's technology industry where machine learning has become essential, the effectiveness of algorithms ultimately depends on a robust data pipeline, and fast model prototyping and tuning require easy feature discovery and consumption. Careful management of ETL processes and their produced datasets is key to both model development in the research stage and model execution in the production environment. In this paper we present QED, an ETL management and curated feature catalog system that provides robust, streamlined machine learning pipelines. First, QED promises dynamic, reliable, and timely data delivery to the production pipeline. Its enhanced ETL process persists data from upstream sources in local data stores and ensures their correctness. Second, in contrast to previous systems, QED is capable not only of producing a daily scoring dataset, but also a training dataset with minimized bias by preserving the historical observations of feature values. Third, QED's multiple data store design allows batch process of large datasets as well as fast random access to single records. Finally, its curated feature catalog system enables sharing and reuse of machine learning features. QED serves as the data backend for a variety of machine learning models that provide key insights into the global business, and optimize the daily operations of Groupon.;https://ieeexplore.ieee.org/abstract/document/7840776/;xYY7peVTi3QJ
Fritchman, K., Saminathan, K., Dowsley, R., Hughes, T., De Cock, M., Nascimento, A., & Teredesai, A. (2018, December). Privacy-preserving scoring of tree ensembles: A novel framework for AI in healthcare. In 2018 IEEE international conference on big data (Big Data) (pp. 2413-2422). Ieee.;0_federated_learning_data_privacy;2018;Privacy-preserving scoring of tree ensembles: A novel framework for AI in healthcare;Kyle Fritchman, Keerthanaa Saminathan, Rafael Dowsley, Tyler Hughes, Martine De Cock, Anderson Nascimento, Ankur Teredesai;2018 IEEE international conference on big data (Big Data), 2413-2422, 2018;Machine Learning (ML) techniques now impact a wide variety of domains. Highly regulated industries such as healthcare and finance have stringent compliance and data governance policies around data sharing. Advances in secure multiparty computation (SMC) for privacy-preserving machine learning (PPML) can help transform these regulated industries by allowing ML computations over encrypted data with personally identifiable information (PII). Yet very little of SMC-based PPML has been put into practice so far. In this paper we present the very first framework for privacy-preserving classification of tree ensembles with application in healthcare. We first describe the underlying cryptographic protocols that enable a healthcare organization to send encrypted data securely to a ML scoring service and obtain encrypted class labels without the scoring service actually seeing that input in the clear. We then describe the deployment challenges we solved to integrate these protocols in a cloud based scalable risk-prediction platform with multiple ML models for healthcare AI. Included are system internals, and evaluations of our deployment for supporting physicians to drive better clinical outcomes in an accurate, scalable, and provably secure manner. To the best of our knowledge, this is the first such applied framework with SMC-based privacy-preserving machine learning for healthcare.;https://ieeexplore.ieee.org/abstract/document/8622627/;o3XFKScqdwYJ
Usama, M., Qadir, J., Al-Fuqaha, A., & Hamdi, M. (2019). The adversarial machine learning conundrum: can the insecurity of ML become the achilles' heel of cognitive networks?. IEEE Network, 34(1), 196-203.;5_adversarial_attack_example_model;2019;The adversarial machine learning conundrum: can the insecurity of ML become the achilles' heel of cognitive networks?;Muhammad Usama, Junaid Qadir, Ala Al-Fuqaha, Mounir Hamdi;IEEE Network 34 (1), 196-203, 2019;The holy grail of networking is to create cognitive networks that organize, manage, and drive themselves. Such a vision now seems attainable thanks in large part to the progress in the field of machine learning (ML), which has now already disrupted a number of industries and revolutionized practically all fields of research. But are the ML models foolproof and robust to security attacks to be in charge of managing the network? Unfortunately, many modern ML models are easily misled by simple and easily-crafted adversarial perturbations, which does not bode well for the future of ML-based cognitive networks unless ML vulnerabilities for the cognitive networking environment are identified, addressed, and fixed. The purpose of this article is to highlight the problem of unsecure ML and to sensitize the readers to the danger of adversarial ML by showing how an easily crafted adversarial ML example can compromise the operations of the cognitive self-driving network. In this article, we demonstrate adversarial attacks on two simple yet representative cognitive networking applications (namely, intrusion detection and network traffic classification). We also provide some guidelines to design secure ML models for cognitive networks that are robust to adversarial attacks on the ML pipeline of cognitive networks.;https://ieeexplore.ieee.org/abstract/document/8884228/;Q56LugtQtHEJ
Xing, E. P., Ho, Q., Dai, W., Kim, J. K., Wei, J., Lee, S., ... & Yu, Y. (2015, August). Petuum: A new platform for distributed machine learning on big data. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1335-1344).;1_ml_machine_data_learning;2015;Petuum: A new platform for distributed machine learning on big data;Eric P Xing, Qirong Ho, Wei Dai, Jin-Kyu Kim, Jinliang Wei, Seunghak Lee, Xun Zheng, Pengtao Xie, Abhimanu Kumar, Yaoliang Yu;Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1335-1344, 2015;"How can one build a distributed framework that allows efficient deployment of a wide spectrum of modern advanced machine learning (ML) programs for industrial-scale problems using Big Models (100s of billions of parameters) on Big Data (terabytes or petabytes)- Contemporary parallelization strategies employ fine-grained operations and scheduling beyond the classic bulk-synchronous processing paradigm popularized by MapReduce, or even specialized operators relying on graphical representations of ML programs. The variety of approaches tends to pull systems and algorithms design in different directions, and it remains difficult to find a universal platform applicable to a wide range of different ML programs at scale. We propose a general-purpose framework that systematically addresses data- and model-parallel challenges in large-scale ML, by leveraging several fundamental properties underlying ML programs that make them different from conventional operation-centric programs: error tolerance, dynamic structure, and nonuniform convergence; all stem from the optimization-centric nature shared in ML programs' mathematical definitions, and the iterative-convergent behavior of their algorithmic solutions. These properties present unique opportunities for an integrative system design, built on bounded-latency network synchronization and dynamic load-balancing scheduling, which is efficient, programmable, and enjoys provable correctness guarantees. We demonstrate how such a design in light of ML-first principles leads to significant performance improvements versus well-known implementations of several ML programs, allowing them to run in much less time and at considerably larger model sizes, on modestly-sized computer clusters.";https://dl.acm.org/doi/abs/10.1145/2783258.2783323;Glh4FAkoMgkJ
Yuan, X., He, P., Zhu, Q., & Li, X. (2019). Adversarial examples: Attacks and defenses for deep learning. IEEE transactions on neural networks and learning systems, 30(9), 2805-2824.;5_adversarial_attack_example_model;2019;Adversarial examples: Attacks and defenses for deep learning;Xiaoyong Yuan, Pan He, Qile Zhu, Xiaolin Li;IEEE transactions on neural networks and learning systems 30 (9), 2805-2824, 2019;With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks (DNNs) have been recently found vulnerable to well-designed input samples called adversarial examples. Adversarial perturbations are imperceptible to human but can easily fool DNNs in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying DNNs in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for DNNs, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples. In addition, three major challenges in adversarial examples and the potential solutions are discussed.;https://ieeexplore.ieee.org/abstract/document/8611298/;HaxX9euwGzoJ
Shokri, R. (2019, July). Trusting machine learning: Privacy, robustness, and transparency challenges. In Proceedings of the ACM Workshop on Information Hiding and Multimedia Security (pp. 150-150).;3_explanation_model_machine_learning;2019;Trusting machine learning: Privacy, robustness, and transparency challenges;Reza Shokri;Proceedings of the ACM Workshop on Information Hiding and Multimedia Security, 150-150, 2019;Machine learning algorithms have shown an unprecedented predictive power for many complex learning tasks. As they are increasingly being deployed in large scale critical applications for processing various types of data, new questions related to their trustworthiness would arise. Can machine learning algorithms be trusted to have access to individualsâ€™ sensitive data? Can they be robust against noisy or adversarially perturbed data? Can we reliably interpret their learning process, and explain their predictions? In this talk, I will go over the challenges of building trustworthy machine learning algorithms in centralized and distributed (federated) settings, and will discuss the inter-relation between privacy, robustness, and interpretability.;https://dl.acm.org/doi/pdf/10.1145/3335203.3335728;q7bxQyO-wb8J
Shahriari, K., & Shahriari, M. (2017, July). IEEE standard review—Ethically aligned design: A vision for prioritizing human wellbeing with artificial intelligence and autonomous systems. In 2017 IEEE Canada International Humanitarian Technology Conference (IHTC) (pp. 197-201). IEEE.;12_ai_ethical_ethic_intelligence;2017;IEEE standard review—Ethically aligned design: A vision for prioritizing human wellbeing with artificial intelligence and autonomous systems;Kyarash Shahriari, Mana Shahriari;2017 IEEE Canada International Humanitarian Technology Conference (IHTC), 197-201, 2017;In September 2009, the IEEE Board of Directors approved the new IEEE tagline - Advancing Technology for Humanity - as recommended by the IEEE Public Visibility Committee. Aligned with the IEEE tagline, IEEE Standards Association takes the initiative to address ethics in engineering design under “Ethically Aligned Design: A Vision for Prioritizing Human Wellbeing with Artificial Intelligence and Autonomous Systems” focusing on Artificial Intelligence and Autonomous Systems (AI/AS). The intention is to cover, as much as possible, the ethical concerns on AI/AS through a rigorous regard to the problem from different perspectives. The ultimate objective of this ongoing initiative is to provide guidelines/procedures/standards to prioritize human wellbeing in the forthcoming evolutions on artificial intelligence and autonomous systems. This article reviews different aspects addressed in Version 1 of this initiative.;https://ieeexplore.ieee.org/abstract/document/8058187/;hCyWRAec5fQJ
Biran, O., & Cotton, C. (2017, August). Explanation and justification in machine learning: A survey. In IJCAI-17 workshop on explainable AI (XAI) (Vol. 8, No. 1, pp. 8-13).;3_explanation_model_machine_learning;2017;Explanation and justification in machine learning: A survey;Or Biran, Courtenay Cotton;IJCAI-17 workshop on explainable AI (XAI) 8 (1), 8-13, 2017;We present a survey of the research concerning explanation and justification in the Machine Learning literature and several adjacent fields. Within Machine Learning, we differentiate between two main branches of current research: interpretable models, and prediction interpretation and justification.;http://www.cs.columbia.edu/~orb/papers/xai_survey_paper_2017.pdf;EgBUK9T20hMJ
Thelisson, E. (2017, August). Towards Trust, Transparency and Liability in AI/AS systems. In IJCAI (pp. 5215-5216).;12_ai_ethical_ethic_intelligence;2017;Towards Trust, Transparency and Liability in AI/AS systems.;Eva Thelisson;IJCAI, 5215-5216, 2017;The research problem being investigated in this article is how to develop governance mechanisms and collective decision-making processes at a global level for Artificial Intelligence (AI) systems and Autonomous systems (AS), which would enhance confidence in AI and AS.;https://www.ijcai.org/proceedings/2017/0767.pdf;cn-FlVc_XsAJ
Epstein, Z., Payne, B. H., Shen, J. H., Hong, C. J., Felbo, B., Dubey, A., ... & Rahwan, I. (2018). TuringBox: An experimental platform for the evaluation of AI systems. In IJCAI 2018 (pp. 5826-5828). International Joint Conferences on Artificial Intelligence.;12_ai_ethical_ethic_intelligence;2018;TuringBox: An experimental platform for the evaluation of AI systems;Ziv Epstein, Blakeley H Payne, Judy Hanwen Shen, Casey Jisoo Hong, Bjarke Felbo, Abhimanyu Dubey, Matthew Groh, Nick Obradovich, Manuel Cebrian, Iyad Rahwan;IJCAI 2018, 5826-5828, 2018;We introduce TuringBox, a platform to democratize the study of AI. On one side of the platform, AI contributors upload existing and novel algorithms to be studied scientifically by others. On the other side, AI examiners develop and post machine intelligence tasks to evaluate and characterize the outputs of algorithms. We outline the architecture of such a platform, and describe two interactive case studies of algorithmic auditing on the platform.;https://pure.mpg.de/rest/items/item_3020343/component/file_3036194/content;Lq-e1T3nqOoJ
Yu, F., Qin, Z., Liu, C., Zhao, L., Wang, Y., & Chen, X. (2019). Interpreting and evaluating neural network robustness. arXiv preprint arXiv:1905.04270.;5_adversarial_attack_example_model;2019;Interpreting and evaluating neural network robustness;Fuxun Yu, Zhuwei Qin, Chenchen Liu, Liang Zhao, Yanzhi Wang, Xiang Chen;arXiv preprint arXiv:1905.04270, 2019;"Recently, adversarial deception becomes one of the most considerable threats to deep neural networks. However, compared to extensive research in new designs of various adversarial attacks and defenses, the neural networks' intrinsic robustness property is still lack of thorough investigation. This work aims to qualitatively interpret the adversarial attack and defense mechanism through loss visualization, and establish a quantitative metric to evaluate the neural network model's intrinsic robustness. The proposed robustness metric identifies the upper bound of a model's prediction divergence in the given domain and thus indicates whether the model can maintain a stable prediction. With extensive experiments, our metric demonstrates several advantages over conventional adversarial testing accuracy based robustness estimation: (1) it provides a uniformed evaluation to models with different structures and parameter scales; (2) it over-performs conventional accuracy based robustness estimation and provides a more reliable evaluation that is invariant to different test settings; (3) it can be fast generated without considerable testing cost.";https://arxiv.org/abs/1905.04270;_DFMq0v88wYJ
Byrne, R. M. (2019, August). Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from Human Reasoning. In IJCAI (pp. 6276-6282).;3_explanation_model_machine_learning;2019;Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from Human Reasoning.;Ruth MJ Byrne;IJCAI, 6276-6282, 2019;Counterfactuals about what could have happened are increasingly used in an array of Artificial Intelligence (AI) applications, and especially in explainable AI (XAI). Counterfactuals can aid the provision of interpretable models to make the decisions of inscrutable systems intelligible to developers and users. However, not all counterfactuals are equally helpful in assisting human comprehension. Discoveries about the nature of the counterfactuals that humans create are a helpful guide to maximize the effectiveness of counterfactual use in AI.;https://www.researchgate.net/profile/Ruth-Byrne-3/publication/334844529_Counterfactuals_in_Explainable_Artificial_Intelligence_XAI_Evidence_from_Human_Reasoning/links/6123d4160c2bfa282a6505dd/Counterfactuals-in-Explainable-Artificial-Intelligence-XAI-Evidence-from-Human-Reasoning.pdf;VXiQo0tMOewJ
Patterson, C., Galluppi, F., Rast, A., & Furber, S. (2012, June). Visualising large-scale neural network models in real-time. In The 2012 International Joint Conference on Neural Networks (IJCNN) (pp. 1-8). IEEE.;11_deep_network_model_layer;2012;Visualising large-scale neural network models in real-time;Cameron Patterson, Francesco Galluppi, Alexander Rast, Steve Furber;The 2012 International Joint Conference on Neural Networks (IJCNN), 1-8, 2012;As models of neural networks scale in concert with increasing computational performance, gaining insight into their operation becomes increasingly important. This paper proposes an efficient and generalised method to access simulation data via in-system aggregation, providing visualised representation at all layers of the network in real-time. Enabling neural networks for real-time visualisation allows a user to gain insight into the network dynamics of their systems as they operate over time. This visibility also permits users (or a computational agent) to determine whether early intervention is required to adjust parameters, or even to terminate operation of experimental networks that are not operating correctly. Conventionally the determination of correctness would occur post-simulation, so with sufficient `in-flight' insight, a significant advantage may be obtained, and compute time minimised. For this paper we apply the real-time visualisation platform to the SpiNNaker programmable neuromimetic system and a variety of neural network models. The visualisation platform is shown to be capable across a range of diverse simulations, and at supporting differing layers of network abstraction, requiring minimal configuration to represent each model. The resulting general-purpose visualisation platform for neural networks, is effective at presenting data to users in order to aid their comprehension of the network dynamics during operation, and scales from small to biologically-significant network sizes.;https://ieeexplore.ieee.org/abstract/document/6252490/;eTS-NeJeqcwJ
Zhou, J., & Chen, F. (2015). Making machine learning useable. International Journal of Intelligent Systems Technologies and Applications, 14(2), 91-109.;1_ml_machine_data_learning;2015;Making machine learning useable;Jianlong Zhou, Fang Chen;International Journal of Intelligent Systems Technologies and Applications 14 (2), 91-109, 2015;Despite the recognised value of machine learning (ML) techniques and high expectation of applying ML techniques within various applications, users often find it difficult to effectively apply ML techniques in practise because of complicated interfaces between ML algorithms and users. This paper focuses on investigating making ML useable from the point of view of how human-computer interaction (HCI) techniques benefit ML in order to simplify the interface between users and ML algorithms. We formulate possible research directions in making ML useable based on human factors, decision making and trust in ML. We strongly believe that a trustworthy decision making based on ML results, which is the ultimate goal of ML-based applications, contributes to the overall application performance and makes ML more useable. Two case studies of measurable decision making and revealing internal states of ML process are presented to show how HCI techniques are used to make ML useable.;https://www.inderscienceonline.com/doi/abs/10.1504/IJISTA.2015.074069;TppHTh_Ix5UJ
Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision (pp. 618-626).;11_deep_network_model_layer;2017;Grad-cam: Visual explanations from deep networks via gradient-based localization;Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra;Proceedings of the IEEE international conference on computer vision, 618-626, 2017;We propose a technique for producing'visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach-Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for'dog'or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families:(1) CNNs with fully-connected layers (eg VGG),(2) CNNs used for structured outputs (eg captioning),(3) CNNs used in tasks with multi-modal inputs (eg VQA) or reinforcement learning, and needs no architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations),(b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task,(c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a'stronger'deep network from a'weaker'one even when both make identical predictions. Our code is available at https://github. com/ramprs/grad-cam/along with a demo on CloudCV [2] 1 and video at youtu. be/COjUB9Izk6E.;http://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html;Ad045yn4zKMJ
Cueva-Lovelle, J. M., García-Díaz, V., Pelayo, G., Bustelo, C., & Pascual-Espada, J. (2015). Towards a standard-based domain-specific platform to solve machine learning-based problems.;1_ml_machine_data_learning;2015;Towards a standard-based domain-specific platform to solve machine learning-based problems;Juan Manuel Cueva-Lovelle, Vicente García-Díaz, G Pelayo, Cristina Bustelo, Jórdan Pascual-Espada;International Journal of Interactive Multimedia and Artificial Intelligence (IJIMAI), 2015;Machine learning is one of the most important subfields of computer science and can be used to solve a variety of interesting artificial intelligence problems. There are different languages, framework and tools to define the data needed to solve machine learning-based problems. However, there is a great number of very diverse alternatives which makes it difficult the intercommunication, portability and re-usability of the definitions, designs or algorithms that any developer may create. In this paper, we take the first step towards a language and a development environment independent of the underlying technologies, allowing developers to design solutions to solve machine learning-based problems in a simple and fast way, automatically generating code for other technologies. That can be considered a transparent bridge among current technologies. We rely on Model-Driven Engineering approach, focusing on the creation of models to abstract the definition of artifacts from the underlying technologies.;https://reunir.unir.net/handle/123456789/10169;uwcNYKOlDXsJ
Semerikov, S., Teplytskyi, I., Yechkalo, Y., Markova, O., Soloviev, V., & Kiv, A. (2019). Computer simulation of neural networks using spreadsheets: Dr. Anderson, welcome back.;1_ml_machine_data_learning;2019;Computer Simulation of Neural Networks Using Spreadsheets: Dr. Anderson, Welcome Back;S Semerikov, I Teplytskyi, Y Yechkalo, O Markova…;Todo;The authors of the given article continue the series presented by the 2018 paper “Computer Simulation of Neural Networks Using Spreadsheets: The Dawn of the Age of Camelot”. This time, they consider mathematical informatics as the basis of higher engineering education fundamentalization. Mathematical informatics deals with smart simulation, information security, long-term data storage and big data management, artificial intelligence systems, etc. The authors suggest studying basic principles of mathematical informatics by applying cloud-oriented means of various levels including those traditionally considered supplementary – spreadsheets. The article considers ways of building neural network models in cloud-oriented spreadsheets, Google Sheets. The model is based on the problem of classifying multi-dimensional data provided in “The Use of Multiple Measurements in Taxonomic Problems” by R. A. Fisher. Edgar Anderson’s role in collecting and preparing the data in the 1920s-1930s is discussed as well as some peculiarities of data selection. There are presented data on the method of multi-dimensional data presentation in the form of an ideograph developed by Anderson and considered one of the first efficient ways of data visualization.;http://ds.knu.edu.ua/jspui/handle/123456789/988;Todo
Nguyen, A., Yosinski, J., & Clune, J. (2019). Understanding neural networks via feature visualization: A survey. Explainable AI: interpreting, explaining and visualizing deep learning, 55-76.;11_deep_network_model_layer;2019;Understanding neural networks via feature visualization: A survey;Anh Nguyen, Jason Yosinski, Jeff Clune;Explainable AI: interpreting, explaining and visualizing deep learning, 55-76, 2019;"A neuroscience method to understanding the brain is to find and study the preferred stimuli that highly activate an individual cell or groups of cells. Recent advances in machine learning enable a family of methods to synthesize preferred stimuli that cause a neuron in an artificial or biological brain to fire strongly. Those methods are known as Activation Maximization (AM) [10] or Feature Visualization via Optimization. In this chapter, we (1) review existing AM techniques in the literature; (2) discuss a probabilistic interpretation for AM; and (3) review the applications of AM in debugging and explaining networks.";https://link.springer.com/chapter/10.1007/978-3-030-28954-6_4;gT3piFHknA4J
Oh, S. J., Schiele, B., & Fritz, M. (2019). Towards reverse-engineering black-box neural networks. Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, 121-144.;5_adversarial_attack_example_model;2019;Towards reverse-engineering black-box neural networks;Seong Joon Oh, Bernt Schiele, Mario Fritz;Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, 121-144, 2019;Much progress in interpretable AI is built around scenarios where the user, one who interprets the model, has a full ownership of the model to be diagnosed. The user either owns the training data and computing resources to train an interpretable model herself or owns a full access to an already trained model to be interpreted post-hoc. In this chapter, we consider a less investigated scenario of diagnosing black-box neural networks, where the user can only send queries and read off outputs. Black-box access is a common deployment mode for many public and commercial models, since internal details, such as architecture, optimisation procedure, and training data, can be proprietary and aggravate their vulnerability to attacks like adversarial examples. We propose a method for exposing internals of black-box models and show that the method is surprisingly effective at inferring a diverse set of internal information. We further show how the exposed internals can be exploited to strengthen adversarial examples against the model. Our work starts an important discussion on the security implications of diagnosing deployed models with limited accessibility. The code is available at goo.gl/MbYfsv .;https://link.springer.com/chapter/10.1007/978-3-030-28954-6_7;cx3gj4N1FLUJ
Ras, G., van Gerven, M., & Haselager, P. (2018). Explanation methods in deep learning: Users, values, concerns and challenges. Explainable and interpretable models in computer vision and machine learning, 19-36.;3_explanation_model_machine_learning;2018;Explanation methods in deep learning: Users, values, concerns and challenges;GabriÃ«lle Ras, Marcel van Gerven, Pim Haselager;Explainable and interpretable models in computer vision and machine learning, 19-36, 2018;Issues regarding explainable AI involve four components: users, laws and regulations, explanations and algorithms. Together these components provide a context in which explanation methods can be evaluated regarding their adequacy. The goal of this chapter is to bridge the gap between expert users and lay users. Different kinds of users are identified and their concerns revealed, relevant statements from the General Data Protection Regulation are analyzed in the context of Deep Neural Networks (DNNs), a taxonomy for the classification of existing explanation methods is introduced, and finally, the various classes of explanation methods are analyzed to verify if user concerns are justified. Overall, it is clear that (visual) explanations can be given about various aspects of the influence of the input on the output. However, it is noted that explanation methods or interfaces for lay users are missing and we speculate which criteria these methods/interfaces should satisfy. Finally it is noted that two important concerns are difficult to address with explanation methods: the concern about bias in datasets that leads to biased DNNs, as well as the suspicion about unfair outcomes.;https://link.springer.com/chapter/10.1007/978-3-319-98131-4_2;kKkWt-zQah0J
Goebel, R., Chander, A., Holzinger, K., Lecue, F., Akata, Z., Stumpf, S., ... & Holzinger, A. (2018). Explainable AI: the new 42?. In Machine Learning and Knowledge Extraction: Second IFIP TC 5, TC 8/WG 8.4, 8.9, TC 12/WG 12.9 International Cross-Domain Conference, CD-MAKE 2018, Hamburg, Germany, August 27–30, 2018, Proceedings 2 (pp. 295-303). Springer International Publishing.;3_explanation_model_machine_learning;2018;Explainable AI: the new 42?;Randy Goebel, Ajay Chander, Katharina Holzinger, Freddy Lecue, Zeynep Akata, Simone Stumpf, Peter Kieseberg, Andreas Holzinger;Machine Learning and Knowledge Extraction: Second IFIP TC 5, TC 8/WG 8.4, 8.9, TC 12/WG 12.9 International Cross-Domain Conference, CD-MAKE 2018, Hamburg, Germany, August 27–30 …, 2018;"Explainable AI is not a new field. Since at least the early exploitation of C.S. Pierce’s abductive reasoning in expert systems of the 1980s, there were reasoning architectures to support an explanation function for complex AI systems, including applications in medical diagnosis, complex multi-component design, and reasoning about the real world. So explainability is at least as old as early AI, and a natural consequence of the design of AI systems. While early expert systems consisted of handcrafted knowledge bases that enabled reasoning over narrowly well-defined domains (e.g., INTERNIST, MYCIN), such systems had no learning capabilities and had only primitive uncertainty handling. But the evolution of formal reasoning architectures to incorporate principled probabilistic reasoning helped address the capture and use of uncertain knowledge.There has been recent and relatively rapid success of AI/machine learning solutions arises from neural network architectures. A new generation of neural methods now scale to exploit the practical applicability of statistical and algebraic learning approaches in arbitrarily high dimensional spaces. But despite their huge successes, largely in problems which can be cast as classification problems, their effectiveness is still limited by their un-debuggability, and their inability to “explain” their decisions in a human understandable and reconstructable way. So while AlphaGo or DeepStack can crush the best humans at Go or Poker, neither program has any internal model of its task; its representations defy interpretation by humans, there is no mechanism to explain their actions and behaviour, and furthermore, there is no obvious instructional value ... the high performance systems can not help humans improve.Even when we understand the underlying mathematical scaffolding of current machine learning architectures, it is often impossible to get insight into the internal working of the models; we need explicit modeling and reasoning tools to explain how and why a result was achieved. We also know that a significant challenge for future AI is contextual adaptation, i.e., systems that incrementally help to construct explanatory models for solving real-world problems. Here it would be beneficial not to exclude human expertise, but to augment human intelligence with artificial intelligence.";https://link.springer.com/chapter/10.1007/978-3-319-99740-7_21;a6IKitU1GboJ
Arrieta, A. B., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., ... & Herrera, F. (2020). Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information fusion, 58, 82-115.;3_explanation_model_machine_learning;2020;Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI;Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, Raja Chatila, Francisco Herrera;Information fusion 58, 82-115, 2020;In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.;https://www.sciencedirect.com/science/article/pii/S1566253519308103;-pzy9CQ3YAQJ
Ma, X., Zhang, F., Chen, X., & Shen, J. (2018). Privacy preserving multi-party computation delegation for deep learning in cloud computing. Information Sciences, 459, 103-116.;0_federated_learning_data_privacy;2018;Privacy preserving multi-party computation delegation for deep learning in cloud computing;Xu Ma, Fangguo Zhang, Xiaofeng Chen, Jian Shen;Information Sciences 459, 103-116, 2018;The recent advances in deep learning have improved the state of the art in artificial intelligence, and one of the most important stimulants of this success is the large volume of data. Although collaborative learning can improve the learning accuracy by incorporating more datasets into the learning process, serious privacy issues have also emerged from the training data. In this paper, we propose a new framework for privacy-preserving multi-party deep learning in cloud computing, where the large volume of training data is distributed among many parties. Our system enables multiple parties to learn the same neural network model, which is generated based on the aggregate dataset, and the privacy of the local dataset and learning model is protected against the cloud server. Extensive analysis shows that our schemes satisfy the security requirements of verifiability and privacy. Our implementation and experiments demonstrate that our system has a manageable computational efficiency and can be applied to a wide range of privacy-sensitive areas in deep learning.;https://www.sciencedirect.com/science/article/pii/S0020025518303608;_JUK73Bbh8AJ
Quemy, A. (2020). Two-stage optimization for machine learning workflow. Information Systems, 92, 101483.;1_ml_machine_data_learning;2020;Two-stage optimization for machine learning workflow;Alexandre Quemy;Information Systems 92, 101483, 2020;Machine learning techniques play a preponderant role in dealing with massive amount of data and are employed in almost every possible domain. Building a high quality machine learning model to be deployed in production is a challenging task, from both, the subject matter experts and the machine learning practitioners.For a broader adoption and scalability of machine learning systems, the construction and configuration of machine learning workflow need to gain in automation. In the last few years, several techniques have been developed in this …;https://www.sciencedirect.com/science/article/pii/S0306437919305356;iLbSPRrCXIsJ
Freitas, J., Ribeiro, J., Baldewijns, D., Oliveira, S., & Braga, D. (2018). Machine Learning Powered Data Platform for High-Quality Speech and NLP Workflows. In INTERSPEECH (pp. 1962-1963).;1_ml_machine_data_learning;2018;Machine Learning Powered Data Platform for High-Quality Speech and NLP Workflows.;JoÃ£o Freitas, Jorge Ribeiro, Daan Baldewijns, Sara Oliveira, Daniela Braga;INTERSPEECH, 1962-1963, 2018;Machine learning (ML) models-like deep neural networksrequire substantial amounts of training data. Also, the training dataset should be properly annotated to obtain satisfactory results. This paper describes a platform designed to create high-quality datasets. By using data workflows adapted for speech technologies and natural language processing systems, the user can collect and enrich speech and text data. Depending on the end goal, the data is passed through multiple processing steps based on human input and ML services. To guarantee data quality, the platform combines several mechanisms like language tests, real-time audits, and user behavior into several ML models that act as quality gateways.;;UXl67lWzzCcJ
Meloni, P., Loi, D., Deriu, G., Pimentel, A. D., Sapra, D., Moser, B., ... & Palumbo, F. (2018, October). ALOHA: an architectural-aware framework for deep learning at the edge. In Proceedings of the workshop on INTelligent embedded systems architectures and applications (pp. 19-26).;1_ml_machine_data_learning;2018;ALOHA: an architectural-aware framework for deep learning at the edge;Paolo Meloni, Daniela Loi, Gianfranco Deriu, Andy D Pimentel, Dolly Sapra, Bernhard Moser, Natalia Shepeleva, Francesco Conti, Luca Benini, Oscar Ripolles, David Solans, Maura Pintor, Battista Biggio, Todor Stefanov, Svetlana Minakova, Nikolaos Fragoulis, Ilias Theodorakopoulos, Michael Masin, Francesca Palumbo;Proceedings of the workshop on INTelligent embedded systems architectures and applications, 19-26, 2018;Novel Deep Learning (DL) algorithms show ever-increasing accuracy and precision in multiple application domains. However, some steps further are needed towards the ubiquitous adoption of this kind of instrument. First, effort and skills required to develop new DL models, or to adapt existing ones to new use-cases, are hardly available for small- and medium-sized businesses. Second, DL inference must be brought at the edge, to overcome limitations posed by the classically-used cloud computing paradigm. This requires implementation on low-energy computing nodes, often heterogenous and parallel, that are usually more complex to program and to manage. This work describes the ALOHA framework, that proposes a solution to these issue by means of an integrated tool flow that automates most phases of the development process. The framework introduces architecture-awareness, considering the target inference platform very early, already during algorithm selection, and driving the optimal porting of the resulting embedded application. Moreover it considers security, power efficiency and adaptiveness as main objectives during the whole development process.;https://dl.acm.org/doi/abs/10.1145/3285017.3285019;jmzwG8Oe5jYJ
Wei, Y., & Low, J. X. (2020). expanAI: A Smart End-to-End Platform for the Development of AI Applications. In Internet of Vehicles. Technologies and Services Toward Smart Cities: 6th International Conference, IOV 2019, Kaohsiung, Taiwan, November 18–21, 2019, Proceedings 6 (pp. 361-365). Springer International Publishing.;1_ml_machine_data_learning;2020;expanAI: A Smart End-to-End Platform for the Development of AI Applications;Yongmei Wei, Jia Xin Low;Internet of Vehicles. Technologies and Services Toward Smart Cities: 6th International Conference, IOV 2019, Kaohsiung, Taiwan, November 18â€“21, 2019, Proceedings 6, 361-365, 2020;Building Modern Artificial Intelligence (AI) applications is a complicated process involving data preparation, model selection, and intensive training over large-scale data. It usually requires expertise in various domains, namely resource management, distribute storage, parallel computing, machine learning and deep learning. Acquiring all these skills for many small and medium companies to build an efficient AI application can be extremely hard. ExpanAI is proposed as a smart end-to-end platform for building efficient AI applications. ExpanAI provides a set of microservices to abstract away low-level implementation, like infrastructure and resource management, from the end users. Frequently used middleware, such as Spark, Kafka, Cassandra, etc., are first-class residences in the ExpanAI and are always available to users. Furthermore, ExpanAI introduces a smart interpreter to provide easy-to-use interface to execute data-intensive jobs. This interpreter automatically optimizes the execution plans according to the profile of the data and available resources. Lastly, a workflow optimization recommender is also proposed to conduct self-analysis over all jobs and automatically generates reports to suggest ways to improve performance or to avoid failures.;https://link.springer.com/chapter/10.1007/978-3-030-38651-1_29;R2JDbVrM7xgJ
Moor, L., Bitter, L., De Prado, M., Pazos, N., & Ouerhani, N. (2019, October). IoT meets distributed AI-Deployment scenarios of Bonseyes AI applications on FIWARE. In 2019 IEEE 38th International Performance Computing and Communications Conference (IPCCC) (pp. 1-2). IEEE.;1_ml_machine_data_learning;2019;IoT meets distributed AI-Deployment scenarios of Bonseyes AI applications on FIWARE;Lucien Moor, Lukas Bitter, Miguel De Prado, Nuria Pazos, Nabil Ouerhani;2019 IEEE 38th International Performance Computing and Communications Conference (IPCCC), 1-2, 2019;Bonseyes is an Artificial Intelligence (AI) platform composed of a Data Marketplace, a Deep Learning Toolbox, and Developer Reference Platforms with the aim of facilitating tech and non-tech companies a rapid adoption of AI as an enabler for their business. Bonseyes provides methods and tools to speed up the development and deployment of AI solutions on low power Internet of Things (IoT) devices, embedded computing systems, and data centre servers. In this work, we address the deployment and the integration of Bonseyes AI applications in a wider enterprise application landscape involving different applications and services. We leverage the well-established IoT platform FIWARE to integrate the Bonseyes AI applications into an enterprise ecosystem. This paper presents two AI application deployment and integration scenarios using FIWARE. The first scenario addresses use cases where edge devices have enough compute power to run the AI applications and there is only need to transmit the results to the enterprise ecosystem. The second scenario copes with use cases where an edge device may delegate most of the computation to an external/cloud server. Further, we employ FIWARE IoT Agent generic enabler to manage all edge devices related to Bonseyes AI applications. Both scenarios have been validated on concrete use cases and demonstrators.;https://ieeexplore.ieee.org/abstract/document/8958742/;JzhVbWjDoPUJ
Zhou, J., Khawaja, M. A., Li, Z., Sun, J., Wang, Y., & Chen, F. (2016). Making machine learning useable by revealing internal states update-a transparent approach. International Journal of Computational Science and Engineering, 13(4), 378-389.;1_ml_machine_data_learning;2016;Making machine learning useable by revealing internal states update - a transparent approach;Jianlong Zhou, M. Asif Khawaja, Zhidong Li, Jinjun Sun, Yang Wang, Fang Chen;International Journal of Computational Science and EngineeringVol. 13, No. 4;Machine learning (ML) techniques are often found difficult to apply effectively in practice because of their complexities. Therefore, making ML useable is emerging as one of active research fields recently. Furthermore, an ML algorithm is still a 'black-box'. This 'black-box' approach makes it difficult for users to understand complicated ML models. As a result, the user is uncertain about the usefulness of ML results and this affects the effectiveness of ML methods. This paper focuses on making a 'black-box' ML process transparent by presenting real-time internal status update of the ML process to users explicitly. A user study was performed to investigate the impact of revealing internal status update to users on the easiness of understanding data analysis process, meaningfulness of real-time status update, and convincingness of ML results. The study showed that revealing of the internal states of ML process can help improve easiness of understanding the data analysis process, make real-time status update more meaningful, and make ML results more convincing.;https://www.inderscienceonline.com/doi/abs/10.1504/IJCSE.2016.080214;Todo
Gale, W., Oakden-Rayner, L., Carneiro, G., Palmer, L. J., & Bradley, A. P. (2019, April). Producing radiologist-quality reports for interpretable deep learning. In 2019 IEEE 16th international symposium on biomedical imaging (ISBI 2019) (pp. 1275-1279). IEEE.;11_deep_network_model_layer;2019;Producing Radiologist-Quality Reports for Interpretable Deep Learning;William Gale, Luke Oakden-Rayner, Gustavo Carneiro, Lyle J Palmer, Andrew P Bradley;2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019);Current approaches to explaining the decisions of deep learning systems for medical tasks have focused on visualising the elements that have contributed to each decision. We argue that such approaches are not enough to “open the black box” of medical decision making systems because they are missing a key component that has been used as a standard communication tool between doctors for centuries: language. We propose a model-agnostic interpretability method that involves training a simple recurrent neural network model to produce descriptive sentences to clarify the decision of deep learning classifiers. We test our method on the task of detecting hip fractures from frontal pelvic x-rays. This process requires minimal additional labelling despite producing text containing elements that the original deep learning classification model was not specifically trained to detect. The experimental results show that: 1) the sentences produced by our method consistently contain the desired information, 2) the generated sentences are preferred by the cohort of doctors tested compared to current tools that create saliency maps, and 3) the combination of visualisations and generated text is better than either alone.;https://ieeexplore.ieee.org/abstract/document/8759236;Todo
Ma, L., Zhang, F., Sun, J., Xue, M., Li, B., Juefei-Xu, F., ... & Wang, Y. (2018, October). Deepmutation: Mutation testing of deep learning systems. In 2018 IEEE 29th international symposium on software reliability engineering (ISSRE) (pp. 100-111). IEEE.;4_dl_testing_deep_network;2018;Deepmutation: Mutation testing of deep learning systems;Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao Xie, Li Li, Yang Liu, Jianjun Zhao, Yadong Wang;2018 IEEE 29th international symposium on software reliability engineering (ISSRE), 100-111, 2018;Deep learning (DL) defines a new data-driven programming paradigm where the internal system logic is largely shaped by the training data. The standard way of evaluating DL models is to examine their performance on a test dataset. The quality of the test dataset is of great importance to gain confidence of the trained models. Using an inadequate test dataset, DL models that have achieved high test accuracy may still lack generality and robustness. In traditional software testing, mutation testing is a well-established technique for quality evaluation of test suites, which analyzes to what extent a test suite detects the injected faults. However, due to the fundamental difference between traditional software and deep learning-based software, traditional mutation testing techniques cannot be directly applied to DL systems. In this paper, we propose a mutation testing framework specialized for DL systems to measure the quality of test data. To do this, by sharing the same spirit of mutation testing in traditional software, we first define a set of source-level mutation operators to inject faults to the source of DL (i.e., training data and training programs). Then we design a set of model-level mutation operators that directly inject faults into DL models without a training process. Eventually, the quality of test data could be evaluated from the analysis on to what extent the injected faults could be detected. The usefulness of the proposed mutation testing techniques is demonstrated on two public datasets, namely MNIST and CIFAR-10, with three DL models.;https://ieeexplore.ieee.org/abstract/document/8539073/;GQX2lF0WQrkJ
Lécué, F., Abeloos, B., Anctil, J., Bergeron, M., Dalla-Rosa, D., Corbeil-Letourneau, S., ... & Ziaeefard, M. (2019, October). Thales XAI Platform: Adaptable Explanation of Machine Learning Systems-A Knowledge Graphs Perspective. In ISWC (Satellites) (pp. 315-316).;3_explanation_model_machine_learning;2019;Thales XAI Platform: Adaptable Explanation of Machine Learning Systems-A Knowledge Graphs Perspective.;Freddy Lécué, Baptiste Abeloos, Jonathan Anctil, Manuel Bergeron, Damien Dalla-Rosa, Simon Corbeil-Letourneau, Florian Martet, Tanguy Pommellet, Laura Salvan, Simon Veilleux, Maryam Ziaeefard;ISWC (Satellites), 315-316, 2019;"Explanation in Machine Learning systems has been identified to be the main asset to have for large scale deployment of Artificial Intelligence (AI) in critical systems. Explanations could be example-, features-, semantics-based or even counterfactual to potentially action on an AI system; they could be represented in many different ways eg, textual, graphical, or visual. All representations serve different means, purpose and operators. We built the first-of-its-kind XAI (eXplainable AI) platform for critical systems ie, Thales XAI Platform which aims at …";https://scholar.archive.org/work/tul7uhhxbfadrdwiuak2uum7he/access/wayback/http://ceur-ws.org/Vol-2456/paper85.pdf;h7WgneXJM7AJ
Vidnerová, P., & Neruda, R. (2016, September). Vulnerability of Machine Learning Models to Adversarial Examples. In ITAT (pp. 187-194).;5_adversarial_attack_example_model;2016;Vulnerability of Machine Learning Models to Adversarial Examples.;Petra Vidnerová, Roman Neruda;ITAT, 187-194, 2016;We propose a genetic algorithm for generating adversarial examples for machine learning models. Such approach is able to find adversarial examples without the access to model’s parameters. Different models are tested, including both deep and shallow neural networks architectures. We show that RBF networks and SVMs with Gaussian kernels tend to be rather robust and not prone to misclassification of adversarial examples.;https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=c186009ea6d19f5af9df0087002ab5381b954e92;mWETgcNJN8QJ
Samek, W., Wiegand, T., & Müller, K. R. (2017). Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models. arXiv preprint arXiv:1708.08296.;11_deep_network_model_layer;2017;Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models;Wojciech Samek, Thomas Wiegand, Klaus-Robert MÃ¼ller;arXiv preprint arXiv:1708.08296, 2017;With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks.;https://arxiv.org/abs/1708.08296;kd-rEvM9VtYJ
Liu, Y., Chen, P. H. C., Krause, J., & Peng, L. (2019). How to read articles that use machine learning: users’ guides to the medical literature. Jama, 322(18), 1806-1816.;1_ml_machine_data_learning;2019;How to read articles that use machine learning: users’ guides to the medical literature;Yun Liu, Po-Hsuan Cameron Chen, Jonathan Krause, Lily Peng;Jama 322 (18), 1806-1816, 2019;In recent years, many new clinical diagnostic tools have been developed using complicated machine learning methods. Irrespective of how a diagnostic tool is derived, it must be evaluated using a 3-step process of deriving, validating, and establishing the clinical effectiveness of the tool. Machine learning–based tools should also be assessed for the type of machine learning model used and its appropriateness for the input data type and data set size. Machine learning models also generally have additional prespecified settings called …;https://jamanetwork.com/journals/jama/article-abstract/2754798;Bscq-37m82cJ
Beam, A. L., Manrai, A. K., & Ghassemi, M. (2020). Challenges to the reproducibility of machine learning models in health care. Jama, 323(4), 305-306.;1_ml_machine_data_learning;2020;Challenges to the reproducibility of machine learning models in health care;Andrew L Beam, Arjun K Manrai, Marzyeh Ghassemi;Jama 323 (4), 305-306, 2020;Reproducibilityhasbeenanimportantandintenselyde-bated topic in science and medicine for the past few decades. 1 As the scientific enterprise has grown in scope and complexity, concerns regarding how well new findings can be reproduced and validated across different scientific teams and study populations have emerged. In some instances, 2 the failure to replicate numerous previous studies has added to the growing concern that science and biomedicine may be in the midst of a “reproducibilitycrisis.” Againstthisbackdrop, high …;https://jamanetwork.com/journals/jama/article-abstract/2758612;i_3g1KLJsvUJ
Xiao, C., Choi, E., & Sun, J. (2018). Opportunities and challenges in developing deep learning models using electronic health records data: a systematic review. Journal of the American Medical Informatics Association, 25(10), 1419-1428.;11_deep_network_model_layer;2018;Opportunities and challenges in developing deep learning models using electronic health records data: a systematic review;Cao Xiao, Edward Choi, Jimeng Sun;Journal of the American Medical Informatics Association 25 (10), 1419-1428, 2018;To conduct a systematic review of deep learning models for electronic health record (EHR) data, and illustrate various deep learning architectures for analyzing different data sources and their target applications. We also highlight ongoing research and identify open challenges in building deep learning models of EHRs.We searched PubMed and Google Scholar for papers on deep learning studies using EHR data published between January 1, 2010, and January 31, 2018. We summarize them according to these axes: types of analytics tasks, types of deep learning model architectures, special challenges arising from health data and tasks and their potential solutions, as well as evaluation strategies.We surveyed and analyzed multiple aspects of the 98 articles we found and identified the following analytics tasks: disease detection/classification, sequential prediction of clinical events, concept embedding, data augmentation, and EHR data privacy. We then studied how deep architectures were applied to these tasks. We also discussed some special challenges arising from modeling EHR data and reviewed a few popular approaches. Finally, we summarized how performance evaluations were conducted for each task.Despite the early success in using deep learning for health analytics applications, there still exist a number of issues to be addressed. We discuss them in detail including data and label availability, the interpretability and transparency of the model, and ease of deployment.;https://academic.oup.com/jamia/article-abstract/25/10/1419/5035024;b3DjY_KTxzAJ
Lapuschkin, S., Binder, A., Montavon, G., Müller, K. R., & Samek, W. (2016). The LRP toolbox for artificial neural networks. The Journal of Machine Learning Research, 17(1), 3938-3942.;11_deep_network_model_layer;2016;The LRP toolbox for artificial neural networks;Sebastian Lapuschkin, Alexander Binder, Grégoire Montavon, Klaus-Robert Müller, Wojciech Samek;The Journal of Machine Learning Research 17 (1), 3938-3942, 2016;The Layer-wise Relevance Propagation (LRP) algorithm explains a classifier’s prediction specific to a given data point by attributing relevance scores to important components of the input by using the topology of the learned model itself. With the LRP Toolbox we provide platform-agnostic implementations for explaining the predictions of pre-trained state of the art Caffe networks and stand-alone implementations for fully connected Neural Network models. The implementations for Matlab and python shall serve as a playing field to familiarize oneself with the LRP algorithm and are implemented with readability and transparency in mind. Models and data can be imported and exported using raw text formats, Matlab’s. mat files and the. npy format for numpy or plain text.;https://www.jmlr.org/papers/volume17/15-618/15-618.pdf?ref=https://githubhelp.com;L8NEFDYV6kMJ
Wang, J., Ma, Y., Zhang, L., Gao, R. X., & Wu, D. (2018). Deep learning for smart manufacturing: Methods and applications. Journal of manufacturing systems, 48, 144-156.;1_ml_machine_data_learning;2018;Deep learning for smart manufacturing: Methods and applications;Jinjiang Wang, Yulin Ma, Laibin Zhang, Robert X Gao, Dazhong Wu;Journal of manufacturing systems 48, 144-156, 2018;Smart manufacturing refers to using advanced data analytics to complement physical science for improving system performance and decision making. With the widespread deployment of sensors and Internet of Things, there is an increasing need of handling big manufacturing data characterized by high volume, high velocity, and high variety. Deep learning provides advanced analytics tools for processing and analysing big manufacturing data. This paper presents a comprehensive survey of commonly used deep learning algorithms and discusses …;https://www.sciencedirect.com/science/article/pii/S0278612518300037;qX17zK5naEIJ
Wang, X., Li, J., Kuang, X., Tan, Y. A., & Li, J. (2019). The security of machine learning in an adversarial setting: A survey. Journal of Parallel and Distributed Computing, 130, 12-23.;5_adversarial_attack_example_model;2019;The security of machine learning in an adversarial setting: A survey;Xianmin Wang, Jing Li, Xiaohui Kuang, Yu-an Tan, Jin Li;Journal of Parallel and Distributed Computing 130, 12-23, 2019;Machine learning (ML) methods have demonstrated impressive performance in many application fields such as autopilot, facial recognition, and spam detection. Traditionally, ML models are trained and deployed in a benign setting, in which the testing and training data have identical statistical characteristics. However, this assumption usually does not hold in the sense that the ML model is designed in an adversarial setting, where some statistical properties of the data can be tampered with by a capable adversary. Specifically, it has been observed …;https://www.sciencedirect.com/science/article/pii/S0743731518309183;KOHciuclwlcJ
Yosinski, J., Clune, J., Nguyen, A., Fuchs, T., & Lipson, H. (2015). Understanding neural networks through deep visualization. arXiv preprint arXiv:1506.06579.;11_deep_network_model_layer;2015;Understanding neural networks through deep visualization;Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, Hod Lipson;arXiv preprint arXiv:1506.06579, 2015;Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.;https://arxiv.org/abs/1506.06579;rj_FETy4rLEJ
Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & Mané, D. (2016). Concrete problems in AI safety. arXiv preprint arXiv:1606.06565.;2_safety_system_autonomous_vehicle;2016;Concrete problems in AI safety;Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan ManÃ©;arXiv preprint arXiv:1606.06565, 2016;"Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (""avoiding side effects"" and ""avoiding reward hacking""), an objective function that is too expensive to evaluate frequently (""scalable supervision""), or undesirable behavior during the learning process (""safe exploration"" and ""distributional shift""). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.";https://arxiv.org/abs/1606.06565;NCBbFTQ421UJ
Gopinath, D., Katz, G., Pasareanu, C. S., & Barrett, C. (2017). Deepsafe: A data-driven approach for checking adversarial robustness in neural networks. arXiv preprint arXiv:1710.00486.;4_dl_testing_deep_network;2017;Deepsafe: A data-driven approach for checking adversarial robustness in neural networks;Divya Gopinath, Guy Katz, Corina S Pasareanu, Clark Barrett;arXiv preprint arXiv:1710.00486, 2017;Deep neural networks have become widely used, obtaining remarkable results in domains such as computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, and bio-informatics, where they have produced results comparable to human experts. However, these networks can be easily fooled by adversarial perturbations: minimal changes to correctly-classified inputs, that cause the network to mis-classify them. This phenomenon represents a concern for both safety and security, but it is currently unclear how to measure a network's robustness against such perturbations. Existing techniques are limited to checking robustness around a few individual input points, providing only very limited guarantees. We propose a novel approach for automatically identifying safe regions of the input space, within which the network is robust against adversarial perturbations. The approach is data-guided, relying on clustering to identify well-defined geometric regions as candidate safe regions. We then utilize verification techniques to confirm that these regions are safe or to provide counter-examples showing that they are not safe. We also introduce the notion of targeted robustness which, for a given target label and region, ensures that a NN does not map any input in the region to the target label. We evaluated our technique on the MNIST dataset and on a neural network implementation of a controller for the next-generation Airborne Collision Avoidance System for unmanned aircraft (ACAS Xu). For these networks, our approach identified multiple regions which were completely safe as well as some which were only safe for specific labels. It also discovered several adversarial perturbations of interest.;https://arxiv.org/abs/1710.00486;vF2SIFTUoOoJ
Bantilan, N. (2018). Themis-ml: A fairness-aware machine learning interface for end-to-end discrimination discovery and mitigation. Journal of Technology in Human Services, 36(1), 15-30.;6_fairness_discrimination_bias_decision;2018;Themis-ml: A fairness-aware machine learning interface for end-to-end discrimination discovery and mitigation;Niels Bantilan;Journal of Technology in Human Services 36 (1), 15-30, 2018;As more industries integrate machine learning into socially sensitive decision processes like hiring, loan-approval, and parole-granting, we are at risk of perpetuating historical and contemporary socioeconomic disparities. This is a critical problem because on the one hand, organizations who use but do not understand the discriminatory potential of such systems will facilitate the widening of social disparities under the assumption that algorithms are categorically objective. On the other hand, the responsible use of machine learning can help us measure, understand, and mitigate the implicit historical biases in socially sensitive data by expressing implicit decision-making mental models in terms of explicit statistical models. In this article we specify, implement, and evaluate a “fairness-aware” machine learning interface called themis-ml, which is intended for use by individual data scientists and engineers, academic research teams, or larger product teams who use machine learning in production systems.;https://www.tandfonline.com/doi/abs/10.1080/15228835.2017.1416512;lNQnt_6iAuMJ
Al-Shedivat, M., Dubey, A., & Xing, E. P. (2018). The intriguing properties of model explanations. arXiv preprint arXiv:1801.09808.;3_explanation_model_machine_learning;2018;The intriguing properties of model explanations;Maruan Al-Shedivat, Avinava Dubey, Eric P Xing;arXiv preprint arXiv:1801.09808, 2018;Linear approximations to the decision boundary of a complex model have become one of the most popular tools for interpreting predictions. In this paper, we study such linear explanations produced either post-hoc by a few recent methods or generated along with predictions with contextual explanation networks (CENs). We focus on two questions: (i) whether linear explanations are always consistent or can be misleading, and (ii) when integrated into the prediction process, whether and how explanations affect the performance of the model. Our analysis sheds more light on certain properties of explanations produced by different methods and suggests that learning models that explain and predict jointly is often advantageous.;https://arxiv.org/abs/1801.09808;6rUKGy4gihEJ
Goodman, D., Xin, H., Yang, W., Yuesheng, W., Junfeng, X., & Huan, Z. (2020). Advbox: a toolbox to generate adversarial examples that fool neural networks. arXiv preprint arXiv:2001.05574.;5_adversarial_attack_example_model;2020;Advbox: a toolbox to generate adversarial examples that fool neural networks;Dou Goodman, Hao Xin, Wang Yang, Wu Yuesheng, Xiong Junfeng, Zhang Huan;arXiv preprint arXiv:2001.05574, 2020;In recent years, neural networks have been extensively deployed for computer vision tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance. Recent studies have shown that they are all vulnerable to the attack of adversarial examples. Small and often imperceptible perturbations to the input images are sufficient to fool the most powerful neural networks. \emph{Advbox} is a toolbox to generate adversarial examples that fool neural networks in PaddlePaddle, PyTorch, Caffe2, MxNet, Keras, TensorFlow and it can benchmark the robustness of machine learning models. Compared to previous work, our platform supports black box attacks on Machine-Learning-as-a-service, as well as more attack scenarios, such as Face Recognition Attack, Stealth T-shirt, and DeepFake Face Detect. The code is licensed under the Apache 2.0 and is openly available at https://github.com/advboxes/AdvBox. Advbox now supports Python 3.;https://arxiv.org/abs/2001.05574;VptSDnIaNnAJ
Apley, D. W., & Zhu, J. (2020). Visualizing the effects of predictor variables in black box supervised learning models. Journal of the Royal Statistical Society Series B: Statistical Methodology, 82(4), 1059-1086.;3_explanation_model_machine_learning;2020;Visualizing the effects of predictor variables in black box supervised learning models;Daniel W Apley, Jingyu Zhu;Journal of the Royal Statistical Society Series B: Statistical Methodology 82 (4), 1059-1086, 2020;In many supervised learning applications, understanding and visualizing the effects of the predictor variables on the predicted response is of paramount importance. A shortcoming of black box supervised learning models (e.g. complex trees, neural networks, boosted trees, random forests, nearest neighbours, local kernel-weighted methods and support vector regression) in this regard is their lack of interpretability or transparency. Partial dependence plots, which are the most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to partial dependence plots, we present a new visualization approach that we term accumulated local effects plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, accumulated local effects plots are far less computationally expensive than partial dependence plots. We also provide an R package ALEPlot as supplementary material to implement our proposed method.;https://academic.oup.com/jrsssb/article-abstract/82/4/1059/7056085;FyH4DuNHPvQJ
Jia, S., Lin, P., Li, Z., Zhang, J., & Liu, S. (2020). Visualizing surrogate decision trees of convolutional neural networks. Journal of Visualization, 23, 141-156.;11_deep_network_model_layer;2020;Visualizing surrogate decision trees of convolutional neural networks;Shichao Jia, Peiwen Lin, Zeyu Li, Jiawan Zhang, Shixia Liu;Journal of Visualization 23, 141-156, 2020;Interpreting the decision-making of black boxes in machine learning becomes urgent nowadays due to their lack of transparency. One effective way to interpret these models is to transform them into interpretable surrogate models such as decision trees and rule lists. Compared with other methods that open the black boxes, rule extraction is a universal method which can theoretically extend to any black boxes. However, in practice, it is not appropriate for deep learning models such as convolutional neural networks (CNNs), since the extracted rules or decision trees are too large to interpret and the rules are not at the semantic level. These two drawbacks limit the usability of rule extraction for deep learning models. In this paper, we adopt a new strategy to solve the problem. We first decompose a CNN into a feature extractor and a classifier. Then extract the decision tree only from the classifier. Then, we leverage lots of segmented labeled images to learn the concepts of each feature. This method can extract human-readable decision trees from CNNs. Finally, we build CNN2DT, a visual analysis system to enable users to explore the surrogate decision trees. Use cases show that CNN2DT provides global and local interpretations of the CNN decision process. Besides, users can easily find the misclassification reasons for single images and the discriminating capacity of different models. A user study has demonstrated the effectiveness of CNN2DT on AlexNet and VGG16 for image classification.;https://link.springer.com/article/10.1007/s12650-019-00607-z;BMK1xwtsQB0J
Kamiran, F., & Calders, T. (2012). Data preprocessing techniques for classification without discrimination. Knowledge and information systems, 33(1), 1-33.;6_fairness_discrimination_bias_decision;2012;Data preprocessing techniques for classification without discrimination;Faisal Kamiran, Toon Calders;Knowledge and information systems 33 (1), 1-33, 2012;"Recently, the following Discrimination-Aware Classification Problem was introduced: Suppose we are given training data that exhibit unlawful discrimination; e.g., toward sensitive attributes such as gender or ethnicity. The task is to learn a classifier that optimizes accuracy, but does not have this discrimination in its predictions on test data. This problem is relevant in many settings, such as when the data are generated by a biased decision process or when the sensitive attribute serves as a proxy for unobserved features. In this paper, we concentrate on the case with only one binary sensitive attribute and a two-class classification problem. We first study the theoretically optimal trade-off between accuracy and non-discrimination for pure classifiers. Then, we look at algorithmic solutions that preprocess the data to remove discrimination before a classifier is learned. We survey and extend our existing data preprocessing techniques, being suppression of the sensitive attribute, massaging the dataset by changing class labels, and reweighing or resampling the data to remove discrimination without relabeling instances. These preprocessing techniques have been implemented in a modified version of Weka and we present the results of experiments on real-life data.";https://link.springer.com/article/10.1007/s10115-011-0463-8;fPFKfCOknQYJ
Lakkaraju, H., Kamar, E., Caruana, R., & Leskovec, J. (2017). Interpretable & explorable approximations of black box models. arXiv preprint arXiv:1707.01154.;3_explanation_model_machine_learning;2017;Interpretable & explorable approximations of black box models;Himabindu Lakkaraju, Ece Kamar, Rich Caruana, Jure Leskovec;arXiv preprint arXiv:1707.01154, 2017;We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation. To this end, we develop a novel objective function which allows us to learn (with optimality guarantees), a small number of compact decision sets each of which explains the behavior of the black box model in unambiguous, well-defined regions of feature space. Furthermore, our framework also is capable of accepting user input when generating these approximations, thus allowing users to interactively explore how the black-box model behaves in different subspaces that are of interest to the user. To the best of our knowledge, this is the first approach which can produce global explanations of the behavior of any given black box model through joint optimization of unambiguity, fidelity, and interpretability, while also allowing users to explore model behavior based on their preferences. Experimental evaluation with real-world datasets and user studies demonstrates that our approach can generate highly compact, easy-to-understand, yet accurate approximations of various kinds of predictive models compared to state-of-the-art baselines.;https://arxiv.org/abs/1707.01154;5bwOhg9PDtkJ
Cheng, H. T., Haque, Z., Hong, L., Ispir, M., Mewald, C., Polosukhin, I., ... & Xie, J. (2017, August). Tensorflow estimators: Managing simplicity vs. flexibility in high-level machine learning frameworks. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1763-1771).;1_ml_machine_data_learning;2017;Tensorflow estimators: Managing simplicity vs. flexibility in high-level machine learning frameworks;Heng-Tze Cheng, Zakaria Haque, Lichan Hong, Mustafa Ispir, Clemens Mewald, Illia Polosukhin, Georgios Roumpos, D Sculley, Jamie Smith, David Soergel, Yuan Tang, Philipp Tucker, Martin Wicke, Cassandra Xia, Jianwei Xie;Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 1763-1771, 2017;We present a framework for specifying, training, evaluating, and deploying machine learning models. Our focus is on simplifying cutting edge machine learning for practitioners in order to bring such technologies into production. Recognizing the fast evolution of the field of deep learning, we make no attempt to capture the design space of all possible model architectures in a domain-specific language (DSL) or similar configuration language. We allow users to write code to define their models, but provide abstractions that guide developers to write models in ways conducive to productionization. We also provide a unifying Estimator interface, making it possible to write downstream infrastructure (e.g. distributed training, hyperparameter tuning) independent of the model implementation.We balance the competing demands for flexibility and simplicity by offering APIs at different levels of abstraction, making common model architectures available out of the box, while providing a library of utilities designed to speed up experimentation with model architectures. To make out of the box models flexible and usable across a wide range of problems, these canned Estimators are parameterized not only over traditional hyperparameters, but also using feature columns, a declarative specification describing how to interpret input dataWe discuss our experience in using this framework in research and production environments, and show the impact on code health, maintainability, and development speed.;https://dl.acm.org/doi/abs/10.1145/3097983.3098171;JpYd5qSRJ-QJ
Ackermann, K., Walsh, J., De Unánue, A., Naveed, H., Navarrete Rivera, A., Lee, S. J., ... & Ghani, R. (2018, July). Deploying machine learning models for public policy: A framework. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp. 15-22).;1_ml_machine_data_learning;2018;Deploying machine learning models for public policy: A framework;Klaus Ackermann, Joe Walsh, Adolfo De UnÃ¡nue, Hareem Naveed, Andrea Navarrete Rivera, Sun-Joo Lee, Jason Bennett, Michael Defoe, Crystal Cody, Lauren Haynes, Rayid Ghani;Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 15-22, 2018;Machine learning research typically focuses on optimization and testing on a few criteria, but deployment in a public policy setting requires more. Technical and non-technical deployment issues get relatively little attention. However, for machine learning models to have real-world benefit and impact, effective deployment is crucial. In this case study, we describe our implementation of a machine learning early intervention system (EIS) for police officers in the Charlotte-Mecklenburg (North Carolina) and Metropolitan Nashville (Tennessee) Police Departments. The EIS identifies officers at high risk of having an adverse incident, such as an unjustified use of force or sustained complaint. We deployed the same code base at both departments, which have different underlying data sources and data structures. Deployment required us to solve several new problems, covering technical implementation, governance of the system, the cost to use the system, and trust in the system. In this paper we describe how we addressed and solved several of these challenges and provide guidance and a framework of important issues to consider for future deployments.;https://dl.acm.org/doi/abs/10.1145/3219819.3219911;HkFQ47d4yekJ
Bird, S., Kenthapadi, K., Kiciman, E., & Mitchell, M. (2019, January). Fairness-aware machine learning: Practical challenges and lessons learned. In Proceedings of the twelfth ACM international conference on web search and data mining (pp. 834-835).;6_fairness_discrimination_bias_decision;2019;Fairness-aware machine learning: Practical challenges and lessons learned;Sarah Bird, Krishnaram Kenthapadi, Emre Kiciman, Margaret Mitchell;Proceedings of the twelfth ACM international conference on web search and data mining, 834-835, 2019;"Researchers and practitioners from different disciplines have highlighted the ethical and legal challenges posed by the use of machine learned models and data-driven systems, and the potential for such systems to discriminate against certain population groups, due to biases in algorithmic decision-making systems. This tutorial aims to present an overview of algorithmic bias / discrimination issues observed over the last few years and the lessons learned, key regulations and laws, and evolution of techniques for achieving fairness in machine learning systems. We will motivate the need for adopting a ""fairness-first"" approach (as opposed to viewing algorithmic bias / fairness considerations as an afterthought), when developing machine learning based models and systems for different consumer and enterprise applications. Then, we will focus on the application of fairness-aware machine learning techniques in practice, by presenting case studies from different technology companies. Based on our experiences in industry, we will identify open problems and research challenges for the data mining / machine learning community.";https://dl.acm.org/doi/abs/10.1145/3289600.3291383;g9LywLHJwcgJ
Sokol, K., & Flach, P. (2020). One explanation does not fit all: The promise of interactive explanations for machine learning transparency. KI-Künstliche Intelligenz, 34(2), 235-250.;3_explanation_model_machine_learning;2020;One explanation does not fit all: The promise of interactive explanations for machine learning transparency;Kacper Sokol, Peter Flach;KI-Künstliche Intelligenz 34 (2), 235-250, 2020;"The need for transparency of predictive systems based on Machine Learning algorithms arises as a consequence of their ever-increasing proliferation in the industry. Whenever black-box algorithmic predictions influence human affairs, the inner workings of these algorithms should be scrutinised and their decisions explained to the relevant stakeholders, including the system engineers, the system’s operators and the individuals whose case is being decided. While a variety of interpretability and explainability methods is available, none of them is a panacea that can satisfy all diverse expectations and competing objectives that might be required by the parties involved. We address this challenge in this paper by discussing the promises of Interactive Machine Learning for improved transparency of black-box systems using the example of contrastive explanations—a state-of-the-art approach to Interpretable Machine Learning. Specifically, we show how to personalise counterfactual explanations by interactively adjusting their conditional statements and extract additional explanations by asking follow-up “What if?” questions. Our experience in building, deploying and presenting this type of system allowed us to list desired properties as well as potential limitations, which can be used to guide the development of interactive explainers. While customising the medium of interaction, i.e., the user interface comprising of various communication channels, may give an impression of personalisation, we argue that adjusting the explanation itself and its content is more important. To this end, properties such as breadth, scope, context, purpose and target of the explanation have to be considered, in addition to explicitly informing the explainee about its limitations and caveats. Furthermore, we discuss the challenges of mirroring the explainee’s mental model, which is the main building block of intelligible human–machine interactions. We also deliberate on the risks of allowing the explainee to freely manipulate the explanations and thereby extracting information about the underlying predictive model, which might be leveraged by malicious actors to steal or game the model. Finally, building an end-to-end interactive explainability system is a challenging engineering task; unless the main goal is its deployment, we recommend “Wizard of Oz” studies as a proxy for testing and evaluating standalone interactive explainability algorithms.";https://link.springer.com/article/10.1007/s13218-020-00637-y;g4NmxL_dRh4J
Alshemali, B., & Kalita, J. (2020). Improving the reliability of deep neural networks in NLP: A review. Knowledge-Based Systems, 191, 105210.;5_adversarial_attack_example_model;2020;Improving the reliability of deep neural networks in NLP: A review;Basemah Alshemali, Jugal Kalita;Knowledge-Based Systems 191, 105210, 2020;Deep learning models have achieved great success in solving a variety of natural language processing (NLP) problems. An ever-growing body of research, however, illustrates the vulnerability of deep neural networks (DNNs) to adversarial examples â€” inputs modified by introducing small perturbations to deliberately fool a target model into outputting incorrect results. The vulnerability to adversarial examples has become one of the main hurdles precluding neural network deployment into safety-critical environments. This paper discusses the contemporary usage of adversarial examples to foil DNNs and presents a comprehensive review of their use to improve the robustness of DNNs in NLP applications. In this paper, we summarize recent approaches for generating adversarial texts and propose a taxonomy to categorize them. We further review various types of defensive strategies against adversarial examples, explore their main challenges, and highlight some future research directions.;https://www.sciencedirect.com/science/article/pii/S0950705119305428;MF4ln9H5YYAJ
Bethard, S., Ogren, P., & Becker, L. (2014, May). ClearTK 2.0: Design patterns for machine learning in UIMA. In LREC... International Conference on Language Resources & Evaluation:[proceedings]. International Conference on Language Resources and Evaluation (Vol. 2014, p. 3289). NIH Public Access.;1_ml_machine_data_learning;2014;ClearTK 2.0: Design patterns for machine learning in UIMA;Steven Bethard, Philip Ogren, Lee Becker;LREC... International Conference on Language Resources & Evaluation:[proceedings]. International Conference on Language Resources and Evaluation 2014, 3289, 2014;ClearTK adds machine learning functionality to the UIMA framework, providing wrappers to popular machine learning libraries, a rich feature extraction library that works across different classifiers, and utilities for applying and evaluating machine learning models. Since its inception in 2008, ClearTK has evolved in response to feedback from developers and the community. This evolution has followed a number of important design principles including: conceptually simple annotator interfaces, readable pipeline descriptions, minimal collection readers, type system agnostic code, modules organized for ease of import, and assisting user comprehension of the complex UIMA framework.;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5667672/;0VhozTIJE1oJ
Sehgal, A., & Kehtarnavaz, N. (2019). Guidelines and benchmarks for deployment of deep learning models on smartphones as real-time apps. Machine Learning and Knowledge Extraction, 1(1), 450-465.;7_edge_computing_deep_learning;2019;Guidelines and benchmarks for deployment of deep learning models on smartphones as real-time apps;Abhishek Sehgal, Nasser Kehtarnavaz;Machine Learning and Knowledge Extraction 1 (1), 450-465, 2019;Deep learning solutions are being increasingly used in mobile applications. Although there are many open-source software tools for the development of deep learning solutions, there are no guidelines in one place in a unified manner for using these tools toward real-time deployment of these solutions on smartphones. From the variety of available deep learning tools, the most suited ones are used in this paper to enable real-time deployment of deep learning inference networks on smartphones. A uniform flow of implementation is devised for both Android and iOS smartphones. The advantage of using multi-threading to achieve or improve real-time throughputs is also showcased. A benchmarking framework consisting of accuracy, CPU/GPU consumption, and real-time throughput is considered for validation purposes. The developed deployment approach allows deep learning models to be turned into real-time smartphone apps with ease based on publicly available deep learning and smartphone software tools. This approach is applied to six popular or representative convolutional neural network models, and the validation results based on the benchmarking metrics are reported.;https://www.mdpi.com/2504-4990/1/1/27;LO_t_GKNVmwJ
Mehrtash, A., Pesteie, M., Hetherington, J., Behringer, P. A., Kapur, T., Wells III, W. M., ... & Abolmaesumi, P. (2017, March). DeepInfer: Open-source deep learning deployment toolkit for image-guided therapy. In Medical Imaging 2017: Image-Guided Procedures, Robotic Interventions, and Modeling (Vol. 10135, pp. 410-416). SPIE.;11_deep_network_model_layer;2017;DeepInfer: Open-source deep learning deployment toolkit for image-guided therapy;Alireza Mehrtash, Mehran Pesteie, Jorden Hetherington, Peter A Behringer, Tina Kapur, William M Wells III, Robert Rohling, Andriy Fedorov, Purang Abolmaesumi;Medical Imaging 2017: Image-Guided Procedures, Robotic Interventions, and Modeling 10135, 410-416, 2017;"Deep learning models have outperformed some of the previous state-of-the-art approaches in medical image analysis. Instead of using hand-engineered features, deep models attempt to automatically extract hierarchical representations at multiple levels of abstraction from the data. Therefore, deep models are usually considered to be more flexible and robust solutions for image analysis problems compared to conventional computer vision models. They have demonstrated significant improvements in computer-aided diagnosis and automatic medical image analysis applied to such tasks as image segmentation, classification and registration. However, deploying deep learning models often has a steep learning curve and requires detailed knowledge of various software packages. Thus, many deep models have not been integrated into the clinical research work ows causing a gap between the state-of-the-art machine learning in medical applications and evaluation in clinical research procedures. In this paper, we propose ""DeepInfer"" - an open-source toolkit for developing and deploying deep learning models within the 3D Slicer medical image analysis platform. Utilizing a repository of task-specific models, DeepInfer allows clinical researchers and biomedical engineers to deploy a trained model selected from the public registry, and apply it to new data without the need for software development or configuration. As two practical use cases, we demonstrate the application of DeepInfer in prostate segmentation for targeted MRI-guided biopsy and identification of the target plane in 3D ultrasound for spinal injections.";https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10135/101351K/DeepInfer--open-source-deep-learning-deployment-toolkit-for-image/10.1117/12.2256011.short;03VEN2Y8YIAJ
Hagendorff, T. (2020). The ethics of AI ethics: An evaluation of guidelines. Minds and machines, 30(1), 99-120.;12_ai_ethical_ethic_intelligence;2020;The ethics of AI ethics: An evaluation of guidelines;Thilo Hagendorff;Minds and machines 30 (1), 99-120, 2020;Current advances in research, development and application of artificial intelligence (AI) systems have yielded a far-reaching discourse on AI ethics. In consequence, a number of ethics guidelines have been released in recent years. These guidelines comprise normative principles and recommendations aimed to harness the “disruptive” potentials of new AI technologies. Designed as a semi-systematic evaluation, this paper analyzes and compares 22 guidelines, highlighting overlaps but also omissions. As a result, I give a detailed overview of the field of AI ethics. Finally, I also examine to what extent the respective ethical principles and values are implemented in the practice of research, development and application of AI systems—and how the effectiveness in the demands of AI ethics can be improved.;https://link.springer.com/article/10.1007/s11023-020-09517-8;WAl2-4d-1PQJ
Páez, A. (2019). The pragmatic turn in explainable artificial intelligence (XAI). Minds and Machines, 29(3), 441-459.;3_explanation_model_machine_learning;2019;The pragmatic turn in explainable artificial intelligence (XAI);Andrés Páez;Minds and Machines 29 (3), 441-459, 2019;In this paper I argue that the search for explainable models and interpretable decisions in AI must be reformulated in terms of the broader project of offering a pragmatic and naturalistic account of understanding in AI. Intuitively, the purpose of providing an explanation of a model or a decision is to make it understandable to its stakeholders. But without a previous grasp of what it means to say that an agent understands a model or a decision, the explanatory strategies will lack a well-defined goal. Aside from providing a clearer objective for XAI, focusing on understanding also allows us to relax the factivity condition on explanation, which is impossible to fulfill in many machine learning models, and to focus instead on the pragmatic conditions that determine the best fit between a model and the methods and devices deployed to understand it. After an examination of the different types of understanding discussed in the philosophical and psychological literature, I conclude that interpretative or approximation models not only provide the best way to achieve the objectual understanding of a machine learning model, but are also a necessary condition to achieve post hoc interpretability. This conclusion is partly based on the shortcomings of the purely functionalist approach to post hoc interpretability that seems to be predominant in most recent literature.;https://link.springer.com/article/10.1007/s11023-019-09502-w;lv0zCCAuoh4J
Došilović, F. K., Brčić, M., & Hlupić, N. (2018, May). Explainable artificial intelligence: A survey. In 2018 41st International convention on information and communication technology, electronics and microelectronics (MIPRO) (pp. 0210-0215). IEEE.;3_explanation_model_machine_learning;2018;Explainable artificial intelligence: A survey;Filip Karlo Došilović, Mario Brčić, Nikica Hlupić;2018 41st International convention on information and communication technology, electronics and microelectronics (MIPRO), 0210-0215, 2018;In the last decade, with availability of large datasets and more computing power, machine learning systems have achieved (super)human performance in a wide variety of tasks. Examples of this rapid development can be seen in image recognition, speech analysis, strategic game planning and many more. The problem with many state-of-the-art models is a lack of transparency and interpretability. The lack of thereof is a major drawback in many applications, e.g. healthcare and finance, where rationale for model's decision is a requirement for trust. In the light of these issues, explainable artificial intelligence (XAI) has become an area of interest in research community. This paper summarizes recent developments in XAI in supervised learning, starts a discussion on its connection with artificial general intelligence, and gives proposals for further research directions.;https://ieeexplore.ieee.org/abstract/document/8400040/;7O4ygCIm7rAJ
Vidnerová, P., & Neruda, R. (2016, August). Evolutionary generation of adversarial examples for deep and shallow machine learning models. In Proceedings of the The 3rd Multidisciplinary International Social Networks Conference on SocialInformatics 2016, Data Science 2016 (pp. 1-7).;5_adversarial_attack_example_model;2016;Evolutionary generation of adversarial examples for deep and shallow machine learning models;Petra VidnerovÃ¡, Roman Neruda;Proceedings of the The 3rd Multidisciplinary International Social Networks Conference on SocialInformatics 2016, Data Science 2016, 1-7, 2016;Studying vulnerability of machine learning models to adversarial examples is an important way to understand their robustness and generalization properties. In this paper, we propose a genetic algorithm for generating adversarial examples for machine learning models. Such approach is able to find adversarial examples without the access to model's parameters. Different models are tested, including both deep and shallow neural networks architectures. We show that RBF networks and SVMs with RBF kernels tend to be rather robust and not prone to misclassification of adversarial examples.;https://dl.acm.org/doi/abs/10.1145/2955129.2955178;-aX2kgWUt2EJ
Wang, W., Chen, G., Dinh, A. T. T., Gao, J., Ooi, B. C., Tan, K. L., & Wang, S. (2015, October). SINGA: Putting deep learning in the hands of multimedia users. In Proceedings of the 23rd ACM international conference on Multimedia (pp. 25-34).;7_edge_computing_deep_learning;2015;SINGA: Putting deep learning in the hands of multimedia users;Wei Wang, Gang Chen, Anh Tien Tuan Dinh, Jinyang Gao, Beng Chin Ooi, Kian-Lee Tan, Sheng Wang;Proceedings of the 23rd ACM international conference on Multimedia, 25-34, 2015;Recently, deep learning techniques have enjoyed success in various multimedia applications, such as image classification and multi-modal data analysis. Two key factors behind deep learning's remarkable achievement are the immense computing power and the availability of massive training datasets, which enable us to train large models to capture complex regularities of the data. There are two challenges to overcome before deep learning can be widely adopted in multimedia and other applications. One is usability, namely the implementation of different models and training algorithms must be done by non-experts without much effort. The other is scalability, that is the deep learning system must be able to provision for a huge demand of computing resources for training large models with massive datasets. To address these two challenges, in this paper, we design a distributed deep learning platform called SINGA which has an intuitive programming model and good scalability. Our experience with developing and training deep learning models for real-life multimedia applications in SINGA shows that the platform is both usable and scalable.;https://dl.acm.org/doi/abs/10.1145/2733373.2806232;g7kj8GvGvykJ
Bailer, W. (2018). On the traceability of results from deep learning-based cloud services. In MultiMedia Modeling: 24th International Conference, MMM 2018, Bangkok, Thailand, February 5-7, 2018, Proceedings, Part I 24 (pp. 620-631). Springer International Publishing.;11_deep_network_model_layer;2018;On the traceability of results from deep learning-based cloud services;Werner Bailer;MultiMedia Modeling: 24th International Conference, MMM 2018, Bangkok, Thailand, February 5-7, 2018, Proceedings, Part I 24, 620-631, 2018;Deep learning-based approaches have become an important method for media content analysis, and are useful tools for multimedia analytics, as they enable organising and visualising multimedia content items. However, the use of deep neural networks also raises issues of traceability, reproducability and understanding analysis results. The issues are caused by the dependency on training data sets and their possible bias, the change of training data sets over time and the lack of transparent and interoperable representations of models. In this paper we analyse these problems in detail and provide examples. We propose six recommendations to address these issues, which include having interoperable representations of trained models, the identification of training data and models (including versions) and the description of provenance of data sets, models and results.;https://link.springer.com/chapter/10.1007/978-3-319-73603-7_50;Yqr-02rmUSAJ
Le, H. V., Mayer, S., & Henze, N. (2017, November). Machine learning with tensorflow for mobile and ubiquitous interaction. In Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia (pp. 567-572).;7_edge_computing_deep_learning;2017;Machine learning with tensorflow for mobile and ubiquitous interaction;Huy Viet Le, Sven Mayer, Niels Henze;Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia, 567-572, 2017;Due to the increasing amount of sensors integrated into the environment and worn by the user, a sheer amount of context-sensitive data become available. While interpreting them with traditional methods (e.g., formulas and simple heuristics) is challenging, the latest machine learning techniques require only a set of labeled data. TensorFlow is an open-source library for machine learning which implements a wide range of neural network models. With TensorFlow Mobile, researchers and developers can further deploy the trained models on low-end mobile devices for ubiquitous scenarios. This facilitates the model export and offers techniques to optimize the model for a mobile deployment. In this tutorial, we teach attendees two basic steps to a deployment of neural networks on smartphones: Firstly, we will teach how to develop neural network architectures and train them in TensorFlow. Secondly, we show the process to run the trained models on a mobile phone using TensorFlow Mobile.;https://dl.acm.org/doi/abs/10.1145/3152832.3156559;DArRDCMgZpIJ
Rudin, C. (2019). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature machine intelligence, 1(5), 206-215.;3_explanation_model_machine_learning;2019;Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead;Cynthia Rudin;Nature machine intelligence 1 (5), 206-215, 2019;Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.;https://www.nature.com/articles/s42256-019-0048-x;mVsM3j41sPcJ
Takabe, Y., & Uehara, M. (2013, September). Rapid Deployment for Machine Learning in Educational Cloud. In 2013 16th International Conference on Network-Based Information Systems (pp. 372-376). IEEE.;1_ml_machine_data_learning;2013;Rapid Deployment for Machine Learning in Educational Cloud;Yuichiro Takabe, Minoru Uehara;16th International Conference on Network-Based Information Systems, 2013;In the cloud era, the acquisition of new cloud skills is a constant requirement of IT specialists. Educational organizations such as universities have a need to provide educational cloud curriculums for their students. In our current research, we are constructing a private cloud based on super-saturation, which is defined as the allocation of a much greater amount of logical resources than physical resources. Super-saturated clouds therefore realize up to 10 times more running instances than conventional clouds. While the performance of super-saturated clouds decreases somewhat compared with conventional clouds, their costs also greatly decrease. Moreover, in the post-cloud era, i.e., the big data era, data scientists will be increasingly required to process big data in the cloud. Mahout and Hadoop are two popular tools used in the fields of data science and machine learning. However, a certain level of skill is required to build such machine learning systems, and because it takes a long time to build such systems, the curriculums available to learners are limited. In this paper, we propose a method of rapid deployment for machine learning systems in the educational cloud. We show that our proposed method can reduce the required preparation time.;https://ieeexplore.ieee.org/abstract/document/6685427;TODO
Salem, A., Zhang, Y., Humbert, M., Berrang, P., Fritz, M., & Backes, M. (2018). Ml-leaks: Model and data independent membership inference attacks and defenses on machine learning models. arXiv preprint arXiv:1806.01246.;5_adversarial_attack_example_model;2018;Ml-leaks: Model and data independent membership inference attacks and defenses on machine learning models;Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, Michael Backes;arXiv preprint arXiv:1806.01246, 2018;Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. However, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. In addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.;https://arxiv.org/abs/1806.01246;luXuOn12qiYJ
Shafahi, A., Huang, W. R., Najibi, M., Suciu, O., Studer, C., Dumitras, T., & Goldstein, T. (2018). Poison frogs! targeted clean-label poisoning attacks on neural networks. Advances in neural information processing systems, 31.;5_adversarial_attack_example_model;2018;Poison frogs! targeted clean-label poisoning attacks on neural networks;Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, Tom Goldstein;Advances in neural information processing systems 31, 2018;"Data poisoning is an attack on machine learning models wherein the attacker adds examples to the training set to manipulate the behavior of the model at test time. This paper explores poisoning attacks on neural nets. The proposed attacks use``clean-labels''; they don't require the attacker to have any control over the labeling of training data. They are also targeted; they control the behavior of the classifier on a specific test instance without degrading overall classifier performance. For example, an attacker could add a seemingly innocuous image (that is properly labeled) to a training set for a face recognition engine, and control the identity of a chosen person at test time. Because the attacker does not need to control the labeling function, poisons could be entered into the training set simply by putting them online and waiting for them to be scraped by a data collection bot.";https://proceedings.neurips.cc/paper_files/paper/2018/hash/22722a343513ed45f14905eb07621686-Abstract.html;-3XntyN4XygJ
Babcock, J., Kramar, J., & Yampolskiy, R. V. (2019). Guidelines for artificial intelligence containment. Next-Generation Ethics: Engineering a Better Society (Ed.) Ali. E. Abbas, 90-112.;12_ai_ethical_ethic_intelligence;2019;Guidelines for artificial intelligence containment;James Babcock, Janos Kramar, Roman V Yampolskiy;Next-Generation Ethics: Engineering a Better Society (Ed.) Ali. E. Abbas, 90-112, 2019;"The past few years have seen a remarkable amount of attention on the long-term future of artificial intelligence (AI). Icons of science and technology such as Stephen Hawking (Cellan-Jones, 2014), Elon Musk (Musk, 2014), and Bill Gates (Gates, 2015) have expressed concern that superintelligent AI may wipe out humanity in the long run. Stuart Russell, coauthor of the most-cited textbook of AI (Russell & Norvig, 2003), recently began prolifically advocating (Dafoe & Russell, 2016) for the field of AI to take this possibility seriously. AI conferences now frequently have panels and workshops on the topic. There has been an outpouring of support from many leading AI researchers for an open letter calling for greatly increased research dedicated to ensuring that increasingly capable AI remains “robust and beneficial,” and gradually a field of “AI safety” is coming into being (Pistono & Yampolskiy, 2016; Yampolskiy, 2016, 2018; Yampolskiy & Spellchecker, 2016). Why all this attention? Since the dawn of modern computing, the possibility of AI has prompted leading thinkers in the field to speculate (Good, 1966; Turing, 1996; Wiener, 1961) about whether AI would end up overtaking and replacing humanity. However, for decades, while computing quickly found many important applications, AI remained a niche field, with modest successes, making such speculation seem irrelevant. But fast-forwarding to the present, machine learning has seen grand successes and very substantial R&D investments, and it is rapidly improving in major domains, such as natural language processing and image recognition, largely via advances in deep learning (LeCun, Bengio, & Hinton, 2015). Artificial general intelligence (AGI), with the ability to perform at a human-comparable level at most cognitive tasks, is likely to be created in the coming century; most predictions, by both experts and non-experts, range from fifteen to twenty-five years (Armstrong & Sotala, 2015).";https://books.google.com/books?hl=en&lr=&id=sYK0DwAAQBAJ&oi=fnd&pg=PA90&dq=Babcock,+J.,+Kram%C3%A1r,+J.,+%26+Yampolskiy,+R.+V.+(2019).+Guidelines+for+artificial+intelligence+containment.+Next-Generation+Ethics:+Engineering+a+Better+Society+(Ed.)+Ali.+E.+Abbas,+90-112.&ots=uNnzinEgi9&sig=H4TXhw6ZJ3J7iW8ylN4iPxNTCJw;7lavBbPu5WMJ
Feurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum, M., & Hutter, F. (2015). Efficient and robust automated machine learning. Advances in neural information processing systems, 28.;1_ml_machine_data_learning;2015;Efficient and robust automated machine learning;Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, Frank Hutter;Advances in neural information processing systems 28, 2015;The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. In this work we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub auto-sklearn, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of auto-sklearn.;https://proceedings.neurips.cc/paper_files/paper/2015/hash/11d0e6287202fced83f79975ec59a3a6-Abstract.html;eaI1EyUfO6QJ
Hardt, M., Price, E., & Srebro, N. (2016). Equality of opportunity in supervised learning. Advances in neural information processing systems, 29.;6_fairness_discrimination_bias_decision;2016;Equality of opportunity in supervised learning;Moritz Hardt, Eric Price, Nati Srebro;Advances in neural information processing systems 29, 2016;We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.;https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html;8iM1lt4xoRwJ
Kim, B., Khanna, R., & Koyejo, O. O. (2016). Examples are not enough, learn to criticize! criticism for interpretability. Advances in neural information processing systems, 29.;3_explanation_model_machine_learning;2016;Examples are not enough, learn to criticize! criticism for interpretability;Been Kim, Rajiv Khanna, Oluwasanmi O Koyejo;Advances in neural information processing systems 29, 2016;Example-based explanations are widely used in the effort to improve the interpretability of highly complex distributions. However, prototypes alone are rarely sufficient to represent the gist of the complexity. In order for users to construct better mental models and understand complex data distributions, we also need {\em criticism} to explain what are\textit {not} captured by prototypes. Motivated by the Bayesian model criticism framework, we develop\texttt {MMD-critic} which efficiently learns prototypes and criticism, designed to aid human interpretability. A human subject pilot study shows that the\texttt {MMD-critic} selects prototypes and criticism that are useful to facilitate human understanding and reasoning. We also evaluate the prototypes selected by\texttt {MMD-critic} via a nearest prototype classifier, showing competitive performance compared to baselines.;https://proceedings.neurips.cc/paper_files/paper/2016/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html;DAzMp3xTE9MJ
Sung, N., Kim, M., Jo, H., Yang, Y., Kim, J., Lausen, L., ... & Kim, S. (2017). Nsml: A machine learning platform that enables you to focus on your models. arXiv preprint arXiv:1712.05902.;1_ml_machine_data_learning;2017;Nsml: A machine learning platform that enables you to focus on your models;Nako Sung, Minkyu Kim, Hyunwoo Jo, Youngil Yang, Jingwoong Kim, Leonard Lausen, Youngkwan Kim, Gayoung Lee, Donghyun Kwak, Jung-Woo Ha, Sunghun Kim;arXiv preprint arXiv:1712.05902, 2017;Machine learning libraries such as TensorFlow and PyTorch simplify model implementation. However, researchers are still required to perform a non-trivial amount of manual tasks such as GPU allocation, training status tracking, and comparison of models with different hyperparameter settings. We propose a system to handle these tasks and help researchers focus on models. We present the requirements of the system based on a collection of discussions from an online study group comprising 25k members. These include automatic GPU allocation, learning status visualization, handling model parameter snapshots as well as hyperparameter modification during learning, and comparison of performance metrics between models via a leaderboard. We describe the system architecture that fulfills these requirements and present a proof-of-concept implementation, NAVER Smart Machine Learning (NSML). We test the system and confirm substantial efficiency improvements for model development.;https://arxiv.org/abs/1712.05902;RCUT6IwMwOMJ
Lundberg, S. M., & Lee, S. I. (2017). A unified approach to interpreting model predictions. Advances in neural information processing systems, 30.;3_explanation_model_machine_learning;2017;A unified approach to interpreting model predictions;Scott M Lundberg, Su-In Lee;Advances in neural information processing systems 30, 2017;Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include:(1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.;https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html;q-_bHThYxV4J
Pleiss, G., Raghavan, M., Wu, F., Kleinberg, J., & Weinberger, K. Q. (2017). On fairness and calibration. Advances in neural information processing systems, 30.;6_fairness_discrimination_bias_decision;2017;On fairness and calibration;Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, Kilian Q Weinberger;Advances in neural information processing systems 30, 2017;"The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be"" fair."" In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (ie equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets.";https://proceedings.neurips.cc/paper/2017/hash/b8b9c74ac526fffbeb2d39ab038d1cd7-Abstract.html;nIbA_ra3LvYJ
Kusner, M. J., Loftus, J., Russell, C., & Silva, R. (2017). Counterfactual fairness. Advances in neural information processing systems, 30.;6_fairness_discrimination_bias_decision;2017;Counterfactual fairness;Matt J Kusner, Joshua Loftus, Chris Russell, Ricardo Silva;Advances in neural information processing systems 30, 2017;Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.;https://proceedings.neurips.cc/paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html;LUIypo54A7YJ
Steinhardt, J., Koh, P. W. W., & Liang, P. S. (2017). Certified defenses for data poisoning attacks. Advances in neural information processing systems, 30.;5_adversarial_attack_example_model;2017;Certified defenses for data poisoning attacks;Jacob Steinhardt, Pang Wei W Koh, Percy S Liang;Advances in neural information processing systems 30, 2017;Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions:(1) that the dataset is large enough for statistical concentration between train and test error to hold, and (2) that outliers within the clean (non-poisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12% to 23% test error by adding only 3% poisoned data.;https://proceedings.neurips.cc/paper/2017/hash/9d7311ba459f9e45ed746755a32dcd11-Abstract.html;TVmrk5N6XpEJ
Chen, I., Johansson, F. D., & Sontag, D. (2018). Why is my classifier discriminatory?. Advances in neural information processing systems, 31.;6_fairness_discrimination_bias_decision;2018;Why is my classifier discriminatory?;Irene Chen, Fredrik D Johansson, David Sontag;Advances in neural information processing systems 31, 2018;Recent attempts to achieve fairness in predictive models focus on the balance between fairness and accuracy. In sensitive applications such as healthcare or criminal justice, this trade-off is often undesirable as any increase in prediction error could have devastating consequences. In this work, we argue that the fairness of predictions should be evaluated in context of the data, and that unfairness induced by inadequate samples sizes or unmeasured predictive variables should be addressed through data collection, rather than by constraining the model. We decompose cost-based metrics of discrimination into bias, variance, and noise, and propose actions aimed at estimating and reducing each term. Finally, we perform case-studies on prediction of income, mortality, and review ratings, confirming the value of this analysis. We find that data collection is often a means to reduce discrimination without sacrificing accuracy.;https://proceedings.neurips.cc/paper/2018/hash/1f1baa5b8edac74eb4eaa329f14a0361-Abstract.html;g7qpc7QBsX4J
Poursabzi-Sangdeh, F., Goldstein, D. G., Hofman, J. M., Wortman Vaughan, J. W., & Wallach, H. (2021, May). Manipulating and measuring model interpretability. In Proceedings of the 2021 CHI conference on human factors in computing systems (pp. 1-52).;3_explanation_model_machine_learning;2021;Manipulating and measuring model interpretability;Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Wortman Vaughan, Hanna Wallach;CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems;With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model’s predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N = 3, 800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model’s predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model’s sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.;https://dl.acm.org/doi/abs/10.1145/3411764.3445315;Todo
Slack, D., Friedler, S. A., Scheidegger, C., & Roy, C. D. (2019). Assessing the local interpretability of machine learning models. arXiv preprint arXiv:1902.03501.;3_explanation_model_machine_learning;2019;Assessing the local interpretability of machine learning models;Dylan Slack, Sorelle A Friedler, Carlos Scheidegger, Chitradeep Dutta Roy;arXiv preprint arXiv:1902.03501, 2019;"The increasing adoption of machine learning tools has led to calls for accountability via model interpretability. But what does it mean for a machine learning model to be interpretable by humans, and how can this be assessed? We focus on two definitions of interpretability that have been introduced in the machine learning literature: simulatability (a user's ability to run a model on a given input) and ""what if"" local explainability (a user's ability to correctly determine a model's prediction under local changes to the input, given knowledge of the model's original prediction). Through a user study with 1,000 participants, we test whether humans perform well on tasks that mimic the definitions of simulatability and ""what if"" local explainability on models that are typically considered locally interpretable. To track the relative interpretability of models, we employ a simple metric, the runtime operation count on the simulatability task. We find evidence that as the number of operations increases, participant accuracy on the local interpretability tasks decreases. In addition, this evidence is consistent with the common intuition that decision trees and logistic regression models are interpretable and are more interpretable than neural networks.";https://arxiv.org/abs/1902.03501;uBNk7zFo8pgJ
Tang, W., Qin, B., Zhao, S., Zhao, B., Xue, Y., & Chen, H. (2019). Privacy Preserving Machine Learning with Limited Information Leakage. In Network and System Security: 13th International Conference, NSS 2019, Sapporo, Japan, December 15–18, 2019, Proceedings 13 (pp. 352-370). Springer International Publishing.;0_federated_learning_data_privacy;2019;Privacy Preserving Machine Learning with Limited Information Leakage;Wenyi Tang, Bo Qin, Suyun Zhao, Boning Zhao, Yunzhi Xue, Hong Chen;Network and System Security: 13th International Conference, NSS 2019, Sapporo, Japan, December 15â€“18, 2019, Proceedings 13, 352-370, 2019;Machine learning is now playing important roles in daily lives, however, the privacy leakages are increasingly getting serious in the meantime. Current solutions to the privacy issues in machine learning, like differential privacy or homomorphic encryption either could only be applied to some specific scenarios or bring huge modification to the model construction, not to mention massive efficiency loss. In this paper, we consider addressing the privacy issue in machine learning from another perspective, without modification to models or severe efficiency loss. We proposed a straightforward privacy preserving machine learning scheme, training machine learning models directly over encrypted data. Ideally, this scheme could provide privacy protection to both training data and test data. We gave it a try by applying order preserving encryption (OPE) to the scheme. We discussed the possibility of using OPE to reveal the order information confidentially for model training. Several OPE algorithms were chosen to utilize the proposed method. Finally, comprehensive experiments were deployed on both synthetic and real datasets. The experiments on real datasets show that the learning performance of several well-known classifiers on before and after encryption changes slightly. The experiments on synthetic datasets show the classifier performance could be ranked according to fidelity and reliability.;https://link.springer.com/chapter/10.1007/978-3-030-36938-5_21;ZyPiMz29Ik4J
Yocum, K., Rowan, S., Lunt, J., & Wong, T. M. (2019). Disdat: Bundle Data Management for Machine Learning Pipelines. In 2019 USENIX Conference on Operational Machine Learning (OpML 19) (pp. 35-37).;1_ml_machine_data_learning;2019;Disdat: Bundle Data Management for Machine Learning Pipelines;Ken Yocum, Sean Rowan, Jonathan Lunt, Theodore M Wong;2019 USENIX Conference on Operational Machine Learning (OpML 19), 35-37, 2019;Modern machine learning pipelines can produce hundreds of data artifacts (such as features, models, and predictions) throughout their lifecycle. During that time, data scientists need to reproduce errors, update features, re-train on specific data, validate/inspect outputs, and share models and predictions. Doing so requires the ability to publish, discover, and version those artifacts.;https://www.usenix.org/conference/opml19/presentation/yocum;rd7UzndW4gkJ
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., ... & Zheng, X. (2016). {TensorFlow}: a system for {Large-Scale} machine learning. In 12th USENIX symposium on operating systems design and implementation (OSDI 16) (pp. 265-283).;7_edge_computing_deep_learning;2016;{TensorFlow}: a system for {Large-Scale} machine learning;Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, Xiaoqiang Zheng;12th USENIX symposium on operating systems design and implementation (OSDI 16), 265-283, 2016;TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous “parameter server” designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that Tensor-Flow achieves for several real-world applications.;https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi;d_TuHVzZvbgJ
Miguel, L. B., Takabayashi, D., Pizani, J. R., Andrade, T., & West, B. (2018, August). Marvin-Open source artificial intelligence platform. In International Conference on Predictive Applications and APIs (pp. 33-44). PMLR.;1_ml_machine_data_learning;2018;Marvin-Open source artificial intelligence platform;Lucas B. Miguel, Daniel Takabayashi, Jose R. Pizani, Tiago Andrade, Brody West;Proceedings of The 4th International Conference on Predictive Applications and APIs, PMLR 82:33-44, 2018.;Marvin is an open source project that focuses on empowering data science teams to deliver industrial-grade applications supported by a high-scale, low-latency, language agnostic and standardized architecture platform, while simplifying the process of exploration and modeling. Building model-dependent applications in a robust way is not trivial, one is required to have knowledge in advanced areas of sciences like computing, statistics and math. Marvin aims at abstracting the complexities in the creation process of scalable, highly available, interoperable and maintainable predictive software.;https://proceedings.mlr.press/v82/miguel18a;TODO
Kauffmann, J., Müller, K. R., & Montavon, G. (2020). Towards explaining anomalies: a deep Taylor decomposition of one-class models. Pattern Recognition, 101, 107198.;5_adversarial_attack_example_model;2020;Towards explaining anomalies: a deep Taylor decomposition of one-class models;Jacob Kauffmann, Klaus-Robert Müller, Grégoire Montavon;Pattern Recognition 101, 107198, 2020;Detecting anomalies in the data is a common machine learning task, with numerous applications in the sciences and industry. In practice, it is not always sufficient to reach high detection accuracy, one would also like to be able to understand why a given data point has been predicted to be anomalous. We propose a principled approach for one-class SVMs (OC-SVM), that draws on the novel insight that these models can be rewritten as distance/pooling neural networks. This ‘neuralization’ step lets us apply deep Taylor decomposition (DTD), a methodology that leverages the model structure in order to quickly and reliably explain decisions in terms of input features. The proposed method (called ‘OC-DTD’) is applicable to a number of common distance-based kernel functions, and it outperforms baselines such as sensitivity analysis, distance to nearest neighbor, or edge detection.;https://www.sciencedirect.com/science/article/pii/S0031320320300054;t2daitQi7eMJ
Chard, R., Ward, L., Li, Z., Babuji, Y., Woodard, A., Tuecke, S., ... & Foster, I. (2019). Publishing and serving machine learning models with dlhub. In Proceedings of the Practice and Experience in Advanced Research Computing on Rise of the Machines (learning) (pp. 1-7).;1_ml_machine_data_learning;2019;Publishing and serving machine learning models with dlhub;Ryan Chard, Logan Ward, Zhuozhao Li, Yadu Babuji, Anna Woodard, Steven Tuecke, Kyle Chard, Ben Blaiszik, Ian Foster;Proceedings of the Practice and Experience in Advanced Research Computing on Rise of the Machines (learning), 1-7, 2019;In this paper we introduce the Data and Learning Hub for Science (DLHub). DLHub serves as a nexus for publishing, sharing, discovering, and reusing machine learning models. It provides a flexible publication platform that enables researchers to describe and deposit models by associating publication and model-specific metadata and assigning a persistent identifier for subsequent citation. DLHub also supports scalable model inference, allowing researchers to execute inference tasks using a distributed execution engine, containerized models, and Kubernetes. Here we describe DLHub and present four scientific use cases that illustrate how DLHub can be used to reliably, efficiently, and scalably integrate ML into scientific processes.;https://dl.acm.org/doi/abs/10.1145/3332186.3332246;W04_Y2QVGhUJ
Wong, P. H. (2020). Democratizing algorithmic fairness. Philosophy & Technology, 33, 225-244.;6_fairness_discrimination_bias_decision;2020;Democratizing algorithmic fairness;Pak-Hang Wong;Philosophy & Technology 33, 225-244, 2020;Machine learning algorithms can now identify patterns and correlations in (big) datasets and predict outcomes based on the identified patterns and correlations. They can then generate decisions in accordance with the outcomes predicted, and decision-making processes can thereby be automated. Algorithms can inherit questionable values from datasets and acquire biases in the course of (machine) learning. While researchers and developers have taken the problem of algorithmic bias seriously, the development of fair algorithms is primarily conceptualized as a technical task. In this paper, I discuss the limitations and risks of this view. Since decisions on “fairness measure” and the related techniques for fair algorithms essentially involve choices between competing values, “fairness” in algorithmic fairness should be conceptualized first and foremost as a political question and be resolved politically. In short, this paper aims to foreground the political dimension of algorithmic fairness and supplement the current discussion with a deliberative approach to algorithmic fairness based on the accountability for reasonableness framework (AFR).;https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s13347-019-00355-w&casa_token=mDxfMkXXP4YAAAAA:kf7_ndtteT9m_3xWnZvrd5pNRwXf-Z757ekdE1hoGPjmVMU42AOaAhL3QOuVC-1Xb598kjheLXh0rs21cQ;bwbhfmnNMGsJ
Zhang, X., Khalili, M. M., & Liu, M. (2020). Long-term impacts of fair machine learning. Ergonomics in Design, 28(3), 7-11.;6_fairness_discrimination_bias_decision;2020;Long-term impacts of fair machine learning;Xueru Zhang, Mohammad Mahdi Khalili, Mingyan Liu;Ergonomics in Design 28 (3), 7-11, 2020;Machine learning models developed from real-world data can inherit potential, preexisting bias in the dataset. When these models are used to inform decisions involving human beings, fairness concerns inevitably arise. Imposing certain fairness constraints in the training of models can be effective only if appropriate criteria are applied. However, a fairness criterion can be defined/assessed only when the interaction between the decisions and the underlying population is well understood. We introduce two feedback models describing how people react when receiving machine-aided decisions and illustrate that some commonly used fairness criteria can end with undesirable consequences while reinforcing discrimination.;https://journals.sagepub.com/doi/abs/10.1177/1064804619884160;anjwkRs3g98J
Kuwajima, H., Tanaka, M., & Okutomi, M. (2019). Improving transparency of deep neural inference process. Progress in Artificial Intelligence, 8, 273-285.;11_deep_network_model_layer;2019;Improving transparency of deep neural inference process;Hiroshi Kuwajima, Masayuki Tanaka, Masatoshi Okutomi;Progress in Artificial Intelligence 8, 273-285, 2019;Deep learning techniques are rapidly advanced recently and becoming a necessity component for widespread systems. However, the inference process of deep learning is black box and is not very suitable to safety-critical systems which must exhibit high transparency. In this paper, to address this black-box limitation, we develop a simple analysis method which consists of (1) structural feature analysis: lists of the features contributing to inference process, (2) linguistic feature analysis: lists of the natural language labels describing the visual attributes for each feature contributing to inference process, and (3) consistency analysis: measuring consistency among input data, inference (label), and the result of our structural and linguistic feature analysis. Our analysis is simplified to reflect the actual inference process for high transparency, whereas it does not include any additional black-box mechanisms such as LSTM for highly human readable results. We conduct experiments and discuss the results of our analysis qualitatively and quantitatively and come to believe that our work improves the transparency of neural networks. Evaluated through 12,800 human tasks, 75% workers answer that input data and result of our feature analysis are consistent, and 70% workers answer that inference (label) and result of our feature analysis are consistent. In addition to the evaluation of the proposed analysis, we find that our analysis also provides suggestions, or possible next actions such as expanding neural network complexity or collecting training data to improve a neural network.;https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s13748-019-00179-x&casa_token=8vDBqmVUR1YAAAAA:WV8OC6NoOT6x89tRjYrqG0K3iKJCFcELB8aWU66-2-1bBcfGXC60Od6CpZcFgEIyj5b9I17qJjZs8OEiWA;r7X0cTdO3kUJ
Kim, H., Kim, Y., & Hong, J. (2019, September). Cluster management framework for autonomic machine learning platform. In Proceedings of the Conference on Research in Adaptive and Convergent Systems (pp. 128-130).;1_ml_machine_data_learning;2019;Cluster management framework for autonomic machine learning platform;Heejin Kim, Younggwan Kim, Jiman Hong;Proceedings of the Conference on Research in Adaptive and Convergent Systems, 128-130, 2019;Autonomic machine learning platforms must provide the necessary management tasks while monitoring the execution status of remotely running machine learning tasks and the performance of the model being trained. In this paper, we design a cluster management framework. The proposed cluster management framework monitors distributed computing resources so that it helps the autonomic machine learning platform to select the proper machine learning algorithm and to execute the proper machine learning model.;https://dl.acm.org/doi/abs/10.1145/3338840.3355691?casa_token=KMAvbw-S5UIAAAAA:zl0DFLLWeLwvoLE1ehKDGkmIrmd9sQ_XJmmKn_83Leg8af72GSE0ehXFfJzi-43FKy0yhWcbgRTN1g;CMAKru9bDPUJ
Gharibi, G., Walunj, V., Rella, S., & Lee, Y. (2019, May). Modelkb: towards automated management of the modeling lifecycle in deep learning. In 2019 IEEE/ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE) (pp. 28-34). IEEE.;1_ml_machine_data_learning;2019;Modelkb: towards automated management of the modeling lifecycle in deep learning;Gharib Gharibi, Vijay Walunj, Sirisha Rella, Yugyung Lee;2019 IEEE/ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE), 28-34, 2019;"Deep Learning has improved the state-of-the-art results in an ever-growing number of domains. This success heavily relies on the development and training of deep learning models, also known as deep neural networks (DNN). Often, developing a DNN is an ad-hoc, iterative process that results in producing tens to hundreds of models before arriving at a satisfactory result. While there has been a surge in the number of tools and frameworks that aim at facilitating deep learning, the issues of model management have been largely ignored. In particular, deep learning practitioners have to manually track their experiments using text files, spreadsheets or folder hierarchies, which is expensive, time-consuming, and error-prone. In this paper, we present our ongoing work and vision towards automating end-to-end model management in deep learning. Specifically, we introduce a tool prototype, named ModelKB, that can automatically (1) extract and store the model's metadata-including its architecture, weights, and configuration; (2) visualize, query, and compare experiments; and (3) reproduce experiments. Our overarching goal is to automate the model management process with minimal user intervention using the user's favorite framework. We report the current status of ModelKB, a pilot user study, and the challenges of automating model management in deep learning.";https://ieeexplore.ieee.org/abstract/document/8823655/?casa_token=VqsYhXE7r4wAAAAA:_JmTvYom9xd7uNzUtnaXLHii3emqiAmWTp-p0qorz-oFLuqXdjr6xP8TgANj0Sm_3cL-NqXIhQY;5cuJ8CcQlY4J
Colomo-Palacios, R. (2019, April). Towards a Software engineering framework for the design, construction and deployment of machine learning-based solutions in digitalization processes. In The International Research & Innovation Forum (pp. 343-349). Cham: Springer International Publishing.;1_ml_machine_data_learning;2019;Towards a Software engineering framework for the design, construction and deployment of machine learning-based solutions in digitalization processes;Ricardo Colomo-Palacios;The International Research & Innovation Forum, 343-349, 2019;There is an increasing demand of digitalization technologies in almost all aspects in modern life. A swelling part of these technologies and solutions are based on Machine Learning technologies. As a consequence of this, there is a need to develop these solutions in a sound and solid way to increase software quality in its eight characteristics: functional suitability, reliability, performance efficiency, usability, security, compatibility, maintainability and portability. To do so, it is needed to adopt software engineering and information systems standards to support the process. This paper aims to draw the path towards a framework to support digitalization processes based on machine-learning solutions.;https://link.springer.com/chapter/10.1007/978-3-030-30809-4_31;owydefdbH6UJ
Manolache, F. B., & Rusu, O. (2016, September). General valuation framework for artificial intelligence models. In 2016 15th RoEduNet Conference: Networking in Education and Research (pp. 1-7). IEEE.;1_ml_machine_data_learning;2016;General valuation framework for artificial intelligence models;Florin B Manolache, Octavian Rusu;2016 15th RoEduNet Conference: Networking in Education and Research, 1-7, 2016;Artificial Intelligence (AI) is a fast developing area that is applied to many daily problems, replacing the tried and true heuristics used by society for a long time. This paper provides a framework to estimate the value added by different steps and components used to create and apply AI models. Such estimations are useful to decide if deploying AI models makes economical sense, reported to the specific problem, data availability, existing domain expertise, and deployment costs. This framework leads to analysis of interesting subtle concepts like the difference between knowing the future and being able to predict the future, even if the prediction would be perfect. Such concepts are useful for benchmarking the efficiency of the model. As an example of applying the framework, real data from a bank was used to determine ways to decrease the operating costs by convincing new customers to opt for electronic statements. The results show that AI models are useful every time when heuristics can be successfully applied, but the improvement brought by AI may not always be spectacular, even in the case of complete information and perfect model. So estimating the real value of adding AI is important because it allows comparison between savings and deployment costs, as well as comparison between models.;https://ieeexplore.ieee.org/abstract/document/7753204/?casa_token=4s3SY5J5Q9gAAAAA:8kSLyASySue4pjMgzsTpoFpqhOnxR7lM-ZThmHw_Qc45CIh2FXisS1bUsdwlR7LWJrFbZ1xCSXY;pOGhIf5dVQoJ
Piantadosi, G., Marrone, S., & Sansone, C. (2019). On reproducibility of deep convolutional neural networks approaches. In Reproducible Research in Pattern Recognition: Second International Workshop, RRPR 2018, Beijing, China, August 20, 2018, Revised Selected Papers 2 (pp. 104-109). Springer International Publishing.;11_deep_network_model_layer;2019;On reproducibility of deep convolutional neural networks approaches;Gabriele Piantadosi, Stefano Marrone, Carlo Sansone;Reproducible Research in Pattern Recognition: Second International Workshop, RRPR 2018, Beijing, China, August 20, 2018, Revised Selected Papers 2, 104-109, 2019;Nowadays, Machine Learning techniques are more and more pervasive in several application fields. In order to perform an evaluation as reliable as possible, it is necessary to consider the reproducibility of these models both at training and inference time. With the introduction of Deep Learning (DL), the assessment of reproducibility became a critical issue due to heuristic considerations made at training time that, although improving the optimization performances of such complex models, can result in non-deterministic outcomes and, therefore, not reproducible models. The aim of this paper is to quantitatively highlight the reproducibility problem of DL approaches, proposing to overcome it by using statistical considerations. We show that, even if the models generated by using several times the same data show differences in the inference phase, the obtained results are not statistically different. In particular, this short paper analyzes, as a case study, our ICPR2018 DL based approach for the breast segmentation in DCE-MRI, demonstrating the reproducibility of the reported results.;https://link.springer.com/chapter/10.1007/978-3-030-23987-9_10;lRoe9RGQpIQJ
Ma, L., Juefei-Xu, F., Xue, M., Li, B., Li, L., Liu, Y., & Zhao, J. (2019, February). Deepct: Tomographic combinatorial testing for deep learning systems. In 2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER) (pp. 614-618). IEEE.;4_dl_testing_deep_network;2019;DeepCT: Tomographic Combinatorial Testing for Deep Learning Systems;Lei Ma, Felix Juefei-Xu, Minhui Xue, Bo Li, Li Li, Yang Liu, Jianjun Zhao;Todo;Deep learning (DL) has achieved remarkable progress over the past decade and has been widely applied to many industry domains. However, the robustness of DL systems recently becomes great concerns, where minor perturbation on the input might cause the DL malfunction. These robustness issues could potentially result in severe consequences when a DL system is deployed to safety-critical applications and hinder the real-world deployment of DL systems. Testing techniques enable the robustness evaluation and vulnerable issue detection of a DL system at an early stage. The main challenge of testing a DL system attributes to the high dimensionality of its inputs and large internal latent feature space, which makes testing each state almost impossible. For traditional software, combinatorial testing (CT) is an effective testing technique to balance the testing exploration effort and defect detection capabilities. In this paper, we perform an exploratory study of CT on DL systems. We propose a set of combinatorial testing criteria specialized for DL systems, as well as a CT coverage guided test generation technique. Our evaluation demonstrates that CT provides a promising avenue for testing DL systems.;https://ieeexplore.ieee.org/abstract/document/8668044;Todo
Talbot, J., Lee, B., Kapoor, A., & Tan, D. S. (2009, April). EnsembleMatrix: interactive visualization to support machine learning with multiple classifiers. In Proceedings of the SIGCHI conference on human factors in computing systems (pp. 1283-1292).;3_explanation_model_machine_learning;2009;EnsembleMatrix: interactive visualization to support machine learning with multiple classifiers;Justin Talbot, Bongshin Lee, Ashish Kapoor, Desney S Tan;Proceedings of the SIGCHI conference on human factors in computing systems, 1283-1292, 2009;Machine learning is an increasingly used computational tool within human-computer interaction research. While most researchers currently utilize an iterative approach to refining classifier models and performance, we propose that ensemble classification techniques may be a viable and even preferable alternative. In ensemble learning, algorithms combine multiple classifiers to build one that is superior to its components. In this paper, we present EnsembleMatrix, an interactive visualization system that presents a graphical view of confusion matrices to help users understand relative merits of various classifiers. EnsembleMatrix allows users to directly interact with the visualizations in order to explore and build combination models. We evaluate the efficacy of the system and the approach in a user study. Results show that users are able to quickly combine multiple classifiers operating on multiple feature sets to produce an ensemble classifier with accuracy that approaches best-reported performance classifying images in the CalTech-101 dataset.;https://dl.acm.org/doi/abs/10.1145/1518701.1518895?casa_token=djUfE6IAPG4AAAAA:hrCX9HVWeIq8l03rTc4_oEIk5EoHfMdBVQabRMQWdxkF2O0X-aHqlUn8QH3wE9213A2NM1YZJZddlw;xvkHCuUjMqUJ
Lim, B. Y., Dey, A. K., & Avrahami, D. (2009, April). Why and why not explanations improve the intelligibility of context-aware intelligent systems. In Proceedings of the SIGCHI conference on human factors in computing systems (pp. 2119-2128).;3_explanation_model_machine_learning;2009;Why and why not explanations improve the intelligibility of context-aware intelligent systems;Brian Y Lim, Anind K Dey, Daniel Avrahami;Proceedings of the SIGCHI conference on human factors in computing systems, 2119-2128, 2009;Context-aware intelligent systems employ implicit inputs, and make decisions based on complex rules and machine learning models that are rarely clear to users. Such lack of system intelligibility can lead to loss of user trust, satisfaction and acceptance of these systems. However, automatically providing explanations about a system's decision process can help mitigate this problem. In this paper we present results from a controlled study with over 200 participants in which the effectiveness of different types of explanations was examined. Participants were shown examples of a system's operation along with various automatically generated explanations, and then tested on their understanding of the system. We show, for example, that explanations describing why the system behaved a certain way resulted in better understanding and stronger feelings of trust. Explanations describing why the system did not behave a certain way, resulted in lower understanding yet adequate performance. We discuss implications for the use of our findings in real-world context-aware applications.;https://dl.acm.org/doi/abs/10.1145/1518701.1519023?casa_token=hvlZVZy3vA0AAAAA:6OmVws6DAe2tEB94CF-HgabF6Mhsh0tNXLvpnCKPTrWkJyFhxIgKr5P9v0LN-FVy3WlOse_ehDMysQ;DlMl3RKELBoJ
Freitas, A. A. (2014). Comprehensible classification models: a position paper. ACM SIGKDD explorations newsletter, 15(1), 1-10.;3_explanation_model_machine_learning;2014;Comprehensible classification models: a position paper;Alex A Freitas;ACM SIGKDD explorations newsletter 15 (1), 1-10, 2014;The vast majority of the literature evaluates the performance of classification models using only the criterion of predictive accuracy. This paper reviews the case for considering also the comprehensibility (interpretability) of classification models, and discusses the interpretability of five types of classification models, namely decision trees, classification rules, decision tables, nearest neighbors and Bayesian network classifiers. We discuss both interpretability issues which are specific to each of those model types and more generic interpretability issues, namely the drawbacks of using model size as the only criterion to evaluate the comprehensibility of a model, and the use of monotonicity constraints to improve the comprehensibility and acceptance of classification models by users.;https://dl.acm.org/doi/abs/10.1145/2594473.2594475?casa_token=a19rwpfV3-MAAAAA:ZJWZWmmifcweTiu4DAUQoVETw4BTSAzRlhDLJLi4tcWiIu0nHp9pvZpxxeX_YJpyNGIe5yEfI2NO6A;T7rfS2oWBD0J
"Ribeiro, M. T., Singh, S., & Guestrin, C. (2016, August). "" Why should i trust you?"" Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144).";3_explanation_model_machine_learning;2016;""" Why should i trust you?"" Explaining the predictions of any classifier";Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin;Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 1135-1144, 2016;Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.;https://dl.acm.org/doi/abs/10.1145/2939672.2939778;BzSVTOaoGOgJ
Cai, Z., Gao, Z. J., Luo, S., Perez, L. L., Vagena, Z., & Jermaine, C. (2014, June). A comparison of platforms for implementing and running very large scale machine learning algorithms. In Proceedings of the 2014 ACM SIGMOD international conference on Management of data (pp. 1371-1382).;1_ml_machine_data_learning;2014;A comparison of platforms for implementing and running very large scale machine learning algorithms;Zhuhua Cai, Zekai J Gao, Shangyu Luo, Luis L Perez, Zografoula Vagena, Christopher Jermaine;Proceedings of the 2014 ACM SIGMOD international conference on Management of data, 1371-1382, 2014;"We describe an extensive benchmark of platforms available to a user who wants to run a machine learning (ML) inference algorithm over a very large data set, but cannot find an existing implementation and thus must ""roll her own"" ML code. We have carefully chosen a set of five ML implementation tasks that involve learning relatively complex, hierarchical models. We completed those tasks on four different computational platforms, and using 70,000 hours of Amazon EC2 compute time, we carefully compared running times, tuning requirements, and ease-of-programming of each.";https://dl.acm.org/doi/abs/10.1145/2588555.2593680;LAAiaqI1BbUJ
Krishnan, S., Franklin, M. J., Goldberg, K., Wang, J., & Wu, E. (2016, June). Activeclean: An interactive data cleaning framework for modern machine learning. In Proceedings of the 2016 International Conference on Management of Data (pp. 2117-2120).;1_ml_machine_data_learning;2016;Activeclean: An interactive data cleaning framework for modern machine learning;Sanjay Krishnan, Michael J Franklin, Ken Goldberg, Jiannan Wang, Eugene Wu;Proceedings of the 2016 International Conference on Management of Data, 2117-2120, 2016;Databases can be corrupted with various errors such as missing, incorrect, or inconsistent values. Increasingly, modern data analysis pipelines involve Machine Learning, and the effects of dirty data can be difficult to debug.Dirty data is often sparse, and naive sampling solutions are not suited for high-dimensional models. We propose ActiveClean, a progressive framework for training Machine Learning models with data cleaning. Our framework updates a model iteratively as the analyst cleans small batches of data, and includes numerous optimizations such as importance weighting and dirty data detection. We designed a visual interface to wrap around this framework and demonstrate ActiveClean for a video classification problem and a topic modeling problem.;https://dl.acm.org/doi/abs/10.1145/2882903.2899409;yopX_mDK0UYJ
Xu, L., Huang, S., Hui, S., Elmore, A. J., & Parameswaran, A. (2017, May). Orpheusdb: a lightweight approach to relational dataset versioning. In Proceedings of the 2017 ACM International Conference on Management of Data (pp. 1655-1658).;9_data_science_software_process;2017;Orpheusdb: a lightweight approach to relational dataset versioning;Liqi Xu, Silu Huang, SiLi Hui, Aaron J Elmore, Aditya Parameswaran;Proceedings of the 2017 ACM International Conference on Management of Data, 1655-1658, 2017;"We demonstrate OrpheusDB, a lightweight approach to versioning of relational datasets. OrpheusDB is built as a thin layer on top of standard relational databases, and therefore inherits much of their benefits while also compactly storing, tracking, and recreating dataset versions on demand. OrpheusDB also supports a range of querying modalities spanning both SQL and git-style version commands. Conference attendees will be able to interact with OrpheusDB via an interactive version browser interface. The demo will highlight underlying design decisions of OrpheusDB, and provide an understanding of how OrpheusDB translates versioning commands into commands understood by a database system that is unaware of the presence of versions. OrpheusDB has been developed as open-source software; code is available at http://orpheus-db.github.io.";https://dl.acm.org/doi/abs/10.1145/3035918.3058744;K77P_lEQYzgJ
Van Der Weide, T., Papadopoulos, D., Smirnov, O., Zielinski, M., & Van Kasteren, T. (2017, May). Versioning for end-to-end machine learning pipelines. In Proceedings of the 1st Workshop on Data Management for End-to-End Machine Learning (pp. 1-9).;1_ml_machine_data_learning;2017;Versioning for end-to-end machine learning pipelines;Tom Van Der Weide, Dimitris Papadopoulos, Oleg Smirnov, Michal Zielinski, Tim Van Kasteren;Proceedings of the 1st Workshop on Data Management for End-to-End Machine Learning, 1-9, 2017;End-to-end machine learning pipelines that run in shared environments are challenging to implement. Production pipelines typically consist of multiple interdependent processing stages. Between stages, the intermediate results are persisted to reduce redundant computation and to improve robustness. Those results might come in the form of datasets for data processing pipelines or in the form of model coefficients in case of model training pipelines. Reusing persisted results improves efficiency but at the same time creates complicated dependencies. Every time one of the processing stages is changed, either due to code change or due to parameters change, it becomes difficult to find which datasets can be reused and which should be recomputed.In this paper we build upon previous work to produce derivations of datasets to ensure that multiple versions of a pipeline can run in parallel while minimizing the amount of redundant computations. Our extensions include partial derivations to simplify navigation and reuse, explicit support for schema changes of pipelines, and a central registry of running pipelines to coordinate upgrading pipelines between teams.;https://dl.acm.org/doi/abs/10.1145/3076246.3076248;f3zh62ZjhkAJ
Binnig, C., Buratti, B., Chung, Y., Cousins, C., Kraska, T., Shang, Z., ... & Zgraggen, E. (2018, June). Towards interactive curation & automatic tuning of ml pipelines. In Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning (pp. 1-4).;1_ml_machine_data_learning;2018;Towards interactive curation & automatic tuning of ml pipelines;Carsten Binnig, Benedetto Buratti, Yeounoh Chung, Cyrus Cousins, Tim Kraska, Zeyuan Shang, Eli Upfal, Robert Zeleznik, Emanuel Zgraggen;Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning, 1-4, 2018;Democratizing Data Science requires a fundamental rethinking of the way data analytics and model discovery is done. Available tools for analyzing massive data sets and curating machine learning models are limited in a number of fundamental ways. First, existing tools require well-trained data scientists to select the appropriate techniques to build models and to evaluate their outcomes. Second, existing tools require heavy data preparation steps and are often too slow to give interactive feedback to domain experts in the model building process, severely limiting the possible interactions. Third, current tools do not provide adequate analysis of statistical risk factors in the model development. In this work, we present the first iteration of QuIC-M (pronounced quick-m), an interactive human-in-the-loop data exploration and model building suite. The goal is to enable domain experts to build the machine learning pipelines an order of magnitude faster than machine learning experts while having model qualities comparable to expert solutions.;https://dl.acm.org/doi/abs/10.1145/3209889.3209891;aWiiSWUYy1oJ
Gharibi, G., Walunj, V., Alanazi, R., Rella, S., & Lee, Y. (2019, June). Automated management of deep learning experiments. In Proceedings of the 3rd International Workshop on Data Management for End-to-End Machine Learning (pp. 1-4).;1_ml_machine_data_learning;2019;Automated management of deep learning experiments;Gharib Gharibi, Vijay Walunj, Rakan Alanazi, Sirisha Rella, Yugyung Lee;Proceedings of the 3rd International Workshop on Data Management for End-to-End Machine Learning, 1-4, 2019;Developing a deep learning model is an iterative, experimental process that produces tens to hundreds of models before arriving at a satisfactory result. While there has been a surge in the number of software tools that aim to facilitate deep learning, the process of managing the models and their artifacts is still surprisingly challenging and time-consuming. Existing model-management solutions are either tailored for commercial platforms or require significant code changes. In this paper, we introduce a lightweight system, named ModelKB, that can automatically extract and manage the model's metadata and provenance information (e.g., the used datasets and hyperparameters). Our overarching goal is to automate the management of deep learning experiments with minimal user intervention. Moreover, ModelKB provides a stepping stone to facilitate model selection and reproducibility.;https://dl.acm.org/doi/abs/10.1145/3329486.3329495;E9Bs5XqRdwcJ
Shang, Z., Zgraggen, E., Buratti, B., Kossmann, F., Eichmann, P., Chung, Y., ... & Kraska, T. (2019, June). Democratizing data science through interactive curation of ml pipelines. In Proceedings of the 2019 international conference on management of data (pp. 1171-1188).;1_ml_machine_data_learning;2019;Democratizing data science through interactive curation of ml pipelines;Zeyuan Shang, Emanuel Zgraggen, Benedetto Buratti, Ferdinand Kossmann, Philipp Eichmann, Yeounoh Chung, Carsten Binnig, Eli Upfal, Tim Kraska;Proceedings of the 2019 international conference on management of data, 1171-1188, 2019;"Statistical knowledge and domain expertise are key to extract actionable insights out of data, yet such skills rarely coexist together. In Machine Learning, high-quality results are only attainable via mindful data preprocessing, hyperparameter tuning and model selection. Domain experts are often overwhelmed by such complexity, de-facto inhibiting a wider adoption of ML techniques in other fields. Existing libraries that claim to solve this problem, still require well-trained practitioners. Those frameworks involve heavy data preparation steps and are often too slow for interactive feedback from the user, severely limiting the scope of such systems.In this paper we present Alpine Meadow, a first Interactive Automated Machine Learning tool. What makes our system unique is not only the focus on interactivity, but also the combined systemic and algorithmic design approach; on one hand we leverage ideas from query optimization, on the other we devise novel selection and pruning strategies combining cost-based Multi-Armed Bandits and Bayesian Optimization.We evaluate our system on over 300 datasets and compare against other AutoML tools, including the current NIPS winner, as well as expert solutions. Not only is Alpine Meadow able to significantly outperform the other AutoML systems while --- in contrast to the other systems --- providing interactive latencies, but also outperforms in 80% of the cases expert solutions over data sets we have never seen before.";https://dl.acm.org/doi/abs/10.1145/3299869.3319863;RK2SVGDN53YJ
Lécuyer, M., Spahn, R., Vodrahalli, K., Geambasu, R., & Hsu, D. (2019, October). Privacy accounting and quality control in the sage differentially private ML platform. In Proceedings of the 27th ACM Symposium on Operating Systems Principles (pp. 181-195).;0_federated_learning_data_privacy;2019;Privacy accounting and quality control in the sage differentially private ML platform;Mathias LÃ©cuyer, Riley Spahn, Kiran Vodrahalli, Roxana Geambasu, Daniel Hsu;Proceedings of the 27th ACM Symposium on Operating Systems Principles, 181-195, 2019;Companies increasingly expose machine learning (ML) models trained over sensitive user data to untrusted domains, such as end-user devices and wide-access model stores. This creates a need to control the data's leakage through these models. We present Sage, a differentially private (DP) ML platform that bounds the cumulative leakage of training data through models. Sage builds upon the rich literature on DP ML algorithms and contributes pragmatic solutions to two of the most pressing systems challenges of global DP: running out of privacy budget and the privacy-utility tradeoff. To address the former, we develop block composition, a new privacy loss accounting method that leverages the growing database regime of ML workloads to keep training models endlessly on a sensitive data stream while enforcing a global DP guarantee for the stream. To address the latter, we develop privacy-adaptive training, a process that trains a model on growing amounts of data and/or with increasing privacy parameters until, with high probability, the model meets developer-configured quality criteria. Sage's methods are designed to integrate with TensorFlow-Extended, Google's open-source ML platform. They illustrate how a systems focus on characteristics of ML workloads enables pragmatic solutions that are not apparent when one focuses on individual algorithms, as most DP ML literature does.;https://dl.acm.org/doi/abs/10.1145/3341301.3359639;Em78sa3GoRwJ
Santos, A., Castelo, S., Felix, C., Ono, J. P., Yu, B., Hong, S. R., ... & Freire, J. (2019, July). Visus: An interactive system for automatic machine learning model building and curation. In Proceedings of the Workshop on Human-In-the-Loop Data Analytics (pp. 1-7).;1_ml_machine_data_learning;2019;Visus: An interactive system for automatic machine learning model building and curation;Aécio Santos, Sonia Castelo, Cristian Felix, Jorge Piazentin Ono, Bowen Yu, Sungsoo Ray Hong, Cláudio T Silva, Enrico Bertini, Juliana Freire;Proceedings of the Workshop on Human-In-the-Loop Data Analytics, 1-7, 2019;While the demand for machine learning (ML) applications is booming, there is a scarcity of data scientists capable of building such models. Automatic machine learning (AutoML) approaches have been proposed that help with this problem by synthesizing end-to-end ML data processing pipelines. However, these follow a best-effort approach and a user in the loop is necessary to curate and refine the derived pipelines. Since domain experts often have little or no expertise in machine learning, easy-to-use interactive interfaces that guide them throughout the model building process are necessary. In this paper, we present Visus, a system designed to support the model building process and curation of ML data processing pipelines generated by AutoML systems. We describe the framework used to ground our design choices and a usage scenario enabled by Visus. Finally, we discuss the feedback received in user testing sessions with domain experts.;https://dl.acm.org/doi/abs/10.1145/3328519.3329134;y1lfFlZqif0J
Sellam, T., Lin, K., Huang, I., Yang, M., Vondrick, C., & Wu, E. (2019, June). Deepbase: Deep inspection of neural networks. In Proceedings of the 2019 International Conference on Management of Data (pp. 1117-1134).;11_deep_network_model_layer;2019;Deepbase: Deep inspection of neural networks;Thibault Sellam, Kevin Lin, Ian Huang, Michelle Yang, Carl Vondrick, Eugene Wu;Proceedings of the 2019 International Conference on Management of Data, 1117-1134, 2019;Although deep learning models perform remarkably well across a range of tasks such as language translation and object recognition, it remains unclear what high-level logic, if any, they follow. Understanding this logic may lead to more transparency, better model design, and faster experimentation. Recent machine learning research has leveraged statistical methods to identify hidden units that behave (e.g., activate) similarly to human understandable logic, but those analyses require considerable manual effort. Our insight is that many of those studies follow a common analysis pattern, and therefore there is opportunity to provide a declarative abstraction to easily express, execute and optimize them. This paper describes DeepBase, a system to inspect neural network behaviors through a unified interface. We model logic with user-provided hypothesis functions that annotate the data with high-level labels (e.g., part-of-speech tags, image captions). DeepBase lets users quickly identify individual or groups of units that have strong statistical dependencies with desired hypotheses. We discuss how DeepBase can express existing analyses, propose a set of simple and effective optimizations to speed up a standard Python implementation by up to 72x, and reproduce recent studies from the NLP literature.;https://dl.acm.org/doi/abs/10.1145/3299869.3300073;X4G9vISL_mgJ
Whang, S. E., Roh, Y., Song, H., & Lee, J. G. (2023). Data collection and quality challenges in deep learning: A data-centric ai perspective. The VLDB Journal, 32(4), 791-813.;1_ml_machine_data_learning;2023;Data collection and quality challenges in deep learning: A data-centric ai perspective;Steven Euijong Whang, Yuji Roh, Hwanjun Song, Jae-Gil Lee;The VLDB Journal 32 (4), 791-813, 2023;Data-centric AI is at the center of a fundamental shift in software engineering where machine learning becomes the new software, powered by big data and computing infrastructure. Here, software engineering needs to be re-thought where data become a first-class citizen on par with code. One striking observation is that a significant portion of the machine learning process is spent on data preparation. Without good data, even the best machine learning algorithms cannot perform well. As a result, data-centric AI practices are now becoming mainstream. Unfortunately, many datasets in the real world are small, dirty, biased, and even poisoned. In this survey, we study the research landscape for data collection and data quality primarily for deep learning applications. Data collection is important because there is lesser need for feature engineering for recent deep learning approaches, but instead more need for large amounts of data. For data quality, we study data validation, cleaning, and integration techniques. Even if the data cannot be fully cleaned, we can still cope with imperfect data during model training using robust model training techniques. In addition, while bias and fairness have been less studied in traditional data management research, these issues become essential topics in modern machine learning applications. We thus study fairness measures and unfairness mitigation techniques that can be applied before, during, or after model training. We believe that the data management community is well poised to solve these problems.;https://link.springer.com/article/10.1007/s00778-022-00775-9;iE_VJJR_13EJ
Lourenço, R., Freire, J., & Shasha, D. (2019, June). Debugging machine learning pipelines. In Proceedings of the 3rd International Workshop on Data Management for End-to-End Machine Learning (pp. 1-10).;1_ml_machine_data_learning;2019;Debugging machine learning pipelines;Raoni LourenÃ§o, Juliana Freire, Dennis Shasha;Proceedings of the 3rd International Workshop on Data Management for End-to-End Machine Learning, 1-10, 2019;Machine learning tasks entail the use of complex computational pipelines to reach quantitative and qualitative conclusions. If some of the activities in a pipeline produce erroneous or uninformative outputs, the pipeline may fail or produce incorrect results. Inferring the root cause of failures and unexpected behavior is challenging, usually requiring much human thought, and is both time consuming and error prone. We propose a new approach that makes use of iteration and provenance to automatically infer the root causes and derive succinct explanations of failures. Through a detailed experimental evaluation, we assess the cost, precision, and recall of our approach compared to the state of the art. Our source code and experimental data will be available for reproducibility and enhancement.;https://dl.acm.org/doi/abs/10.1145/3329486.3329489;ucmQrcC6b74J
Urban, C., Christakis, M., Wüstholz, V., & Zhang, F. (2020). Perfectly parallel fairness certification of neural networks. Proceedings of the ACM on Programming Languages, 4(OOPSLA), 1-30.;6_fairness_discrimination_bias_decision;2020;Perfectly parallel fairness certification of neural networks;Caterina Urban, Maria Christakis, Valentin WÃ¼stholz, Fuyuan Zhang;Proceedings of the ACM on Programming Languages 4 (OOPSLA), 1-30, 2020;Recently, there is growing concern that machine-learned software, which currently assists or even automates decision making, reproduces, and in the worst case reinforces, bias present in the training data. The development of tools and techniques for certifying fairness of this software or describing its biases is, therefore, critical. In this paper, we propose a perfectly parallel static analysis for certifying fairness of feed-forward neural networks used for classification of tabular data. When certification succeeds, our approach provides definite guarantees, otherwise, it describes and quantifies the biased input space regions. We design the analysis to be sound, in practice also exact, and configurable in terms of scalability and precision, thereby enabling pay-as-you-go certification. We implement our approach in an open-source tool called Libra and demonstrate its effectiveness on neural networks trained on popular datasets.;https://dl.acm.org/doi/abs/10.1145/3428253;r9dlmkk7sVwJ
Agrawal, P., Arya, R., Bindal, A., Bhatia, S., Gagneja, A., Godlewski, J., ... & Wu, M. C. (2019, June). Data platform for machine learning. In Proceedings of the 2019 International Conference on Management of Data (pp. 1803-1816).;1_ml_machine_data_learning;2019;Data Platform for Machine Learning;"Pulkit Agrawal, Rajat Arya, Aanchal Bindal, Sandeep Bhatia, Anupriya Gagneja,
 Joseph Godlewski, Yucheng Low, Timothy Muss, Mudit Manu Paliwal, Sethu Raman,
Vishrut Shah, Bochao Shen, Laura Sugden, Kaiyu Zhao, Ming-Chuan Wu
";SIGMOD '19: Proceedings of the 2019 International Conference on Management of Data;In this paper, we present a purpose-built data management system, MLdp, for all machine learning (ML) datasets. ML applications pose some unique requirements different from common conventional data processing applications, including but not limited to: data lineage and provenance tracking, rich data semantics and formats, integration with diverse ML frameworks and access patterns, trial-and-error driven data exploration and evolution, rapid experimentation, reproducibility of the model training, strict compliance and privacy regulations, etc. Current ML systems/services, often named MLaaS, to-date focus on the ML algorithms, and offer no integrated data management system. Instead, they require users to bring their own data and to manage their own data on either blob storage or on file systems. The burdens of data management tasks, such as versioning and access control, fall onto the users, and not all compliance features, such as terms of use, privacy measures, and auditing, are available. MLdp offers a minimalist and flexible data model for all varieties of data, strong version management to guarantee re-producibility of ML experiments, and integration with major ML frameworks. MLdp also maintains the data provenance to help users track lineage and dependencies among data versions and models in their ML pipelines. In addition to table-stake features, such as security, availability and scalability, MLdp's internal design choices are strongly influenced by the goal to support rapid ML experiment iterations, which cycle through data discovery, data exploration, feature engineering, model training, model evaluation, and back to data discovery. The contributions of this paper are: 1) to recognize the needs and to call out the requirements of an ML data platform, 2) to share our experiences in building MLdp by adopting existing database technologies to the new problem as well as by devising new solutions, and 3) to call for actions from our communities on future challenges.;https://dl.acm.org/doi/abs/10.1145/3299869.3314050;Todo
Shu, A., Markov, K., Vazhenin, A., Cortez, R., Bhalla, S., & Shrestha, S. (2017). Unified user-interface and protocol for managing heterogeneous deep learning services. In New Trends in Intelligent Software Methodologies, Tools and Techniques (pp. 563-575). IOS Press.;7_edge_computing_deep_learning;2017;Unified user-interface and protocol for managing heterogeneous deep learning services;Absalom Shu, Konstantin Markov, Alexander Vazhenin, Ruth Cortez, Subhash Bhalla, Shashank Shrestha;New Trends in Intelligent Software Methodologies, Tools and Techniques, 563-575, 2017;In the last decade, cheaper and more powerful computations have favored a sufficient surge in research and development of applications in the fields of machine and deep learning. Though often varying in approach, these activities aim mostly at solving similar tasks such as speech synthesis, emotion detection, image recognition, mathematical computations etc. Usually, the typical scenario of using designed algorithms/applications includes inputting data represented in some predefined formats and launching a corresponding program tool that produces expected output data as well as analyzing obtained results. These applications are numerous, and are created by different research teams and laboratories using different techniques and environments including locations, interfaces and procedures to access, query and use them. In a collaborative working environment, developing independent user interfaces for each deep learning application could entail a lot of additional development efforts. In the presented paper, we propose a standardized and flexible interface to reduce design efforts, based on integration of various Deep Learning services (DL services). We also demonstrate a protocol for communication between the user interface and the heterogeneous services. This platform will enable developers of deep learning models to be concerned solely with developing and tuning their models, which can be easily plugged into the central user interface, conveniently exposing their services to users who will have homogeneous central access to a wide-range of DL services, from a unified interface.;https://ebooks.iospress.nl/volumearticle/47598;6_oqLojPuUcJ
Carlini, N., & Wagner, D. (2017, May). Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp) (pp. 39-57). Ieee.;5_adversarial_attack_example_model;2017;Towards evaluating the robustness of neural networks;Nicholas Carlini, David Wagner;2017 ieee symposium on security and privacy (sp), 39-57, 2017;Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x' that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from 95% to 0.5%. In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100% probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.;https://ieeexplore.ieee.org/abstract/document/7958570/;s2cQAMRecF0J
Mohassel, P., & Zhang, Y. (2017, May). Secureml: A system for scalable privacy-preserving machine learning. In 2017 IEEE symposium on security and privacy (SP) (pp. 19-38). IEEE.;0_federated_learning_data_privacy;2017;Secureml: A system for scalable privacy-preserving machine learning;Payman Mohassel, Yupeng Zhang;2017 IEEE symposium on security and privacy (SP), 19-38, 2017;Machine learning is widely used in practice to produce predictive models for applications such as image processing, speech and text recognition. These models are more accurate when trained on large amount of data collected from different sources. However, the massive data collection raises privacy concerns. In this paper, we present new and efficient protocols for privacy preserving machine learning for linear regression, logistic regression and neural network training using the stochastic gradient descent method. Our protocols fall in the two-server model where data owners distribute their private data among two non-colluding servers who train various models on the joint data using secure two-party computation (2PC). We develop new techniques to support secure arithmetic operations on shared decimal numbers, and propose MPC-friendly alternatives to non-linear functions such as sigmoid and softmax that are superior to prior work. We implement our system in C++. Our experiments validate that our protocols are several orders of magnitude faster than the state of the art implementations for privacy preserving linear and logistic regressions, and scale to millions of data samples with thousands of features. We also implement the first privacy preserving system for training neural networks.;https://ieeexplore.ieee.org/abstract/document/7958569/;4HfOKuDVx7EJ
Gehr, T., Mirman, M., Drachsler-Cohen, D., Tsankov, P., Chaudhuri, S., & Vechev, M. (2018, May). Ai2: Safety and robustness certification of neural networks with abstract interpretation. In 2018 IEEE symposium on security and privacy (SP) (pp. 3-18). IEEE.;4_dl_testing_deep_network;2018;Ai2: Safety and robustness certification of neural networks with abstract interpretation;Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, Martin Vechev;2018 IEEE symposium on security and privacy (SP), 3-18, 2018;We present AI 2 , the first sound and scalable analyzer for deep neural networks. Based on overapproximation, AI 2 can automatically prove safety properties (e.g., robustness) of realistic neural networks (e.g., convolutional neural networks). The key insight behind AI 2 is to phrase reasoning about safety and robustness of neural networks in terms of classic abstract interpretation, enabling us to leverage decades of advances in that area. Concretely, we introduce abstract transformers that capture the behavior of fully connected and convolutional neural network layers with rectified linear unit activations (ReLU), as well as max pooling layers. This allows us to handle real-world neural networks, which are often built out of those types of layers. We present a complete implementation of AI 2 together with an extensive evaluation on 20 neural networks. Our results demonstrate that: (i) AI 2 is precise enough to prove useful specifications (e.g., robustness), (ii) AI 2 can be used to certify the effectiveness of state-of-the-art defenses for neural networks, (iii) AI 2 is significantly faster than existing analyzers based on symbolic analysis, which often take hours to verify simple fully connected networks, and (iv) AI 2 can handle deep convolutional networks, which are beyond the reach of existing methods.;https://ieeexplore.ieee.org/abstract/document/8418593/;3KsWR4AIvIQJ
Wang, B., & Gong, N. Z. (2018, May). Stealing hyperparameters in machine learning. In 2018 IEEE symposium on security and privacy (SP) (pp. 36-52). IEEE.;5_adversarial_attack_example_model;2018;Stealing hyperparameters in machine learning;Binghui Wang, Neil Zhenqiang Gong;2018 IEEE symposium on security and privacy (SP), 36-52, 2018;Hyperparameters are critical in machine learning, as different hyperparameters often result in models with significantly different performance. Hyperparameters may be deemed confidential because of their commercial value and the confidentiality of the proprietary algorithms that the learner uses to learn them. In this work, we propose attacks on stealing the hyperparameters that are learned by a learner. We call our attacks hyperparameter stealing attacks. Our attacks are applicable to a variety of popular machine learning algorithms such as ridge regression, logistic regression, support vector machine, and neural network. We evaluate the effectiveness of our attacks both theoretically and empirically. For instance, we evaluate our attacks on Amazon Machine Learning. Our results demonstrate that our attacks can accurately steal hyperparameters. We also study countermeasures. Our results highlight the need for new defenses against our hyperparameter stealing attacks for certain machine learning algorithms.;https://ieeexplore.ieee.org/abstract/document/8418595/;t0Vm0-iXRCkJ
Ling, X., Ji, S., Zou, J., Wang, J., Wu, C., Li, B., & Wang, T. (2019, May). Deepsec: A uniform platform for security analysis of deep learning model. In 2019 IEEE symposium on security and privacy (SP) (pp. 673-690). IEEE.;5_adversarial_attack_example_model;2019;Deepsec: A uniform platform for security analysis of deep learning model;Xiang Ling, Shouling Ji, Jiaxu Zou, Jiannan Wang, Chunming Wu, Bo Li, Ting Wang;2019 IEEE symposium on security and privacy (SP), 673-690, 2019;"Deep learning (DL) models are inherently vulnerable to adversarial examples – maliciously crafted inputs to trigger target DL models to misbehave – which significantly hinders the application of DL in security-sensitive domains. Intensive research on adversarial learning has led to an arms race between adversaries and defenders. Such plethora of emerging attacks and defenses raise many questions: Which attacks are more evasive, preprocessing-proof, or transferable? Which defenses are more effective, utility-preserving, or general? Are ensembles of multiple defenses more robust than individuals? Yet, due to the lack of platforms for comprehensive evaluation on adversarial attacks and defenses, these critical questions remain largely unsolved. In this paper, we present the design, implementation, and evaluation of DEEPSEC, a uniform platform that aims to bridge this gap. In its current implementation, DEEPSEC incorporates 16 state-of-the-art attacks with 10 attack utility metrics, and 13 state-of-the-art defenses with 5 defensive utility metrics. To our best knowledge, DEEPSEC is the first platform that enables researchers and practitioners to (i) measure the vulnerability of DL models, (ii) evaluate the effectiveness of various attacks/defenses, and (iii) conduct comparative studies on attacks/defenses in a comprehensive and informative manner. Leveraging DEEPSEC, we systematically evaluate the existing adversarial attack and defense methods, and draw a set of key findings, which demonstrate DEEPSEC’s rich functionality, such as (1) the trade-off between misclassification and imperceptibility is empirically confirmed; (2) most defenses that claim to be universally applicable can only defend against limited types of attacks under restricted settings; (3) it is not necessary that adversarial examples with higher perturbation magnitude are easier to be detected; (4) the ensemble of multiple defenses cannot improve the overall defense capability, but can improve the lower bound of the defense effectiveness of individuals. Extensive analysis on DEEPSEC demonstrates its capabilities and advantages as a benchmark platform which can benefit future adversarial learning research.";https://ieeexplore.ieee.org/abstract/document/8835375/;J7vEQNow9fIJ
Varley, M., & Belle, V. (2021). Fairness in machine learning with tractable models. Knowledge-Based Systems, 215, 106715.;6_fairness_discrimination_bias_decision;2021;Fairness in machine learning with tractable models;Michael Varley, Vaishak Belle;Knowledge-Based Systems 215, 106715, 2021;Machine Learning techniques have become pervasive across a range of different applications, and are now widely used in areas as disparate as recidivism prediction, consumer credit–risk analysis and insurance pricing. The prevalence of machine learning techniques has raised concerns about the potential for learned algorithms to become biased against certain groups. Many definitions have been proposed in the literature, but the fundamental task of reasoning about probabilistic events is a challenging one, owing to the intractability of inference.The focus of this paper is taking steps towards the application of tractable probabilistic models to fairness in machine learning. Tractable probabilistic models have recently emerged that guarantee that conditional marginal can be computed in time linear in the size of the model.In particular, we show that sum product networks (SPNs) enable an effective technique for determining the statistical relationships between protected attributes and other training variables. We will also motivate the concept of “fairness through percentile equivalence”, a new definition predicated on the notion that individuals at the same percentile of their respective distributions should be treated equivalently, and this prevents unfair penalisation of those individuals who lie at the extremities of their respective distributions.We compare the efficacy of this pre-processing technique with an alternative approach that assumes an additive contribution. It was found that when these two approaches were compared on a data set containing the results of law school applicants, the percentile equivalence method reduced the average underestimation in the exam score of ethnic minority applicants black applicants at the bottom end of their conditional distribution by about a fifth. We conclude by outlining potential improvements to our existing methodology and suggest opportunities for further work in this field.;https://www.sciencedirect.com/science/article/pii/S0950705120308443;GEYlT6Sq2qIJ
Attenberg, J., Ipeirotis, P. G., & Provost, F. J. (2011). Beat the machine: Challenging workers to find the unknown unknowns, in ‘Human Computation’, Vol. WS-11-11 of AAAI Workshops, AAAI.;1_ml_machine_data_learning;2011;Beat the machine: Challenging workers to find the unknown unknowns, in ‘Human Computation’, Vol;J Attenberg, PG Ipeirotis, FJ Provost;WS-11-11 of AAAI Workshops, AAAI, 2011;nothing defined (was only displayed as a citation on google scholar);;ZPQXHswkkSQJ
Luo, B., Liu, Y., Wei, L., & Xu, Q. (2018, April). Towards imperceptible and robust adversarial example attacks against neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 32, No. 1).;5_adversarial_attack_example_model;2018;Towards imperceptible and robust adversarial example attacks against neural networks;Bo Luo, Yannan Liu, Lingxiao Wei, Qiang Xu;Proceedings of the AAAI Conference on Artificial Intelligence 32 (1), 2018;Machine learning systems based on deep neural networks, being able to produce state-of-the-art results on various perception tasks, have gained mainstream adoption in many applications. However, they are shown to be vulnerable to adversarial example attack, which generates malicious output by adding slight perturbations to the input. Previous adversarial example crafting methods, however, use simple metrics to evaluate the distances between the original examples and the adversarial ones, which could be easily detected by human eyes. In addition, these attacks are often not robust due to the inevitable noises and deviation in the physical world. In this work, we present a new adversarial example attack crafting method, which takes the human perceptual system into consideration and maximizes the noise tolerance of the crafted adversarial example. Experimental results demonstrate the efficacy of the proposed technique.;https://ojs.aaai.org/index.php/AAAI/article/view/11499;346FvEY_VxUJ
Ren, K., Zheng, T., Qin, Z., & Liu, X. (2020). Adversarial attacks and defenses in deep learning. Engineering, 6(3), 346-360.;5_adversarial_attack_example_model;2020;Adversarial attacks and defenses in deep learning;Kui Ren, Tianhang Zheng, Zhan Qin, Xue Liu;Engineering 6 (3), 346-360, 2020;With the rapid developments of artificial intelligence (AI) and deep learning (DL) techniques, it is critical to ensure the security and robustness of the deployed algorithms. Recently, the security vulnerability of DL algorithms to adversarial samples has been widely recognized. The fabricated samples can lead to various misbehaviors of the DL models while being perceived as benign by humans. Successful implementations of adversarial attacks in real physical-world scenarios further demonstrate their practicality. Hence, adversarial attack and defense techniques have attracted increasing attention from both machine learning and security communities and have become a hot research topic in recent years. In this paper, we first introduce the theoretical foundations, algorithms, and applications of adversarial attack techniques. We then describe a few research efforts on the defense techniques, which cover the broad frontier in the field. Several open problems and challenges are subsequently discussed, which we hope will provoke further research efforts in this critical area.;https://www.sciencedirect.com/science/article/pii/S209580991930503X;0LIOn4AvkfoJ
Su, J., de Prado, M., Dahyot, R., & Saeed, R. (2019). AI Pipeline-bringing AI to you. End-to-end integration of data, algorithms and deployment tools. In Emerging Deep Learning Accelerators (EDLA) Workshop at HiPEAC 2019 (pp. 1-9). Emerging Deep Learning Accelerators (EDLA) Workshop at HiPEAC 2019.;1_ml_machine_data_learning;2019;AI Pipeline-bringing AI to you. End-to-end integration of data, algorithms and deployment tools;Jing Su, Miguel de Prado, Rozenn Dahyot, Rabia Saeed;Emerging Deep Learning Accelerators (EDLA) Workshop at HiPEAC 2019, 1-9, 2019;Next generation of embedded Information and Communication Technology (ICT) systems are interconnected collaborative intelligent systems able to perform autonomous tasks. Training and deployment of such systems on Edge devices however require a fine-grained integration of data and tools to achieve high accuracy and overcome functional and non-functional requirements. In this work, we present a modular AI pipeline as an integrating framework to bring data, algorithms and deployment tools together. By these means, we are able to interconnect the different entities or stages of particular systems and provide an end-to-end development of AI products. We demonstrate the effectiveness of the AI pipeline by solving an Automatic Speech Recognition challenge and we show that all the steps leading to an end-to-end development for Key-word Spotting tasks: importing, partitioning and pre-processing of speech data, training of different neural network architectures and their deployment on heterogeneous embedded platforms.;http://mural.maynoothuniversity.ie/15157/;VePsJJMxfBYJ
Kahng, M., Andrews, P. Y., Kalro, A., & Chau, D. H. (2017). A cti v is: Visual exploration of industry-scale deep neural network models. IEEE transactions on visualization and computer graphics, 24(1), 88-97.;11_deep_network_model_layer;2017;ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models;Minsuk Kahng, Pierre Y Andrews, Aditya Kalro, Duen Horng Chau;IEEE transactions on visualization and computer graphics 24 (1), 88-97, 2017;While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ActiVis, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance-and subset-level. ActiVis has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ActiVis may work with different models.;https://ieeexplore.ieee.org/abstract/document/8022871/;ci3vzJkk8wAJ
Ming, Y., Qu, H., & Bertini, E. (2018). Rulematrix: Visualizing and understanding classifiers with rules. IEEE transactions on visualization and computer graphics, 25(1), 342-352.;3_explanation_model_machine_learning;2018;Rulematrix: Visualizing and understanding classifiers with rules;Yao Ming, Huamin Qu, Enrico Bertini;IEEE transactions on visualization and computer graphics 25 (1), 342-352, 2018;With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.;https://ieeexplore.ieee.org/abstract/document/8440085/;GKbkffTQYp4J
Hohman, F., Kahng, M., Pienta, R., & Chau, D. H. (2018). Visual analytics in deep learning: An interrogative survey for the next frontiers. IEEE transactions on visualization and computer graphics, 25(8), 2674-2693.;11_deep_network_model_layer;2018;Visual analytics in deep learning: An interrogative survey for the next frontiers;Fred Hohman, Minsuk Kahng, Robert Pienta, Duen Horng Chau;IEEE transactions on visualization and computer graphics 25 (8), 2674-2693, 2018;"Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W's and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.";https://ieeexplore.ieee.org/abstract/document/8371286/;8VPpy4H1FvkJ
Wongsuphasawat, K., Smilkov, D., Wexler, J., Wilson, J., Mane, D., Fritz, D., ... & Wattenberg, M. (2017). Visualizing dataflow graphs of deep learning models in tensorflow. IEEE transactions on visualization and computer graphics, 24(1), 1-12.;7_edge_computing_deep_learning;2017;Visualizing dataflow graphs of deep learning models in tensorflow;Kanit Wongsuphasawat, Daniel Smilkov, James Wexler, Jimbo Wilson, Dandelion Mane, Doug Fritz, Dilip Krishnan, Fernanda B ViÃ©gas, Martin Wattenberg;IEEE transactions on visualization and computer graphics 24 (1), 1-12, 2017;We present a design study of the TensorFlow Graph Visualizer, part of the TensorFlow machine intelligence platform. This tool helps users understand complex machine learning architectures by visualizing their underlying dataflow graphs. The tool works by applying a series of graph transformations that enable standard layout techniques to produce a legible interactive diagram. To declutter the graph, we decouple non-critical nodes from the layout. To provide an overview, we build a clustered graph using the hierarchical structure annotated in the source code. To support exploration of nested structure on demand, we perform edge bundling to enable stable and responsive cluster expansion. Finally, we detect and highlight repeated structures to emphasize a model's modular composition. To demonstrate the utility of the visualizer, we describe example usage scenarios and report user feedback. Overall, users find the visualizer useful for understanding, debugging, and sharing the structures of their models.;https://ieeexplore.ieee.org/abstract/document/8019861/;r3SBu_LqTZQJ
Ahn, Y., & Lin, Y. R. (2019). Fairsight: Visual analytics for fairness in decision making. IEEE transactions on visualization and computer graphics, 26(1), 1086-1095.;6_fairness_discrimination_bias_decision;2019;Fairsight: Visual analytics for fairness in decision making;Yongsu Ahn, Yu-Ru Lin;IEEE transactions on visualization and computer graphics 26 (1), 1086-1095, 2019;"Data-driven decision making related to individuals has become increasingly pervasive, but the issue concerning the potential discrimination has been raised by recent studies. In response, researchers have made efforts to propose and implement fairness measures and algorithms, but those efforts have not been translated to the real-world practice of data-driven decision making. As such, there is still an urgent need to create a viable tool to facilitate fair decision making. We propose FairSight, a visual analytic system to address this need; it is designed to achieve different notions of fairness in ranking decisions through identifying the required actions - understanding, measuring, diagnosing and mitigating biases - that together lead to fairer decision making. Through a case study and user study, we demonstrate that the proposed visual analytic and diagnostic modules in the system are effective in understanding the fairness-aware decision pipeline and obtaining more fair outcomes.";https://ieeexplore.ieee.org/abstract/document/8805420/;RlNKTIefABoJ
Wexler, J., Pushkarna, M., Bolukbasi, T., Wattenberg, M., Viégas, F., & Wilson, J. (2019). The what-if tool: Interactive probing of machine learning models. IEEE transactions on visualization and computer graphics, 26(1), 56-65.;1_ml_machine_data_learning;2019;The what-if tool: Interactive probing of machine learning models;James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda ViÃ©gas, Jimbo Wilson;IEEE transactions on visualization and computer graphics 26 (1), 56-65, 2019;A key challenge in developing and deploying Machine Learning (ML) systems is understanding their performance across a wide range of inputs. To address this challenge, we created the What-If Tool, an open-source application that allows practitioners to probe, visualize, and analyze ML systems, with minimal coding. The What-If Tool lets practitioners test performance in hypothetical situations, analyze the importance of different data features, and visualize model behavior across multiple models and subsets of input data. It also lets practitioners measure systems according to multiple ML fairness metrics. We describe the design of the tool, and report on real-life usage at different organizations.;https://ieeexplore.ieee.org/abstract/document/8807255/;zBeiRiiCOLQJ
Spinner, T., Schlegel, U., Schäfer, H., & El-Assady, M. (2019). explAIner: A visual analytics framework for interactive and explainable machine learning. IEEE transactions on visualization and computer graphics, 26(1), 1064-1074.;3_explanation_model_machine_learning;2019;explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning;Thilo Spinner, Udo Schlegel, Hanna Schäfer, Mennatallah El-Assady;IEEE transactions on visualization and computer graphics, 2019;"We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.";https://ieeexplore.ieee.org/abstract/document/8807299;TODO
Ma, Y., Xie, T., Li, J., & Maciejewski, R. (2019). Explaining vulnerabilities to adversarial machine learning through visual analytics. IEEE transactions on visualization and computer graphics, 26(1), 1075-1085.;5_adversarial_attack_example_model;2019;Explaining vulnerabilities to adversarial machine learning through visual analytics;Yuxin Ma, Tiankai Xie, Jundong Li, Ross Maciejewski;IEEE transactions on visualization and computer graphics 26 (1), 1075-1085, 2019;Machine learning models are currently being deployed in a variety of real-world applications where model predictions are used to make decisions about healthcare, bank loans, and numerous other critical tasks. As the deployment of artificial intelligence technologies becomes ubiquitous, it is unsurprising that adversaries have begun developing methods to manipulate machine learning models to their advantage. While the visual analytics community has developed methods for opening the black box of machine learning models, little work has focused on helping the user understand their model vulnerabilities in the context of adversarial attacks. In this paper, we present a visual analytics framework for explaining and exploring model vulnerabilities to adversarial attacks. Our framework employs a multi-faceted visualization scheme designed to support the analysis of data poisoning attacks from the perspective of models, data instances, features, and local structures. We demonstrate our framework through two case studies on binary classifiers and illustrate model vulnerabilities with respect to varying attack strategies.;https://ieeexplore.ieee.org/abstract/document/8812988/;D_DCE60oF2EJ
Stoffel, F. (2018). Transparency in Interactive Feature-based Machine Learning: Challenges and Solutions (Doctoral dissertation).;3_explanation_model_machine_learning;2018;Transparency in Interactive Feature-based Machine Learning: Challenges and Solutions;Florian Stoffel;-;"Machine learning is ubiquitous in everyday life; techniques from the area of automated data analysis are used in various application scenarios, ranging from recommendations for movies over routes to drive to automated analysis of data in critical domains. To make appropriate use of such techniques, a calibration between human trust and trustworthiness of the machine learning techniques is required. If the calibration does not take place, research shows that disuse and misuse of machine learning techniques may happen. In this thesis, we elaborate on the problem of providing transparency in feature-based machine learning. In particular, we outline a number of challenges and present solutions for transparency. The solutions are based on interactive visual interfaces operating on feature-level. First, we elaborate on the connection between trust and transparency and outline the fundamental framework that builds the ground for this thesis and introduce different audiences of transparency. In the following, we present interactive, visualization and visual analytics-based solutions for specific aspects of transparency. First, the solution for the task of error analysis in supervised learning is presented. The proposed visual analytics system contains a number of coordinated views that facilitate sensemaking and reasoning of the influence of single features or groups of features in the machine learning process. The second solution is a visualization technique tailored to the interactive, visual exploration of ambiguous feature sets that arise in certain machine learning scenarios. Statistical and semantical information is combined to present a clear picture of the targeted type of ambiguities that can be interactively modified, eventually leading to a more specific feature set with fewer ambiguities. Afterward we illustrate how the concept of transparency and observable behavior can be of use in a real-world scenario. We contribute an interactive, visualization-driven system to explore a spatial clustering, giving the human control of the feature set, feature weights, and associated hyperparameters. To observe different behaviors of the spatial clustering, an interactive visualization is provided that allows the comparison of different feature combinations and hyperparameters. In the same application domain, we contribute a visual analytics system that enables analysts to interactively visualize the output of a machine learning system in context with additional data that have a common, spatial context. The system bridges the gap between the analysts utilizing a machine learning system and users of the results, which in the targeted scenario are two different user groups. Our solutions show that both groups profit from insights in the feature set of the machine learning. The thesis concludes with a reflection regarding further research directions and a summary of the results.";https://kops.uni-konstanz.de/bitstream/123456789/44228/3/Stoffel_2-u2rr3yg1on061.pdf;BOTKUCjieuMJ
Hohman, F., Srinivasan, A., & Drucker, S. M. (2019, October). TeleGam: Combining visualization and verbalization for interpretable machine learning. In 2019 IEEE Visualization Conference (VIS) (pp. 151-155). IEEE.;3_explanation_model_machine_learning;2019;TeleGam: Combining visualization and verbalization for interpretable machine learning;Fred Hohman, Arjun Srinivasan, Steven M Drucker;2019 IEEE Visualization Conference (VIS), 151-155, 2019;While machine learning (ML) continues to find success in solving previously-thought hard problems, interpreting and exploring ML models remains challenging. Recent work has shown that visualizations are a powerful tool to aid debugging, analyzing, and interpreting ML models. However, depending on the complexity of the model (e.g., number of features), interpreting these visualizations can be difficult and may require additional expertise. Alternatively, textual descriptions, or verbalizations, can be a simple, yet effective way to communicate or summarize key aspects about a model, such as the overall trend in a model's predictions or comparisons between pairs of data instances. With the potential benefits of visualizations and verbalizations in mind, we explore how the two can be combined to aid ML interpretability. Specifically, we present a prototype system, TeleGam, that demonstrates how visualizations and verbalizations can collectively support interactive exploration of ML models, for example, generalized additive models (GAMs). We describe TELEGAM's interface and underlying heuristics to generate the verbalizations. We conclude by discussing how TeleGam can serve as a platform to conduct future studies for understanding user expectations and designing novel interfaces for interpretable ML.;https://ieeexplore.ieee.org/abstract/document/8933695/;s_7jKplzFj0J
Bock, M., & Schreiber, A. (2018, November). Visualization of neural networks in virtual reality using Unreal Engine. In Proceedings of the 24th ACM symposium on virtual reality software and technology (pp. 1-2).;11_deep_network_model_layer;2018;Visualization of neural networks in virtual reality using Unreal Engine;Marcel Bock, Andreas Schreiber;Proceedings of the 24th ACM symposium on virtual reality software and technology, 1-2, 2018;Many applications today use deep learning to provide intelligent behavior. To understand and explain how deep learning models come to certain decisions can be hard or completely in-transparent. We propose a visualization of convolutional neural networks in Virtual Reality (VR). The interactive application shows the internal processes and allows to inspect the results. Large networks can be visualized in real-time with special rendering techniques.;https://dl.acm.org/doi/abs/10.1145/3281505.3281605;WJygVoCMTrcJ
Wansing, C., Banos, O., Gloesekoetter, P., Pomares, H., & Rojas, I. (2015, November). Development of a platform for the exchange of bio-datasets with integrated opportunities for artificial intelligence using MatLab. In 2015 Third World Conference on Complex Systems (WCCS) (pp. 1-6). IEEE.;1_ml_machine_data_learning;2015;Development of a platform for the exchange of bio-datasets with integrated opportunities for artificial intelligence using MatLab;Christian Wansing, Oresti Banos, Peter Gloesekoetter, Hector Pomares, Ignacio Rojas;2015 Third World Conference on Complex Systems (WCCS), 1-6, 2015;This paper deals with the issue of automating the process of machine learning and analyzing bio-datasets. For this a user-friendly website has been developed for the interaction with the researchers. On this website it is possible to upload datasets and to share them, if desired, with other scientists. The uploaded data can also be analyzed by various methods and functions. The signals inside these datasets can also be visualized. Furthermore, several algorithms have been implemented to create machine learning models with the uploaded data. Based on these generated models new data can be classified or calculated. For all these applications the simplest possible handling was implemented to make the website available to all interested researchers.;https://ieeexplore.ieee.org/abstract/document/7483278/;t6zXL7rvmhIJ
Griebel, M., Dürr, A., & Stein, N. (2019). Applied image recognition: guidelines for using deep learning models in practice.;11_deep_network_model_layer;2019;Applied image recognition: guidelines for using deep learning models in practice;Matthias Griebel, Alexander DÃ¼rr, Nikolai Stein;-;In recent years, novel deep learning techniques, greater data availability, and a significant growth in computing powers have enabled AI researchers to tackle problems that had remained unassailable for many years. Furthermore, the advent of comprehensive AI frameworks offers the unique opportunity for adopting these new tools in applied fields. Information systems research can play a vital role in bridging the gap to practice. To this end, we conceptualize guidelines for applied image recognition spanning task definition, neural net configuration and training procedures. We showcase our guidelines by means of a biomedical research project for image recognition.;https://aisel.aisnet.org/wi2019/track05/papers/2/;IBnoVApG3rYJ
He, W., Wei, J., Chen, X., Carlini, N., & Song, D. (2017). Adversarial example defense: Ensembles of weak defenses are not strong. In 11th USENIX workshop on offensive technologies (WOOT 17).;5_adversarial_attack_example_model;2017;Adversarial example defense: Ensembles of weak defenses are not strong;Warren He, James Wei, Xinyun Chen, Nicholas Carlini, Dawn Song;11th USENIX workshop on offensive technologies (WOOT 17), 2017;Ongoing research has proposed several methods to defend neural networks against adversarial examples, many of which researchers have shown to be ineffective. We ask whether a strong defense can be created by combining multiple (possibly weak) defenses. To answer this question, we study three defenses that follow this approach. Two of these are recently proposed defenses that intentionally combine components designed to work well together. A third defense combines three independent defenses. For all the components of these defenses and the combined defenses themselves, we show that an adaptive adversary can create adversarial examples successfully with low distortion. Thus, our work implies that ensemble of weak defenses is not sufficient to provide strong defense against adversarial examples.;https://www.usenix.org/conference/woot17/workshop-program/presentation/he;diy9fD1i_VMJ
Yeung, J., Wong, S., Tam, A., & So, J. (2019, July). Integrating machine learning technology to data analytics for e-commerce on cloud. In 2019 Third World Conference on Smart Trends in Systems Security and Sustainablity (WorldS4) (pp. 105-109). IEEE.;1_ml_machine_data_learning;2019;Integrating machine learning technology to data analytics for e-commerce on cloud;John Yeung, Simon Wong, Alvin Tam, Joseph So;2019 Third World Conference on Smart Trends in Systems Security and Sustainablity (WorldS4), 105-109, 2019;This paper presents the technical challenges faced bye-commerce players and how cloud computing has been designed and developed to handle these technical challenges. Once these technical challenges have been handled, another challenge of building data analytics on cloud platforms come out. This paper also highlights the main reasons of driving organizations to build data analytics on cloud. Besides, this paper exhibits how to integrate machine learning models to the data analytic processes, for making more sophisticated analysis for e-commerce activities. The platform Amazon SageMaker is adopted to illustrate how machine learning models were integrated in the data analytic processes. The public cloud platform Amazon Web Services is used to present how machine learning models were integrated in the practices with a real-life c-commerce case.;https://ieeexplore.ieee.org/abstract/document/8904026/;VvpbbBsDCTIJ
Harašta, J. (2019, June). Trust by Discrimination: Technology Specific Regulation & Explainable AI. CEUR Workshop Proceedings.;12_ai_ethical_ethic_intelligence;2019;Trust by Discrimination: Technology Specific Regulation & Explainable AI;Jakub HaraÅ¡ta;CEUR Workshop Proceedings, 2019;Paper discusses the possibility to bare non-explainable AI from deployment in certain services requiring higher transparency. Paper argues that in spite of claiming that regulation should be 'technology neutral', regulating specific technology differently in different contexts may yield desirable regulatory outcomes.;https://is.muni.cz/publication/1541157/cs/Trust-by-Discrimination-Technology-Specific-Regulation-Explainable-AI/Harasta;jIY06ewtZOQJ
Biggio, B., Nelson, B., & Laskov, P. (2012). Poisoning attacks against support vector machines. arXiv preprint arXiv:1206.6389.;5_adversarial_attack_example_model;2012;Poisoning attacks against support vector machines;Battista Biggio, Blaine Nelson, Pavel Laskov;arXiv preprint arXiv:1206.6389, 2012;We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data. The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.;https://arxiv.org/abs/1206.6389;vMsUiQAe0ZIJ
Braun, M. L., & Ong, C. S. (2018). Open science in machine learning. In Implementing Reproducible Research (pp. 343-365). Chapman and Hall/CRC.;1_ml_machine_data_learning;2018;Open Science in Machine Learning;Mikio L. Braun, Cheng Soon Ong;Implementing Reproducible Research, 2018;The advent of Big Data has resulted in an urgent need for flexible analysis tools. Machine learning addresses part of this need, providing a stable of potential computational models for extracting knowledge from the flood of data. In contrast to many areas of the natural sciences, such as physics, chemistry, and biology, machine learning can be studied in an algorithmic 344and computational fashion. In principle, machine learning experiments can be precisely defined, leading to perfectly reproducible research. In fact, there have been several efforts in the past of frameworks for reproducible experiments [3,13,15,30]. Since machine learning is a data-driven approach, we need to have access to carefully structured data [25], that would enable direct comparison of the results of statistical estimation procedures. Some headway has been seen in the statistics and bioinformatics community. The success of R and Bioconductor [8,9] as well as projects such as Sweave [15] and Orgmode [29] have resulted in the possibility to embed the code that produces the results of the paper in the paper itself. The idea is to have a unified computation and presentation, with the hope that it results in reproducible research [14,24].;https://www.taylorfrancis.com/chapters/edit/10.1201/9781315373461-13/open-science-machine-learning-mikio-braun-cheng-soon-ong;TODO
Suciu, O., Marginean, R., Kaya, Y., Daume III, H., & Dumitras, T. (2018). When does machine learning {FAIL}? generalized transferability for evasion and poisoning attacks. In 27th USENIX Security Symposium (USENIX Security 18) (pp. 1299-1316).;5_adversarial_attack_example_model;2018;When does machine learning {FAIL}? generalized transferability for evasion and poisoning attacks;Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, Tudor Dumitras;27th USENIX Security Symposium (USENIX Security 18), 1299-1316, 2018;Recent results suggest that attacks against supervised machine learning systems are quite effective, while defenses are easily bypassed by new attacks. However, the specifications for machine learning systems currently lack precise adversary definitions, and the existing attacks make diverse, potentially unrealistic assumptions about the strength of the adversary who launches them. We propose the FAIL attacker model, which describes the adversary's knowledge and control along four dimensions. Our model allows us to consider a wide range of weaker adversaries who have limited control and incomplete knowledge of the features, learning algorithms and training instances utilized. To evaluate the utility of the FAIL model, we consider the problem of conducting targeted poisoning attacks in a realistic setting: the crafted poison samples must have clean labels, must be individually and collectively inconspicuous, and must exhibit a generalized form of transferability, defined by the FAIL model. By taking these constraints into account, we design StingRay, a targeted poisoning attack that is practical against 4 machine learning applications, which use 3 different learning algorithms, and can bypass 2 existing defenses. Conversely, we show that a prior evasion attack is less effective under generalized transferability. Such attack evaluations, under the FAIL adversary model, may also suggest promising directions for future defenses.;https://www.usenix.org/conference/usenixsecurity18/presentation/suciu;h8SffNeyIwEJ
Zliobaite, I. (2015). A survey on measuring indirect discrimination in machine learning. arXiv preprint arXiv:1511.00148.;6_fairness_discrimination_bias_decision;2015;A survey on measuring indirect discrimination in machine learning;Indre Zliobaite;arXiv preprint arXiv:1511.00148, 2015;Nowadays, many decisions are made using predictive models built on historical data.Predictive models may systematically discriminate groups of people even if the computing process is fair and well-intentioned. Discrimination-aware data mining studies how to make predictive models free from discrimination, when historical data, on which they are built, may be biased, incomplete, or even contain past discriminatory decisions. Discrimination refers to disadvantageous treatment of a person based on belonging to a category rather than on individual merit. In this survey we review and organize various discrimination measures that have been used for measuring discrimination in data, as well as in evaluating performance of discrimination-aware predictive models. We also discuss related measures from other disciplines, which have not been used for measuring discrimination, but potentially could be suitable for this purpose. We computationally analyze properties of selected measures. We also review and discuss measuring procedures, and present recommendations for practitioners. The primary target audience is data mining, machine learning, pattern recognition, statistical modeling researchers developing new methods for non-discriminatory predictive modeling. In addition, practitioners and policy makers would use the survey for diagnosing potential discrimination by predictive models.;https://arxiv.org/abs/1511.00148;AW3AngnyFzYJ
Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). Model-agnostic interpretability of machine learning. arXiv preprint arXiv:1606.05386.;3_explanation_model_machine_learning;2016;Model-agnostic interpretability of machine learning;Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin;arXiv preprint arXiv:1606.05386, 2016;Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.;https://arxiv.org/abs/1606.05386;RduEY8UFcn4J
Miao, H., Chavan, A., & Deshpande, A. (2016). ProvDB: A system for lifecycle management of collaborative analysis workflows. arXiv preprint arXiv:1610.04963.;9_data_science_software_process;2016;ProvDB: A system for lifecycle management of collaborative analysis workflows;Hui Miao, Amit Chavan, Amol Deshpande;arXiv preprint arXiv:1610.04963, 2016;"As data-driven methods are becoming pervasive in a wide variety of disciplines, there is an urgent need to develop scalable and sustainable tools to simplify the process of data science, to make it easier to keep track of the analyses being performed and datasets being generated, and to enable introspection of the workflows. In this paper, we describe our vision of a unified provenance and metadata management system to support lifecycle management of complex collaborative data science workflows. We argue that a large amount of information about the analysis processes and data artifacts can, and should be, captured in a semi-passive manner; and we show that querying and analyzing this information can not only simplify bookkeeping and debugging tasks for data analysts but can also enable a rich new set of capabilities like identifying flaws in the data science process itself. It can also significantly reduce the time spent in fixing post-deployment problems through automated analysis and monitoring. We have implemented an initial prototype of our system, called ProvDB, on top of git (a version control system) and Neo4j (a graph database), and we describe its key features and capabilities.";https://arxiv.org/abs/1610.04963;IYQHwsNps50J
Castelvecchi, D. (2016). Can we open the black box of AI?. Nature News, 538(7623), 20.;12_ai_ethical_ethic_intelligence;2016;Can we open the black box of AI?;Davide Castelvecchi;Nature News 538 (7623), 20, 2016;Dean Pomerleau can still remember his first tussle with the black-box problem. The year was 1991, and he was making a pioneering attempt to do something that has now become commonplace in autonomous-vehicle research: teach a computer how to drive.This meant taking the wheel of a specially equipped Humvee military vehicle and guiding it through city streets, says Pomerleau, who was then a robotics graduate student at Carnegie Mellon University in Pittsburgh, Pennsylvania. With him in the Humvee was a computer that he had programmed to peer through a camera, interpret what was happening out on the road and memorize every move that he made in response. Eventually, Pomerleau hoped, the machine would make enough associations to steer on its own.;https://www.nature.com/articles/538020a;i1YprqStMzMJ
Liu, Y., Chen, X., Liu, C., & Song, D. (2016). Delving into transferable adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770.;5_adversarial_attack_example_model;2016;Delving into transferable adversarial examples and black-box attacks;Yanpei Liu, Xinyun Chen, Chang Liu, Dawn Song;arXiv preprint arXiv:1611.02770, 2016;An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.;https://arxiv.org/abs/1611.02770;FggEjpnxZqUJ
Papernot, N., McDaniel, P., Sinha, A., & Wellman, M. (2016). Towards the science of security and privacy in machine learning. arXiv preprint arXiv:1611.03814.;5_adversarial_attack_example_model;2016;Towards the science of security and privacy in machine learning;Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, Michael Wellman;arXiv preprint arXiv:1611.03814, 2016;Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive---new systems and models are being deployed in every domain imaginable, leading to rapid and widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize recent findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date. We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by formally exploring the opposing relationship between model accuracy and resilience to adversarial manipulation. Through these explorations, we show that there are (possibly unavoidable) tensions between model complexity, accuracy, and resilience that must be calibrated for the environments in which they will be used.;https://arxiv.org/abs/1611.03814;Kphj7NRPiYQJ
Papernot, N., McDaniel, P., & Goodfellow, I. (2016). Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277.;5_adversarial_attack_example_model;2016;Transferability in machine learning: from phenomena to black-box attacks using adversarial samples;Nicolas Papernot, Patrick McDaniel, Ian Goodfellow;arXiv preprint arXiv:1605.07277, 2016;Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.;https://arxiv.org/abs/1605.07277;SqSVTBNOtWAJ
Friedler, S. A., Scheidegger, C., & Venkatasubramanian, S. (2021). The (im) possibility of fairness: Different value systems require different mechanisms for fair decision making. Communications of the ACM, 64(4), 136-143.;6_fairness_discrimination_bias_decision;2021;The (im) possibility of fairness: Different value systems require different mechanisms for fair decision making;Sorelle A Friedler, Carlos Scheidegger, Suresh Venkatasubramanian;Communications of the ACM 64 (4), 136-143, 2021;What does it mean to be fair?;https://dl.acm.org/doi/fullHtml/10.1145/3433949;Z_h3GCQDKrMJ
Weller, A. (2017). Challenges for transparency.;12_ai_ethical_ethic_intelligence;2017;Challenges for transparency;Adrian Weller;-;Transparency is often deemed critical to enable effective real-world deployment of intelligent systems. Yet the motivations for and benefits of different types of transparency can vary significantly depending on context, and objective measurement criteria are difficult to identify. We provide a brief survey, suggesting challenges and related concerns. We highlight and review settings where transparency may cause harm, discussing connections across privacy, multi-agent game theory, economics, fairness and trust.;https://openreview.net/forum?id=SJR9L5MQ-;qHrh28AvwJ8J
Shwartz-Ziv, R., & Tishby, N. (2017). Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810.;11_deep_network_model_layer;2017;Opening the black box of deep neural networks via information;Ravid Shwartz-Ziv, Naftali Tishby;arXiv preprint arXiv:1703.00810, 2017;"Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the \textit{Information Plane}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on {\emph compression} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.";https://arxiv.org/abs/1703.00810;M0QXb6gzXv4J
Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608.;3_explanation_model_machine_learning;2017;Towards a rigorous science of interpretable machine learning;Finale Doshi-Velez, Been Kim;arXiv preprint arXiv:1702.08608, 2017;As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.;https://arxiv.org/abs/1702.08608;xQ5BC7Hj-HkJ
Gajane, P., & Pechenizkiy, M. (2017). On formalizing fairness in prediction with machine learning. arXiv preprint arXiv:1710.03184.;6_fairness_discrimination_bias_decision;2017;On formalizing fairness in prediction with machine learning;Pratik Gajane, Mykola Pechenizkiy;arXiv preprint arXiv:1710.03184, 2017;Machine learning algorithms for prediction are increasingly being used in critical decisions affecting human lives. Various fairness formalizations, with no firm consensus yet, are employed to prevent such algorithms from systematically discriminating against people based on certain attributes protected by law. The aim of this article is to survey how fairness is formalized in the machine learning literature for the task of prediction and present these formalizations with their corresponding notions of distributive justice from the social sciences literature. We provide theoretical as well as empirical critiques of these notions from the social sciences literature and explain how these critiques limit the suitability of the corresponding fairness formalizations to certain domains. We also suggest two notions of distributive justice which address some of these critiques and discuss avenues for prospective fairness formalizations.;https://arxiv.org/abs/1710.03184;NH9juG21oBIJ
Shaikh, S., Vishwakarma, H., Mehta, S., Varshney, K. R., Ramamurthy, K. N., & Wei, D. (2017). An end-to-end machine learning pipeline that ensures fairness policies. arXiv preprint arXiv:1710.06876.;6_fairness_discrimination_bias_decision;2017;An end-to-end machine learning pipeline that ensures fairness policies;Samiulla Shaikh, Harit Vishwakarma, Sameep Mehta, Kush R Varshney, Karthikeyan Natesan Ramamurthy, Dennis Wei;arXiv preprint arXiv:1710.06876, 2017;In consequential real-world applications, machine learning (ML) based systems are expected to provide fair and non-discriminatory decisions on candidates from groups defined by protected attributes such as gender and race. These expectations are set via policies or regulations governing data usage and decision criteria (sometimes explicitly calling out decisions by automated systems). Often, the data creator, the feature engineer, the author of the algorithm and the user of the results are different entities, making the task of ensuring fairness in an end-to-end ML pipeline challenging. Manually understanding the policies and ensuring fairness in opaque ML systems is time-consuming and error-prone, thus necessitating an end-to-end system that can: 1) understand policies written in natural language, 2) alert users to policy violations during data usage, and 3) log each activity performed using the data in an immutable storage so that policy compliance or violation can be proven later. We propose such a system to ensure that data owners and users are always in compliance with fairness policies.;https://arxiv.org/abs/1710.06876;nihj-83hMTQJ
Doshi-Velez, F., Kortz, M., Budish, R., Bavitz, C., Gershman, S., O'Brien, D., ... & Wood, A. (2017). Accountability of AI under the law: The role of explanation. arXiv preprint arXiv:1711.01134.;3_explanation_model_machine_learning;2017;Accountability of AI under the law: The role of explanation;Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman, David O'Brien, Kate Scott, Stuart Schieber, James Waldo, David Weinberger, Adrian Weller, Alexandra Wood;arXiv preprint arXiv:1711.01134, 2017;"The ubiquity of systems using artificial intelligence or ""AI"" has brought increasing attention to how those systems should be regulated. The choice of how to regulate AI systems will require care. AI systems have the potential to synthesize large amounts of data, allowing for greater levels of personalization and precision than ever before---applications range from clinical decision support to autonomous driving and predictive policing. That said, there exist legitimate concerns about the intentional and unintentional negative consequences of AI systems. There are many ways to hold AI systems accountable. In this work, we focus on one: explanation. Questions about a legal right to explanation from AI systems was recently debated in the EU General Data Protection Regulation, and thus thinking carefully about when and how explanation from AI systems might improve accountability is timely. In this work, we review contexts in which explanation is currently required under the law, and then list the technical considerations that must be considered if we desired AI systems that could provide kinds of explanations that are currently required of humans.";https://arxiv.org/abs/1711.01134;FG3jPadR2bsJ
Doran, D., Schulz, S., & Besold, T. R. (2017). What does explainable AI really mean? A new conceptualization of perspectives. arXiv preprint arXiv:1710.00794.;3_explanation_model_machine_learning;2017;What does explainable AI really mean? A new conceptualization of perspectives;Derek Doran, Sarah Schulz, Tarek R Besold;arXiv preprint arXiv:1710.00794, 2017;"We characterize three notions of explainable AI that cut across research fields: opaque systems that offer no insight into its algo- rithmic mechanisms; interpretable systems where users can mathemat- ically analyze its algorithmic mechanisms; and comprehensible systems that emit symbols enabling user-driven explanations of how a conclusion is reached. The paper is motivated by a corpus analysis of NIPS, ACL, COGSCI, and ICCV/ECCV paper titles showing differences in how work on explainable AI is positioned in various fields. We close by introducing a fourth notion: truly explainable systems, where automated reasoning is central to output crafted explanations without requiring human post processing as final step of the generative process.";https://arxiv.org/abs/1710.00794;rG5W5Yse2QwJ
Yang, C., Wu, Q., Li, H., & Chen, Y. (2017). Generative poisoning attack method against neural networks. arXiv preprint arXiv:1703.01340.;5_adversarial_attack_example_model;2017;Generative poisoning attack method against neural networks;Chaofei Yang, Qing Wu, Hai Li, Yiran Chen;arXiv preprint arXiv:1703.01340, 2017;Poisoning attack is identified as a severe security threat to machine learning algorithms. In many applications, for example, deep neural network (DNN) models collect public data as the inputs to perform re-training, where the input data can be poisoned. Although poisoning attack against support vector machines (SVM) has been extensively studied before, there is still very limited knowledge about how such attack can be implemented on neural networks (NN), especially DNNs. In this work, we first examine the possibility of applying traditional gradient-based method (named as the direct gradient method) to generate poisoned data against NNs by leveraging the gradient of the target model w.r.t. the normal data. We then propose a generative method to accelerate the generation rate of the poisoned data: an auto-encoder (generator) used to generate poisoned data is updated by a reward function of the loss, and the target NN model (discriminator) receives the poisoned data to calculate the loss w.r.t. the normal data. Our experiment results show that the generative method can speed up the poisoned data generation rate by up to 239.38x compared with the direct gradient method, with slightly lower model accuracy degradation. A countermeasure is also designed to detect such poisoning attack methods by checking the loss of the target model.;https://arxiv.org/abs/1703.01340;TtgaDvotA90J
Fisher, A., Rudin, C., & Dominici, F. (2019). All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously. J. Mach. Learn. Res., 20(177), 1-81.;3_explanation_model_machine_learning;2019;All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously.;Aaron Fisher, Cynthia Rudin, Francesca Dominici;J. Mach. Learn. Res. 20 (177), 1-81, 2019;Variable importance (VI) tools describe how much covariates contribute to a prediction model’s accuracy. However, important variables for one well-performing model (for example, a linear model f (x)= xT β with a fixed coefficient vector β) may be unimportant for another model. In this paper, we propose model class reliance (MCR) as the range of VI values across all well-performing model in a prespecified class. Thus, MCR gives a more comprehensive description of importance by accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. In the process of deriving MCR, we show several informative results for permutation-based VI estimates, based on the VI measures used in Random Forests. Specifically, we derive connections between permutation importance estimates for a single prediction model, U-statistics, conditional variable importance, conditional causal effects, and linear model coefficients. We then give probabilistic bounds for MCR, using a novel, generalizable technique. We apply MCR to a public data set of Broward County criminal records to study the reliance of recidivism prediction models on sex and race. In this application, MCR can be used to help inform VI for unknown, proprietary models.;https://www.jmlr.org/papers/volume20/18-760/18-760.pdf?ref=https://githubhelp.com;_Ldn4g0IkLYJ
Aravantinos, V., & Diehl, F. (2018). Traceability of deep neural networks. arXiv preprint arXiv:1812.06744.;4_dl_testing_deep_network;2018;Traceability of deep neural networks;Vincent Aravantinos, Frederik Diehl;arXiv preprint arXiv:1812.06744, 2018;"[Context]
The success of deep learning makes its usage more and more tempting in safety-critical applications. However such applications have historical standards (e.g., DO178, ISO26262) which typically do not envision the usage of machine learning. We focus in particular on \emph{requirements traceability} of software artifacts, i.e., code modules, functions, or statements (depending on the desired granularity).
[Problem]
Both code and requirements are a problem when dealing with deep neural networks: code constituting the network is not comparable to classical code; furthermore, requirements for applications where neural networks are required are typically very hard to specify: even though high-level requirements can be defined, it is very hard to make such requirements concrete enough, that one can qualify them of low-level requirements. An additional problem is that deep learning is in practice very much based on trial-and-error, which makes the final result hard to explain without the previous iterations.
[Proposed solution]
We investigate which artifacts could play a similar role to code or low-level requirements in neural network development and propose various traces which one could possibly consider as a replacement for classical notions. We also propose a form of traceability (and new artifacts) in order to deal with the particular trial-and-error development process for deep learning.";https://arxiv.org/abs/1812.06744;Chwu9U_Cf3UJ
Sugimura, P., & Hartl, F. (2018). Building a reproducible machine learning pipeline. arXiv preprint arXiv:1810.04570.;1_ml_machine_data_learning;2018;Building a reproducible machine learning pipeline;Peter Sugimura, Florian Hartl;arXiv preprint arXiv:1810.04570, 2018;Reproducibility of modeling is a problem that exists for any machine learning practitioner, whether in industry or academia. The consequences of an irreproducible model can include significant financial costs, lost time, and even loss of personal reputation (if results prove unable to be replicated). This paper will first discuss the problems we have encountered while building a variety of machine learning models, and subsequently describe the framework we built to tackle the problem of model reproducibility. The framework is comprised of four main components (data, feature, scoring, and evaluation layers), which are themselves comprised of well defined transformations. This enables us to not only exactly replicate a model, but also to reuse the transformations across different models. As a result, the platform has dramatically increased the speed of both offline and online experimentation while also ensuring model reproducibility.;https://arxiv.org/abs/1810.04570;otWDfb89gs8J
Schoenfeld, B., Giraud-Carrier, C., Poggemann, M., Christensen, J., & Seppi, K. (2018). Preprocessor selection for machine learning pipelines. arXiv preprint arXiv:1810.09942.;1_ml_machine_data_learning;2018;Preprocessor selection for machine learning pipelines;Brandon Schoenfeld, Christophe Giraud-Carrier, Mason Poggemann, Jarom Christensen, Kevin Seppi;arXiv preprint arXiv:1810.09942, 2018;Much of the work in metalearning has focused on classifier selection, combined more recently with hyperparameter optimization, with little concern for data preprocessing. Yet, it is generally well accepted that machine learning applications require not only model building, but also data preprocessing. In other words, practical solutions consist of pipelines of machine learning operators rather than single algorithms. Interestingly, our experiments suggest that, on average, data preprocessing hinders accuracy, while the best performing pipelines do actually make use of preprocessors. Here, we conduct an extensive empirical study over a wide range of learning algorithms and preprocessors, and use metalearning to determine when one should make use of preprocessors in ML pipeline design.;https://arxiv.org/abs/1810.09942;wDCuFcHYwFUJ
Dakkak, A., Li, C., Xiong, J., & Hwu, W. M. (2018). Frustrated with replicating claims of a shared model? a solution. arXiv preprint arXiv:1811.09737.;4_dl_testing_deep_network;2018;Frustrated with replicating claims of a shared model? a solution;Abdul Dakkak, Cheng Li, Jinjun Xiong, Wen-Mei Hwu;arXiv preprint arXiv:1811.09737, 2018;"Machine Learning (ML) and Deep Learning (DL) innovations are being introduced at such a rapid pace that model owners and evaluators are hard-pressed analyzing and studying them. This is exacerbated by the complicated procedures for evaluation. The lack of standard systems and efficient techniques for specifying and provisioning ML/DL evaluation is the main cause of this ""pain point"". This work discusses common pitfalls for replicating DL model evaluation, and shows that these subtle pitfalls can affect both accuracy and performance. It then proposes a solution to remedy these pitfalls called MLModelScope, a specification for repeatable model evaluation and a runtime to provision and measure experiments. We show that by easing the model specification and evaluation process, MLModelScope facilitates rapid adoption of ML/DL innovations.";https://arxiv.org/abs/1811.09737;-E4X6l831PoJ
Lai, L., & Suda, N. (2018). Rethinking machine learning development and deployment for edge devices. arXiv preprint arXiv:1806.07846.;7_edge_computing_deep_learning;2018;Rethinking machine learning development and deployment for edge devices;Liangzhen Lai, Naveen Suda;arXiv preprint arXiv:1806.07846, 2018;Machine learning (ML), especially deep learning is made possible by the availability of big data, enormous compute power and, often overlooked, development tools or frameworks. As the algorithms become mature and efficient, more and more ML inference is moving out of datacenters/cloud and deployed on edge devices. This model deployment process can be challenging as the deployment environment and requirements can be substantially different from those during model development. In this paper, we propose a new ML development and deployment approach that is specially designed and optimized for inference-only deployment on edge devices. We build a prototype and demonstrate that this approach can address all the deployment challenges and result in more efficient and high-quality solutions.;https://arxiv.org/abs/1806.07846;OXy1C_z4dUUJ
Xie, X., Ma, L., Juefei-Xu, F., Chen, H., Xue, M., Li, B., ... & See, S. (2018). Coverage-guided fuzzing for deep neural networks. arXiv preprint arXiv:1809.01266, 3.;1_ml_machine_data_learning;2018;only citation on google scholar;only citation on google scholar;only citation on google scholar;only citation on google scholar;only citation on google scholar;only citation on google scholar
Lipton, Z. C. (2018). The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue, 16(3), 31-57.;3_explanation_model_machine_learning;2018;The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery.;Zachary C Lipton;Queue 16 (3), 31-57, 2018;Supervised machine-learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world?;https://dl.acm.org/doi/pdf/10.1145/3236386.3241340;aMNh86C6mdYJ
Guo, Q., Xie, X., Ma, L., Hu, Q., Feng, R., Li, L., ... & Li, X. (2018). An orchestrated empirical study on deep learning frameworks and platforms. arXiv preprint arXiv:1811.05187.;4_dl_testing_deep_network;2018;An orchestrated empirical study on deep learning frameworks and platforms;Qianyu Guo, Xiaofei Xie, Lei Ma, Qiang Hu, Ruitao Feng, Li Li, Yang Liu, Jianjun Zhao, Xiaohong Li;arXiv preprint arXiv:1811.05187, 2018;Deep learning (DL) has recently achieved tremendous success in a variety of cutting-edge applications, e.g., image recognition, speech and natural language processing, and autonomous driving. Besides the available big data and hardware evolution, DL frameworks and platforms play a key role to catalyze the research, development, and deployment of DL intelligent solutions. However, the difference in computation paradigm, architecture design and implementation of existing DL frameworks and platforms brings challenges for DL software development, deployment, maintenance, and migration. Up to the present, it still lacks a comprehensive study on how current diverse DL frameworks and platforms influence the DL software development process. In this paper, we initiate the first step towards the investigation on how existing state-of-the-art DL frameworks (i.e., TensorFlow, Theano, and Torch) and platforms (i.e., server/desktop, web, and mobile) support the DL software development activities. We perform an in-depth and comparative evaluation on metrics such as learning accuracy, DL model size, robustness, and performance, on state-of-the-art DL frameworks across platforms using two popular datasets MNIST and CIFAR-10. Our study reveals that existing DL frameworks still suffer from compatibility issues, which becomes even more severe when it comes to different platforms. We pinpoint the current challenges and opportunities towards developing high quality and compatible DL systems. To ignite further investigation along this direction to address urgent industrial demands of intelligent solutions, we make all of our assembled feasible toolchain and dataset publicly available.;https://arxiv.org/abs/1811.05187;mFmAOK2-KAAJ
Hanzlik, L., Zhang, Y., Grosse, K., Salem, A., Augustin, M., Backes, M., & Fritz, M. (2021). Mlcapsule: Guarded offline deployment of machine learning as a service. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 3300-3309).;5_adversarial_attack_example_model;2021;Mlcapsule: Guarded offline deployment of machine learning as a service;Lucjan Hanzlik, Yang Zhang, Kathrin Grosse, Ahmed Salem, Maximilian Augustin, Michael Backes, Mario Fritz;Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 3300-3309, 2021;Machine Learning as a Service (MLaaS) is a popular and convenient way to access a trained machine learning (ML) model trough an API. However, if the user's input is sensitive, sending it to the server is not an option. Equally, the service provider does not want to share the model by sending it to the client for protecting its intellectual property and pay-per-query business model. As a solution, we propose MLCapsule, a guarded offline deployment of MLaaS. MLCapsule executes the machine learning model locally on the user's client and therefore the data never leaves the client. Meanwhile, we show that MLCapsule is able to offer the service provider the same level of control and security of its model as the commonly used server-side execution. Beyond protecting against direct model access, we demonstrate that MLCapsule allows for implementing defenses against advanced attacks on machine learning models such as model stealing, reverse engineering and membership inference.;https://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Hanzlik_MLCapsule_Guarded_Offline_Deployment_of_Machine_Learning_as_a_Service_CVPRW_2021_paper.html;zvVcADHBoHMJ
Corbett-Davies, S., & Goel, S. (2018). The measure and mismeasure of fairness: A critical review of fair machine learning. arXiv preprint arXiv:1808.00023.;6_fairness_discrimination_bias_decision;2018;The measure and mismeasure of fairness: A critical review of fair machine learning;Sam Corbett-Davies, Sharad Goel;arXiv preprint arXiv:1808.00023, 2018;"The nascent field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last several years, three formal definitions of fairness have gained prominence: (1) anti-classification, meaning that protected attributes---like race, gender, and their proxies---are not explicitly used to make decisions; (2) classification parity, meaning that common measures of predictive performance (e.g., false positive and false negative rates) are equal across groups defined by the protected attributes; and (3) calibration, meaning that conditional on risk estimates, outcomes are independent of protected attributes. Here we show that all three of these fairness definitions suffer from significant statistical limitations. Requiring anti-classification or classification parity can, perversely, harm the very groups they were designed to protect; and calibration, though generally desirable, provides little guarantee that decisions are equitable. In contrast to these formal fairness criteria, we argue that it is often preferable to treat similarly risky people similarly, based on the most statistically accurate estimates of risk that one can produce. Such a strategy, while not universally applicable, often aligns well with policy objectives; notably, this strategy will typically violate both anti-classification and classification parity. In practice, it requires significant effort to construct suitable risk estimates. One must carefully define and measure the targets of prediction to avoid retrenching biases in the data. But, importantly, one cannot generally address these difficulties by requiring that algorithms satisfy popular mathematical formalizations of fairness. By highlighting these challenges in the foundation of fair machine learning, we hope to help researchers and practitioners productively advance the area.";https://arxiv.org/abs/1808.00023;ibgnEqgL5CUJ
Narayanan, M., Chen, E., He, J., Kim, B., Gershman, S., & Doshi-Velez, F. (2018). How do humans understand explanations from machine learning systems? an evaluation of the human-interpretability of explanation. arXiv preprint arXiv:1802.00682.;3_explanation_model_machine_learning;2018;How do humans understand explanations from machine learning systems? an evaluation of the human-interpretability of explanation;Menaka Narayanan, Emily Chen, Jeffrey He, Been Kim, Sam Gershman, Finale Doshi-Velez;arXiv preprint arXiv:1802.00682, 2018;Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.;https://arxiv.org/abs/1802.00682;fOfyMxWt8KYJ
Veale, M. (2019). Governing Machine Learning that Matters (Doctoral dissertation, UCL (University College London)).;6_fairness_discrimination_bias_decision;2019;Governing Machine Learning that Matters;Michael Veale;UCL (University College London), 2019;Personal data is increasingly used to augment decision-making and to build digital services, often through machine learning technologies, model-building tools which recognise and operationalise patterns in datasets. Researchers, regulators and civil society have expressed concern around how machine learning might create or reinforce social challenges, such as discrimination, or create new opacities difficult to scrutinise or challenge. This thesis examines how of machine learning systems that matter—those involved in high-stakes decision-making—are and should be gov- erned, in their technical, legal and social contexts. // First, it unpacks the provisions and framework of European data protection law in relation to these social concerns and machine learning’s technical characteristics. In chapter 2, how data protection and machine learning relate is presented and examined, revealing practical weaknesses and inconsistencies. In chapter 3, characteristics of machine learning that might further stress data protection law are high- lighted. The framework’s implicit assumptions and resultant tensions are examined through three lenses. These stresses bring policy opportunities amidst challenges, such as the chance to make clearer trade-offs and expand the collective dimension of data protection rights. // The thesis then pivots to the social dimensions of machine learning on-the- ground. Chapter 4 reports upon interviews with 27 machine learning practitioners in the public sector about how they cope with value-laden choices today, unearthing a range of tensions between practical challenges and those imagined by the ‘fairness, accountability and transparency’ literature in computer science. One tension between fairness and privacy is unpacked and examined in further detail in chapter 5 to demonstrate the kind of change in method and approach that might be needed to grapple with the findings of the thesis. // The thesis concludes by synthesising the findings of the previous chapters, and outlines policy recommendations going forward of relevance to a range of interested parties.;https://discovery.ucl.ac.uk/id/eprint/10078626/;wCx6sE2AxZcJ
Hosny, A., Schwier, M., Berger, C., Örnek, E. P., Turan, M., Tran, P. V., ... & Aerts, H. J. (2019). Modelhub. ai: Dissemination platform for deep learning models. arXiv preprint arXiv:1911.13218.;1_ml_machine_data_learning;2019;Modelhub. ai: Dissemination platform for deep learning models;Ahmed Hosny, Michael Schwier, Christoph Berger, Evin P Ã–rnek, Mehmet Turan, Phi V Tran, Leon Weninger, Fabian Isensee, Klaus H Maier-Hein, Richard McKinley, Michael T Lu, Udo Hoffmann, Bjoern Menze, Spyridon Bakas, Andriy Fedorov, Hugo JWL Aerts;arXiv preprint arXiv:1911.13218, 2019;Recent advances in artificial intelligence research have led to a profusion of studies that apply deep learning to problems in image analysis and natural language processing among others. Additionally, the availability of open-source computational frameworks has lowered the barriers to implementing state-of-the-art methods across multiple domains. Albeit leading to major performance breakthroughs in some tasks, effective dissemination of deep learning algorithms remains challenging, inhibiting reproducibility and benchmarking studies, impeding further validation, and ultimately hindering their effectiveness in the cumulative scientific progress. In developing a platform for sharing research outputs, we present ModelHub.AI (www.modelhub.ai), a community-driven container-based software engine and platform for the structured dissemination of deep learning models. For contributors, the engine controls data flow throughout the inference cycle, while the contributor-facing standard template exposes model-specific functions including inference, as well as pre- and post-processing. Python and RESTful Application programming interfaces (APIs) enable users to interact with models hosted on ModelHub.AI and allows both researchers and developers to utilize models out-of-the-box. ModelHub.AI is domain-, data-, and framework-agnostic, catering to different workflows and contributors' preferences.;https://arxiv.org/abs/1911.13218;fRQGpXfynK0J
Fursin, G. (2019). SysML'19 demo: customizable and reusable Collective Knowledge pipelines to automate and reproduce machine learning experiments. arXiv preprint arXiv:1904.00324.;1_ml_machine_data_learning;2019;SysML'19 demo: customizable and reusable Collective Knowledge pipelines to automate and reproduce machine learning experiments;Grigori Fursin;arXiv preprint arXiv:1904.00324, 2019;Reproducing, comparing and reusing results from machine learning and systems papers is a very tedious, ad hoc and time-consuming process. I will demonstrate how to automate this process using open-source, portable, customizable and CLI-based Collective Knowledge workflows and pipelines developed by the community. I will help participants run several real-world non-virtualized CK workflows from the SysML'19 conference, companies (General Motors, Arm) and MLPerf benchmark to automate benchmarking and co-design of efficient software/hardware stacks for machine learning workloads. I hope that our approach will help authors reduce their effort when sharing reusable and extensible research artifacts while enabling artifact evaluators to automatically validate experimental results from published papers in a standard and portable way.;https://arxiv.org/abs/1904.00324;pXpNw9W1-GoJ
Jobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of AI ethics guidelines. Nature machine intelligence, 1(9), 389-399.;12_ai_ethical_ethic_intelligence;2019;The global landscape of AI ethics guidelines;Anna Jobin, Marcello Ienca, Effy Vayena;Nature machine intelligence 1 (9), 389-399, 2019;In the past five years, private companies, research institutions and public sector organizations have issued principles and guidelines for ethical artificial intelligence (AI). However, despite an apparent agreement that AI should be ‘ethical’, there is debate about both what constitutes ‘ethical AI’ and which ethical requirements, technical standards and best practices are needed for its realization. To investigate whether a global agreement on these questions is emerging, we mapped and analysed the current corpus of principles and guidelines on ethical AI. Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted, why they are deemed important, what issue, domain or actors they pertain to, and how they should be implemented. Our findings highlight the importance of integrating guideline-development efforts with substantive ethical analysis and adequate implementation strategies.;https://www.nature.com/articles/s42256-019-0088-2;7A5k2lX2A3gJ
McDermott, M., Wang, S., Marinsek, N., Ranganath, R., Ghassemi, M., & Foschini, L. (2019). Reproducibility in machine learning for health. arXiv preprint arXiv:1907.01463.;3_explanation_model_machine_learning;2019;Reproducibility in machine learning for health;Matthew McDermott, Shirly Wang, Nikki Marinsek, Rajesh Ranganath, Marzyeh Ghassemi, Luca Foschini;arXiv preprint arXiv:1907.01463, 2019;Machine learning algorithms designed to characterize, monitor, and intervene on human health (ML4H) are expected to perform safely and reliably when operating at scale, potentially outside strict human supervision. This requirement warrants a stricter attention to issues of reproducibility than other fields of machine learning. In this work, we conduct a systematic evaluation of over 100 recently published ML4H research papers along several dimensions related to reproducibility. We find that the field of ML4H compares poorly to more established machine learning fields, particularly concerning data and code accessibility. Finally, drawing from success in other fields of science, we propose recommendations to data providers, academic publishers, and the ML4H research community in order to promote reproducible research moving forward.;https://arxiv.org/abs/1907.01463;CE_6YFRw9IQJ
Weller, A. (2019). Transparency: motivations and challenges. In Explainable AI: interpreting, explaining and visualizing deep learning (pp. 23-40). Cham: Springer International Publishing.;12_ai_ethical_ethic_intelligence;2019;Transparency: motivations and challenges;Adrian Weller;Explainable AI: interpreting, explaining and visualizing deep learning, 23-40, 2019;Transparency is often deemed critical to enable effective real-world deployment of intelligent systems. Yet the motivations for and benefits of different types of transparency can vary significantly depending on context, and objective measurement criteria are difficult to identify. We provide a brief survey, suggesting challenges and related concerns, particularly when agents have misaligned interests. We highlight and review settings where transparency may cause harm, discussing connections across privacy, multi-agent game theory, economics, fairness and trust.;https://link.springer.com/chapter/10.1007/978-3-030-28954-6_2;7tmfvMj71gYJ
Raji, I. D., & Yang, J. (2019). About ml: Annotation and benchmarking on understanding and transparency of machine learning lifecycles. arXiv preprint arXiv:1912.06166.;3_explanation_model_machine_learning;2019;About ml: Annotation and benchmarking on understanding and transparency of machine learning lifecycles;Inioluwa Deborah Raji, Jingying Yang;arXiv preprint arXiv:1912.06166, 2019;"We present the ""Annotation and Benchmarking on Understanding and Transparency of Machine Learning Lifecycles"" (ABOUT ML) project as an initiative to operationalize ML transparency and work towards a standard ML documentation practice. We make the case for the project's relevance and effectiveness in consolidating disparate efforts across a variety of stakeholders, as well as bringing in the perspectives of currently missing voices that will be valuable in shaping future conversations. We describe the details of the initiative and the gaps we hope this project will help address.";https://arxiv.org/abs/1912.06166;aLF1FVt4GPYJ
Xu, L., & Wang, Y. (2019). Xcloud: Design and implementation of ai cloud platform with restful api service. arXiv preprint arXiv:1912.10344.;1_ml_machine_data_learning;2019;Xcloud: Design and implementation of ai cloud platform with restful api service;Lu Xu, Yating Wang;arXiv preprint arXiv:1912.10344, 2019;In recent years, artificial intelligence (AI) has aroused much attention among both industrial and academic areas. However, building and maintaining efficient AI systems are quite difficult for many small business companies and researchers if they are not familiar with machine learning and AI. In this paper, we first evaluate the difficulties and challenges in building AI systems. Then an cloud platform termed XCloud, which provides several common AI services in form of RESTful APIs, is constructed. Technical details are discussed in Section 2. This project is released as open-source software and can be easily accessed for late research. Code is available at https://github.com/lucasxlu/XCloud.git.;https://arxiv.org/abs/1912.10344;YMDRYfCjgrkJ
YANG, R., ZOMAYA, A. Y., WANG, L., & RANJAN, R. (2019). Orchestrating development lifecycle of machine learning based IoT applications: A survey.;7_edge_computing_deep_learning;2019;Orchestrating development lifecycle of machine learning based IoT applications: A survey;RENYU YANG, ALBERT Y ZOMAYA, LIZHE WANG, RAJIV RANJAN;-;Machine Learning (ML) and Internet of Things (IoT) are complementary advances: ML techniques unlock complete potentials of IoT with intelligence, and IoT applications increasingly feed data collected by sensors into ML models, thereby employing results to improve their business processes and services. Hence, orchestrating ML pipelines that encompasses model training and implication involved in holistic development lifecycle of an IoT application often leads to complex system integration. This paper provides a comprehensive and systematic survey on the development lifecycle of ML-based IoT application. We outline core roadmap and taxonomy, and subsequently assess and compare existing standard techniques used in individual stage.;https://www.researchgate.net/profile/Zhenyu-Wen-2/publication/336642133_Orchestrating_the_Development_Lifecycle_of_Machine_Learning-Based_IoT_Applications_A_Taxonomy_and_Survey/links/5da9968292851c577eb824b6/Orchestrating-the-Development-Lifecycle-of-Machine-Learning-Based-IoT-Applications-A-Taxonomy-and-Survey.pdf;1pSWVfetqNkJ
Xie, X., Ma, L., Wang, H., Li, Y., Liu, Y., & Li, X. (2019). Diffchaser: Detecting disagreements for deep neural networks. International Joint Conferences on Artificial Intelligence Organization.;4_dl_testing_deep_network;2019;Diffchaser: Detecting disagreements for deep neural networks;Xiaofei Xie, Lei Ma, Haijun Wang, Yuekang Li, Yang Liu, Xiaohong Li;International Joint Conferences on Artificial Intelligence Organization, 2019;The platform migration and customization have become an indispensable process of deep neural network (DNN) development lifecycle. A highprecision but complex DNN trained in the cloud on massive data and powerful GPUs often goes through an optimization phase (eg, quantization, compression) before deployment to a target device (eg, mobile device). A test set that effectively uncovers the disagreements of a DNN and its optimized variant provides certain feedback to debug and further enhance the optimization procedure. However, the minor inconsistency between a DNN and its optimized version is often hard to detect and easily bypasses the original test set. This paper proposes DiffChaser, an automated black-box testing framework to detect untargeted/targeted disagreements between version variants of a DNN. We demonstrate 1) its effectiveness by comparing with the state-of-the-art techniques, and 2) its usefulness in real-world DNN product deployment involved with quantization and optimization.;https://ink.library.smu.edu.sg/sis_research/7105/;T3INfr1yKMoJ
Zhou, B., Bau, D., Oliva, A., & Torralba, A. (2019). Comparing the interpretability of deep networks via network dissection. Explainable AI: Interpreting, explaining and visualizing deep learning, 243-252.;11_deep_network_model_layer;2019;Comparing the interpretability of deep networks via network dissection;Bolei Zhou, David Bau, Aude Oliva, Antonio Torralba;Explainable AI: Interpreting, explaining and visualizing deep learning, 243-252, 2019;In this chapter, we introduce Network Dissection (The complete paper and code are available at http://netdissect.csail.mit.edu ), a general framework to quantify the interpretability of the units inside a deep convolutional neural networks (CNNs). We compare the different vocabularies of interpretable units as concept detectors emerged from the networks trained to solve different supervised learning tasks such as object recognition on ImageNet and scene classification on Places. The network dissection is further applied to analyze how the units acting as semantic detectors grow and evolve over the training iterations both in the scenario of the train-from-scratch and in the stage of the fine-tuning between data sources. Our results highlight that interpretability is an important property of deep neural networks that provides new insights into their hierarchical structure.;https://link.springer.com/chapter/10.1007/978-3-030-28954-6_12;V_BqYj131g8J
Murdoch, W. J., Singh, C., Kumbier, K., Abbasi-Asl, R., & Yu, B. (2019). Interpretable machine learning: definitions, methods, and applications. arXiv preprint arXiv:1901.04592.;3_explanation_model_machine_learning;2019;Interpretable machine learning: definitions, methods, and applications;W James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, Bin Yu;arXiv preprint arXiv:1901.04592, 2019;Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related, and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the Predictive, Descriptive, Relevant (PDR) framework for discussing interpretations. The PDR framework provides three overarching desiderata for evaluation: predictive accuracy, descriptive accuracy and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post-hoc categories, with sub-groups including sparsity, modularity and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often under-appreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.;https://arxiv.org/abs/1901.04592;xdNAZo6RzdoJ
Han, K., Chen, J., Zhang, H., Xu, H., Peng, Y., Wang, Y., ... & Li, X. (2019). Delta: a deep learning based language technology platform. arXiv preprint arXiv:1908.01853.;1_ml_machine_data_learning;2019;Delta: a deep learning based language technology platform;Kun Han, Junwen Chen, Hui Zhang, Haiyang Xu, Yiping Peng, Yun Wang, Ning Ding, Hui Deng, Yonghu Gao, Tingwei Guo, Yi Zhang, Yahao He, Baochang Ma, Yulong Zhou, Kangli Zhang, Chao Liu, Ying Lyu, Chenxi Wang, Cheng Gong, Yunbo Wang, Wei Zou, Hui Song, Xiangang Li;arXiv preprint arXiv:1908.01853, 2019;In this paper we present DELTA, a deep learning based language technology platform. DELTA is an end-to-end platform designed to solve industry level natural language and speech processing problems. It integrates most popular neural network models for training as well as comprehensive deployment tools for production. DELTA aims to provide easy and fast experiences for using, deploying, and developing natural language processing and speech models for both academia and industry use cases. We demonstrate the reliable performance with DELTA on several natural language processing and speech tasks, including text classification, named entity recognition, natural language inference, speech recognition, speaker verification, etc. DELTA has been used for developing several state-of-the-art algorithms for publications and delivering real production to serve millions of users.;https://arxiv.org/abs/1908.01853;Tj3bq029Ot8J
Arora, A., Nethi, A., Kharat, P., Verghese, V., Jenkins, G., Miff, S., ... & Wang, X. (2019). Isthmus: secure, scalable, real-time and robust machine learning platform for healthcare. arXiv preprint arXiv:1909.13343.;1_ml_machine_data_learning;2019;Isthmus: secure, scalable, real-time and robust machine learning platform for healthcare;Akshay Arora, Arun Nethi, Priyanka Kharat, Vency Verghese, Grant Jenkins, Steve Miff, Vikas Chowdhry, Xiao Wang;arXiv preprint arXiv:1909.13343, 2019;In recent times, machine learning (ML) and artificial intelligence (AI) based systems have evolved and scaled across different industries such as finance, retail, insurance, energy utilities, etc. Among other things, they have been used to predict patterns of customer behavior, to generate pricing models, and to predict the return on investments. But the successes in deploying machine learning models at scale in those industries have not translated into the healthcare setting. There are multiple reasons why integrating ML models into healthcare has not been widely successful, but from a technical perspective, general-purpose commercial machine learning platforms are not a good fit for healthcare due to complexities in handling data quality issues, mandates to demonstrate clinical relevance, and a lack of ability to monitor performance in a highly regulated environment with stringent security and privacy needs. In this paper, we describe Isthmus, a turnkey, cloud-based platform which addresses the challenges above and reduces time to market for operationalizing ML/AI in healthcare. Towards the end, we describe three case studies which shed light on Isthmus capabilities. These include (1) supporting an end-to-end lifecycle of a model which predicts trauma survivability at hospital trauma centers, (2) bringing in and harmonizing data from disparate sources to create a community data platform for inferring population as well as patient level insights for Social Determinants of Health (SDoH), and (3) ingesting live-streaming data from various IoT sensors to build models, which can leverage real-time and longitudinal information to make advanced time-sensitive predictions.;https://arxiv.org/abs/1909.13343;14CKsX0TC6IJ
Zhang, Y., & Zhou, L. (2019). Fairness assessment for artificial intelligence in financial industry. arXiv preprint arXiv:1912.07211.;6_fairness_discrimination_bias_decision;2019;Fairness assessment for artificial intelligence in financial industry;Yukun Zhang, Longsheng Zhou;arXiv preprint arXiv:1912.07211, 2019;Artificial Intelligence (AI) is an important driving force for the development and transformation of the financial industry. However, with the fast-evolving AI technology and application, unintentional bias, insufficient model validation, immature contingency plan and other underestimated threats may expose the company to operational and reputational risks. In this paper, we focus on fairness evaluation, one of the key components of AI Governance, through a quantitative lens. Statistical methods are reviewed for imbalanced data treatment and bias mitigation. These methods and fairness evaluation metrics are then applied to a credit card default payment example.;https://arxiv.org/abs/1912.07211;PultQvf2HHIJ
Aïvodji, U., Arai, H., Fortineau, O., Gambs, S., Hara, S., & Tapp, A. (2019, May). Fairwashing: the risk of rationalization. In International Conference on Machine Learning (pp. 161-170). PMLR.;3_explanation_model_machine_learning;2019;Fairwashing: the risk of rationalization;Ulrich Aïvodji, Hiromi Arai, Olivier Fortineau, Sébastien Gambs, Satoshi Hara, Alain Tapp;International Conference on Machine Learning, 161-170, 2019;Black-box explanation is the problem of explaining how a machine learning model–whose internal logic is hidden to the auditor and generally complex–produces its outcomes. Current approaches for solving this problem include model explanation, outcome explanation as well as model inspection. While these techniques can be beneficial by providing interpretability, they can be used in a negative manner to perform fairwashing, which we define as promoting the false perception that a machine learning model respects some ethical values. In particular, we demonstrate that it is possible to systematically rationalize decisions taken by an unfair black-box model using the model explanation as well as the outcome explanation approaches with a given fairness metric. Our solution, LaundryML, is based on a regularized rule list enumeration algorithm whose objective is to search for fair rule lists approximating an unfair black-box model. We empirically evaluate our rationalization technique on black-box models trained on real-world datasets and show that one can obtain rule lists with high fidelity to the black-box model while being considerably less unfair at the same time.;http://proceedings.mlr.press/v97/aivodji19a;oV0TpVb1BSMJ
Bald, L. (2019). Identifying and Mitigating Bias in Machine Learning Applications.;6_fairness_discrimination_bias_decision;2019;Identifying and Mitigating Bias in Machine Learning Applications;Laura Bald;University of Oregon, 2019;"This study addresses the existence of bias in machine learning applications and examines techniques for identifying and mitigating bias using scholarly literature published between 2012 and 2019. The intended audience is machine learning engineers, system analysts, and data analysts of any industry. This study is significant because there may be considerable ethical implications caused by machine learning bias; identifying and mitigating these biases is key to the development and deployment of effective machine learning algorithms.";https://scholarsbank.uoregon.edu/xmlui/handle/1794/24785;ZWYK6I5iQG8J
Wang, X., & Huang, H. (2019). Approaching machine learning fairness through adversarial network. arXiv preprint arXiv:1909.03013.;6_fairness_discrimination_bias_decision;2019;Approaching machine learning fairness through adversarial network;Xiaoqian Wang, Heng Huang;arXiv preprint arXiv:1909.03013, 2019;Fairness is becoming a rising concern w.r.t. machine learning model performance. Especially for sensitive fields such as criminal justice and loan decision, eliminating the prediction discrimination towards a certain group of population (characterized by sensitive features like race and gender) is important for enhancing the trustworthiness of model. In this paper, we present a new general framework to improve machine learning fairness. The goal of our model is to minimize the influence of sensitive feature from the perspectives of both the data input and the predictive model. In order to achieve this goal, we reformulate the data input by removing the sensitive information and strengthen model fairness by minimizing the marginal contribution of the sensitive feature. We propose to learn the non-sensitive input via sampling among features and design an adversarial network to minimize the dependence between the reformulated input and the sensitive information. Extensive experiments on three benchmark datasets suggest that our model achieve better results than related state-of-the-art methods with respect to both fairness metrics and prediction performance.;https://arxiv.org/abs/1909.03013;dm79lvvepr0J
Schumann, C., Wang, X., Beutel, A., Chen, J., Qian, H., & Chi, E. H. (2019). Transfer of machine learning fairness across domains. arXiv preprint arXiv:1906.09688.;6_fairness_discrimination_bias_decision;2019;Transfer of machine learning fairness across domains;Candice Schumann, Xuezhi Wang, Alex Beutel, Jilin Chen, Hai Qian, Ed H Chi;arXiv preprint arXiv:1906.09688, 2019;If our models are used in new or unexpected cases, do we know if they will make fair predictions? Previously, researchers developed ways to debias a model for a single problem domain. However, this is often not how models are trained and used in practice. For example, labels and demographics (sensitive attributes) are often hard to observe, resulting in auxiliary or synthetic data to be used for training, and proxies of the sensitive attribute to be used for evaluation of fairness. A model trained for one setting may be picked up and used in many others, particularly as is common with pre-training and cloud APIs. Despite the pervasiveness of these complexities, remarkably little work in the fairness literature has theoretically examined these issues. We frame all of these settings as domain adaptation problems: how can we use what we have learned in a source domain to debias in a new target domain, without directly debiasing on the target domain as if it is a completely new problem? We offer new theoretical guarantees of improving fairness across domains, and offer a modeling approach to transfer to data-sparse target domains. We give empirical results validating the theory and showing that these modeling approaches can improve fairness metrics with less data.;https://arxiv.org/abs/1906.09688;O9iPqPujyQ0J
Wang, X., Thain, N., Sinha, A., Prost, F., Chi, E. H., Chen, J., & Beutel, A. (2021, March). Practical compositional fairness: Understanding fairness in multi-component recommender systems. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining (pp. 436-444).;6_fairness_discrimination_bias_decision;2021;Practical compositional fairness: Understanding fairness in multi-component recommender systems;Xuezhi Wang, Nithum Thain, Anu Sinha, Flavien Prost, Ed H Chi, Jilin Chen, Alex Beutel;Proceedings of the 14th ACM International Conference on Web Search and Data Mining, 436-444, 2021;"How can we build recommender systems to take into account fairness? Real-world recommender systems are often composed of multiple models, built by multiple teams. However, most research on fairness focuses on improving fairness in a single model. Further, recent research on classification fairness has shown that combining multiple ""fair"" classifiers can still result in an ""unfair"" classification system. This presents a significant challenge: how do we understand and improve fairness in recommender systems composed of multiple components?In this paper, we study the compositionality of recommender fairness. We consider two recently proposed fairness ranking metrics: equality of exposure and pairwise ranking accuracy. While we show that fairness in recommendation is not guaranteed to compose, we provide theory for a set of conditions under which fairness of individual models does compose. We then present an analytical framework for both understanding whether a real system's signals can achieve compositional fairness, and improving which component would have the greatest impact on the fairness of the overall system. In addition to the theoretical results, we find on multiple datasets---including a large-scale real-world recommender system---that the overall system's end-to-end fairness is largely achievable by improving fairness in individual components.";https://dl.acm.org/doi/abs/10.1145/3437963.3441732;8Vyp5_KryoUJ
Miller, T. (2019). Explanation in artificial intelligence: Insights from the social sciences. Artificial intelligence, 267, 1-38.;3_explanation_model_machine_learning;2019;Explanation in artificial intelligence: Insights from the social sciences;Tim Miller;Artificial intelligence 267, 1-38, 2019;There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.;https://www.sciencedirect.com/science/article/pii/S0004370218305988;IvZdrEBN6VkJ
Rudin, C. (2018). Please stop explaining black box models for high stakes decisions. Stat, 1050, 26.;3_explanation_model_machine_learning;2018;Please stop explaining black box models for high stakes decisions;Cynthia Rudin;Stat 1050, 26, 2018;"There are black box models now being used for high stakes decision-making throughout society. The practice of trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forwardâ€“it is to design models that are inherently interpretable.There has been an increasing trend in healthcare and criminal justice to leverage machine learning (ML) for high-stakes prediction problems that deeply affect human lives. In many of these domains, practitioners are deploying black box machine-learning models that do not explain their predictions in a way that humans can understand. The lack of transparency and accountability of a predictive model can have (and has had already) severe consequences; there have been cases of people wrongly denied parole [Wexler, 2017], poor bail decisions leading to the release of dangerous criminals, machine-learning-based pollution models stating that dangerous situations are safe [McGough, 2018] and generally poor use of limited valuable resources in criminal justice, medicine, energy reliability, finance, and in other domains.";https://www.datascienceassn.org/sites/default/files/Please%20Stop%20Explaining%20Black%20Box%20Models%20for%20High%20Stakes%20Decisions.pdf;7jVa4lquht0J
Lane, H. C., Core, M. G., Van Lent, M., Solomon, S., & Gomboc, D. (2005, July). Explainable Artificial Intelligence for Training and Tutoring. In AIED (pp. 762-764).;3_explanation_model_machine_learning;2005;Explainable Artificial Intelligence for Training and Tutoring.;H Chad Lane, Mark G Core, Michael Van Lent, Steve Solomon, Dave Gomboc;AIED, 762-764, 2005;This paper describes an Explainable Artificial Intelligence XAI tool that allows entities to answer questions about their activities within a tactical simulation. We show how XAI can be used to provide more meaningful after-action reviews and discuss ongoing work to integrate an intelligent tutor into the XAI framework.Descriptors:;https://apps.dtic.mil/sti/citations/ADA459148;toCgEo6B8CoJ
Hall, P. (2019). Guidelines for Responsible and Human-Centered Use of Explainable Machine Learning. stat, 1050, 8.;3_explanation_model_machine_learning;2019;Guidelines for Responsible and Human-Centered Use of Explainable Machine Learning;Patrick Hall;stat 1050, 8, 2019;Explainable machine learning (ML) enables human learning from ML, human appeal of automated model decisions, regulatory compliance, and white-hat hacking and forensic analysis of ML models. 1, 2, 3 Explainable ML (ie explainable artificial intelligence or XAI) has been implemented in numerous open source and commercial packages and explainable ML is also an important, mandatory, or embedded aspect of commercial predictive modeling in industries like financial services. 4, 5, 6 However, like many technologies, explainable ML can be misused, particularly as a faulty safeguard for harmful black-boxes, eg fairwashing, and for other malevolent purposes like model stealing [1],[31],[34]. This text presents several definitions, examples, and qualifications in Section 2 before covering the details of responsible and human-centered use guidelines in Sections 3.1â€“3.4. This text concludes in Section 4 with the seemingly natural argument for a holistic approach to ML that includes interpretable (ie white-box) models along with explanatory, debugging, and disparate impact analysis techniques for any ML system that impacts humans.;http://datascienceprojects.org/papers/Hall2019%20-%20Explainable.pdf;b6MW5S5cnHgJ
Caton, S., & Haas, C. (2020). Fairness in machine learning: A survey. ACM Computing Surveys.;6_fairness_discrimination_bias_decision;2020;Fairness in machine learning: A survey;Simon Caton, Christian Haas;ACM Computing Surveys, 2020;When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as five dilemmas for fairness research.;https://dl.acm.org/doi/abs/10.1145/3616865;GGWAElOoDloJ
Derakhshan, B., Mahdiraji, A. R., Rabl, T., & Markl, V. (2019, March). Continuous Deployment of Machine Learning Pipelines. In EDBT (pp. 397-408).;1_ml_machine_data_learning;2019;Continuous Deployment of Machine Learning Pipelines;Behrouz Derakhshan, Alireza Rezaei Mahdiraji, Tilmann Rabl, Volker Markl;EDBT, 2019;The final goal of all industrial machine learning (ML) projects is to develop ML products and rapidly bring them into production. However, it is highly challenging to automate and operationalize ML products and thus many ML endeavors fail to deliver on their expectations. The paradigm of Machine Learning Operations (MLOps) addresses this issue. MLOps includes several aspects, such as best practices, sets of concepts, and development culture. However, MLOps is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we conduct mixed-method research, including a literature review, a tool review, and expert interviews. As a result of these investigations, we contribute to the body of knowledge by providing an aggregated overview of the necessary principles, components, and roles, as well as the associated architecture and workflows. Furthermore, we provide a comprehensive definition of MLOps and highlight open challenges in the field. Finally, this work provides guidance for ML researchers and practitioners who want to automate and operate their ML products with a designated set of technologies.;https://hpi.de/fileadmin/user_upload/fachgebiete/rabl/publications/2019/ContinousMLDeploymentEDBT2019.pdf;Todo
Radovanović, S., Petrović, A., Delibašić, B., & Suknović, M. (2020, August). Enforcing fairness in logistic regression algorithm. In 2020 International Conference on INnovations in Intelligent SysTems and Applications (INISTA) (pp. 1-7). IEEE.;6_fairness_discrimination_bias_decision;2020;Enforcing fairness in logistic regression algorithm;Sandro Radovanović, Andrija Petrović, Boris Delibašić, Milija Suknović;2020 International Conference on INnovations in Intelligent SysTems and Applications (INISTA), 1-7, 2020;Machine learning has been subject to discussion from the legal and ethical points of view in recent years. Automation of the decision-making process can lead to unethical acts with legal consequences. There are examples where the decision made by machine learning systems was unfairly biased toward some group of people. This is mainly because data used for model training were biased and thus developed a predictive model inherited that bias. Therefore, the process of learning a predictive model must be aware and account for the possible bias in the data. In this paper, we propose a modification of the logistic regression algorithm that adds one known and one novel fairness constraints into the process of model learning, thus forcing the predictive model not to create disparate impact and allow equal opportunity for every subpopulation. We demonstrate our model on real-world problems and show that a small reduction in predictive performance can yield a high improvement in disparate impact and equality of opportunity.;https://ieeexplore.ieee.org/abstract/document/9194676/;2QZ6-9241xAJ
Zhan, Y., Li, P., Qu, Z., Zeng, D., & Guo, S. (2020). A learning-based incentive mechanism for federated learning. IEEE Internet of Things Journal, 7(7), 6360-6368.;0_federated_learning_data_privacy;2020;A learning-based incentive mechanism for federated learning;Yufeng Zhan, Peng Li, Zhihao Qu, Deze Zeng, Song Guo;IEEE Internet of Things Journal 7 (7), 6360-6368, 2020;Internet of Things (IoT) generates large amounts of data at the network edge. Machine learning models are often built on these data, to enable the detection, classification, and prediction of the future events. Due to network bandwidth, storage, and especially privacy concerns, it is often impossible to send all the IoT data to the data center for centralized model training. To address these issues, federated learning has been proposed to let nodes use the local data to train models, which are then aggregated to synthesize a global model. Most of the existing work has focused on designing learning algorithms with provable convergence time, but other issues, such as incentive mechanism, are unexplored. Although incentive mechanisms have been extensively studied in network and computation resource allocation, yet they cannot be applied to federated learning directly due to the unique challenges of information unsharing and difficulties of contribution evaluation. In this article, we study the incentive mechanism for federated learning to motivate edge nodes to contribute model training. Specifically, a deep reinforcement learning-based (DRL) incentive mechanism has been designed to determine the optimal pricing strategy for the parameter server and the optimal training strategies for edge nodes. Finally, numerical experiments have been implemented to evaluate the efficiency of the proposed DRL-based incentive mechanism.;https://ieeexplore.ieee.org/abstract/document/8963610/;YKRy9u1g_jkJ
Zhou, P., Wang, K., Guo, L., Gong, S., & Zheng, B. (2019). A privacy-preserving distributed contextual federated online learning framework with big data support in social recommender systems. IEEE Transactions on Knowledge and Data Engineering, 33(3), 824-838.;0_federated_learning_data_privacy;2019;A privacy-preserving distributed contextual federated online learning framework with big data support in social recommender systems;Pan Zhou, Kehao Wang, Linke Guo, Shimin Gong, Bolong Zheng;IEEE Transactions on Knowledge and Data Engineering 33 (3), 824-838, 2019;Nowadays, the booming demand of big data analytics and the constraints of computational ability and network bandwidth have made it difficult for a stand-alone agent/service provider to provide suitable information for every user from the large volume online data within the limited time. To handle this challenge, a recommender system (RS) can call in a group of agents to collaborate to learn users' preference and taste, which is known as a distributed recommender system (DRS). DRSs can improve the accuracy of a traditional RS by requesting agents to share information with each other. However, it is challenging for DRSs to make personalized recommendations for each user due to the large amount of candidates. In addition, information sharing among agents raises a privacy concern. Thus, we propose a privacy-preserving DRS in this paper, and then model each service provider as a distributed online learner with context-awareness. Service providers collaborate to make personalized recommendations by learning users' preferences according to the user context and users' history behaviors. We adopt the federated learning framework to help train a high quality privacy- preserving centralized model over a large number of distributed agents which is probably unreliable with relatively slow network connections. To handle big data scenario, we build an item-cluster tree to deal with online and increasing datasets from top to the bottom. We further consider the structure of social network and present an efficient algorithm to avoid more performance loss adaptively. Theoretical proofs show that our proposed algorithm can achieve sublinear regret and differential privacy protection simultaneously for service providers and users. Numerical results confirm that our novel framework can handle increasing big datasets and strike a trade-off between privacy-preserving level and the prediction accuracy.;https://ieeexplore.ieee.org/abstract/document/8807242/;4E1cdCmO4CgJ
Wang, S., Tuor, T., Salonidis, T., Leung, K. K., Makaya, C., He, T., & Chan, K. (2019). Adaptive federated learning in resource constrained edge computing systems. IEEE journal on selected areas in communications, 37(6), 1205-1221.;0_federated_learning_data_privacy;2019;Adaptive federated learning in resource constrained edge computing systems;Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian Makaya, Ting He, Kevin Chan;IEEE journal on selected areas in communications 37 (6), 1205-1221, 2019;Emerging technologies and applications including Internet of Things, social networking, and crowd-sourcing generate large amounts of data at the network edge. Machine learning models are often built from the collected data, to enable the detection, classification, and prediction of future events. Due to bandwidth, storage, and privacy concerns, it is often impractical to send all the data to a centralized location. In this paper, we consider the problem of learning model parameters from data distributed across multiple edge nodes, without sending raw data to a centralized place. Our focus is on a generic class of machine learning models that are trained using gradient-descent-based approaches. We analyze the convergence bound of distributed gradient descent from a theoretical point of view, based on which we propose a control algorithm that determines the best tradeoff between local update and global parameter aggregation to minimize the loss function under a given resource budget. The performance of the proposed algorithm is evaluated via extensive experiments with real datasets, both on a networked prototype system and in a larger-scale simulated environment. The experimentation results show that our proposed approach performs near to the optimum with various machine learning models and different data distributions.;https://ieeexplore.ieee.org/abstract/document/8664630/;o3kB34RbmbcJ
Lu, Y., Huang, X., Dai, Y., Maharjan, S., & Zhang, Y. (2019). Blockchain and federated learning for privacy-preserved data sharing in industrial IoT. IEEE Transactions on Industrial Informatics, 16(6), 4177-4186.;0_federated_learning_data_privacy;2019;Blockchain and federated learning for privacy-preserved data sharing in industrial IoT;Yunlong Lu, Xiaohong Huang, Yueyue Dai, Sabita Maharjan, Yan Zhang;IEEE Transactions on Industrial Informatics 16 (6), 4177-4186, 2019;The rapid increase in the volume of data generated from connected devices in industrial Internet of Things paradigm, opens up new possibilities for enhancing the quality of service for the emerging applications through data sharing. However, security and privacy concerns (e.g., data leakage) are major obstacles for data providers to share their data in wireless networks. The leakage of private data can lead to serious issues beyond financial loss for the providers. In this article, we first design a blockchain empowered secure data sharing architecture for distributed multiple parties. Then, we formulate the data sharing problem into a machine-learning problem by incorporating privacy-preserved federated learning. The privacy of data is well-maintained by sharing the data model instead of revealing the actual data. Finally, we integrate federated learning in the consensus process of permissioned blockchain, so that the computing work for consensus can also be used for federated training. Numerical results derived from real-world datasets show that the proposed data sharing scheme achieves good accuracy, high efficiency, and enhanced security.;https://ieeexplore.ieee.org/abstract/document/8843900/;lBPjDXK5qY0J
Kim, Y. J., & Hong, C. S. (2019, September). Blockchain-based node-aware dynamic weighting methods for improving federated learning performance. In 2019 20th Asia-pacific network operations and management symposium (APNOMS) (pp. 1-4). IEEE.;0_federated_learning_data_privacy;2019;Blockchain-based node-aware dynamic weighting methods for improving federated learning performance;You Jun Kim, Choong Seon Hong;2019 20th Asia-pacific network operations and management symposium (APNOMS), 1-4, 2019;Federated learning (FL) is a decentralized learning method that deviated from the conventional centralized learning. The FL progresses learning locally on each device and gradually improves the learning model through interaction with the central server. However, it can cause network overload because of limited communication bandwidth and the participation of a huge number of users. One of the ways to minimize the network load is for the model to converge rapidly and stably with target learning accuracy. In this paper, we propose blockchain based federated learning scenario. Blockchain can efficiently induce users to participate in learning and can separate each participating user as a `node'. In addition, it can be pursued the integrity, stability, and so on. We consider two types of weights to choose the subset of clients for updating the global model. First, we consider the weight based on local learning accuracy of each client. Second, we consider the weight based on participation frequency of each client. We choose two key performance indicators, learning speed and standard deviation, to compare the performance of our proposed scheme with existing schemes. The simulation results show that our proposed scheme achieves higher stability along with fast convergence time for targeted accuracy compared to others.;https://ieeexplore.ieee.org/abstract/document/8893114/;Xm8RhGZKxUwJ
Kim, H., Park, J., Bennis, M., & Kim, S. L. (2019). Blockchained on-device federated learning. IEEE Communications Letters, 24(6), 1279-1283.;0_federated_learning_data_privacy;2019;Blockchained on-device federated learning;Hyesung Kim, Jihong Park, Mehdi Bennis, Seong-Lyun Kim;IEEE Communications Letters 24 (6), 1279-1283, 2019;By leveraging blockchain, this letter proposes a blockchained federated learning (BlockFL) architecture where local learning model updates are exchanged and verified. This enables on-device machine learning without any centralized training data or coordination by utilizing a consensus mechanism in blockchain. Moreover, we analyze an end-to-end latency model of BlockFL and characterize the optimal block generation rate by considering communication, computation, and consensus delays.;https://ieeexplore.ieee.org/abstract/document/8733825/;y9w9U1BbmxIJ
Zhu, G., Wang, Y., & Huang, K. (2019). Broadband analog aggregation for low-latency federated edge learning. IEEE Transactions on Wireless Communications, 19(1), 491-506.;0_federated_learning_data_privacy;2019;Broadband analog aggregation for low-latency federated edge learning;Guangxu Zhu, Yong Wang, Kaibin Huang;IEEE Transactions on Wireless Communications 19 (1), 491-506, 2019;To leverage rich data distributed at the network edge, a new machine-learning paradigm, called edge learning, has emerged where learning algorithms are deployed at the edge for providing intelligent services to mobile users. While computing speeds are advancing rapidly, the communication latency is becoming the bottleneck of fast edge learning. To address this issue, this work is focused on designing a low-latency multi-access scheme for edge learning. To this end, we consider a popular privacy-preserving framework, federated edge learning (FEEL), where a global AI-model at an edge-server is updated by aggregating (averaging) local models trained at edge devices. It is proposed that the updates simultaneously transmitted by devices over broadband channels should be analog aggregated “over-the-air” by exploiting the waveform-superposition property of a multi-access channel. Such broadband analog aggregation (BAA) results in dramatical communication-latency reduction compared with the conventional orthogonal access (i.e., OFDMA). In this work, the effects of BAA on learning performance are quantified targeting a single-cell random network. First, we derive two tradeoffs between communication-and-learning metrics, which are useful for network planning and optimization. The power control (“truncated channel inversion”) required for BAA results in a tradeoff between the update-reliability [as measured by the receive signal-to-noise ratio (SNR)] and the expected update-truncation ratio. Consider the scheduling of cell-interior devices to constrain path loss. This gives rise to the other tradeoff between the receive SNR and fraction of data exploited in learning. Next, the latency-reduction ratio of the proposed BAA with respect to the traditional OFDMA scheme is proved to scale almost linearly with the device population. Experiments based on a neural network and a real dataset are conducted for corroborating the theoretical results.;https://ieeexplore.ieee.org/abstract/document/8870236/;DgSSnwt6_QEJ
Nishio, T., & Yonetani, R. (2019, May). Client selection for federated learning with heterogeneous resources in mobile edge. In ICC 2019-2019 IEEE international conference on communications (ICC) (pp. 1-7). IEEE.;0_federated_learning_data_privacy;2019;Client selection for federated learning with heterogeneous resources in mobile edge;Takayuki Nishio, Ryo Yonetani;ICC 2019-2019 IEEE international conference on communications (ICC), 1-7, 2019;We envision a mobile edge computing (MEC) framework for machine learning (ML) technologies, which leverages distributed client data and computation resources for training high-performance ML models while preserving client privacy. Toward this future goal, this work aims to extend Federated Learning (FL), a decentralized learning framework that enables privacy-preserving training of models, to work with heterogeneous clients in a practical cellular network. The FL protocol iteratively asks random clients to download a trainable model from a server, update it with own data, and upload the updated model to the server, while asking the server to aggregate multiple client updates to further improve the model. While clients in this protocol are free from disclosing own private data, the overall training process can become inefficient when some clients are with limited computational resources (i.e., requiring longer update time) or under poor wireless channel conditions (longer upload time). Our new FL protocol, which we refer to as FedCS, mitigates this problem and performs FL efficiently while actively managing clients based on their resource conditions. Specifically, FedCS solves a client selection problem with resource constraints, which allows the server to aggregate as many client updates as possible and to accelerate performance improvement in ML models. We conducted an experimental evaluation using publicly-available large-scale image datasets to train deep neural networks on MEC environment simulations. The experimental results show that FedCS is able to complete its training process in a significantly shorter time compared to the original FL protocol.;https://ieeexplore.ieee.org/abstract/document/8761315/;KDS_C7mzO3UJ
Damiani, E., & Ardagna, C. A. (2020, January). Certified machine-learning models. In International Conference on Current Trends in Theory and Practice of Informatics (pp. 3-15). Cham: Springer International Publishing.;1_ml_machine_data_learning;2020;Certified machine-learning models;Ernesto Damiani, Claudio A. Ardagna ;Lecture Notes in Computer Science book series (LNTCS,volume 12011);The massive adoption of Machine Learning (ML) has deeply changed the internal structure, the design and the operation of software systems. ML has shifted the focus from code to data, especially in application areas where it is easier to collect samples that embody correct solutions to individual instances of a problem, than to design and code a deterministic algorithm solving it for all instances. There is an increasing awareness of the need to verify key non-functional properties of ML-based software applications like fairness and privacy. However, the traditional approach trying to verify these properties by code inspection is pointless, since ML models’ behavior mostly depends on the data and parameters used to train them. Classic software certification techniques cannot solve the issue as well. The Artificial Intelligence (AI) community has been working on the idea of preventing undesired behavior by controlling a priori the ML models’ training sets and parameters. In this paper, we take a different, online approach to ML verification, where novel behavioral monitoring techniques based on statistical testing are used to support a dynamic certification framework enforcing the desired properties on black-box ML models in operation. Our aim is to deliver a novel framework suitable for practical certification of distributed ML-powered applications in heavily regulated domains like transport, energy, healthcare, even when the certifying authority is not privy to the model training. To achieve this goal, we rely on three key ideas: (i) use test suites to define desired non-functional properties of ML models, (ii) Use statistical monitoring of ML models’ behavior at inference time to check that the desired behavioral properties are achieved, and (iii) compose monitors’ outcome within dynamic, virtual certificates for composite software applications. ;https://link.springer.com/chapter/10.1007/978-3-030-38919-2_1;Todo
Bergstra, J., Komer, B., Eliasmith, C., Yamins, D., & Cox, D. D. (2015). Hyperopt: a python library for model selection and hyperparameter optimization. Computational Science & Discovery, 8(1), 014008.;1_ml_machine_data_learning;2015;Hyperopt: a python library for model selection and hyperparameter optimization;James Bergstra, Brent Komer, Chris Eliasmith, Dan Yamins, David D Cox;Computational Science & Discovery 8 (1), 014008, 2015;Sequential model-based optimization (also known as Bayesian optimization) is one of the most efficient methods (per function evaluation) of function minimization. This efficiency makes it appropriate for optimizing the hyperparameters of machine learning algorithms that are slow to train. The Hyperopt library provides algorithms and parallelization infrastructure for performing hyperparameter optimization (model selection) in Python. This paper presents an introductory tutorial on the usage of the Hyperopt library, including the description of search spaces, minimization (in serial and parallel), and the analysis of the results collected in the course of minimization. This paper also gives an overview of Hyperopt-Sklearn, a software project that provides automatic algorithm configuration of the Scikit-learn machine learning library. Following Auto-Weka, we take the view that the choice of classifier and even the choice of preprocessing module can be taken together to represent a single large hyperparameter optimization problem. We use Hyperopt to define a search space that encompasses many standard components (eg SVM, RF, KNN, PCA, TFIDF) and common patterns of composing them together. We demonstrate, using search algorithms in Hyperopt and standard benchmarking data sets (MNIST, 20-newsgroups, convex shapes), that searching this space is practical and effective. In particular, we improve on best-known scores for the model space for both MNIST and convex shapes. The paper closes with some discussion of ongoing and future work.;https://www.researchgate.net/profile/Chris-Eliasmith/publication/282814671_Hyperopt_A_Python_library_for_model_selection_and_hyperparameter_optimization/links/57d173d008ae5f03b48a7c42/Hyperopt-A-Python-library-for-model-selection-and-hyperparameter-optimization.pdf?origin=journalDetail&_tp=eyJwYWdlIjoiam91cm5hbERldGFpbCJ9;6_BXKurjy4QJ
Chen, Y., Sun, X., & Jin, Y. (2019). Communication-efficient federated deep learning with layerwise asynchronous model update and temporally weighted aggregation. IEEE transactions on neural networks and learning systems, 31(10), 4229-4238.;0_federated_learning_data_privacy;2019;Communication-efficient federated deep learning with layerwise asynchronous model update and temporally weighted aggregation;Yang Chen, Xiaoyan Sun, Yaochu Jin;IEEE transactions on neural networks and learning systems 31 (10), 4229-4238, 2019;Federated learning obtains a central model on the server by aggregating models trained locally on clients. As a result, federated learning does not require clients to upload their data to the server, thereby preserving the data privacy of the clients. One challenge in federated learning is to reduce the client-server communication since the end devices typically have very limited communication bandwidth. This article presents an enhanced federated learning technique by proposing an asynchronous learning strategy on the clients and a temporally weighted aggregation of the local models on the server. In the asynchronous learning strategy, different layers of the deep neural networks (DNNs) are categorized into shallow and deep layers, and the parameters of the deep layers are updated less frequently than those of the shallow layers. Furthermore, a temporally weighted aggregation strategy is introduced on the server to make use of the previously trained local models, thereby enhancing the accuracy and convergence of the central model. The proposed algorithm is empirically on two data sets with different DNNs. Our results demonstrate that the proposed asynchronous federated deep learning outperforms the baseline algorithm both in terms of communication cost and model accuracy.;https://ieeexplore.ieee.org/abstract/document/8945292/;40mOF3VVXv8J
Mills, J., Hu, J., & Min, G. (2019). Communication-efficient federated learning for wireless edge intelligence in IoT. IEEE Internet of Things Journal, 7(7), 5986-5994.;0_federated_learning_data_privacy;2019;Communication-efficient federated learning for wireless edge intelligence in IoT;Jed Mills, Jia Hu, Geyong Min;IEEE Internet of Things Journal 7 (7), 5986-5994, 2019;The rapidly expanding number of Internet of Things (IoT) devices is generating huge quantities of data, but public concern over data privacy means users are apprehensive to send data to a central server for machine learning (ML) purposes. The easily changed behaviors of edge infrastructure that software-defined networking (SDN) provides makes it possible to collate IoT data at edge servers and gateways, where federated learning (FL) can be performed: building a central model without uploading data to the server. FedAvg is an FL algorithm which has been the subject of much study, however, it suffers from a large number of rounds to convergence with non-independent identically distributed (non-IID) client data sets and high communication costs per round. We propose adapting FedAvg to use a distributed form of Adam optimization, greatly reducing the number of rounds to convergence, along with the novel compression techniques, to produce communication-efficient FedAvg (CE-FedAvg). We perform extensive experiments with the MNIST/CIFAR-10 data sets, IID/non-IID client data, varying numbers of clients, client participation rates, and compression rates. These show that CE-FedAvg can converge to a target accuracy in up to 6× less rounds than similarly compressed FedAvg, while uploading up to 3× less data, and is more robust to aggressive compression. Experiments on an edge-computing-like testbed using Raspberry Pi clients also show that CE-FedAvg is able to reach a target accuracy in up to 1.7× less real time than FedAvg.;https://ieeexplore.ieee.org/abstract/document/8917724/;eCheF1kgpnIJ
Luping, W. A. N. G., Wei, W. A. N. G., & Bo, L. I. (2019, July). CMFL: Mitigating communication overhead for federated learning. In 2019 IEEE 39th international conference on distributed computing systems (ICDCS) (pp. 954-964). IEEE.;0_federated_learning_data_privacy;2019;CMFL: Mitigating communication overhead for federated learning;Luping WANG, Wei WANG, Bo LI;2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS);Federated Learning enables mobile users to collaboratively learn a global prediction model by aggregating their individual updates without sharing the privacy-sensitive data. As mobile devices usually have limited data plan and slow network connections to the central server where the global model is maintained, mitigating the communication overhead is of paramount importance. While existing works mainly focus on reducing the total bits transferred in each update via data compression, we study an orthogonal approach that identifies irrelevant updates made by clients and precludes them from being uploaded for reduced network footprint. Following this idea, we propose Communication-Mitigated Federated Learning (CMFL) in this paper. CMFL provides clients with feedback information regarding the global tendency of model updating. Each client checks if its update aligns with this global tendency and is relevant enough to model improvement. By avoiding uploading those irrelevant updates to the server, CMFL can substantially reduce the communication overhead while still guaranteeing the learning convergence. CMFL is shown to achieve general improvement in communication efficiency for almost all of the existing federated learning schemes. We evaluate CMFL through extensive simulations and EC2 emulations. Compared with vanilla Federated Learning, CMFL yields 13.97x communication efficiency in terms of the reduction of network footprint. When applied to Federated Multi-Task Learning, CMFL improves the communication efficiency by 5.7x with 4% higher prediction accuracy.;https://ieeexplore.ieee.org/abstract/document/8885054;todo
Nguyen, T. D., Marchal, S., Miettinen, M., Fereidooni, H., Asokan, N., & Sadeghi, A. R. (2019, July). DÏoT: A federated self-learning anomaly detection system for IoT. In 2019 IEEE 39th International conference on distributed computing systems (ICDCS) (pp. 756-767). IEEE.;0_federated_learning_data_privacy;2019;DÏoT: A federated self-learning anomaly detection system for IoT;Thien Duc Nguyen, Samuel Marchal, Markus Miettinen, Hossein Fereidooni, N Asokan, Ahmad-Reza Sadeghi;2019 IEEE 39th International conference on distributed computing systems (ICDCS), 756-767, 2019;IoT devices are increasingly deployed in daily life. Many of these devices are, however, vulnerable due to insecure design, implementation, and configuration. As a result, many networks already have vulnerable IoT devices that are easy to compromise. This has led to a new category of malware specifically targeting IoT devices. However, existing intrusion detection techniques are not effective in detecting compromised IoT devices given the massive scale of the problem in terms of the number of different types of devices and manufacturers involved. In this paper, we present DÏoT, an autonomous self-learning distributed system for detecting compromised IoT devices. DÏoT builds effectively on device-type-specific communication profiles without human intervention nor labeled data that are subsequently used to detect anomalous deviations in devices' communication behavior, potentially caused by malicious adversaries. DÏoT utilizes a federated learning approach for aggregating behavior profiles efficiently. To the best of our knowledge, it is the first system to employ a federated learning approach to anomaly-detection-based intrusion detection. Consequently, DÏoT can cope with emerging new and unknown attacks. We systematically and extensively evaluated more than 30 off-the-shelf IoT devices over a long term and show that DÏoT is highly effective (95.6% detection rate) and fast (257 ms) at detecting devices compromised by, for instance, the infamous Mirai malware. DÏoT reported no false alarms when evaluated in a real-world smart home deployment setting.;https://ieeexplore.ieee.org/abstract/document/8884802/;tbfJda2_jFYJ
Zhang, X., Peng, M., Yan, S., & Sun, Y. (2019). Deep-reinforcement-learning-based mode selection and resource allocation for cellular V2X communications. IEEE Internet of Things Journal, 7(7), 6380-6391.;0_federated_learning_data_privacy;2019;Deep-reinforcement-learning-based mode selection and resource allocation for cellular V2X communications;Xinran Zhang, Mugen Peng, Shi Yan, Yaohua Sun;IEEE Internet of Things Journal 7 (7), 6380-6391, 2019;Cellular vehicle-to-everything (V2X) communication is crucial to support future diverse vehicular applications. However, for safety-critical applications, unstable vehicle-to-vehicle (V2V) links, and high signaling overhead of centralized resource allocation approaches become bottlenecks. In this article, we investigate a joint optimization problem of transmission mode selection and resource allocation for cellular V2X communications. In particular, the problem is formulated as a Markov decision process, and a deep reinforcement learning (DRL)-based decentralized algorithm is proposed to maximize the sum capacity of vehicle-to-infrastructure users while meeting the latency and reliability requirements of V2V pairs. Moreover, considering training limitation of local DRL models, a two-timescale federated DRL algorithm is developed to help obtain robust models. Wherein, the graph theory-based vehicle clustering algorithm is executed on a large timescale and in turn, the federated learning algorithm is conducted on a small timescale. The simulation results show that the proposed DRL-based algorithm outperforms other decentralized baselines, and validate the superiority of the two-timescale federated DRL algorithm for newly activated V2V pairs.;https://ieeexplore.ieee.org/abstract/document/8944302/;XTtmQX756lkJ
Weng, J., Weng, J., Zhang, J., Li, M., Zhang, Y., & Luo, W. (2019). Deepchain: Auditable and privacy-preserving deep learning with blockchain-based incentive. IEEE Transactions on Dependable and Secure Computing, 18(5), 2438-2455.;0_federated_learning_data_privacy;2019;Deepchain: Auditable and privacy-preserving deep learning with blockchain-based incentive;Jiasi Weng, Jian Weng, Jilian Zhang, Ming Li, Yue Zhang, Weiqi Luo;IEEE Transactions on Dependable and Secure Computing 18 (5), 2438-2455, 2019;Deep learning can achieve higher accuracy than traditional machine learning algorithms in a variety of machine learning tasks. Recently, privacy-preserving deep learning has drawn tremendous attention from information security community, in which neither training data nor the training model is expected to be exposed. Federated learning is a popular learning mechanism, where multiple parties upload local gradients to a server and the server updates model parameters with the collected gradients. However, there are many security problems neglected in federated learning, for example, the participants may behave incorrectly in gradient collecting or parameter updating, and the server may be malicious as well. In this article, we present a distributed, secure, and fair deep learning framework named DeepChain to solve these problems. DeepChain provides a value-driven incentive mechanism based on Blockchain to force the participants to behave correctly. Meanwhile, DeepChain guarantees data privacy for each participant and provides auditability for the whole training process. We implement a prototype of DeepChain and conduct experiments on a real dataset for different settings, and the results show that our DeepChain is promising.;https://ieeexplore.ieee.org/abstract/document/8894364/;t7qI-Z8L78cJ
Zhang, X., Chen, X., Liu, J. K., & Xiang, Y. (2019). DeepPAR and DeepDPA: privacy preserving and asynchronous deep learning for industrial IoT. IEEE Transactions on Industrial Informatics, 16(3), 2081-2090.;0_federated_learning_data_privacy;2019;DeepPAR and DeepDPA: privacy preserving and asynchronous deep learning for industrial IoT;Xiaoyu Zhang, Xiaofeng Chen, Joseph K Liu, Yang Xiang;IEEE Transactions on Industrial Informatics 16 (3), 2081-2090, 2019;Industrial Internet of Things (IIoT) is significant of building powerful industrial systems and applications. Deep learning has provided a promising opportunity to extract useful knowledge by utilizing vast amounts of data in IIoT. However, lacking of massive public datasets will lead to low performance and overfitting of the learned model. Therefore, the federated deep learning over distributed datasets has been proposed. Whereas, it inevitably introduces some new security challenges, i.e., disclosing participant's data privacy. However, existing methods cannot guarantee each participant's data privacy in a learning group. In this article, we propose two privacy-preserving asynchronous deep learning schemes [privacy-preserving and asynchronous deep learning via re-encryption (DeepPAR) and dynamic privacy-preserving and asynchronous deep learning (DeepDPA)]. Compared to the state-of-the-art work, DeepPAR protects each participant's input privacy while preserving dynamic update secrecy inherently. Meanwhile, DeepDPA enables to guarantee backward secrecy of group participants in a lightweight manner. Security analysis and performance evaluations on real dataset show that our proposed schemes are secure, efficient, and effective.;https://ieeexplore.ieee.org/abstract/document/8836609/;lydkcM_V4GIJ
Conway-Jones, D., Tuor, T., Wang, S., & Leung, K. K. (2019, June). Demonstration of federated learning in a resource-constrained networked environment. In 2019 IEEE international conference on smart computing (SMARTCOMP) (pp. 484-486). IEEE.;0_federated_learning_data_privacy;2019;Demonstration of federated learning in a resource-constrained networked environment;Dave Conway-Jones, Tiffany Tuor, Shiqiang Wang, Kin K Leung;2019 IEEE international conference on smart computing (SMARTCOMP), 484-486, 2019;Many modern applications in the area of smart computing are based on machine learning techniques. To train machine learning models, a large amount of data is usually required, which is often not readily available at a central location. Federated learning enables the training of machine learning models from distributed datasets at client devices without transmitting the data to a central place, which has benefits including preserving the privacy of user data and reducing communication bandwidth. In this demonstration, we show a federated learning system deployed in an emulated wide-area communications network with dynamic, heterogeneous, and intermittent resource availability, where the network is emulated using a CORE/EMANE emulator. In our system, the environment is decentralized and each client can ask for assistance by other clients. The availability of clients is intermittent so only those clients that are available can provide assistance. A graphical interface illustrates the network connections and the user can adjust these connections through the interface. A user interface displays the training progress and each client's contribution to training.;https://ieeexplore.ieee.org/abstract/document/8784064/;OZ43sSfow6oJ
Lu, Y., Huang, X., Dai, Y., Maharjan, S., & Zhang, Y. (2019). Differentially private asynchronous federated learning for mobile edge computing in urban informatics. IEEE Transactions on Industrial Informatics, 16(3), 2134-2143.;0_federated_learning_data_privacy;2019;Differentially private asynchronous federated learning for mobile edge computing in urban informatics;Yunlong Lu, Xiaohong Huang, Yueyue Dai, Sabita Maharjan, Yan Zhang;IEEE Transactions on Industrial Informatics 16 (3), 2134-2143, 2019;Driven by technologies such as mobile edge computing and 5G, recent years have witnessed the rapid development of urban informatics, where a large amount of data is generated. To cope with the growing data, artificial intelligence algorithms have been widely exploited. Federated learning is a promising paradigm for distributed edge computing, which enables edge nodes to train models locally without transmitting their data to a server. However, the security and privacy concerns of federated learning hinder its wide deployment in urban applications such as vehicular networks. In this article, we propose a differentially private asynchronous federated learning scheme for resource sharing in vehicular networks. To build a secure and robust federated learning scheme, we incorporate local differential privacy into federated learning for protecting the privacy of updated local models. We further propose a random distributed update scheme to get rid of the security threats led by a centralized curator. Moreover, we perform the convergence boosting in our proposed scheme by updates verification and weighted aggregation. We evaluate our scheme on three real-world datasets. Numerical results show the high accuracy and efficiency of our proposed scheme, whereas preserve the data privacy.;https://ieeexplore.ieee.org/abstract/document/8843942/;KKCXZjmlLV4J
Qian, J., Gochhayat, S. P., & Hansen, L. K. (2019, June). Distributed active learning strategies on edge computing. In 2019 6th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2019 5th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom) (pp. 221-226). IEEE.;7_edge_computing_deep_learning;2019;Distributed active learning strategies on edge computing;Jia Qian, Sarada Prasad Gochhayat, Lars Kai Hansen;2019 6th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2019 5th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom …, 2019;"Fog platform brings the computing power from the remote cloud-side closer to the edge devices to reduce latency, as the unprecedented generation of data causes ineligible latency to process the data in a centralized fashion at the Cloud. In this new setting, edge devices with distributed computing capability, such as sensors, surveillance camera, can communicate with fog nodes with less latency. Furthermore, local computing (at edge side) may improve privacy and trust. In this paper, we present a new method, in which, we decompose the data processing, by dividing them between edge devices and fog nodes, intelligently. We apply active learning on edge devices; and federated learning on the fog node which significantly reduces the data samples to train the model as well as the communication cost. To show the effectiveness of the proposed method, we implemented and evaluated its performance on a benchmark images data set.";https://ieeexplore.ieee.org/abstract/document/8854053/;5DifUl56zRIJ
Samarakoon, S., Bennis, M., Saad, W., & Debbah, M. (2019). Distributed federated learning for ultra-reliable low-latency vehicular communications. IEEE Transactions on Communications, 68(2), 1146-1159.;0_federated_learning_data_privacy;2019;Distributed federated learning for ultra-reliable low-latency vehicular communications;Sumudu Samarakoon, Mehdi Bennis, Walid Saad, MÃ©rouane Debbah;IEEE Transactions on Communications 68 (2), 1146-1159, 2019;In this paper, the problem of joint power and resource allocation (JPRA) for ultra-reliable low-latency communication (URLLC) in vehicular networks is studied. Therein, the network-wide power consumption of vehicular users (VUEs) is minimized subject to high reliability in terms of probabilistic queuing delays. Using extreme value theory (EVT), a new reliability measure is defined to characterize extreme events pertaining to vehicles' queue lengths exceeding a predefined threshold. To learn these extreme events, assuming they are independently and identically distributed over VUEs, a novel distributed approach based on federated learning (FL) is proposed to estimate the tail distribution of the queue lengths. Considering the communication delays incurred by FL over wireless links, Lyapunov optimization is used to derive the JPRA policies enabling URLLC for each VUE in a distributed manner. The proposed solution is then validated via extensive simulations using a Manhattan mobility model. Simulation results show that FL enables the proposed method to estimate the tail distribution of queues with an accuracy that is close to a centralized solution with up to 79% reductions in the amount of exchanged data. Furthermore, the proposed method yields up to 60% reductions of VUEs with large queue lengths, while reducing the average power consumption by two folds, compared to an average queue-based baseline.;https://ieeexplore.ieee.org/abstract/document/8917592/;HmZaCv-7ghQJ
Hao, M., Li, H., Luo, X., Xu, G., Yang, H., & Liu, S. (2019). Efficient and privacy-enhanced federated learning for industrial artificial intelligence. IEEE Transactions on Industrial Informatics, 16(10), 6532-6542.;0_federated_learning_data_privacy;2019;Efficient and privacy-enhanced federated learning for industrial artificial intelligence;Meng Hao, Hongwei Li, Xizhao Luo, Guowen Xu, Haomiao Yang, Sen Liu;IEEE Transactions on Industrial Informatics 16 (10), 6532-6542, 2019;By leveraging deep learning-based technologies, industrial artificial intelligence (IAI) has been applied to solve various industrial challenging problems in Industry 4.0. However, for privacy reasons, traditional centralized training may be unsuitable for sensitive data-driven industrial scenarios, such as healthcare and autopilot. Recently, federated learning has received widespread attention, since it enables participants to collaboratively learn a shared model without revealing their local data. However, studies have shown that, by exploiting the shared parameters adversaries can still compromise industrial applications such as auto-driving navigation systems, medical data in wearable devices, and industrial robots' decision making. In this article, to solve this problem, we propose an efficient and privacy-enhanced federated learning (PEFL) scheme for IAI. Compared with existing solutions, PEFL is noninteractive, and can prevent private data from being leaked even if multiple entities collude with each other. Moreover, extensive experiments with real-world data demonstrate the superiority of PEFL in terms of accuracy and efficiency.;https://ieeexplore.ieee.org/abstract/document/8859260/;8zFTykeYi34J
Anh, T. T., Luong, N. C., Niyato, D., Kim, D. I., & Wang, L. C. (2019). Efficient training management for mobile crowd-machine learning: A deep reinforcement learning approach. IEEE Wireless Communications Letters, 8(5), 1345-1348.;0_federated_learning_data_privacy;2019;Efficient training management for mobile crowd-machine learning: A deep reinforcement learning approach;Tran The Anh, Nguyen Cong Luong, Dusit Niyato, Dong In Kim, Li-Chun Wang;IEEE Wireless Communications Letters 8 (5), 1345-1348, 2019;In this letter, we consider the concept of mobile crowd-machine learning (MCML) for a federated learning model. The MCML enables mobile devices in a mobile network to collaboratively train neural network models required by a server while keeping data on the mobile devices. The MCML thus addresses data privacy issues of traditional machine learning. However, the mobile devices are constrained by energy, CPU, and wireless bandwidth. Thus, to minimize the energy consumption, training time, and communication cost, the server needs to determine proper amounts of data and energy that the mobile devices use for training. However, under the dynamics and uncertainty of the mobile environment, it is challenging for the server to determine the optimal decisions on mobile device resource management. In this letter, we propose to adopt a deep -learning algorithm that allows the server to learn and find optimal decisions without any a priori knowledge of network dynamics. Simulation results show that the proposed algorithm outperforms the static algorithms in terms of energy consumption and training latency.;https://ieeexplore.ieee.org/abstract/document/8716527/;gVkyKSjicSgJ
Troglia, M., Melcher, J., Zheng, Y., Anthony, D., Yang, A., & Yang, T. (2019, November). Fair: Federated incumbent detection in cbrs band. In 2019 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN) (pp. 1-6). IEEE.;0_federated_learning_data_privacy;2019;Fair: Federated incumbent detection in cbrs band;Matthew Troglia, Jordan Melcher, Yao Zheng, Dylan Anthony, Alvin Yang, Thomas Yang;2019 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN), 1-6, 2019;"The next-generation spectrum access system (SAS) for the Citizens Broadband Radio Service band is equipped with environmental sensors (ESCs) to detect the presence of non-informed incumbent users, which allows the SAS to dynamically reassign spectrum resource for low privilege users to avoid interference. However, the performance of existing single-node detection model is limited by the sensorâ€™s geo-locations; whereas a naive distributed sensing network with improved detection accuracy introduces a high bandwidth overhead due to the frequent communication of spectrum data. In addition, many existing coherent spectrum sensing methods are not feasible for CBRS band due to the unknown operational characteristics of incumbent military wireless applications. To address these issues, we propose a machine learning based non-coherent spectrum sensing system: (F)eder(a)ted (I)ncumbent Detection in CB(R)S (FaIR). FaIR leverages a communication-efficient distributed learning framework, federated learning, for ESCs to collaborate and train a data-driven machine learning model for incumbent detection under minimal communication bandwidth. Our preliminary results show that the federated learning method can exploit the spatial diversity of ESCs and obtain an improved detection model comparing to a naive distributed sensing and centralized model framework. We evaluate the FaIR model with a variety of spectrum waveforms at varying SNRs. Our experiments showed that FaIR improves the average detection accuracy compared to the single-node method, using a fraction of the bandwidth compared to the naive multi-node method.";https://ieeexplore.ieee.org/abstract/document/8935736/;tuLNwh9ddPgJ
Li, Z., Wang, L., Jiang, L., & Xu, C. Z. (2019, December). FC-SLAM: Federated learning enhanced distributed visual-LiDAR SLAM in cloud robotic system. In 2019 IEEE International Conference on Robotics and Biomimetics (ROBIO) (pp. 1995-2000). IEEE.;0_federated_learning_data_privacy;2019;FC-SLAM: Federated learning enhanced distributed visual-LiDAR SLAM in cloud robotic system;Zhaoran Li, Lujia Wang, Lingxin Jiang, Cheng-Zhong Xu;2019 IEEE International Conference on Robotics and Biomimetics (ROBIO), 1995-2000, 2019;SLAM has shown great values in many fields such as self-driving cars, virtual reality and robotic localization, etc. Cloud robot based collaborative SLAM can effectively improve the efficiency of mapping tasks. However, place recognition and matching accuracy among different robots can greatly affect the map fusion performance of the entire SLAM system. Therefore, this paper presents a learning architecture for cooperative SLAM named FC-SLAM, a distributed SLAM in cloud robotic systems by taking advantage of federated learning to enhance the performance of visual-LiDAR SLAM. Additionally, we propose a federated deep learning algorithm for feature extraction and dynamic vocabulary designation which works in real-time on cloud workstation. FC-SLAM can ensure real-time collaborative SLAM by keep the original images on robot side instead of sending them to the cloud server. We test our system on open datasets and in simulated environment. The results show that it has better feature extraction performance than SIFT and ORB under illumination and viewpoint changes. Besides, map fusion is conducted to generate a global map according to place matching relation of distributed robots.;https://ieeexplore.ieee.org/abstract/document/8961798/;ul43R4x6SM0J
Chen, M., Semiari, O., Saad, W., Liu, X., & Yin, C. (2019). Federated echo state learning for minimizing breaks in presence in wireless virtual reality networks. IEEE Transactions on Wireless Communications, 19(1), 177-191.;7_edge_computing_deep_learning;2019;Federated echo state learning for minimizing breaks in presence in wireless virtual reality networks;Mingzhe Chen, Omid Semiari, Walid Saad, Xuanlin Liu, Changchuan Yin;"Mingzhe Chen; Omid Semiari; Walid Saad; Xuanlin Liu; Changchuan Yin";In this paper, the problem of enhancing the virtual reality (VR) experience for wireless users is investigated by minimizing the occurrence of breaks in presence (BIP) that can detach the users from their virtual world. To measure the BIP for wireless VR users, a novel model that jointly considers the VR application type, transmission delay, VR video quality, and users' awareness of the virtual environment is proposed. In the developed model, base stations (BSs) transmit VR videos to the wireless VR users using directional transmission links so as to provide high data rates for the VR users, thus, reducing the number of BIP for each user. Since the body movements of a VR user may result in a blockage of its wireless link, the location and orientation of VR users must also be considered when minimizing BIP. The BIP minimization problem is formulated as an optimization problem which jointly considers the predictions of users' locations, orientations, and their BS association. To predict the orientation and locations of VR users, a distributed learning algorithm based on the machine learning framework of deep echo state networks (ESNs) is proposed. The proposed algorithm uses federated learning to enable multiple BSs to locally train their deep ESNs using their collected data and cooperatively build a learning model to predict the entire users' locations and orientations. Using these predictions, the user association policy that minimizes BIP is derived. Simulation results demonstrate that the developed algorithm reduces the users' BIP by up to 16% and 26%, respectively, compared to centralized ESN and deep learning algorithms.;https://ieeexplore.ieee.org/abstract/document/8851408;Todo
Yu, Z., Hu, J., Min, G., Lu, H., Zhao, Z., Wang, H., & Georgalas, N. (2018, December). Federated learning based proactive content caching in edge computing. In 2018 IEEE Global Communications Conference (GLOBECOM) (pp. 1-6). IEEE.;0_federated_learning_data_privacy;2018;Federated learning based proactive content caching in edge computing;Zhengxin Yu, Jia Hu, Geyong Min, Haochuan Lu, Zhiwei Zhao, Haozhe Wang, Nektarios Georgalas;2018 IEEE Global Communications Conference (GLOBECOM), 1-6, 2018;Content caching is a promising approach in edge computing to cope with the explosive growth of mobile data on 5G networks, where contents are typically placed on local caches for fast and repetitive data access. Due to the capacity limit of caches, it is essential to predict the popularity of files and cache those popular ones. However, the fluctuated popularity of files makes the prediction a highly challenging task. To tackle this challenge, many recent works propose learning based approaches which gather the users' data centrally for training, but they bring a significant issue: users may not trust the central server and thus hesitate to upload their private data. In order to address this issue, we propose a Federated learning based Proactive Content Caching (FPCC) scheme, which does not require to gather users' data centrally for training. The FPCC is based on a hierarchical architecture in which the server aggregates the users' updates using federated averaging, and each user performs training on its local data using hybrid filtering on stacked autoencoders. The experimental results demonstrate that, without gathering user's private data, our scheme still outperforms other learning-based caching algorithms such as m-epsilon-greedy and Thompson sampling in terms of cache efficiency.;https://ieeexplore.ieee.org/abstract/document/8647616/;lRJdcxor9zAJ
Leroy, D., Coucke, A., Lavril, T., Gisselbrecht, T., & Dureau, J. (2019, May). Federated learning for keyword spotting. In ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP) (pp. 6341-6345). IEEE.;0_federated_learning_data_privacy;2019;Federated learning for keyword spotting;David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht, Joseph Dureau;ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP), 6341-6345, 2019;"We propose a practical approach based on federated learning to solve out-of-domain issues with continuously running embedded speech-based models such as wake word detectors. We conduct an extensive empirical study of the federated averaging algorithm for the ""Hey Snips"" wake word based on a crowdsourced dataset that mimics a federation of wake word users. We empirically demonstrate that using an adaptive averaging strategy inspired from Adam in place of standard weighted model averaging highly reduces the number of communication rounds required to reach our target performance. The associated upstream communication costs per user are estimated at 8 MB, which is a reasonable in the context of smart home voice assistants. Additionally, the dataset used for these experiments is being open sourced with the aim of fostering further transparent research in the application of federated learning to speech data.";https://ieeexplore.ieee.org/abstract/document/8683546/;JxWET2fsRL8J
Samarakoon, S., Bennis, M., Saad, W., & Debbah, M. (2018, December). Federated learning for ultra-reliable low-latency V2V communications. In 2018 IEEE global communications conference (GLOBECOM) (pp. 1-7). IEEE.;0_federated_learning_data_privacy;2018;Federated learning for ultra-reliable low-latency V2V communications;Sumudu Samarakoon, Mehdi Bennis, Walid Saad, Merouane Debbah;2018 IEEE global communications conference (GLOBECOM), 1-7, 2018;In this paper, a novel joint transmit power and resource allocation approach for enabling ultra-reliable low-latency communication (URLLC) in vehicular networks is proposed. The objective is to minimize the network-wide power consumption of vehicular users (VUEs) while ensuring high reliability in terms of probabilistic queuing delays. In particular, a reliability measure is defined to characterize extreme events (i.e., when vehicles' queue lengths exceed a predefined threshold with non-negligible probability) using extreme value theory (EVT). Leveraging principles from federated learning (FL), the distribution of these extreme events corresponding to the tail distribution of queues is estimated by VUEs in a decentralized manner. Finally, Lyapunov optimization is used to find the joint transmit power and resource allocation policies for each VUE in a distributed manner. The proposed solution is validated via extensive simulations using a Manhattan mobility model. It is shown that FL enables the proposed distributed method to estimate the tail distribution of queues with an accuracy that is very close to a centralized solution with up to 79% reductions in the amount of data that need to be exchanged. Furthermore, the proposed method yields up to 60% reductions of VUEs with large queue lengths, without an additional power consumption, compared to an average queue-based baseline. Compared to systems with fixed power consumption and focusing on queue stability while minimizing average power consumption, the reductions in extreme events of the proposed method is about two orders of magnitude.;https://ieeexplore.ieee.org/abstract/document/8647927/;wyx8aslq73QJ
Silva, S., Gutman, B. A., Romero, E., Thompson, P. M., Altmann, A., & Lorenzi, M. (2019, April). Federated learning in distributed medical databases: Meta-analysis of large-scale subcortical brain data. In 2019 IEEE 16th international symposium on biomedical imaging (ISBI 2019) (pp. 270-274). IEEE.;0_federated_learning_data_privacy;2019;Federated learning in distributed medical databases: Meta-analysis of large-scale subcortical brain data;Santiago Silva, Boris A Gutman, Eduardo Romero, Paul M Thompson, Andre Altmann, Marco Lorenzi;2019 IEEE 16th international symposium on biomedical imaging (ISBI 2019), 270-274, 2019;At this moment, databanks worldwide contain brain images of previously unimaginable numbers. Combined with developments in data science, these massive data provide the potential to better understand the genetic underpinnings of brain diseases. However, different datasets, which are stored at different institutions, cannot always be shared directly due to privacy and legal concerns, thus limiting the full exploitation of big data in the study of brain disorders. Here we propose a federated learning framework for securely accessing and meta-analyzing any biomedical data without sharing individual information. We illustrate our framework by investigating brain structural relationships across diseases and clinical cohorts. The framework is first tested on synthetic data and then applied to multi-centric, multi-database studies including ADNI, PPMI, MIRIAD and UK Biobank, showing the potential of the approach for further applications in distributed analysis of multi-centric cohorts.;https://ieeexplore.ieee.org/abstract/document/8759317/;2fMD5qfCU4wJ
Ye, D., Yu, R., Pan, M., & Han, Z. (2020). Federated learning in vehicular edge computing: A selective model aggregation approach. IEEE Access, 8, 23920-23935.;0_federated_learning_data_privacy;2020;Federated learning in vehicular edge computing: A selective model aggregation approach;Dongdong Ye, Rong Yu, Miao Pan, Zhu Han;IEEE Access 8, 23920-23935, 2020;Federated learning is a newly emerged distributed machine learning paradigm, where the clients are allowed to individually train local deep neural network (DNN) models with local data and then jointly aggregate a global DNN model at the central server. Vehicular edge computing (VEC) aims at exploiting the computation and communication resources at the edge of vehicular networks. Federated learning in VEC is promising to meet the ever-increasing demands of artificial intelligence (AI) applications in intelligent connected vehicles (ICV). Considering image classification as a typical AI application in VEC, the diversity of image quality and computation capability in vehicular clients potentially affects the accuracy and efficiency of federated learning. Accordingly, we propose a selective model aggregation approach, where “fine” local DNN models are selected and sent to the central server by evaluating the local image quality and computation capability. Regarding the implementation of model selection, the central server is not aware of the image quality and computation capability in the vehicular clients, whose privacy is protected under such a federated learning framework. To overcome this information asymmetry, we employ two-dimension contract theory as a distributed framework to facilitate the interactions between the central server and vehicular clients. The formulated problem is then transformed into a tractable problem through successively relaxing and simplifying the constraints, and eventually solved by a greedy algorithm. Using two datasets, i.e., MNIST and BelgiumTSC, our selective model aggregation approach is demonstrated to outperform the original federated averaging (FedAvg) approach in terms of accuracy and efficiency. Meanwhile, our approach also achieves higher utility at the central server compared with the baseline approaches.;https://ieeexplore.ieee.org/abstract/document/8964354/;VDVeTYPZ-CYJ
Yang, K., Jiang, T., Shi, Y., & Ding, Z. (2020). Federated learning via over-the-air computation. IEEE transactions on wireless communications, 19(3), 2022-2035.;0_federated_learning_data_privacy;2020;Federated learning via over-the-air computation;Kai Yang, Tao Jiang, Yuanming Shi, Zhi Ding;IEEE transactions on wireless communications 19 (3), 2022-2035, 2020;The stringent requirements for low-latency and privacy of the emerging high-stake applications with intelligent devices such as drones and smart vehicles make the cloud computing inapplicable in these scenarios. Instead, edge machine learning becomes increasingly attractive for performing training and inference directly at network edges without sending data to a centralized data center. This stimulates a nascent field termed as federated learning for training a machine learning model on computation, storage, energy and bandwidth limited mobile devices in a distributed manner. To preserve data privacy and address the issues of unbalanced and non-IID data points across different devices, the federated averaging algorithm has been proposed for global model aggregation by computing the weighted average of locally updated model at each selected device. However, the limited communication bandwidth becomes the main bottleneck for aggregating the locally computed updates. We thus propose a novel over-the-air computation based approach for fast global model aggregation via exploring the superposition property of a wireless multiple-access channel. This is achieved by joint device selection and beamforming design, which is modeled as a sparse and low-rank optimization problem to support efficient algorithms design. To achieve this goal, we provide a difference-of-convex-functions (DC) representation for the sparse and low-rank function to enhance sparsity and accurately detect the fixed-rank constraint in the procedure of device selection. A DC algorithm is further developed to solve the resulting DC program with global convergence guarantees. The algorithmic advantages and admirable performance of the proposed methodologies are demonstrated through extensive numerical results.;https://ieeexplore.ieee.org/abstract/document/8952884/;aIfv9tyCnuAJ
Savazzi, S., Nicoli, M., & Rampa, V. (2020). Federated learning with cooperating devices: A consensus approach for massive IoT networks. IEEE Internet of Things Journal, 7(5), 4641-4654.;0_federated_learning_data_privacy;2020;Federated learning with cooperating devices: A consensus approach for massive IoT networks;Stefano Savazzi, Monica Nicoli, Vittorio Rampa;IEEE Internet of Things Journal 7 (5), 4641-4654, 2020;"Federated learning (FL) is emerging as a new paradigm to train machine learning (ML) models in distributed systems. Rather than sharing and disclosing the training data set with the server, the model parameters (e.g., neural networks' weights and biases) are optimized collectively by large populations of interconnected devices, acting as local learners. FL can be applied to power-constrained Internet of Things (IoT) devices with slow and sporadic connections. In addition, it does not need data to be exported to third parties, preserving privacy. Despite these benefits, a main limit of existing approaches is the centralized optimization which relies on a server for aggregation and fusion of local parameters; this has the drawback of a single point of failure and scaling issues for increasing network size. This article proposes a fully distributed (or serverless) learning approach: the proposed FL algorithms leverage the cooperation of devices that perform data operations inside the network by iterating local computations and mutual interactions via consensus-based methods. The approach lays the groundwork for integration of FL within 5G and beyond networks characterized by decentralized connectivity and computing, with intelligence distributed over the end devices. The proposed methodology is verified by the experimental data sets collected inside an Industrial IoT (IIoT) environment.";https://ieeexplore.ieee.org/abstract/document/8950073/;NWTxc9QebzoJ
Choi, J., & Pokhrel, S. R. (2019). Federated learning with multichannel ALOHA. IEEE Wireless Communications Letters, 9(4), 499-502.;0_federated_learning_data_privacy;2019;Federated learning with multichannel ALOHA;Jinho Choi, Shiva Raj Pokhrel;IEEE Wireless Communications Letters 9 (4), 499-502, 2019;In this letter, we study federated learning in a cellular system with a base station (BS) and a large number of users with local data sets. We show that multichannel random access can provide a better performance than sequential polling when some users are unable to compute local updates (due to other tasks) or in dormant state. In addition, for better aggregation in federated learning, the access probabilities of users can be optimized for given local updates. To this end, we formulate an optimization problem and show that a distributed approach can be used within federated learning to adaptively decide the access probabilities.;https://ieeexplore.ieee.org/abstract/document/8935424/;_QpJ_eyxJboJ
Mowla, N. I., Tran, N. H., Doh, I., & Chae, K. (2019). Federated learning-based cognitive detection of jamming attack in flying ad-hoc network. IEEE Access, 8, 4338-4350.;0_federated_learning_data_privacy;2019;Federated learning-based cognitive detection of jamming attack in flying ad-hoc network;Nishat I Mowla, Nguyen H Tran, Inshil Doh, Kijoon Chae;IEEE Access 8, 4338-4350, 2019;Flying Ad-hoc Network (FANET) is a decentralized communication system solely formed by Unmanned Aerial Vehicles (UAVs). In FANET, the UAV clients are vulnerable to various malicious attacks such as the jamming attack. The aerial adversaries in the jamming attack disrupt the communication of the victim network through interference on the receiver side. Jamming attack detection in FANET poses new challenges for its key differences from other ad-hoc networks. First, because of the varying communication range and power consumption constraints, any centralized detection system becomes trivial in FANET. Second, the existing decentralized solutions, disregarding the unbalanced sensory data from new spatial environments, are unsuitable for the highly mobile and spatially heterogeneous UAVs in FANET. Third, given a huge number of UAV clients, the global model may need to choose a sub-group of UAV clients for providing a timely global update. Recently, federated learning has gained attention, as it addresses unbalanced data properties besides providing communication efficiency, thus making it a suitable choice for FANET. Therefore, we propose a federated learning-based on-device jamming attack detection security architecture for FANET. We enhance the proposed federated learning model with a client group prioritization technique leveraging the Dempster-Shafer theory. The proposed client group prioritization mechanism allows the aggregator node to identify better client groups for calculating the global update. We evaluated our mechanism with datasets from publicly available standardized jamming attack scenarios by CRAWDAD and the ns-3 simulated FANET architecture and showed that, in terms of accuracy, our proposed solution (82.01% for the CRAWDAD dataset and 89.73% for the ns-3 simulated FANET dataset) outperforms the traditional distributed solution (49.11% for the CRAWDAD dataset and 65.62% for the ns-3 simulated FANET dataset). Moreover, the Dempster-Shafer-based client group prioritization mechanism identifies the best client groups out of 56 client group combinations for efficient federated averaging.;https://ieeexplore.ieee.org/abstract/document/8945183/;Eyru5fck394J
Han, Y., Li, D., Qi, H., Ren, J., & Wang, X. (2019, May). Federated learning-based computation offloading optimization in edge computing-supported internet of things. In Proceedings of the ACM Turing Celebration Conference-China (pp. 1-5).;0_federated_learning_data_privacy;2019;Federated learning-based computation offloading optimization in edge computing-supported internet of things;Yiwen Han, Ding Li, Haotian Qi, Jianji Ren, Xiaofei Wang;Proceedings of the ACM Turing Celebration Conference-China, 1-5, 2019;Recent visualizations of smart cities, factories, healthcare system and etc. raise challenges on the capability and connectivity of massive Internet of Things (IoT) devices. Hence, edge computing is emerged to complement these capability-constrained devices with an idea offloading intensive computation tasks from them to edge nodes. By taking advantage of this feature, IoT devices are able to conserve more energy and still maintain the quality of services they shall provide. Nevertheless, computation offloading decisions concern joint and complex resource management and should be determined in real time facing dynamic workloads and radio environment. Therefore, in this work, we use multiple Deep Reinforcement Learning (DRL) agents deployed on IoT devices to instruct the decision making of themselves. On the other hand, Federated Learning is utilized to train DRL agents in a distributed fashion, aiming to make the DRL-based decision making practical and further decrease the transmission cost between IoT devices and Edge Nodes. Experimental results corroborate the effectiveness of both the DRL and Federated Learning in the dynamic IoT system.;https://dl.acm.org/doi/abs/10.1145/3321408.3321586;1d7lIAVf2v0J
Hu, B., Gao, Y., Liu, L., & Ma, H. (2018, December). Federated region-learning: An edge computing based framework for urban environment sensing. In 2018 ieee global communications conference (globecom) (pp. 1-7). IEEE.;0_federated_learning_data_privacy;2018;Federated region-learning: An edge computing based framework for urban environment sensing;Binxuan Hu, Yujia Gao, Liang Liu, Huadong Ma;2018 ieee global communications conference (globecom), 1-7, 2018;Sparse sensory data caused by insufficient monitoring sites and their incomplete records becomes the main challenge of fine-grained environment sensing. In this paper, we develop a novel inference framework, named Federated Region- Learning (FRL), for urban environment sensing. The proposed framework inherits the basic idea of federated learning, and also considers the regional characteristics during the distribution of training samples so as to improve the inference accuracy. Moreover, we exploit an edge computing architecture to implement the FRL for improving the computational efficiency. We also apply FRL to PM2.5 monitoring in Beijing. The evaluation shows that our FRL improves computational efficiency nearly 3 times than centralized training mode and increases accuracy by more than 5% compared with normal distributed training.;https://ieeexplore.ieee.org/abstract/document/8647649/;53e5Ps9qvcIJ
Nadiger, C., Kumar, A., & Abdelhak, S. (2019, June). Federated reinforcement learning for fast personalization. In 2019 IEEE Second International Conference on Artificial Intelligence and Knowledge Engineering (AIKE) (pp. 123-127). IEEE.;0_federated_learning_data_privacy;2019;Federated reinforcement learning for fast personalization;Chetan Nadiger, Anil Kumar, Sherine Abdelhak;2019 IEEE Second International Conference on Artificial Intelligence and Knowledge Engineering (AIKE), 123-127, 2019;Understanding user behavior and adapting to it has been an important focus area for applications. That adaptation is commonly called Personalization. Personalization has been sought after in gaming, personal assistants, dialogue managers, and other popular application categories. One of the challenges of personalization methods is the time they take to adapt to the user behavior or reactions. This sometimes is detrimental to user experience. The contribution of this work is twofold: (1) showing the applicability of granular (per user) personalization through the use of reinforcement learning, and (2) proposing a novel mitigation strategy to decrease the personalization time, through federated learning. To our knowledge, this paper is among the first to present an overall architecture for federated reinforcement learning (FRL), which includes the grouping policy, the learning policy, and the federation policy. We demonstrate the efficacy of the proposed architecture on a non-player character in the Atari game Pong, and scale the implementation across 3, 4, and 5 users. We demonstrate the success of the proposal through achieving a median improvement of ~17% on the personalization time.;https://ieeexplore.ieee.org/abstract/document/8791693/;_5CVQsTBt2kJ
Choi, I., Song, Q., & Sun, K. (2019, November). Federated-cloud based deep neural networks with privacy preserving image filtering techniques. In 2019 IEEE Conference on Dependable and Secure Computing (DSC) (pp. 1-8). IEEE.;0_federated_learning_data_privacy;2019;Federated-cloud based deep neural networks with privacy preserving image filtering techniques;Isabelle Choi, Qiyang Song, Kun Sun;2019 IEEE Conference on Dependable and Secure Computing (DSC), 1-8, 2019;Training Deep Neural Network (DNN) models often require significant computational resources due to the large dataset sizes and a huge number of parameters to be optimized. A cloud-based approach may be utilized to accommodate such resource needs with flexibility and efficiency. But, protecting data privacy is a challenge in such approaches. Most of the encryption-based approaches for providing privacy incurs substantial overheads. However, in many instances, only part of data information needs to be protected, and the level of privacy is often dependent on the application requirements. Various types of image filtering techniques are utilized to generate distorted DNN datasets with application-specific privacy requirements satisfied. In general, high distortion level provides strong protection on data privacy but degrades the DNN accuracy. To find the appropriate type and level of image filtering prior to the training process, we identify an image similarity metric that can be used as a DNN accuracy predictor as well as the distortion level indicator. Furthermore, to improve the DNN accuracy of highly distorted datasets, we propose a privacy-preserving federated-cloud DNN training/classification on multiple distorted datasets. Each cloud trains an independent DNN model with a different image filtering algorithm, and then the client combines and utilizes the multiple models to obtain a well-performing model. Experiments were conducted to validate the effectiveness of the proposed schemes.;https://ieeexplore.ieee.org/abstract/document/8937635/;vl0lXfI6BNQJ
Bao, X., Su, C., Xiong, Y., Huang, W., & Hu, Y. (2019, August). Flchain: A blockchain for auditable federated learning with trust and incentive. In 2019 5th International Conference on Big Data Computing and Communications (BIGCOM) (pp. 151-159). IEEE.;0_federated_learning_data_privacy;2019;Flchain: A blockchain for auditable federated learning with trust and incentive;Xianglin Bao, Cheng Su, Yan Xiong, Wenchao Huang, Yifei Hu;2019 5th International Conference on Big Data Computing and Communications (BIGCOM), 151-159, 2019;Federated learning (shorted as FL) recently proposed by Google is a privacy-preserving method to integrate distributed data trainers. FL is extremely useful due to its ensuring privacy, lower latency, less power consumption and smarter models, but it could fail if multiple trainers abort training or send malformed messages to its partners. Such misbehavior are not auditable and parameter server may compute incorrectly due to single point failure. Furthermore, FL has no incentive to attract sufficient distributed training data and computation power. In this paper, we propose FLChain to build a decentralized, public auditable and healthy FL ecosystem with trust and incentive. FLChain replace traditional FL parameter server whose computation result must be consensual on-chain. Our work is not trivial when it is vital and hard to provide enough incentive and deterrence to distributed trainers. We achieve model commercialization by providing a healthy marketplace for collaborative-training models. Honest trainer can gain fairly partitioned profit from well-trained model according to its contribution and the malicious can be timely detected and heavily punished. To reduce the time cost of misbehavior detecting and model query, we design DDCBF for accelerating the query of blockchain-documented information. Finally, we implement a prototype of our work and measure the cost of various operations.;https://ieeexplore.ieee.org/abstract/document/8905038/;B4FE4zjBUE8J
Majeed, U., & Hong, C. S. (2019, September). FLchain: Federated learning via MEC-enabled blockchain network. In 2019 20th Asia-Pacific Network Operations and Management Symposium (APNOMS) (pp. 1-4). IEEE.;0_federated_learning_data_privacy;2019;FLchain: Federated learning via MEC-enabled blockchain network;Umer Majeed, Choong Seon Hong;2019 20th Asia-Pacific Network Operations and Management Symposium (APNOMS), 1-4, 2019;In this paper, we propose blockchain network based architecture called “FLchain” for enhancing security of Federated Learning (FL). We leverage the concept of channels for learning multiple global models on FLchain. Local model parameters for each global iteration are stored as a block on the channel-specific ledger. We introduce the notion of “the global model state trie” which is stored and updated on the blockchain network based on the aggregation of local model updates collected from mobile devices. Qualitative evaluation shows that FLchain is more robust than traditional FL schemes as it ensures provenance and maintains auditable aspects of FL model in an immutable manner.;https://ieeexplore.ieee.org/abstract/document/8892848/;IndoJdrOB-sJ
Sozinov, K., Vlassov, V., & Girdzijauskas, S. (2018, December). Human activity recognition using federated learning. In 2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom) (pp. 1103-1111). IEEE.;0_federated_learning_data_privacy;2018;Human activity recognition using federated learning;Konstantin Sozinov, Vladimir Vlassov, Sarunas Girdzijauskas;2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking …, 2018;State-of-the-art deep learning models for human activity recognition use large amount of sensor data to achieve high accuracy. However, training of such models in a data center using data collected from smart devices leads to high communication costs and possible privacy infringement. In order to mitigate aforementioned issues, federated learning can be employed to train a generic classifier by combining multiple local models trained on data originating from multiple clients. In this work we evaluate federated learning to train a human activity recognition classifier and compare its performance to centralized learning by building two models, namely a deep neural network and a softmax regression trained on both synthetic and real-world datasets. We study communication costs as well as the influence of erroneous clients with corrupted data in federated learning setting. We have found that federated learning for the task of human activity recognition is capable of producing models with slightly worse, but acceptable, accuracy compared to centralized models. In our experiments federated learning achieved an accuracy of up to 89% compared to 93% in centralized training for the deep neural network. The global model trained with federated learning on skewed datasets achieves accuracy comparable to centralized learning. Furthermore, we identified an important issue of clients with corrupted data and proposed a federated learning algorithm that identifies and rejects erroneous clients. Lastly, we have identified a trade-off between communication cost and the complexity of a model. We show that more complex models such as deep neural network require more communication in federated learning settings for human activity recognition compared to less complex models, such as multinomial logistic regression.;https://ieeexplore.ieee.org/abstract/document/8672262/;pvbpEdJpeboJ
Kang, J., Xiong, Z., Niyato, D., Yu, H., Liang, Y. C., & Kim, D. I. (2019, August). Incentive design for efficient federated learning in mobile networks: A contract theory approach. In 2019 IEEE VTS Asia Pacific Wireless Communications Symposium (APWCS) (pp. 1-5). IEEE.;0_federated_learning_data_privacy;2019;Incentive design for efficient federated learning in mobile networks: A contract theory approach;Jiawen Kang, Zehui Xiong, Dusit Niyato, Han Yu, Ying-Chang Liang, Dong In Kim;2019 IEEE VTS Asia Pacific Wireless Communications Symposium (APWCS), 1-5, 2019;"The following topics are dealt with: MIMO communication; error statistics; 5G mobile communication; wireless channels; telecommunication power management; cellular radio; probability; interference suppression; Internet of Things; radio spectrum management.";https://ieeexplore.ieee.org/abstract/document/8851649/;xPkm3m6aKvAJ
Kang, J., Xiong, Z., Niyato, D., Xie, S., & Zhang, J. (2019). Incentive mechanism for reliable federated learning: A joint optimization approach to combining reputation and contract theory. IEEE Internet of Things Journal, 6(6), 10700-10714.;0_federated_learning_data_privacy;2019;Incentive mechanism for reliable federated learning: A joint optimization approach to combining reputation and contract theory;Jiawen Kang, Zehui Xiong, Dusit Niyato, Shengli Xie, Junshan Zhang;IEEE Internet of Things Journal 6 (6), 10700-10714, 2019;Federated learning is an emerging machine learning technique that enables distributed model training using local datasets from large-scale nodes, e.g., mobile devices, but shares only model updates without uploading the raw training data. This technique provides a promising privacy preservation for mobile devices while simultaneously ensuring high learning performance. The majority of existing work has focused on designing advanced learning algorithms with an aim to achieve better learning performance. However, the challenges, such as incentive mechanisms for participating in training and worker (i.e., mobile devices) selection schemes for reliable federated learning, have not been explored yet. These challenges have hindered the widespread adoption of federated learning. To address the above challenges, in this article, we first introduce reputation as the metric to measure the reliability and trustworthiness of the mobile devices. We then design a reputation-based worker selection scheme for reliable federated learning by using a multiweight subjective logic model. We also leverage the blockchain to achieve secure reputation management for workers with nonrepudiation and tamper-resistance properties in a decentralized manner. Moreover, we propose an effective incentive mechanism combining reputation with contract theory to motivate high-reputation mobile devices with high-quality data to participate in model learning. Numerical results clearly indicate that the proposed schemes are efficient for reliable federated learning in terms of significantly improving the learning accuracy.;https://ieeexplore.ieee.org/abstract/document/8832210/;PYpNWUjJSyMJ
Aïvodji, U. M., Gambs, S., & Martin, A. (2019, May). IOTFLA: A secured and privacy-preserving smart home architecture implementing federated learning. In 2019 IEEE security and privacy workshops (SPW) (pp. 175-180). IEEE.;0_federated_learning_data_privacy;2019;IOTFLA: A secured and privacy-preserving smart home architecture implementing federated learning;Ulrich Matchi Aïvodji, Sébastien Gambs, Alexandre Martin;2019 IEEE security and privacy workshops (SPW), 175-180, 2019;Slowly but steadily, the Internet of Things (IoT) is becoming more and more ubiquitous in our daily life. However, it also brings important security and privacy challenges along with it, especially in a sensitive context such as the smart home. In this position paper, we propose a novel architecture for smart home, called our, focusing on the security and privacy aspects, which combines federated learning with secure data aggregation. We hope that our proposition will provide a step forward towards achieving more security and privacy in smart homes.;https://ieeexplore.ieee.org/abstract/document/8844592/;9d9z_E53nZYJ
Feng, S., Niyato, D., Wang, P., Kim, D. I., & Liang, Y. C. (2019, July). Joint service pricing and cooperative relay communication for federated learning. In 2019 International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) (pp. 815-820). IEEE.;0_federated_learning_data_privacy;2019;Joint service pricing and cooperative relay communication for federated learning;Shaohan Feng, Dusit Niyato, Ping Wang, Dong In Kim, Ying-Chang Liang;2019 International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and …, 2019;For the sake of protecting data privacy and due to the rapid development of mobile devices, e.g., powerful central processing unit (CPU) and nascent neural processing unit (NPU), collaborative machine learning on mobile devices, e.g., federated learning, has been envisioned as a new AI approach with broad application prospects. However, the learning process of the existing federated learning platforms rely on the direct communication between the model owner, e.g., central cloud or edge server, and the mobile devices for transferring the model update. Such a direct communication may be energy inefficient or even unavailable in mobile environments. In this paper, we consider adopting the relay network to construct a cooperative communication platform for supporting model update transfer and trading. In the system, the mobile devices generate model updates based on their training data. The model updates are then forwarded to the model owner through the cooperative relay network. The model owner enjoys the learning service provided by the mobile devices. In return, the mobile devices charge the model owner certain prices. Due to the coupled interference of wireless transmission among the mobile devices that use the same relay node, the rational mobile devices have to choose their relay nodes as well as deciding on their transmission powers. Thus, we formulate a Stackelberg game model to investigate the interaction among the mobile devices and that between the mobile devices and the model owner. The Stackelberg equilibrium is investigated by capitalizing on the exterior point method. Moreover, we provide a series of insightful analytical and numerical results on the equilibrium of the Stackelberg game.;https://ieeexplore.ieee.org/abstract/document/8875430/;nHLnwOixP-wJ
Duan, S., Zhang, D., Wang, Y., Li, L., & Zhang, Y. (2019). JointRec: A deep-learning-based joint cloud video recommendation framework for mobile IoT. IEEE Internet of Things Journal, 7(3), 1655-1666.;0_federated_learning_data_privacy;2019;JointRec: A deep-learning-based joint cloud video recommendation framework for mobile IoT;Sijing Duan, Deyu Zhang, Yanbo Wang, Lingxiang Li, Yaoxue Zhang;IEEE Internet of Things Journal 7 (3), 1655-1666, 2019;In the era of Internet of Things (IoT), watching videos on mobile devices has been a popular application in our daily life. How to recommend videos to users is one of the most concerned problem for Internet video service providers (IVSPs). In order to provide better recommendation service to users, they deploy cloud servers in a geo-distributed manner. Each server is responsible for analyzing a local area of user data. Therefore, these cloud servers form information islands and the characteristics of data present nonindependent and identically distribution (non-i.i.d). In this scenario, it is difficult to provide accurate video recommendation service to the minority of users in each area. To tackle this issue, we propose JointRec, a deep learning-based joint cloud video recommendation framework. JointRec integrates the JointCloud architecture into mobile IoT and achieves federated training among distributed cloud servers. Specifically, we first design a dual-convolutional probabilistic matrix factorization (Dual-CPMF) model to conduct video recommendation. Based on this model, each cloud can recommend videos by exploiting the user's profiles and description of videos that users rate, thereby providing more accurate video recommendation services. Then, we present a federated recommendation algorithm which enables each cloud to share their weights and train a model cooperatively. Furthermore, considering the heavy communication costs in the process of federated training, we combine low-rank matrix factorization and 8-bit quantization method to reduce uplink communication costs and network bandwidth. We validate the proposed approach on the real-world data set, and the experimental results indicate the effectiveness of our proposed approach.;https://ieeexplore.ieee.org/abstract/document/8854245/;tWqVmL9aPMMJ
Ji, S., Pan, S., Long, G., Li, X., Jiang, J., & Huang, Z. (2019, July). Learning private neural language modeling with attentive aggregation. In 2019 International joint conference on neural networks (IJCNN) (pp. 1-8). IEEE.;0_federated_learning_data_privacy;2019;Learning private neural language modeling with attentive aggregation;Shaoxiong Ji, Shirui Pan, Guodong Long, Xue Li, Jing Jiang, Zi Huang;2019 International joint conference on neural networks (IJCNN), 1-8, 2019;Mobile keyboard suggestion is typically regarded as a word-level language modeling problem. Centralized machine learning techniques require the collection of massive user data for training purposes, which may raise privacy concerns in relation to users' sensitive data. Federated learning (FL) provides a promising approach to learning private language modeling for intelligent personalized keyboard suggestions by training models on distributed clients rather than training them on a central server. To obtain a global model for prediction, existing FL algorithms simply average the client models and ignore the importance of each client during model aggregation. Furthermore, there is no optimization for learning a well-generalized global model on the central server. To solve these problems, we propose a novel model aggregation with an attention mechanism considering the contribution of client models to the global model, together with an optimization technique during server aggregation. Our proposed attentive aggregation method minimizes the weighted distance between the server model and client models by iteratively updating parameters while attending to the distance between the server model and client models. Experiments on two popular language modeling datasets and a social media dataset show that our proposed method outperforms its counterparts in terms of perplexity and communication cost in most settings of comparison.;https://ieeexplore.ieee.org/abstract/document/8852464/;hniH8B2jfboJ
Liu, B., Wang, L., & Liu, M. (2019). Lifelong federated reinforcement learning: a learning architecture for navigation in cloud robotic systems. IEEE Robotics and Automation Letters, 4(4), 4555-4562.;0_federated_learning_data_privacy;2019;Lifelong federated reinforcement learning: a learning architecture for navigation in cloud robotic systems;Boyi Liu, Lujia Wang, Ming Liu;IEEE Robotics and Automation Letters 4 (4), 4555-4562, 2019;This letter was motivated by the problem of how to make robots fuse and transfer their experience so that they can effectively use prior knowledge and quickly adapt to new environments. To address the problem, we present a learning architecture for navigation in cloud robotic systems: Lifelong Federated Reinforcement Learning (LFRL). In the letter, we propose a knowledge fusion algorithm for upgrading a shared model deployed on the cloud. Then, effective transfer learning methods in LFRL are introduced. LFRL is consistent with human cognitive science and fits well in cloud robotic systems. Experiments show that LFRL greatly improves the efficiency of reinforcement learning for robot navigation. The cloud robotic system deployment also shows that LFRL is capable of fusing prior knowledge. In addition, we release a cloud robotic navigation-learning website to provide the service based on LFRL: www.shared-robotics.com.;https://ieeexplore.ieee.org/abstract/document/8772088/;2knU0J_kIJ0J
Zou, Y., Feng, S., Niyato, D., Jiao, Y., Gong, S., & Cheng, W. (2019, July). Mobile device training strategies in federated learning: An evolutionary game approach. In 2019 International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) (pp. 874-879). IEEE.;0_federated_learning_data_privacy;2019;Mobile device training strategies in federated learning: An evolutionary game approach;Yuze Zou, Shaohan Feng, Dusit Niyato, Yutao Jiao, Shimin Gong, Wenqing Cheng;2019 International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and …, 2019;With the tremendous success of machine learning and increasingly powerful mobile devices, federated learning has gained growing attention from both academia and industry. It capitalizes on vast number of distributed data to support machine learning based applications while maintaining data privacy. In this paper, we consider a federated learning system, in which the mobile devices allocate their data and computation resources among the machine learning applications, i.e., model owners. Specifically, we formulate an evolutionary game mode for the mobile devices with bounded rationality to adapt their training strategies aiming to maximize the device's individual utility. The uniqueness and stability of the equilibrium of the game are analysed theoretically. Besides, Extensive experiments are conducted to determine the functions fitted for the accuracy and energy consumption metrics.;https://ieeexplore.ieee.org/abstract/document/8875353/;r_XIqpUxTj4J
Sarikaya, Y., & Ercetin, O. (2019). Motivating workers in federated learning: A stackelberg game perspective. IEEE Networking Letters, 2(1), 23-27.;0_federated_learning_data_privacy;2019;Motivating workers in federated learning: A stackelberg game perspective;Yunus Sarikaya, Ozgur Ercetin;IEEE Networking Letters 2 (1), 23-27, 2019;Due to the large size of the training data, distributed learning approaches such as federated learning have gained attention recently. However, the convergence rate of distributed learning suffers from heterogeneous worker performance. In this letter, we consider an incentive mechanism for workers to mitigate the delays in completion of each batch. We analytically obtained equilibrium solution of a Stackelberg game. Our numerical results indicate that with a limited budget, the model owner should judiciously decide on the number of workers due to trade off between the diversity provided by the number of workers and the latency of completing the training.;https://ieeexplore.ieee.org/abstract/document/8867906/;0YQI19PbXxYJ
Zhu, H., & Jin, Y. (2019). Multi-objective evolutionary federated learning. IEEE transactions on neural networks and learning systems, 31(4), 1310-1322.;0_federated_learning_data_privacy;2019;Multi-objective evolutionary federated learning;Hangyu Zhu, Yaochu Jin;IEEE transactions on neural networks and learning systems 31 (4), 1310-1322, 2019;Federated learning is an emerging technique used to prevent the leakage of private information. Unlike centralized learning that needs to collect data from users and store them collectively on a cloud server, federated learning makes it possible to learn a global model while the data are distributed on the users' devices. However, compared with the traditional centralized approach, the federated setting consumes considerable communication resources of the clients, which is indispensable for updating global models and prevents this technique from being widely used. In this paper, we aim to optimize the structure of the neural network models in federated learning using a multi-objective evolutionary algorithm to simultaneously minimize the communication costs and the global model test errors. A scalable method for encoding network connectivity is adapted to federated learning to enhance the efficiency in evolving deep neural networks. Experimental results on both multilayer perceptrons and convolutional neural networks indicate that the proposed optimization method is able to find optimized neural network models that can not only significantly reduce communication costs but also improve the learning performance of federated learning compared with the standard fully connected neural networks.;https://ieeexplore.ieee.org/abstract/document/8744465/;KvgVDnBLUbYJ
Hua, S., Yang, K., & Shi, Y. (2019, September). On-device federated learning via second-order optimization with over-the-air computation. In 2019 IEEE 90th Vehicular Technology Conference (VTC2019-Fall) (pp. 1-5). IEEE.;0_federated_learning_data_privacy;2019;On-device federated learning via second-order optimization with over-the-air computation;Sheng Hua, Kai Yang, Yuanming Shi;2019 IEEE 90th Vehicular Technology Conference (VTC2019-Fall), 1-5, 2019;Federated learning becomes a promising approach for preserving privacy by keeping user data locally. The basic idea is that a central server iteratively aggregates distributed local models trained directly on mobile usersâ€™ local datasets to form a high-quality global model by computing the weighted sum of the locally updated models. However, the communication cost becomes the main bottleneck as a large number of communication rounds are involved in the federated learning procedure. We propose to update local models by second- order optimization methods with fast convergence rates, thereby significantly reducing the communication rounds for global model updates. Furthermore, the over-the-air computation technique is adopted to improve communication efficiency for model aggregation by utilizing the superposition property of wireless channels. A nonconvex low-rank beamforming approach is then developed to support over- the-air computation via difference-of-convexfunctions (DC) programming. Through extensive experiments, we reveal that the proposed DC algorithm is able to significantly minimize the aggregation error, and the second- order methods are quite robust to the model aggregation errors.;https://ieeexplore.ieee.org/abstract/document/8891310/;4IrtXgCL_X8J
Wang, T., Cao, Z., Wang, S., Wang, J., Qi, L., Liu, A., ... & Li, X. (2019). Privacy-enhanced data collection based on deep learning for internet of vehicles. IEEE Transactions on Industrial Informatics, 16(10), 6663-6672.;0_federated_learning_data_privacy;2019;Privacy-enhanced data collection based on deep learning for internet of vehicles;Tian Wang, Zhihan Cao, Shuo Wang, Jianhuang Wang, Lianyong Qi, Anfeng Liu, Mande Xie, Xiaolong Li;IEEE Transactions on Industrial Informatics 16 (10), 6663-6672, 2019;The development of smart cities and deep learning technology is changing our physical world to a cyber world. As one of the main applications, the Internet of Vehicles has been developing rapidly. However, privacy leakage and delay problem for data collection remain as the key concerns behind the fast development of the cyber intelligence technologies. If the original data collected are directly uploaded to the cloud for processing, it will bring huge load pressure and delay to the network communication. Moreover, during this process, it will lead to the leakage of data privacy. To this end, in this article we design a data collection and preprocessing scheme based on deep learning, which adopts the semisupervised learning algorithm of data augmentation and label guessing. Data filtering is performed at the edge layer, and a large amount of similar data and irrelevant data are cleared. If the edge device cannot process some complex data independently, it will send the processed and reliable data to the cloud for further processing, which maximizes the protection of user privacy. Our method significantly reduces the amount of data uploaded to the cloud, and meanwhile protects the user's data privacy effectively.;https://ieeexplore.ieee.org/abstract/document/8945334/?casa_token=fr_J15xLoNkAAAAA:cXWGzozChTbh1-jBsyaMultggbEYHz6CHTPvH_vR7Cxw63Naw2yK_jOnBuJBZwHsqROw1zzTEwg;jQNIl3YdKxsJ
Zhou, W., Li, Y., Chen, S., & Ding, B. (2018, October). Real-time data processing architecture for multi-robots based on differential federated learning. In 2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI) (pp. 462-471). IEEE.;0_federated_learning_data_privacy;2018;Real-time data processing architecture for multi-robots based on differential federated learning;Wei Zhou, Yiying Li, Shuhui Chen, Bo Ding;2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and …, 2018;The emergency of ubiquitous intelligence in various things has become the ultimate cornerstone in building a smart interconnection of the physical world and the human world, which also caters to the idea of Internet of Things (IoT). Nowadays, robots as a new type of ubiquitous IoT devices have gained much attention. With the increasing number of distributed multi-robots, such smart environment generates unprecedented amounts of data. Robotic applications are faced with challenges of such big data: the serious real-time assurance and data privacy. Therefore, in order to obtain the big data values via knowledge sharing under the premise of ensuring the real-time data processing and data privacy, we propose a real-time data processing architecture for multi-robots based on the differential federated learning, called RT-robots architecture. A global shared model with differential privacy protection is trained on the cloud iteratively and distributed to multiple edge robots in each round, and the robotic tasks are processed locally in real time. Our implementation and experiments demonstrate that our architecture can be applied on multiple robotic recognition tasks, balance the trade-off between the performance and privacy.;https://ieeexplore.ieee.org/abstract/document/8560084/;dkxXVfg1q0QJ
Martinez, I., Francis, S., & Hafid, A. S. (2019, October). Record and reward federated learning contributions with blockchain. In 2019 International conference on cyber-enabled distributed computing and knowledge discovery (CyberC) (pp. 50-57). IEEE.;0_federated_learning_data_privacy;2019;Record and reward federated learning contributions with blockchain;Ismael Martinez, Sreya Francis, Abdelhakim Senhaji Hafid;2019 International conference on cyber-enabled distributed computing and knowledge discovery (CyberC), 50-57, 2019;Although Federated Learning allows for participants to contribute their local data without it being revealed, it faces issues in data security and in accurately paying participants for quality data contributions. In this paper, we propose an EOS Blockchain design and workflow to establish data security, a novel validation error based metric upon which we qualify gradient uploads for payment, and implement a small example of our blockchain Federated Learning model to analyze its performance.;https://ieeexplore.ieee.org/abstract/document/8945913/;eQG6yJ_PH8QJ
Sattler, F., Wiedemann, S., Müller, K. R., & Samek, W. (2019). Robust and communication-efficient federated learning from non-iid data. IEEE transactions on neural networks and learning systems, 31(9), 3400-3413.;0_federated_learning_data_privacy;2019;Robust and communication-efficient federated learning from non-iid data;Felix Sattler, Simon Wiedemann, Klaus-Robert MÃ¼ller, Wojciech Samek;IEEE transactions on neural networks and learning systems 31 (9), 3400-3413, 2019;Federated learning allows multiple parties to jointly train a deep learning model on their combined data, without any of the participants having to reveal their local data to a centralized server. This form of privacy-preserving collaborative learning, however, comes at the cost of a significant communication overhead during training. To address this problem, several compression methods have been proposed in the distributed training literature that can reduce the amount of required communication by up to three orders of magnitude. These existing methods, however, are only of limited utility in the federated learning setting, as they either only compress the upstream communication from the clients to the server (leaving the downstream communication uncompressed) or only perform well under idealized conditions, such as i.i.d. distribution of the client data, which typically cannot be found in federated learning. In this article, we propose sparse ternary compression (STC), a new compression framework that is specifically designed to meet the requirements of the federated learning environment. STC extends the existing compression technique of top-k gradient sparsification with a novel mechanism to enable downstream compression as well as ternarization and optimal Golomb encoding of the weight updates. Our experiments on four different learning tasks demonstrate that STC distinctively outperforms federated averaging in common federated learning scenarios. These results advocate for a paradigm shift in federated optimization toward high-frequency low-bitwidth communication, in particular in the bandwidth-constrained learning environments.;https://ieeexplore.ieee.org/abstract/document/8889996/;9X60sow2a30J
Yang, H. H., Liu, Z., Quek, T. Q., & Poor, H. V. (2019). Scheduling policies for federated learning in wireless networks. IEEE transactions on communications, 68(1), 317-333.;0_federated_learning_data_privacy;2019;Scheduling policies for federated learning in wireless networks;Howard H Yang, Zuozhu Liu, Tony QS Quek, H Vincent Poor;IEEE transactions on communications 68 (1), 317-333, 2019;Motivated by the increasing computational capacity of wireless user equipments (UEs), e.g., smart phones, tablets, or vehicles, as well as the increasing concerns about sharing private data, a new machine learning model has emerged, namely federated learning (FL), that allows a decoupling of data acquisition and computation at the central unit. Unlike centralized learning taking place in a data center, FL usually operates in a wireless edge network where the communication medium is resource-constrained and unreliable. Due to limited bandwidth, only a portion of UEs can be scheduled for updates at each iteration. Due to the shared nature of the wireless medium, transmissions are subjected to interference and are not guaranteed. The performance of FL system in such a setting is not well understood. In this paper, an analytical model is developed to characterize the performance of FL in wireless networks. Particularly, tractable expressions are derived for the convergence rate of FL in a wireless setting, accounting for effects from both scheduling schemes and inter-cell interference. Using the developed analysis, the effectiveness of three different scheduling policies, i.e., random scheduling (RS), round robin (RR), and proportional fair (PF), are compared in terms of FL convergence rate. It is shown that running FL with PF outperforms RS and RR if the network is operating under a high signal-to-interference-plus-noise ratio (SINR) threshold, while RR is more preferable when the SINR threshold is low. Moreover, the FL convergence rate decreases rapidly as the SINR threshold increases, thus confirming the importance of compression and quantization of the update parameters. The analysis also reveals a trade-off between the number of scheduled UEs and subchannel bandwidth under a fixed amount of available spectrum.;https://ieeexplore.ieee.org/abstract/document/8851249/?casa_token=Px00wep2LRMAAAAA:L-hFP2x4thrRMc-prnnwpAhEjNBlJlqZYCAk3k5YSmcfMwfMaGiFUpscY3dZnHXCkIh3JRtNqwI;j3HRonM8UlcJ
Lugan, S., Desbordes, P., Brion, E., Tormo, L. X. R., Legay, A., & Macq, B. (2019). Secure architectures implementing trusted coalitions for blockchained distributed learning (TCLearn). Ieee Access, 7, 181789-181799.;0_federated_learning_data_privacy;2019;Secure architectures implementing trusted coalitions for blockchained distributed learning (TCLearn;Sébastien Lugan, Paul Desbordes, Eliott Brion, Luis Xavier Ramos Tormo, Axel Legay, Benoît Macq;Ieee Access 7, 181789-181799, 2019;Distributed learning across coalitions is becoming popular for multi-centric implementation of deep learning models. However, the level of trust between the members of a coalition can vary and requires different security architectures. Privacy of the training data has been largely described in distributed learning. In this paper, we present a scalable security architecture providing additional features such as validation on the sources quality, confidentiality on the model within a trusted coalition or confidentiality among untrusted partners inside the coalition. More specifically, we propose solutions that guarantee preservation not only of data privacy but also of data quality, enforce a trustworthy sequence of iterative learning, and that lead to equitable sharing of the learned model among the coalition's members. We give an example of its deployment in the case of the distributed optimization of a deep learning convolutional neural network trained on medical images.;https://ieeexplore.ieee.org/abstract/document/8932389/;YqaYsstAJR4J
Hao, M., Li, H., Xu, G., Liu, S., & Yang, H. (2019, May). Towards efficient and privacy-preserving federated deep learning. In ICC 2019-2019 IEEE international conference on communications (ICC) (pp. 1-6). IEEE.;0_federated_learning_data_privacy;2019;Towards efficient and privacy-preserving federated deep learning;Meng Hao, Hongwei Li, Guowen Xu, Sen Liu, Haomiao Yang;ICC 2019-2019 IEEE international conference on communications (ICC), 1-6, 2019;Deep learning has been applied in many areas, such as computer vision, natural language processing and emotion analysis. Differing from the traditional deep learning that collects users' data centrally, federated deep learning requires participants to train the networks on private datasets and share the training results, and hence has more gratifying efficiency and stronger security. However, it still presents some privacy issues since adversaries can deduce users' privacy from local outputs, such as gradients. While the problem of private federated deep learning has been an active research issue, the latest research findings are still inadequate in terms of security, accuracy and efficiency. In this paper, we propose an efficient and privacy-preserving federated deep learning protocol based on stochastic gradient descent method by integrating the additively homomorphic encryption with differential privacy. Specifically, users add noises to each local gradients before encrypting them to obtain the optical performance and security. Moreover, our scheme is secure to honest-but-curious server setting even if the cloud server colludes with multiple users. Besides, our scheme supports federated learning for large-scale users scenarios and extensive experiments demonstrate our scheme has high efficiency and high accuracy compared with non-private model.;https://ieeexplore.ieee.org/abstract/document/8761267/?casa_token=VJxDBh15gj8AAAAA:g7vLep8XFaK-Xf-2ShCJj3x2X-lMMZPbEGYOSNkg9ObNt1SeKSfDx9aav_ySBRuKPj435bvtAXo;8AfDzSoIORIJ
Yao, X., Huang, T., Wu, C., Zhang, R., & Sun, L. (2019, September). Towards faster and better federated learning: A feature fusion approach. In 2019 IEEE International Conference on Image Processing (ICIP) (pp. 175-179). IEEE.;0_federated_learning_data_privacy;2019;Towards faster and better federated learning: A feature fusion approach;Xin Yao, Tianchi Huang, Chenglei Wu, Ruixiao Zhang, Lifeng Sun;2019 IEEE International Conference on Image Processing (ICIP), 175-179, 2019;Federated learning enables on-device training over distributed networks consisting of a massive amount of modern smart devices, such as smartphones and IoT devices. However, the leading optimization algorithm in such settings, i.e., federated averaging, suffers from heavy communication cost and inevitable performance drop, especially when the local data is distributed in a Non-IID way. In this paper, we propose a feature fusion method to address this problem. By aggregating the features from both the local and global models, we achieve a higher accuracy at less communication cost. Furthermore, the feature fusion modules offer better initialization for newly incoming clients and thus speed up the process of convergence. Experiments in popular federated learning scenarios show that our federated learning algorithm with feature fusion mechanism outperforms baselines in both accuracy and generalization ability while reducing the number of communication rounds by more than 60%.;https://ieeexplore.ieee.org/abstract/document/8803001/?casa_token=DAKXGsHOkssAAAAA:qucImdUUTkg4dWtT4YbLQl9B2OFsWpkBogrW6-ixGKNgRs4SFPhgd_APdQuKqon5Q_VhqU-pb5g;AQ22zN2R6XQJ
Doku, R., Rawat, D. B., & Liu, C. (2019, July). Towards federated learning approach to determine data relevance in big data. In 2019 IEEE 20th international conference on information reuse and integration for data science (IRI) (pp. 184-192). IEEE.;0_federated_learning_data_privacy;2019;Towards federated learning approach to determine data relevance in big data;Ronald Doku, Danda B Rawat, Chunmei Liu;2019 IEEE 20th international conference on information reuse and integration for data science (IRI), 184-192, 2019;"In the past few years, data has proliferated to astronomical proportions; as a result, big data has become the driving force behind the growth of many machine learning innovations. However, the incessant generation of data in the information age poses a needle in the haystack problem, where it has become challenging to determine useful data from a heap of irrelevant ones. This has resulted in a quality over quantity issue in data science where a lot of data is being generated, but the majority of it is irrelevant. Furthermore, most of the data and the resources needed to effectively train machine learning models are owned by major tech companies, resulting in a centralization problem. As such, federated learning seeks to transform how machine learning models are trained by adopting a distributed machine learning approach. Another promising technology is the blockchain, whose immutable nature ensures data integrity. By combining the blockchain's trust mechanism and federated learning's ability to disrupt data centralization, we propose an approach that determines relevant data and stores the data in a decentralized manner.";https://ieeexplore.ieee.org/abstract/document/8843451/;Lcs0z0H_LDYJ
Yao, X., Huang, C., & Sun, L. (2018, December). Two-stream federated learning: Reduce the communication costs. In 2018 IEEE Visual Communications and Image Processing (VCIP) (pp. 1-4). IEEE.;0_federated_learning_data_privacy;2018;Two-stream federated learning: Reduce the communication costs;Xin Yao, Chaofeng Huang, Lifeng Sun;2018 IEEE Visual Communications and Image Processing (VCIP), 1-4, 2018;Federated learning algorithm solves the problem of training machine learning models over distributed networks that consist of a massive amount of modern smart devices. It overcomes the challenge of privacy preservation, unbalanced and Non-IID data distributions, and does its best to reduce the required communication rounds. However, communication costs are still the principle constraint compared to other factors, such as computation costs. In this paper, we adopt a two-stream model with MMD (Maximum Mean Discrepancy) constraint instead of the single model to be trained on devices in standard federated learning settings. Following experiments show that the proposed model outperforms baseline methods, especially in Non-IID data distributions, and achieves a reduction of more than 20% in required communication rounds.;https://ieeexplore.ieee.org/abstract/document/8698609/;kDTpEm7cKFEJ
Xu, G., Li, H., Liu, S., Yang, K., & Lin, X. (2019). Verifynet: Secure and verifiable federated learning. IEEE Transactions on Information Forensics and Security, 15, 911-926.;0_federated_learning_data_privacy;2019;Verifynet: Secure and verifiable federated learning;Guowen Xu, Hongwei Li, Sen Liu, Kan Yang, Xiaodong Lin;IEEE Transactions on Information Forensics and Security 15, 911-926, 2019;As an emerging training model with neural networks, federated learning has received widespread attention due to its ability to update parameters without collecting users' raw data. However, since adversaries can track and derive participants' privacy from the shared gradients, federated learning is still exposed to various security and privacy threats. In this paper, we consider two major issues in the training process over deep neural networks (DNNs): 1) how to protect user's privacy (i.e., local gradients) in the training process and 2) how to verify the integrity (or correctness) of the aggregated results returned from the server. To solve the above problems, several approaches focusing on secure or privacy-preserving federated learning have been proposed and applied in diverse scenarios. However, it is still an open problem enabling clients to verify whether the cloud server is operating correctly, while guaranteeing user's privacy in the training process. In this paper, we propose VerifyNet, the first privacy-preserving and verifiable federated learning framework. In specific, we first propose a double-masking protocol to guarantee the confidentiality of users' local gradients during the federated learning. Then, the cloud server is required to provide the Proof about the correctness of its aggregated results to each user. We claim that it is impossible that an adversary can deceive users by forging Proof, unless it can solve the NP-hard problem adopted in our model. In addition, VerifyNet is also supportive of users dropping out during the training process. The extensive experiments conducted on real-world data also demonstrate the practical performance of our proposed scheme.;https://ieeexplore.ieee.org/abstract/document/8765347/?casa_token=jXYGyiSnDG0AAAAA:gtiwx7vPcE3wEBjCLD59PAt7zckL2yFY3rqiJYn-jyl0UypgCEXM18yEi6w4MvJrbTosPiZD-Uk;gsa-k24p0uwJ
Ahn, J. H., Simeone, O., & Kang, J. (2019, September). Wireless federated distillation for distributed edge learning with heterogeneous data. In 2019 IEEE 30th Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC) (pp. 1-6). IEEE.;0_federated_learning_data_privacy;2019;Wireless federated distillation for distributed edge learning with heterogeneous data;Jin-Hyun Ahn, Osvaldo Simeone, Joonhyuk Kang;2019 IEEE 30th Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC), 1-6, 2019;Cooperative training methods for distributed machine learning typically assume noiseless and ideal communication channels. This work studies some of the opportunities and challenges arising from the presence of wireless communication links. We specifically consider wireless implementations of Federated Learning (FL) and Federated Distillation (FD), as well as of a novel Hybrid Federated Distillation (HFD) scheme. Both digital implementations based on separate source-channel coding and over-the-air computing implementations based on joint source-channel coding are proposed and evaluated over Gaussian multiple-access channels.;https://ieeexplore.ieee.org/abstract/document/8904164/?casa_token=24CkliI0troAAAAA:kQgdYPiAPq4iyJIMvsZ74FDfd7ul_n43p5DA_Ywpet8aoi7B0ksJ4qJG2PHVVbtye600-xBjCsU;oCLk69zJnMgJ
Truex, S., Baracaldo, N., Anwar, A., Steinke, T., Ludwig, H., Zhang, R., & Zhou, Y. (2019, November). A hybrid approach to privacy-preserving federated learning. In Proceedings of the 12th ACM workshop on artificial intelligence and security (pp. 1-11).;0_federated_learning_data_privacy;2019;A hybrid approach to privacy-preserving federated learning;Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui Zhang, Yi Zhou;Proceedings of the 12th ACM workshop on artificial intelligence and security, 1-11, 2019;Federated learning facilitates the collaborative training of models without the sharing of raw data. However, recent attacks demonstrate that simply maintaining data locality during training processes does not provide sufficient privacy guarantees. Rather, we need a federated learning system capable of preventing inference over both the messages exchanged during training and the final trained model while ensuring the resulting model also has acceptable predictive accuracy. Existing federated learning approaches either use secure multiparty computation (SMC) which is vulnerable to inference or differential privacy which can lead to low accuracy given a large number of parties with relatively small amounts of data each. In this paper, we present an alternative approach that utilizes both differential privacy and SMC to balance these trade-offs. Combining differential privacy with secure multiparty computation enables us to reduce the growth of noise injection as the number of parties increases without sacrificing privacy while maintaining a pre-defined rate of trust. Our system is therefore a scalable approach that protects against inference threats and produces models with high accuracy. Additionally, our system can be used to train a variety of machine learning models, which we validate with experimental results on 3 different machine learning algorithms. Our experiments demonstrate that our approach out-performs state of the art solutions.;https://dl.acm.org/doi/abs/10.1145/3338501.3357370?casa_token=fA17OHGGoOEAAAAA:ZdBmPkC1_7iLqVcdwNxDCuwy3jk5dEbbSIxYbxmsCgf7qwk_KgbAT_A1L222I9Q88bABtHH8y6Q9WQ;w3YnhimEp2oJ
Jalalirad, A., Scavuzzo, M., Capota, C., & Sprague, M. (2019, December). A simple and efficient federated recommender system. In Proceedings of the 6th IEEE/ACM international conference on big data computing, applications and technologies (pp. 53-58).;0_federated_learning_data_privacy;2019;A simple and efficient federated recommender system;Amir Jalalirad, Marco Scavuzzo, Catalin Capota, Michael Sprague;Proceedings of the 6th IEEE/ACM international conference on big data computing, applications and technologies, 53-58, 2019;Federated Learning (FL) is recently explored as a machine learning paradigm to communally gain generalizable knowledge from the data available in a collection of edge devices without the requirement to transfer the data. FL gives rise to the opportunity to train models on edge devices while preserving user's privacy as the data never leaves user's premises. In this paper, we introduce a simple yet efficient extension of FL for recommender systems to improve on personalization and discuss closely-related meta-learning algorithms. Compared to state-of-the-art federated recommenders, our proposed algorithm is simpler and more robust in real-life scenarios. Through experiments on benchmark data, we evaluate our algorithm in root mean squared error (RMSE) of user's rating prediction.;https://dl.acm.org/doi/abs/10.1145/3365109.3368788?casa_token=KAS1Nots11QAAAAA:5SK24oaBZQCMEiQMgBQ_RX86K6GbPlUEQoy5KPrS07JbVAz4JPSn0WDVdCjE9tF6zg06JnLyxlHMfQ;llEyLTA7h9AJ
Daga, H., Nicholson, P. K., Gavrilovska, A., & Lugones, D. (2019, November). Cartel: A system for collaborative transfer learning at the edge. In Proceedings of the ACM Symposium on Cloud Computing (pp. 25-37).;0_federated_learning_data_privacy;2019;Cartel: A system for collaborative transfer learning at the edge;Harshit Daga, Patrick K Nicholson, Ada Gavrilovska, Diego Lugones;Proceedings of the ACM Symposium on Cloud Computing, 25-37, 2019;As Multi-access Edge Computing (MEC) and 5G technologies evolve, new applications are emerging with unprecedented capacity and real-time requirements. At the core of such applications there is a need for machine learning (ML) to create value from the data at the edge. Current ML systems transfer data from geo-distributed streams to a central datacenter for modeling. The model is then moved to the edge and used for inference or classification. These systems can be ineffective because they introduce significant demand for data movement and model transfer in the critical path of learning. Furthermore, a full model may not be needed at each edge location. An alternative is to train and update the models online at each edge with local data, in isolation from other edges. Still, this approach can worsen the accuracy of models due to reduced data availability, especially in the presence of local data shifts.In this paper we propose Cartel, a system for collaborative learning in edge clouds, that creates a model-sharing environment in which tailored models at each edge can quickly adapt to changes, and can be as robust and accurate as centralized models. Results show that Cartel adapts to workload changes 4 to 8x faster than isolated learning, and reduces model size, training time and total data transfer by 3x, 5.7x and ~1500x, respectively, when compared to centralized learning.;https://dl.acm.org/doi/abs/10.1145/3357223.3362708?casa_token=mPPPOfalaBwAAAAA:2POVQPSuHaIgSw2dWZgYK5A2BcuwigrKyccqCHVB0qgNbmCGpUzSBCDsqfgG0IyKXJzUSqj9ctrjBw;lnRLR2bTLkYJ
Shen, S., Han, Y., Wang, X., & Wang, Y. (2019). Computation offloading with multiple agents in edge-computing–supported IoT. ACM Transactions on Sensor Networks (TOSN), 16(1), 1-27.;0_federated_learning_data_privacy;2019;Computation offloading with multiple agents in edge-computing–supported IoT;Shihao Shen, Yiwen Han, Xiaofei Wang, Yan Wang;ACM Transactions on Sensor Networks (TOSN) 16 (1), 1-27, 2019;With the development of the Internet of Things (IoT) and the birth of various new IoT devices, the capacity of massive IoT devices is facing challenges. Fortunately, edge computing can optimize problems such as delay and connectivity by offloading part of the computational tasks to edge nodes close to the data source. Using this feature, IoT devices can save more resources while still maintaining the quality of service. However, since computation offloading decisions concern joint and complex resource management, we use multiple Deep Reinforcement Learning (DRL) agents deployed on IoT devices to guide their own decisions. Besides, Federated Learning (FL) is utilized to train DRL agents in a distributed fashion, aiming to make the DRL-based decision making practical and further decrease the transmission cost between IoT devices and Edge Nodes. In this article, we first study the problem of computation offloading optimization and prove the problem is an NP-hard problem. Then, based on DRL and FL, we propose an offloading algorithm that is different from the traditional method. Finally, we studied the effects of various parameters on the performance of the algorithm and verified the effectiveness of both the DRL and FL in the IoT system.;https://dl.acm.org/doi/abs/10.1145/3372025?casa_token=xa1EfHdSwqkAAAAA:zc6OiX_6Z8GSmFyF7KEGmhFNhBrdCxc6l1HibMd1_BmobyQLeImrv0s4g_biGm5I3sPBHKFnzSYd3w;Ahova2H30QYJ
Fan, Z., Song, X., Jiang, R., Chen, Q., & Shibasaki, R. (2019). Decentralized attention-based personalized human mobility prediction. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 3(4), 1-26.;0_federated_learning_data_privacy;2019;Decentralized attention-based personalized human mobility prediction;Zipei Fan, Xuan Song, Renhe Jiang, Quanjun Chen, Ryosuke Shibasaki;Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 3 (4), 1-26, 2019;Human mobility prediction is essential to a variety of human-centered computing applications achieved through upgrading of location-based services (LBS) to future-location-based services (FLBS). Previous studies on human mobility prediction have mainly focused on centralized human mobility prediction, where user mobility data are collected, trained and predicted at the cloud server side. However, such a centralized approach leads to a high risk of privacy issues, and a real-time centralized system for processing such a large volume of distributed data is extremely difficult to apply. Moreover, a large and dynamic set of users makes the predictive model extremely challenging to personalize. In this paper, we propose a novel decentralized attention-based human mobility predictor in which 1) no additional training procedure is required for personalized prediction, 2) no additional training procedure is required for incremental learning, and 3) the predictor can be trained and predicted in a decentralized way. We tested our method on big data of real-world mobile phone user GPS and on Android devices, and achieved a low-power consumption and a good prediction accuracy without collecting user data in the server or applying additional training on the user side.;https://dl.acm.org/doi/abs/10.1145/3369830?casa_token=9f2-CFVkXpkAAAAA:cRcM_gatbT3ExldHJxROIugx8BxzRJlLeUUWtr_9gtWynBVTjBzwGzbmHuj1Nv3XUukhqGIU8p41fw;4iQu5S-9rkcJ
Xu, M., Qian, F., Mei, Q., Huang, K., & Liu, X. (2018). Deeptype: On-device deep learning for input personalization service with minimal privacy concern. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2(4), 1-26.;0_federated_learning_data_privacy;2018;Deeptype: On-device deep learning for input personalization service with minimal privacy concern;Mengwei Xu, Feng Qian, Qiaozhu Mei, Kang Huang, Xuanzhe Liu;Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2 (4), 1-26, 2018;Mobile users spend an extensive amount of time on typing. A more efficient text input instrument brings a significant enhancement of user experience. Deep learning techniques have been recently applied to suggesting the next words of input, but to achieve more accurate predictions, these models should be customized for individual users. Personalization is often at the expense of privacy concerns. Existing solutions require users to upload the historical logs of their input text to the cloud so that a deep learning predictor can be trained. In this work, we propose a novel approach, called DeepType, to personalize text input with better privacy. The basic idea is intuitive: training deep learning predictors on the device instead of on the cloud, so that the model makes personalized and private data never leaves the device to externals. With DeepType, a global model is first trained on the cloud using massive public corpora, and our personalization is done by incrementally customizing the global model with data on individual devices. We further propose a set of techniques that effectively reduce the computation cost of training deep learning models on mobile devices at the cost of negligible accuracy loss. Experiments using real-world text input from millions of users demonstrate that DeepType significantly improves the input efficiency for individual users, and its incurred computation and energy costs are within the performance and battery restrictions of typical COTS mobile devices.;https://dl.acm.org/doi/abs/10.1145/3287075?casa_token=YSP1UcUofVMAAAAA:05bfnBkgu4zRBb6TnY1NZ9Xkgv6jQYgstbrbfxm5Z9a8neaQT8F9yw-umIicE4tGTJd-YmxkkkEJAw;n00e68Sak1gJ
Benditkis, D., Keren, A., Mor-Yosef, L., Avidor, T., Shoham, N., & Tal-Israel, N. (2019, November). Distributed deep neural network training on edge devices. In Proceedings of the 4th ACM/IEEE Symposium on Edge Computing (pp. 304-306).;0_federated_learning_data_privacy;2019;Distributed deep neural network training on edge devices;Daniel Benditkis, Aviv Keren, Liron Mor-Yosef, Tomer Avidor, Neta Shoham, Nadav Tal-Israel;Proceedings of the 4th ACM/IEEE Symposium on Edge Computing, 304-306, 2019;Deep Neural Network (Deep Learning) models have been traditionally trained on dedicated servers, after collecting data from various edge devices and sending them to the server. In recent years new methodologies have emerged for training models in a distributed manner over edge devices, keeping the data on the devices themselves. This allows for better data privacy and reduces the training costs. One of the main challenges for such methodologies is reducing the communication costs to and mainly from the edge devices. In this work we compare the two main methodologies used for distributed edge training: Federated Learning and Large Batch Training. For each of the methodologies we examine their convergence rates, communication costs, and final model performance. In addition, we present two techniques for compressing the communication between the edge devices, and examine their suitability for each one of the training methodologies.;https://dl.acm.org/doi/abs/10.1145/3318216.3363324?casa_token=TKFeFIEQm0gAAAAA:PcXOJLlyT0cNtsoBSRaMIdTNB1ljbpU1oAjaJIC8E8wxPGRc-ngmoFHnv88T0Kwb2RweHPJq-A98MQ;ecE7UHuzBhMJ
Xu, Z., Li, L., & Zou, W. (2019, May). Exploring federated learning on battery-powered devices. In Proceedings of the ACM turing celebration conference-China (pp. 1-6).;0_federated_learning_data_privacy;2019;Exploring federated learning on battery-powered devices;Zichen Xu, Li Li, Wenting Zou;Proceedings of the ACM turing celebration conference-China, 1-6, 2019;Smartphones generate private data ubiquitously that serves Big data analysis. Without violating privacy issue, analytic companies want to understand and learn features from these data, which fits the nature of federated learning. Federated learning invites multiple participants to train a learning model (e.g., Artificial Neural Network) while guaranteeing the data privacy. However, processing federated learning is heavy on batteries, which is against the ubiquity feature of smartphone. In this paper, we explore the possibility of enabling federated learning on many battery-powered devices. We share our observations on supporting federated learning using battery power, and propose a two-layered strategy to process the learning on batteries with a reasonable tradeoff. The first layer improves the initialization of the federated learning while the second layer explores local energy saving potential. The results show that our work can finish the learning process successfully, consuming 20% less energy on average, and pays negligible overhead (average 0.1%) and accuracy loss (2% at most), as compared to the default setting.;https://dl.acm.org/doi/abs/10.1145/3321408.3323080?casa_token=JS0gOxLAM3cAAAAA:BkY6OV5C8Y5HeYns6lEKjnA8NWTySJlp81Fz2V_Bi7CrGKLt_AoxDI3i40lbNT6r1emCe5eXnKkQnw;gW3qBHOsx5MJ
Kharitonov, E. (2019, January). Federated online learning to rank with evolution strategies. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining (pp. 249-257).;0_federated_learning_data_privacy;2019;Federated online learning to rank with evolution strategies;Eugene Kharitonov;Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, 249-257, 2019;Online Learning to Rank is a powerful paradigm that allows to train ranking models using only online feedback from its users.In this work, we consider Federated Online Learning to Rank setup (FOLtR) where on-mobile ranking models are trained in a way that respects the users' privacy. We require that the user data, such as queries, results, and their feature representations are never communicated for the purpose of the ranker's training. We believe this setup is interesting, as it combines unique requirements for the learning algorithm: (a) preserving the user privacy, (b) low communication and computation costs, (c) learning from noisy bandit feedback, and (d) learning with non-continuous ranking quality measures. We propose a learning algorithm FOLtR-ES that satisfies these requirements. A part of FOLtR-ES is a privatization procedure that allows it to provide Îµ-local differential privacy guarantees, i.e. protecting the clients from an adversary who has access to the communicated messages. This procedure can be applied to any absolute online metric that takes finitely many values or can be discretized to a finite domain. Our experimental study is based on a widely used click simulation approach and publicly available learning to rank datasets MQ2007 and MQ2008. We evaluate FOLtR-ES against offline baselines that are trained using relevance labels, linear regression model and RankingSVM. From our experiments, we observe that FOLtR-ES can optimize a ranking model to perform similarly to the baselines in terms of the optimized online metric, Max Reciprocal Rank.;https://dl.acm.org/doi/abs/10.1145/3289600.3290968?casa_token=tHVZpno3bC4AAAAA:Ydp9XFfRQ5NAyd0dZm4NXBl9XvZm13sHapGwP7WAfmZWCWqxdxeJjXGwe89pzJnikZklLbx6Ju3VAA;KIu9KaWbhNYJ
Deng, K., Chen, Z., Zhang, S., Gong, C., & Zhu, J. (2019, October). Content compression coding for federated learning. In 2019 11th International Conference on Wireless Communications and Signal Processing (WCSP) (pp. 1-6). IEEE.;0_federated_learning_data_privacy;2019;Content compression coding for federated learning;Kaihe Deng, Zhikun Chen, Sihai Zhang, Chen Gong, Jinkang Zhu;2019 11th International Conference on Wireless Communications and Signal Processing (WCSP), 1-6, 2019;As machine learning requires distributed framework, Federated Learning(FL) is proposed to address the distributed computations in machine learning, especially for privacy protection. In FL, an important problem is to reduce the transmission quantity between the terminals and the centralized server. In this paper the content compression coding for FL is proposed, which can efficiently reduce the required transmission bandwidth. The correlation among the parameters of the distributed ANN models is first verified based on the entropy analysis and simulation results validate the propsed content compression coding using the WinRAR compression. Our work demonstrates that the proposed coding can compress the parameters in the distributed ANN with acceptable learning performance.;https://ieeexplore.ieee.org/abstract/document/8928018/;R7i1gT02aKYJ
Xu, R., Baracaldo, N., Zhou, Y., Anwar, A., & Ludwig, H. (2019, November). Hybridalpha: An efficient approach for privacy-preserving federated learning. In Proceedings of the 12th ACM workshop on artificial intelligence and security (pp. 13-23).;0_federated_learning_data_privacy;2019;Hybridalpha: An efficient approach for privacy-preserving federated learning;Runhua Xu, Nathalie Baracaldo, Yi Zhou, Ali Anwar, Heiko Ludwig;Proceedings of the 12th ACM workshop on artificial intelligence and security, 13-23, 2019;Federated learning has emerged as a promising approach for collaborative and privacy-preserving learning. Participants in a federated learning process cooperatively train a model by exchanging model parameters instead of the actual training data, which they might want to keep private. However, parameter interaction and the resulting model still might disclose information about the training data used. To address these privacy concerns, several approaches have been proposed based on differential privacy and secure multiparty computation (SMC), among others. They often result in large communication overhead and slow training time. In this paper, we propose HybridAlpha, an approach for privacy-preserving federated learning employing an SMC protocol based on functional encryption. This protocol is simple, efficient and resilient to participants dropping out. We evaluate our approach regarding the training time and data volume exchanged using a federated learning process to train a CNN on the MNIST data set. Evaluation against existing crypto-based SMC solutions shows that HybridAlpha can reduce the training time by 68% and data transfer volume by 92% on average while providing the same model performance and privacy guarantees as the existing solutions.;https://dl.acm.org/doi/abs/10.1145/3338501.3357371?casa_token=yYmM3-HdQKYAAAAA:GZrExA6Ye15aUlVDZseQsQ3VfBv1zDqLVGBCpJglj1ao2z2h7GHIGRl6OVqqVsT0vQ_Ls8xY4N4KFg;5qmaPrKNqT4J
Zhao, Y., Chen, J., Wu, D., Teng, J., & Yu, S. (2019, December). Multi-task network anomaly detection using federated learning. In Proceedings of the 10th international symposium on information and communication technology (pp. 273-279).;0_federated_learning_data_privacy;2019;Multi-task network anomaly detection using federated learning;Ying Zhao, Junjun Chen, Di Wu, Jian Teng, Shui Yu;Proceedings of the 10th international symposium on information and communication technology, 273-279, 2019;Because of the complexity of network traffic, there are various significant challenges in the network anomaly detection fields. One of the major challenges is the lack of labeled training data. In this paper, we use federated learning to tackle data scarcity problem and to preserve data privacy, where multiple participants collaboratively train a global model. Unlike the centralized training architecture, participants do not need to share their training to the server in federated learning, which can prevent the training data from being exploited by attackers. Moreover, most of the previous works focus on one specific task of anomaly detection, which restricts the application areas and can not provide more valuable information to network administrators. Therefore, we propose a multi-task deep neural network in federated learning (MT-DNN-FL) to perform network anomaly detection task, VPN (Tor) traffic recognition task, and traffic classification task, simultaneously. Compared with multiple single-task models, the multi-task method can reduce training time overhead. Experiments conducted on well-known CICIDS2017, ISCXVPN2016, and ISCXTor2016 datasets, show that the detection and classification performance achieved by the proposed method is better than the baseline methods in centralized training architecture.;https://dl.acm.org/doi/abs/10.1145/3368926.3369705?casa_token=TrHWFOZxjm0AAAAA:jpnkxfVQJYEKm0XQCvXRqL1QMAr6z6G6VN3Ys-U6p33RMUTjO-veROnGv5joBjrxEbydk5Ss9Z9cYA;MCZ87j1rQc4J
Jiang, L., Tan, R., Lou, X., & Lin, G. (2019, April). On lightweight privacy-preserving collaborative learning for internet-of-things objects. In Proceedings of the international conference on internet of things design and implementation (pp. 70-81).;0_federated_learning_data_privacy;2019;On lightweight privacy-preserving collaborative learning for internet-of-things objects;Linshan Jiang, Rui Tan, Xin Lou, Guosheng Lin;Proceedings of the international conference on internet of things design and implementation, 70-81, 2019;The Internet of Things (IoT) will be a main data generation infrastructure for achieving better system intelligence. This paper considers the design and implementation of a practical privacy-preserving collaborative learning scheme, in which a curious learning coordinator trains a better machine learning model based on the data samples contributed by a number of IoT objects, while the confidentiality of the raw forms of the training data is protected against the coordinator. Existing distributed machine learning and data encryption approaches incur significant computation and communication overhead, rendering them ill-suited for resource-constrained IoT objects. We study an approach that applies independent Gaussian random projection at each IoT object to obfuscate data and trains a deep neural network at the coordinator based on the projected data from the IoT objects. This approach introduces light computation overhead to the IoT objects and moves most workload to the coordinator that can have sufficient computing resources. Although the independent projections performed by the IoT objects address the potential collusion between the curious coordinator and some compromised IoT objects, they significantly increase the complexity of the projected data. In this paper, we leverage the superior learning capability of deep learning in capturing sophisticated patterns to maintain good learning performance. Extensive comparative evaluation shows that this approach outperforms other lightweight approaches that apply additive noisification for differential privacy and/or support vector machines for learning in the applications with light data pattern complexities.;https://dl.acm.org/doi/abs/10.1145/3302505.3310070?casa_token=bPgizDlpJkoAAAAA:uW34ivPeoavwMIM_VChIiobWWtjQRXeuE4yjapdqK-RGb_JvDa2F_HCcrcyEuM4TeGzs98Afm4dEDQ;snYIb2ME1AEJ
Awan, S., Li, F., Luo, B., & Liu, M. (2019, November). Poster: A reliable and accountable privacy-preserving federated learning framework using the blockchain. In Proceedings of the 2019 ACM SIGSAC conference on computer and communications security (pp. 2561-2563).;0_federated_learning_data_privacy;2019;Poster: A reliable and accountable privacy-preserving federated learning framework using the blockchain;Sana Awan, Fengjun Li, Bo Luo, Mei Liu;Proceedings of the 2019 ACM SIGSAC conference on computer and communications security, 2561-2563, 2019;Federated learning (FL) is promising in supporting collaborative learning applications that involve large datasets, massively distributed data owners and unreliable network connectivity. To protect data privacy, existing FL approaches adopt (k,n)-threshold secret sharing schemes, based on the semi-honest assumption for clients, to enable secure multiparty computation in local model update exchange which deals with random client dropouts at the cost of increasing data size. These approaches adopt the semi-honest assumption for clients, therefore they are vulnerable to malicious clients. In this work, we propose a blockchain-based privacy-preserving federated learning (BC-based PPFL) framework, which leverages the immutability and decentralized trust properties of blockchain to provide provenance of model updates. Our proof-of-concept implementation of BC-based PPFL demonstrates it is practical for secure aggregation of local model updates in the federated setting.;https://dl.acm.org/doi/abs/10.1145/3319535.3363256?casa_token=ZbgD-1wBUUEAAAAA:sKFrefdSQzLMGz_M0Cutrqd_tUtkWy6-reY8G9OXhOUNVwApj6T_jR50ZNmNz8KREeJb08HWVJyDFQ;-dQg9P8Oq1cJ
Bonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H. B., Patel, S., ... & Seth, K. (2017, October). Practical secure aggregation for privacy-preserving machine learning. In proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (pp. 1175-1191).;0_federated_learning_data_privacy;2017;Practical secure aggregation for privacy-preserving machine learning;Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, Karn Seth;proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 1175-1191, 2017;We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user's individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers $1.73 x communication expansion for 210 users and 220-dimensional vectors, and 1.98 x expansion for 214 users and 224-dimensional vectors over sending data in the clear.;https://dl.acm.org/doi/abs/10.1145/3133956.3133982;jW0Scs4p9b0J
Puri, C., Dolui, K., Kooijman, G., Masculo, F., Van Sambeek, S., Boer, S. D., ... & Vanrumste, B. (2019, November). Privacy preserving pregnancy weight gain management: demo abstract. In Proceedings of the 17th Conference on Embedded Networked Sensor Systems (pp. 398-399).;0_federated_learning_data_privacy;2019;Privacy preserving pregnancy weight gain management: demo abstract;Chetanya Puri, Koustabh Dolui, Gerben Kooijman, Felipe Masculo, Shannon Van Sambeek, Sebastiaan Den Boer, Sam Michiels, Hans Hallez, Stijn Luca, Bart Vanrumste;Proceedings of the 17th Conference on Embedded Networked Sensor Systems, 398-399, 2019;Early gestational weight gain prediction can help expecting women overcome several associated risks. However, training the model requires access to centrally stored privacy sensitive weight and other meta-data. In this demo, we present a privacy preserving federated learning approach where we train a global weight gain prediction model by aggregating client models trained locally on their personal data. We showcase a software data-exploration tool that exhibits local model generation, sharing and updating across users and server for proposed collaborative learning. Our proposed model predicts the final weight category with 61.3% accuracy on day 140, with a 8.8% compromise on the centralized training accuracy.;https://dl.acm.org/doi/abs/10.1145/3356250.3361941?casa_token=ZXlmeJtuSgEAAAAA:TMzol0EVnLc6I4BNGDHb-WnREFo5D3qHVEIup7xWu2WsEL_UV4fgXvipLHh8DqJznqutwpyDdN37kg;JCC_2pxk_AcJ
Ickin, S., Vandikas, K., & Fiedler, M. (2019, October). Privacy preserving qoe modeling using collaborative learning. In Proceedings of the 4th Internet-QoE Workshop on QoE-Based Analysis and Management of Data Communication Networks (pp. 13-18).;0_federated_learning_data_privacy;2019;Privacy preserving qoe modeling using collaborative learning;Selim Ickin, Konstantinos Vandikas, Markus Fiedler;Proceedings of the 4th Internet-QoE Workshop on QoE-Based Analysis and Management of Data Communication Networks, 13-18, 2019;Machine Learning (ML) based Quality of Experience (QoE) models potentially suffer from over-fitting due to limitations including low data volume, and limited participant profiles. This prevents models from becoming generic. Consequently, these trained models may under-perform when tested outside the experimented population. One reason for the limited datasets, which we refer in this paper as small QoE data lakes, is due to the fact that often these datasets potentially contain user sensitive information and are only collected throughout expensive user studies with special user consent. Thus, sharing of datasets amongst researchers is often not allowed. In recent years, privacy preserving machine learning models have become important and so have techniques that enable model training without sharing datasets but instead relying on secure communication protocols. Following this trend, in this paper, we present Round-Robin based Collaborative Machine Learning model training, where the model is trained in a sequential manner amongst the collaborated partner nodes. We benchmark this work using our customized Federated Learning mechanism as well as conventional Centralized and Isolated Learning methods.;https://dl.acm.org/doi/abs/10.1145/3349611.3355548?casa_token=3-aD22uswoMAAAAA:Sp7lWdGjOd2SW4Ildh1rf_cCwRyeaKL8Uj5NDPB8IAOCptCa3z32vKvaasWCUSMZWKwUhG69Q4fZ-w;DD-xjA8BZeYJ
Ma, J., Zhang, Q., Lou, J., Ho, J. C., Xiong, L., & Jiang, X. (2019, November). Privacy-preserving tensor factorization for collaborative health data analysis. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management (pp. 1291-1300).;0_federated_learning_data_privacy;2019;Privacy-preserving tensor factorization for collaborative health data analysis;Jing Ma, Qiuchen Zhang, Jian Lou, Joyce C Ho, Li Xiong, Xiaoqian Jiang;Proceedings of the 28th ACM International Conference on Information and Knowledge Management, 1291-1300, 2019;Tensor factorization has been demonstrated as an efficient approach for computational phenotyping, where massive electronic health records (EHRs) are converted to concise and meaningful clinical concepts. While distributing the tensor factorization tasks to local sites can avoid direct data sharing, it still requires the exchange of intermediary results which could reveal sensitive patient information. Therefore, the challenge is how to jointly decompose the tensor under rigorous and principled privacy constraints, while still support the model's interpretability. We propose DPFact, a privacy-preserving collaborative tensor factorization method for computational phenotyping using EHR. It embeds advanced privacy-preserving mechanisms with collaborative learning. Hospitals can keep their EHR database private but also collaboratively learn meaningful clinical concepts by sharing differentially private intermediary results. Moreover, DPFact solves the heterogeneous patient population using a structured sparsity term. In our framework, each hospital decomposes its local tensors and sends the updated intermediary results with output perturbation every several iterations to a semi-trusted server which generates the phenotypes. The evaluation on both real-world and synthetic datasets demonstrated that under strict privacy constraints, our method is more accurate and communication-efficient than state-of-the-art baseline methods.;https://dl.acm.org/doi/abs/10.1145/3357384.3357878;Eg3_tM8SKrsJ
Mandal, K., & Gong, G. (2019, November). PrivFL: Practical privacy-preserving federated regressions on high-dimensional data over mobile networks. In Proceedings of the 2019 ACM SIGSAC Conference on Cloud Computing Security Workshop (pp. 57-68).;0_federated_learning_data_privacy;2019;PrivFL: Practical privacy-preserving federated regressions on high-dimensional data over mobile networks;Kalikinkar Mandal, Guang Gong;Proceedings of the 2019 ACM SIGSAC Conference on Cloud Computing Security Workshop, 57-68, 2019;Federated Learning (FL) enables a large number of users to jointly learn a shared machine learning (ML) model, coordinated by a centralized server, where the data is distributed across multiple devices. This approach enables the server or users to train and learn an ML model using gradient descent, while keeping all the training data on users' devices. We consider training an ML model over a mobile network where user dropout is a common phenomenon. Although federated learning was aimed at reducing data privacy risks, the ML model privacy has not received much attention.In this work, we present PrivFL, a privacy-preserving system for training (predictive) linear and logistic regression models and oblivious predictions in the federated setting, while guaranteeing data and model privacy as well as ensuring robustness to users dropping out in the network. We design two privacy-preserving protocols for training linear and logistic regression models based on an additive homomorphic encryption (HE) scheme and an aggregation protocol. Exploiting the training algorithm of federated learning, at the core of our training protocols is a secure multiparty global gradient computation on alive users' data. We analyze the security of our training protocols against semi-honest adversaries. As long as the aggregation protocol is secure under the aggregation privacy game and the additive HE scheme is semantically secure, PrivFL guarantees the users' data privacy against the server, and the server's regression model privacy against the users. We demonstrate the performance of PrivFL on real-world datasets and show its applicability in the federated learning system.;https://dl.acm.org/doi/abs/10.1145/3338466.3358926;btf2e4phZYYJ
Zhang, J., Wang, J., Zhao, Y., & Chen, B. (2019). An efficient federated learning scheme with differential privacy in mobile edge computing. In Machine Learning and Intelligent Communications: 4th International Conference, MLICOM 2019, Nanjing, China, August 24–25, 2019, Proceedings 4 (pp. 538-550). Springer International Publishing.;0_federated_learning_data_privacy;2019;An efficient federated learning scheme with differential privacy in mobile edge computing;Jiale Zhang, Junyu Wang, Yanchao Zhao, Bing Chen;Machine Learning and Intelligent Communications: 4th International Conference, MLICOM 2019, Nanjing, China, August 24â€“25, 2019, Proceedings 4, 538-550, 2019;"In this paper, we consider a mobile edge computing (MEC) system that multiple users participate in the federated learning protocol by jointly training a deep neural network (DNN) with their private training datasets. The main challenges of applying federated learning to MEC are: (1) it incurs tremendous computational cost by carrying out the deep neural network training phase on the resource-constraint mobile edge devices; (2) existing literature demonstrates that the parameters of a DNN trained on a dataset can be exploited to partially reconstruct the training samples in original dataset. To address the aforementioned issues, we introduce an efficiently private federated learning scheme in mobile edge computing, named FedMEC, with model partition technique and differential privacy method in this work. The experimental results demonstrate that our proposed FedMEC scheme can achieve high model accuracy under different perturbation strengths.";https://link.springer.com/chapter/10.1007/978-3-030-32388-2_46;tU_y0u0ynn8J
Zhu, X., Li, H., & Yu, Y. (2019). Blockchain-based privacy preserving deep learning. In Information Security and Cryptology: 14th International Conference, Inscrypt 2018, Fuzhou, China, December 14-17, 2018, Revised Selected Papers 14 (pp. 370-383). Springer International Publishing.;0_federated_learning_data_privacy;2019;Blockchain-based privacy preserving deep learning;Xudong Zhu, Hui Li, Yang Yu ;Information security and cryptology: 14th International Conference, Inscrypt 2018, Fuzhou, China, December 14-17, 2018: revised selected papers;Smart mobile devices have access to huge amounts of data appropriate to deep learning models, which in turn can significantly improve the end-user experience on mobile devices. But massive data collection required for machine learning introduce obvious privacy issues. To this end, the notion of federated learning (FL) was proposed, which leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. However, in many applications one or more Byzantine devices may suffice to let current coordination learning mechanisms fail with unpredictable or disastrous outcomes. In this paper, we provide a proof-of-concept for managing security issues in federated learning systems via blockchain technology. Our approach uses decentralized programs executed via blockchain technology to establish secure learning coordination mechanisms and to identify and exclude Byzantine members. We studied the performance of our blockchain-based approach in a collective deep-learning scenario both in the presence and absence of Byzantine devices and compared our results to those obtained with an existing collective decision approach. The results show a clear advantage of the blockchain approach when Byzantine devices are part of the members.;https://link.springer.com/chapter/10.1007/978-3-030-14234-6_20;Todo
Ji, S., Long, G., Pan, S., Zhu, T., Jiang, J., & Wang, S. (2019, April). Detecting suicidal ideation with data protection in online communities. In International conference on database systems for advanced applications (pp. 225-229). Cham: Springer International Publishing.;1_ml_machine_data_learning;2019;Detecting suicidal ideation with data protection in online communities;Shaoxiong Ji, Guodong Long, Shirui Pan, Tianqing Zhu, Jing Jiang, Sen Wang;International conference on database systems for advanced applications, 225-229, 2019;Recent advances in Artificial Intelligence empower proactive social services that use virtual intelligent agents to automatically detect people’s suicidal ideation. Conventional machine learning methods require a large amount of individual data to be collected from users’ Internet activities, smart phones and wearable healthcare devices, to amass them in a central location. The centralized setting arises significant privacy and data misuse concerns, especially where vulnerable people are concerned. To address this problem, we propose a novel data-protecting solution to learn a model. Instead of asking users to share all their personal data, our solution is to train a local data-preserving model for each user which only shares their own model’s parameters with the server rather than their personal information. To optimize the model’s learning capability, we have developed a novel updating algorithm, called average difference descent, to aggregate parameters from different client models. An experimental study using real-world online social community datasets has been included to mimic the scenario of private communities for suicide discussion. The results of experiments demonstrate the effectiveness of our technology solution and paves the way for mental health service providers to apply this technology to real applications.;https://link.springer.com/chapter/10.1007/978-3-030-18590-9_17;V3pGckJvwrsJ
Hao, T., Huang, Y., Wen, X., Gao, W., Zhang, F., Zheng, C., ... & Zhan, J. (2019). Edge AIBench: towards comprehensive end-to-end edge computing benchmarking. In Benchmarking, Measuring, and Optimizing: First BenchCouncil International Symposium, Bench 2018, Seattle, WA, USA, December 10-13, 2018, Revised Selected Papers 1 (pp. 23-30). Springer International Publishing.;7_edge_computing_deep_learning;2019;Edge AIBench: towards comprehensive end-to-end edge computing benchmarking;Tianshu Hao, Yunyou Huang, Xu Wen, Wanling Gao, Fan Zhang, Chen Zheng, Lei Wang, Hainan Ye, Kai Hwang, Zujie Ren, Jianfeng Zhan;Benchmarking, Measuring, and Optimizing: First BenchCouncil International Symposium, Bench 2018, Seattle, WA, USA, December 10-13, 2018, Revised Selected Papers 1, 23-30, 2019;In edge computing scenarios, the distribution of data and collaboration of workloads on different layers are serious concerns for performance, privacy, and security issues. So for edge computing benchmarking, we must take an end-to-end view, considering all three layers: client-side devices, edge computing layer, and cloud servers. Unfortunately, the previous work ignores this most important point. This paper presents the BenchCouncilâ€™s coordinated effort on edge AI benchmarks, named Edge AIBench. In total, Edge AIBench models four typical application scenarios: ICU Patient Monitor, Surveillance Camera, Smart Home, and Autonomous Vehicle with the focus on data distribution and workload collaboration on three layers. Edge AIBench is publicly available from http://www.benchcouncil.org/EdgeAIBench/index.html . We also build an edge computing testbed with a federated learning framework to resolve performance, privacy, and security issues.;https://link.springer.com/chapter/10.1007/978-3-030-32813-9_3;zwCQGP9KOJ4J
Kamp, M., Adilova, L., Sicking, J., Hüger, F., Schlicht, P., Wirtz, T., & Wrobel, S. (2019). Efficient decentralized deep learning by dynamic model averaging. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10–14, 2018, Proceedings, Part I 18 (pp. 393-409). Springer International Publishing.;0_federated_learning_data_privacy;2019;Efficient decentralized deep learning by dynamic model averaging;Michael Kamp, Linara Adilova, Joachim Sicking, Fabian Hüger, Peter Schlicht, Tim Wirtz, Stefan Wrobel;Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10–14, 2018, Proceedings, Part I 18, 393-409, 2019;We propose an efficient protocol for decentralized training of deep neural networks from distributed data sources. The proposed protocol allows to handle different phases of model training equally well and to quickly adapt to concept drifts. This leads to a reduction of communication by an order of magnitude compared to periodically communicating state-of-the-art approaches. Moreover, we derive a communication bound that scales well with the hardness of the serialized learning problem. The reduction in communication comes at almost no cost, as the predictive performance remains virtually unchanged. Indeed, the proposed protocol retains loss bounds of periodically averaging schemes. An extensive empirical evaluation validates major improvement of the trade-off between model performance and communication which could be beneficial for numerous decentralized learning applications, such as autonomous driving, or voice recognition and image classification on mobile phones. Code related to this paper is available at: https://bitbucket.org/Michael_Kamp/decentralized-machine-learning .;https://link.springer.com/chapter/10.1007/978-3-030-10925-7_24;NiNP2eCKeDIJ
Yang, W., Zhang, Y., Ye, K., Li, L., & Xu, C. Z. (2019). Ffd: A federated learning based method for credit card fraud detection. In Big Data–BigData 2019: 8th International Congress, Held as Part of the Services Conference Federation, SCF 2019, San Diego, CA, USA, June 25–30, 2019, Proceedings 8 (pp. 18-32). Springer International Publishing.;0_federated_learning_data_privacy;2019;Ffd: A federated learning based method for credit card fraud detection;Wensi Yang, Yuhang Zhang, Kejiang Ye, Li Li, Cheng-Zhong Xu;Big Data–BigData 2019: 8th International Congress, Held as Part of the Services Conference Federation, SCF 2019, San Diego, CA, USA, June 25–30, 2019, Proceedings 8, 18-32, 2019;Credit card fraud has caused a huge loss to both banks and consumers in recent years. Thus, an effective Fraud Detection System (FDS) is important to minimize the loss for banks and cardholders. Based on our analysis, the credit card transaction dataset is very skewed, there are much fewer samples of frauds than legitimate transactions. Furthermore, due to the data security and privacy, different banks are usually not allowed to share their transaction datasets. These problems make FDS difficult to learn the patterns of frauds and also difficult to detect them. In this paper, we propose a framework to train a fraud detection model using behavior features with federated learning, we term this detection framework FFD (Federated learning for Fraud Detection). Different from the traditional FDS trained with data centralized in the cloud, FFD enables banks to learn fraud detection model with the training data distributed on their own local database. Then, a shared FDS is constructed by aggregating locally-computed updates of fraud detection model. Banks can collectively reap the benefits of shared model without sharing the dataset and protect the sensitive information of cardholders. Furthermore, an oversampling approach is combined to balance the skewed dataset. We evaluate the performance of our credit card FDS with FFD framework on a large scale dataset of real-world credit card transactions. Experimental results show that the federated learning based FDS achieves an average of test AUC to 95.5%, which is about 10% higher than traditional FDS.;https://link.springer.com/chapter/10.1007/978-3-030-23551-2_2;lJwZEA1T8swJ
Ulm, G., Gustavsson, E., & Jirstrand, M. (2019). Functional federated learning in erlang (ffl-erl). In Functional and Constraint Logic Programming: 26th International Workshop, WFLP 2018, Frankfurt/Main, Germany, September 6, 2018, Revised Selected Papers 26 (pp. 162-178). Springer International Publishing.;0_federated_learning_data_privacy;2019;Functional federated learning in erlang (ffl-erl;Gregor Ulm, Emil Gustavsson, Mats Jirstrand;Functional and Constraint Logic Programming: 26th International Workshop, WFLP 2018, Frankfurt/Main, Germany, September 6, 2018, Revised Selected Papers 26, 162-178, 2019;The functional programming language Erlang is well-suited for concurrent and distributed applications, but numerical computing is not seen as one of its strengths. Yet, the recent introduction of Federated Learning, which leverages client devices for decentralized machine learning tasks, while a central server updates and distributes a global model, motivated us to explore how well Erlang is suited to that problem. We present the Federated Learning framework ffl-erl and evaluate it in two scenarios: one in which the entire system has been written in Erlang, and another in which Erlang is relegated to coordinating client processes that rely on performing numerical computations in the programming language C. There is a concurrent as well as a distributed implementation of each case. We show that Erlang incurs a performance penalty, but for certain use cases this may not be detrimental, considering the trade-off between speed of development (Erlang) versus performance (C). Thus, Erlang may be a viable alternative to C for some practical machine learning tasks.;https://link.springer.com/chapter/10.1007/978-3-030-16202-3_10;y1u9qVaelTMJ
Sun, X., Bommert, A., Pfisterer, F., Rähenfürher, J., Lang, M., & Bischl, B. (2020). High dimensional restrictive federated model selection with multi-objective bayesian optimization over shifted distributions. In Intelligent Systems and Applications: Proceedings of the 2019 Intelligent Systems Conference (IntelliSys) Volume 1 (pp. 629-647). Springer International Publishing.;0_federated_learning_data_privacy;2020;High dimensional restrictive federated model selection with multi-objective bayesian optimization over shifted distributions;Xudong Sun, Andrea Bommert, Florian Pfisterer, Jörg Rähenfürher, Michel Lang, Bernd Bischl;Intelligent Systems and Applications: Proceedings of the 2019 Intelligent Systems Conference (IntelliSys) Volume 1, 629-647, 2020;A novel machine learning optimization process coined Restrictive Federated Model Selection (RFMS) is proposed under the scenario, for example, when data from healthcare units can not leave the site it is situated on and it is forbidden to carry out training algorithms on remote data sites due to either technical or privacy and trust concerns. To carry out a clinical research in this scenario, an analyst could train a machine learning model only on local data site, but it is still possible to execute a statistical query at a certain cost in the form of sending a machine learning model to some of the remote data sites and get the performance measures as feedback, maybe due to prediction being usually much cheaper. Compared to federated learning, which is optimizing the model parameters directly by carrying out training across all data sites, RFMS trains model parameters only on one local data site but optimizes hyper parameters across other data sites jointly since hyper-parameters play an important role in machine learning performance. The aim is to get a Pareto optimal model with respective to both local and remote unseen prediction losses, which could generalize well across data sites. In this work, we specifically consider high dimensional data with different distributions over data sites. As an initial investigation, Bayesian Optimization especially multi-objective Bayesian Optimization is used to guide an adaptive hyper-parameter optimization process to select models under the RFMS scenario. Empirical results shows that solely using the local data site to tune hyper-parameters generalizes poorly across data sites, compared to methods that utilize the local and remote performances. Furthermore, in terms of hypervolumes, multi-objective Bayesian Optimization algorithms show increased performance across multiple data sites among other candidates.;https://link.springer.com/chapter/10.1007/978-3-030-29516-5_48;mZ88uEZ5rh4J
Hu, Y., Sun, X., Chen, Y., & Lu, Z. (2019, May). Model and feature aggregation based federated learning for multi-sensor time series trend following. In International Work-Conference on Artificial Neural Networks (pp. 233-246). Cham: Springer International Publishing.;0_federated_learning_data_privacy;2019;Model and feature aggregation based federated learning for multi-sensor time series trend following;Yao Hu, Xiaoyan Sun, Yang Chen, Zishuai Lu;International Work-Conference on Artificial Neural Networks, 233-246, 2019;In the industrial field, especially the work or environment condition monitoring, it is crucial but difficult to follow the trend of the time series monitoring data (TSD) when the TSD come from different kinds of sensors and are collected by different companies. The privacy of the multi-sensor TSD must be carefully treated. Few studies, however, have been devoted to solving such problems. Federated learning (FL) is a good structure developed by Google for well keeping the personal privacy. Motivated by this, we here present an improved FL structure for not only keeping the data privacy but also extracting and fusing the trends features of the multi-sensor TSD. In our work, the client models of FL are first designed and optimized for getting the initial parameters and features w.r.t. the corresponding sensorâ€™s TSD, and then both the model parameters and the extracted features of all the activated clients (sensors) are sent to the central server and aggregated. The fused parameters and features are returned to the clients and used to update the optimization of the model. Finally, the fused features of all multi-sensor TSD are put into an echo state network (ESN) to fulfill the trend following of the multi-sensor TSD. The proposed algorithm is applied to the multi-sensor electromagnetic radiation intensity TSD sampled from an actual coal mine, and its superiority in promoting the accuracy on every sensor is demonstrated.;https://link.springer.com/chapter/10.1007/978-3-030-20521-8_20;xIksVZ7RHVIJ
Sheller, M. J., Reina, G. A., Edwards, B., Martin, J., & Bakas, S. (2019). Multi-institutional deep learning modeling without sharing patient data: A feasibility study on brain tumor segmentation. In Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Revised Selected Papers, Part I 4 (pp. 92-104). Springer International Publishing.;0_federated_learning_data_privacy;2019;Multi-institutional deep learning modeling without sharing patient data: A feasibility study on brain tumor segmentation;Micah J Sheller, G Anthony Reina, Brandon Edwards, Jason Martin, Spyridon Bakas;Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with MICCAI 2018, Granada, Spain …, 2019;Deep learning models for semantic segmentation of images require large amounts of data. In the medical imaging domain, acquiring sufficient data is a significant challenge. Labeling medical image data requires expert knowledge. Collaboration between institutions could address this challenge, but sharing medical data to a centralized location faces various legal, privacy, technical, and data-ownership challenges, especially among international institutions. In this study, we introduce the first use of federated learning for multi-institutional collaboration, enabling deep learning modeling without sharing patient data. Our quantitative results demonstrate that the performance of federated semantic segmentation models (Dice = 0.852) on multimodal brain scans is similar to that of models trained by sharing data (Dice = 0.862). We compare federated learning with two alternative collaborative learning methods and find that they fail to match the performance of federated learning.;https://link.springer.com/chapter/10.1007/978-3-030-11723-8_9;rXIBbnSEsXkJ
Zhao, Y., Chen, J., Zhang, J., Wu, D., Teng, J., & Yu, S. (2020). PDGAN: A novel poisoning defense method in federated learning using generative adversarial network. In Algorithms and Architectures for Parallel Processing: 19th International Conference, ICA3PP 2019, Melbourne, VIC, Australia, December 9–11, 2019, Proceedings, Part I 19 (pp. 595-609). Springer International Publishing.;0_federated_learning_data_privacy;2020;PDGAN: A novel poisoning defense method in federated learning using generative adversarial network;Ying Zhao, Junjun Chen, Jiale Zhang, Di Wu, Jian Teng, Shui Yu;Algorithms and Architectures for Parallel Processing: 19th International Conference, ICA3PP 2019, Melbourne, VIC, Australia, December 9â€“11, 2019, Proceedings, Part I 19, 595-609, 2020;Federated learning can complete an enormous training task efficiently by inviting participants to train a deep learning model collaboratively, and the user privacy will be well preserved for the users only upload model parameters to the centralized server. However, the attackers can initiate poisoning attacks by uploading malicious updates in federated learning. Therefore, the accuracy of the global model will be impacted significantly after the attack. To address this vulnerability, we propose a novel poisoning defense generative adversarial network (PDGAN) to defend the poising attack. The PDGAN can reconstruct training data from model updates and audit the accuracy for each participant model by using the generated data. Precisely, the participant whose accuracy is lower than a predefined threshold will be identified as an attacker and model parameters of the attacker will be removed from the training procedure in this iteration. Experiments conducted on MNIST and Fashion-MNIST datasets demonstrate that our approach can indeed defend the poisoning attacks in federated learning.;https://link.springer.com/chapter/10.1007/978-3-030-38991-8_39;hi3DIUOAZt4J
Fadaeddini, A., Majidi, B., & Eshghi, M. (2019). Privacy preserved decentralized deep learning: A blockchain based solution for secure ai-driven enterprise. In High-Performance Computing and Big Data Analysis: Second International Congress, TopHPC 2019, Tehran, Iran, April 23–25, 2019, Revised Selected Papers 2 (pp. 32-40). Springer International Publishing.;0_federated_learning_data_privacy;2019;Privacy preserved decentralized deep learning: A blockchain based solution for secure ai-driven enterprise;Amin Fadaeddini, Babak Majidi, Mohammad Eshghi;High-Performance Computing and Big Data Analysis: Second International Congress, TopHPC 2019, Tehran, Iran, April 23â€“25, 2019, Revised Selected Papers 2, 32-40, 2019;Deep learning and Blockchain attracted the attention of both the research community and the industry. In the financial enterprise by using the Blockchain technology, financial transactions could be performed in shorter periods and with higher transparency and security. In Blockchain ecosystem, there is no need for having a central reliable authority to regulate and control the system. In Blockchain many entities which cannot trust each other in normal conditions can join together to achieve a mutual goal. Deep learning algorithms are currently the best solution for many machine learning applications and provide high accuracy models for robotics, computer vision, smart cities and other AI-driven enterprise. However, availability of more data can boost the performance of deep models considerably. In this paper, a secure decentralized deep learning framework for big data analytics on Blockchain for AI-driven enterprise is proposed. The proposed framework uses the Stellar Blockchain infrastructure for secure decentralized training of the deep models. A Deep Learning Coin (DLC) is used for Blockchain compensation. The security of the proposed framework incentivizes people and organizations to share their valuable data for training the deep neural models while the privacy of their data is preserved.;https://link.springer.com/chapter/10.1007/978-3-030-33495-6_3;XgHY6cLQ98sJ
Li, W., Milletarì, F., Xu, D., Rieke, N., Hancox, J., Zhu, W., ... & Feng, A. (2019). Privacy-preserving federated brain tumour segmentation. In Machine Learning in Medical Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings 10 (pp. 133-141). Springer International Publishing.;0_federated_learning_data_privacy;2019;Privacy-preserving federated brain tumour segmentation;Wenqi Li, Fausto Milletarì, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao Zhu, Maximilian Baust, Yan Cheng, Sébastien Ourselin, M Jorge Cardoso, Andrew Feng;Machine Learning in Medical Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings 10, 133-141, 2019;Due to medical data privacy regulations, it is often infeasible to collect and share patient data in a centralised data lake. This poses challenges for training machine learning algorithms, such as deep convolutional networks, which often require large numbers of diverse training examples. Federated learning sidesteps this difficulty by bringing code to the patient data owners and only sharing intermediate model training updates among them. Although a high-accuracy model could be achieved by appropriately aggregating these model updates, the model shared could indirectly leak the local training examples. In this paper, we investigate the feasibility of applying differential-privacy techniques to protect the patient data in a federated learning setup. We implement and evaluate practical federated learning systems for brain tumour segmentation on the BraTS dataset. The experimental results show that there is a trade-off between model performance and privacy protection costs.;https://link.springer.com/chapter/10.1007/978-3-030-32692-0_16;W9WuAVjgIXkJ
Liu, C., Chakraborty, S., & Verma, D. (2019). Secure model fusion for distributed learning using partial homomorphic encryption. Policy-Based Autonomic Data Governance, 154-179.;0_federated_learning_data_privacy;2019;Secure model fusion for distributed learning using partial homomorphic encryption;Changchang Liu, Supriyo Chakraborty, Dinesh Verma;Policy-Based Autonomic Data Governance, 154-179, 2019;Distributed learning has emerged as a useful tool for analyzing data stored in multiple geographic locations, especially when the distributed data sets are large and hard to move around, or the data owner is reluctant to put data into the Cloud due to privacy concerns. In distributed learning, only the locally computed models are uploaded to the fusion server, which however may still cause privacy issues since the fusion server could implement various inference attacks from its observations. To address this problem, we propose a secure distributed learning system that aims to utilize the additive property of partial homomorphic encryption to prevent direct exposure of the computed models to the fusion server. Furthermore, we propose two optimization mechanisms for applying partial homomorphic encryption to model parameters in order to improve the overall efficiency. Through experimental analysis, we demonstrate the effectiveness of our proposed mechanisms in practical distributed learning systems. Furthermore, we analyze the relationship between the computational time in the training process and several important system parameters, which can serve as a useful guide for selecting proper parameters for balancing the trade-off among model accuracy, model security and system overhead.;https://link.springer.com/chapter/10.1007/978-3-030-17277-0_9;CcarQyRnAesJ
Song, C., Li, T., Huang, X., Wang, Z., & Zeng, P. (2019). Towards edge computing based distributed data analytics framework in smart grids. In Artificial Intelligence and Security: 5th International Conference, ICAIS 2019, New York, NY, USA, July 26-28, 2019, Proceedings, Part I 5 (pp. 283-292). Springer International Publishing.;7_edge_computing_deep_learning;2019;Towards edge computing based distributed data analytics framework in smart grids;Chunhe Song, Tong Li, Xu Huang, Zhongfeng Wang, Peng Zeng;Artificial Intelligence and Security: 5th International Conference, ICAIS 2019, New York, NY, USA, July 26-28, 2019, Proceedings, Part I 5, 283-292, 2019;Edge computing, as an emerging paradigm empower the network edge devices with intelligence, has become a prominent and promising future for Internet of things. Meanwhile, machine learning method, especially deep learning method has experience tremendous success recently in many application scenario. Recently, deep learning method applied in IoT scenario is also explored in many literatures. However, how to combine edge computing and deep learning method to advance the data analytics in smart grids has not been fully studied. To this end, in this paper, we propose ECNN (Edge-deployed Convolution Neural Network) in edge computing assisted smart grids to greatly enhance the ability in data aggregation and analytics. We also discuss how to train such network in edge computing distributively. Experiments shows the advantage of our paradigm.;https://link.springer.com/chapter/10.1007/978-3-030-24274-9_25;mFDPbnZOV6MJ
Anelli, V. W., Deldjoo, Y., Di Noia, T., & Ferrara, A. (2019). Towards effective device-aware federated learning. In AI* IA 2019–Advances in Artificial Intelligence: XVIIIth International Conference of the Italian Association for Artificial Intelligence, Rende, Italy, November 19–22, 2019, Proceedings 18 (pp. 477-491). Springer International Publishing.;0_federated_learning_data_privacy;2019;Towards effective device-aware federated learning;Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, Antonio Ferrara;AI* IA 2019–Advances in Artificial Intelligence: XVIIIth International Conference of the Italian Association for Artificial Intelligence, Rende, Italy, November 19–22, 2019 …, 2019;With the wealth of information produced by social networks, smartphones, medical or financial applications, speculations have been raised about the sensitivity of such data in terms of users’ personal privacy and data security. To address the above issues, Federated Learning (FL) has been recently proposed as a means to leave data and computational resources distributed over a large number of nodes (clients) where a central coordinating server aggregates only locally computed updates without knowing the original data. In this work, we extend the FL framework by pushing forward the state the art in the field on several dimensions: (i) unlike the original FedAvg approach relying solely on single criteria (i.e., local dataset size), a suite of domain- and client-specific criteria constitute the basis to compute each local client’s contribution, (ii) the multi-criteria contribution of each device is computed in a prioritized fashion by leveraging a priority-aware aggregation operator used in the field of information retrieval, and (iii) a mechanism is proposed for online-adjustment of the aggregation operator parameters via a local search strategy with backtracking. Extensive experiments on a publicly available dataset indicate the merits of the proposed approach compared to standard FedAvg baseline.;https://link.springer.com/chapter/10.1007/978-3-030-35166-3_34;fGu0LrO7WVEJ
Han, X., Yu, H., & Gu, H. (2019). Visual inspection with federated learning. In Image Analysis and Recognition: 16th International Conference, ICIAR 2019, Waterloo, ON, Canada, August 27–29, 2019, Proceedings, Part II 16 (pp. 52-64). Springer International Publishing.;0_federated_learning_data_privacy;2019;Visual inspection with federated learning;Xu Han, Haoran Yu, Haisong Gu;Image Analysis and Recognition: 16th International Conference, ICIAR 2019, Waterloo, ON, Canada, August 27â€“29, 2019, Proceedings, Part II 16, 52-64, 2019;In industrial applications of AI, challenges for visual inspection include data shortage and security. In this paper, we propose a Federated Learning (FL) framework to address these issues. This method is incorporated with our novel DataonomySM approach which can overcome the limited size of industrial dataset in each inspection task. The models pre-trained in the server can continuously and regularly update, and help each client upgrade its inspection model over time. The FL approach only requires clients to send to the server certain information derived from raw images, and thus does not sacrifice data security. Some preliminary tests are done to examine the workability of the proposed framework. This study is expected to bring the field of automated inspection to a new level of security, reliability, and efficiency, and to unlock significant potentials of deep learning applications.;https://link.springer.com/chapter/10.1007/978-3-030-27272-2_5;vRlXwT3o9ycJ
Deist, T. M., Dankers, F. J., Ojha, P., Marshall, M. S., Janssen, T., Faivre-Finn, C., ... & Dekker, A. (2020). Distributed learning on 20 000+ lung cancer patients–The Personal Health Train. Radiotherapy and Oncology, 144, 189-200.;0_federated_learning_data_privacy;2020;Distributed learning on 20 000+ lung cancer patients–The Personal Health Train;Timo M Deist, Frank JWM Dankers, Priyanka Ojha, M Scott Marshall, Tomas Janssen, Corinne Faivre-Finn, Carlotta Masciocchi, Vincenzo Valentini, Jiazhou Wang, Jiayan Chen, Zhen Zhang, Emiliano Spezi, Mick Button, Joost Jan Nuyttens, René Vernhout, Johan Van Soest, Arthur Jochems, René Monshouwer, Johan Bussink, Gareth Price, Philippe Lambin, Andre Dekker;Radiotherapy and Oncology 144, 189-200, 2020;Access to healthcare data is indispensable for scientific progress and innovation. Sharing healthcare data is time-consuming and notoriously difficult due to privacy and regulatory concerns. The Personal Health Train (PHT) provides a privacy-by-design infrastructure connecting FAIR (Findable, Accessible, Interoperable, Reusable) data sources and allows distributed data analysis and machine learning. Patient data never leaves a healthcare institute.Lung cancer patient-specific databases (tumor …;https://www.sciencedirect.com/science/article/pii/S0167814019334899;Aj8recEZcdgJ
Brisimi, T. S., Chen, R., Mela, T., Olshevsky, A., Paschalidis, I. C., & Shi, W. (2018). Federated learning of predictive models from federated electronic health records. International journal of medical informatics, 112, 59-67.;0_federated_learning_data_privacy;2018;Federated learning of predictive models from federated electronic health records;Theodora S Brisimi, Ruidi Chen, Theofanie Mela, Alex Olshevsky, Ioannis Ch Paschalidis, Wei Shi;International journal of medical informatics 112, 59-67, 2018;In an era of “big data,” computationally efficient and privacy-aware solutions for large-scale machine learning problems become crucial, especially in the healthcare domain, where large amounts of data are stored in different locations and owned by different entities. Past research has been focused on centralized algorithms, which assume the existence of a central data repository (database) which stores and can process the data from all participants. Such an architecture, however, can be impractical when data are not centrally located, it does not scale well to very large datasets, and introduces single-point of failure risks which could compromise the integrity and privacy of the data. Given scores of data widely spread across hospitals/individuals, a decentralized computationally scalable methodology is very much in need.We aim at solving a binary supervised classification problem to predict hospitalizations for cardiac events using a distributed algorithm. We seek to develop a general decentralized optimization framework enabling multiple data holders to collaborate and converge to a common predictive model, without explicitly exchanging raw data.We focus on the soft-margin l1-regularized sparse Support Vector Machine (sSVM) classifier. We develop an iterative cluster Primal Dual Splitting (cPDS) algorithm for solving the large-scale sSVM problem in a decentralized fashion. Such a distributed learning scheme is relevant for multi-institutional collaborations or peer-to-peer applications, allowing the data holders to collaborate, while keeping every participant's data private.We test cPDS on the problem of predicting hospitalizations due to heart diseases within a calendar year based on information in the patients Electronic Health Records prior to that year. cPDS converges faster than centralized methods at the cost of some communication between agents. It also converges faster and with less communication overhead compared to an alternative distributed algorithm. In both cases, it achieves similar prediction accuracy measured by the Area Under the Receiver Operating Characteristic Curve (AUC) of the classifier. We extract important features discovered by the algorithm that are predictive of future hospitalizations, thus providing a way to interpret the classification results and inform prevention efforts.;https://www.sciencedirect.com/science/article/pii/S138650561830008X;gBj9Yi-KPkQJ
Huang, L., Shea, A. L., Qian, H., Masurkar, A., Deng, H., & Liu, D. (2019). Patient clustering improves efficiency of federated machine learning to predict mortality and hospital stay time using distributed electronic medical records. Journal of biomedical informatics, 99, 103291.;0_federated_learning_data_privacy;2019;Patient clustering improves efficiency of federated machine learning to predict mortality and hospital stay time using distributed electronic medical records;Li Huang, Andrew L Shea, Huining Qian, Aditya Masurkar, Hao Deng, Dianbo Liu;Journal of biomedical informatics 99, 103291, 2019;Electronic medical records (EMRs) support the development of machine learning algorithms for predicting disease incidence, patient response to treatment, and other healthcare events. But so far most algorithms have been centralized, taking little account of the decentralized, non-identically independently distributed (non-IID), and privacy-sensitive characteristics of EMRs that can complicate data collection, sharing and learning. To address this challenge, we introduced a community-based federated machine learning (CBFL) algorithm and evaluated it on non-IID ICU EMRs. Our algorithm clustered the distributed data into clinically meaningful communities that captured similar diagnoses and geographical locations, and learnt one model for each community. Throughout the learning process, the data was kept local at hospitals, while locally-computed results were aggregated on a server. Evaluation results show that CBFL outperformed the baseline federated machine learning (FL) algorithm in terms of Area Under the Receiver Operating Characteristic Curve (ROC AUC), Area Under the Precision-Recall Curve (PR AUC), and communication cost between hospitals and the server. Furthermore, communitiesâ€™ performance difference could be explained by how dissimilar one community was to others.;https://www.sciencedirect.com/science/article/pii/S1532046419302102;ihlaZnYVTaMJ
Qian, Y., Hu, L., Chen, J., Guan, X., Hassan, M. M., & Alelaiwi, A. (2019). Privacy-aware service placement for mobile edge computing via federated learning. Information Sciences, 505, 562-570.;7_edge_computing_deep_learning;2019;Privacy-aware service placement for mobile edge computing via federated learning;Yongfeng Qian, Long Hu, Jing Chen, Xin Guan, Mohammad Mehedi Hassan, Abdulhameed Alelaiwi;Information Sciences 505, 562-570, 2019;Mobile edge clouds can offer delay-sensitive services to users by deploying storage and computing resources at the network edge. Considering resource-limited edge server, it is impossible to deploy all services on the edge clouds. Thus, many existing works have addressed the problem of arranging services on mobile edge clouds for better quality of services (QoS) to users. In terms of existing service placement strategies, the historical data of requesting services by users are collected to analyze. However, those historical data belong to users’ sensitive information, without appropriate privacy preserving measures may hinder the implementation of traditional service arrangement strategies. Service placement with considering users’ privacy and limited resources of mobile edge clouds, is an extremely urgent problem to be solved. In this paper, we propose a privacy-aware service placement (PSP) scheme to address the problem of service placement with privacy-awareness in the edge cloud system. The purpose of PSP mechanism is to protect users’ privacy and provide better QoS to users when obtaining services from mobile edge clouds. Specifically, whether service placement on mobile edge clouds or not is modeled as a 0–1 problem. Then, a hybrid service placement algorithm is proposed that combines a centralized greedy algorithm and distributed federated learning. Compared with other optimization schemes, the simulation experiments show that PSP algorithm could not only protect users’ privacy but also meet users’ service demands through mobile edge clouds.;https://www.sciencedirect.com/science/article/pii/S0020025519306814;Pw3ytjy3zWIJ
Verma, D. C., White, G., Julier, S., Pasteris, S., Chakraborty, S., & Cirincione, G. (2019, May). Approaches to address the data skew problem in federated learning. In Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications (Vol. 11006, pp. 542-557). SPIE.;0_federated_learning_data_privacy;2019;Approaches to address the data skew problem in federated learning;Dinesh C Verma, Graham White, Simon Julier, Stepehen Pasteris, Supriyo Chakraborty, Greg Cirincione;Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications 11006, 542-557, 2019;A Federated Learning approach consists of creating an AI model from multiple data sources, without moving large amounts of data across to a central environment. Federated learning can be very useful in a tactical coalition environment, where data can be collected individually by each of the coalition partners, but network connectivity is inadequate to move the data to a central environment. However, such data collected is often dirty and imperfect. The data can be imbalanced, and in some cases, some classes can be completely missing from some coalition partners. Under these conditions, traditional approaches for federated learning can result in models that are highly inaccurate. In this paper, we propose approaches that can result in good machine learning models even in the environments where the data may be highly skewed, and study their performance under different environments.;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11006/110061I/Approaches-to-address-the-data-skew-problem-in-federated-learning/10.1117/12.2519621.short;msq3QwqOr5oJ
Preuveneers, D., Rimmer, V., Tsingenopoulos, I., Spooren, J., Joosen, W., & Ilie-Zudor, E. (2018). Chained anomaly detection models for federated learning: An intrusion detection case study. Applied Sciences, 8(12), 2663.;0_federated_learning_data_privacy;2018;Chained anomaly detection models for federated learning: An intrusion detection case study;Davy Preuveneers, Vera Rimmer, Ilias Tsingenopoulos, Jan Spooren, Wouter Joosen, Elisabeth Ilie-Zudor;Applied Sciences 8 (12), 2663, 2018;The adoption of machine learning and deep learning is on the rise in the cybersecurity domain where these AI methods help strengthen traditional system monitoring and threat detection solutions. However, adversaries too are becoming more effective in concealing malicious behavior amongst large amounts of benign behavior data. To address the increasing time-to-detection of these stealthy attacks, interconnected and federated learning systems can improve the detection of malicious behavior by joining forces and pooling together monitoring data. The major challenge that we address in this work is that in a federated learning setup, an adversary has many more opportunities to poison one of the local machine learning models with malicious training samples, thereby influencing the outcome of the federated learning and evading detection. We present a solution where contributing parties in federated learning can be held accountable and have their model updates audited. We describe a permissioned blockchain-based federated learning method where incremental updates to an anomaly detection machine learning model are chained together on the distributed ledger. By integrating federated learning with blockchain technology, our solution supports the auditing of machine learning models without the necessity to centralize the training data. Experiments with a realistic intrusion detection use case and an autoencoder for anomaly detection illustrate that the increased complexity caused by blockchain technology has a limited performance impact on the federated learning, varying between 5 and 15%, while providing full transparency over the distributed training process of the neural network. Furthermore, our blockchain-based federated learning solution can be generalized and applied to more sophisticated neural network architectures and other use cases.;https://www.mdpi.com/2076-3417/8/12/2663;k6aXsnGd4JEJ
Jiang, D., Song, Y., Tong, Y., Wu, X., Zhao, W., Xu, Q., & Yang, Q. (2019, November). Federated topic modeling. In Proceedings of the 28th ACM international conference on information and knowledge management (pp. 1071-1080).;0_federated_learning_data_privacy;2019;Federated topic modeling;Di Jiang, Yuanfeng Song, Yongxin Tong, Xueyang Wu, Weiwei Zhao, Qian Xu, Qiang Yang;Proceedings of the 28th ACM international conference on information and knowledge management, 1071-1080, 2019;Topic modeling has been widely applied in a variety of industrial applications. Training a high-quality model usually requires massive amount of in-domain data, in order to provide comprehensive co-occurrence information for the model to learn. However, industrial data such as medical or financial records are often proprietary or sensitive, which precludes uploading to data centers. Hence training topic models in industrial scenarios using conventional approaches faces a dilemma: a party (i.e., a company or institute) has to either tolerate data scarcity or sacrifice data privacy. In this paper, we propose a novel framework named Federated Topic Modeling (FTM), in which multiple parties collaboratively train a high-quality topic model by simultaneously alleviating data scarcity and maintaining immune to privacy adversaries. FTM is inspired by federated learning and consists of novel techniques such as private Metropolis Hastings, topic-wise normalization and heterogeneous model integration. We conduct a series of quantitative evaluations to verify the effectiveness of FTM and deploy FTM in an Automatic Speech Recognition (ASR) system to demonstrate its utility in real-life applications. Experimental results verify FTM's superiority over conventional topic modeling.;https://dl.acm.org/doi/abs/10.1145/3357384.3357909;Ef8uySOwIUsJ
Du, W., Zeng, X., Yan, M., & Zhang, M. (2018). Efficient federated learning via variational dropout.;0_federated_learning_data_privacy;2018;Efficient federated learning via variational dropout.;Wei Du, Xiao Zeng, Ming Yan, Mi Zhang;-;As an emerging field, federated learning has recently attracted considerable attention. Compared to distributed learning in the datacenter setting, federated learning has more strict constraints on computate efficiency of the learned model and communication cost during the training process. In this work, we propose an efficient federated learning framework based on variational dropout. Our approach is able to jointly learn a sparse model while reducing the amount of gradients exchanged during the iterative training process. We demonstrate the superior performance of our approach on achieving significant model compression and communication reduction ratios with no accuracy loss.;https://openreview.net/forum?id=BkeAf2CqY7;nAhpwA6D858J
Zhan, Y., Li, P., & Guo, S. (2020, May). Experience-driven computational resource allocation of federated learning by deep reinforcement learning. In 2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS) (pp. 234-243). IEEE.;0_federated_learning_data_privacy;2020;Experience-driven computational resource allocation of federated learning by deep reinforcement learning;Yufeng Zhan, Peng Li, Song Guo;2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS), 234-243, 2020;Federated learning is promising in enabling large-scale machine learning by massive mobile devices without exposing the raw data of users with strong privacy concerns. Existing work of federated learning struggles for accelerating the learning process, but ignores the energy efficiency that is critical for resource-constrained mobile devices. In this paper, we propose to improve the energy efficiency of federated learning by lowering CPU-cycle frequency of mobile devices who are faster in the training group. Since all the devices are synchronized by iterations, the federated learning speed is preserved as long as they complete the training before the slowest device in each iteration. Based on this idea, we formulate an optimization problem aiming to minimize the total system cost that is defined as a weighted sum of training time and energy consumption. Due to the hardness of nonlinear constraints and unawareness of network quality, we design an experience-driven algorithm based on the Deep Reinforcement Learning (DRL), which can converge to the near-optimal solution without knowledge of network quality. Experiments on a small-scale testbed and large-scale simulations are conducted to evaluate our proposed algorithm. The results show that it outperforms the start-of-the-art by 40% at most.;https://ieeexplore.ieee.org/abstract/document/9139873/;j0H7YklK2FkJ
Caldas, S., Smith, V., & Talwalkar, A. (2018, February). Federated kernelized multi-task learning. In Proc. SysML Conf (pp. 1-3).;0_federated_learning_data_privacy;2018;Federated kernelized multi-task learning;Sebastian Caldas, Virginia Smith, Ameet Talwalkar;Proc. SysML Conf, 1-3, 2018;Federated learning poses new statistical and systems challenges in the training of machine learning models over distributed networks of devices. In this ongoing work, we develop a state of the art MTL federated system that bypasses the modelling limitations of previous efforts through the inclusion of non-linear mappings in its formulation. We address the new issues that arise due to this inclusion and that are associated with the particulars of the federated scenario, such as communication and storage costs, introducing this way the first fully practical kernelized federated framework.;https://mlsys.org/Conferences/2019/doc/2018/30.pdf;ojHBmM8Cg1EJ
Thomas, M. A., Abraham, D. S., & Liu, D. (2018). Federated machine learning for translational research.;0_federated_learning_data_privacy;2018;Federated machine learning for translational research.;Manoj A Thomas, Diya Suzanne Abraham, Dapeng Liu;-;Translational research (TR) is the harnessing of knowledge from basic science and clinical research to advance healthcare. As a sister discipline, Translational informatics (TI) concerns the application of informatics theories, methods, and frameworks to TR. This research builds upon TR concepts, and aims to bring advances in machine learning (ML) and data analytics for improving clinical decision support. A federated machine learning (FML) architecture is proposed to aggregate multiple sources, and intermediate data analytic processes and products to output high quality knowledge discovery and decision making. The proposed architecture is evaluated for its operational performance based on three propositions, and a case for clinical decision support in the prediction of adult Sepsis is presented. Our research illustrates how IS scholarship may provide valuable contributions to the advancement of TI.;https://aisel.aisnet.org/amcis2018/Health/Presentations/34/;IlY3-T11NZMJ
Mo, F., & Haddadi, H. (2019, March). Efficient and private federated learning using tee. In Proc. EuroSys Conf., Dresden, Germany.;0_federated_learning_data_privacy;2019;Efficient and private federated learning using tee;Fan Mo, Hamed Haddadi;Proc. EuroSys Conf., Dresden, Germany, 2019;Background Federated Learning has received great attention since it enables edge devices to collaboratively train shared or personal models while keeping the raw training data local [3]. However, recent works have demonstrated that private information can leak from gradients present in shared models [4]. Due to the limited resources of edge devices, they do not support complicated defence methods against inference attacks. Therefore, guaranteeing data privacy during federated learning without compromising accuracy and efficiency is of great importance. Recently, Trusted Execution Environments (TEE) have been used for privacy-preserving machine learning. By allocating private regions of computing resources (eg memory), it is possible to provide both hardware and software isolation. The TEE provides lower overhead and higher privacy compared to traditional software protections such as homomorphic encryption. After [5] first deployed deep neural networks (DNN) in a real TEE using Intel SGX technique, several studies added outsource support or multiple memory blacks to improve the efficiency of DNNs [2]. However, all these studies used advanced machines with Intel SGX as a setup to simulate high-performance computation, a solution that is hard to be extended in edge computing. For federated learning on edge devices, such as mobile phones or small form factor platforms such as the Raspberry Pi, only limited computing resources are accessible. Leveraging a TEE implementation from ARM, TrustZone, we propose an efficient and private federated learning framework at edge computing. Differential privacy and data obliviousness will be used to enhance the privacy protection. FrameworkThe proposed framework trains DNN models in the Trust-Zone to prevent the privacy leakage from model parameters. The layers are separated [6], and the DNN model F () is partitioned as two parts. The untrusted part FU () is running in TrustZone’s normal execution mode-the client application (CA). The trusted part FT () is running in TrustZone’s trusted execution mode-the trusted application (TA). We enhance the protection of the trusted part of models using data obliviousness. Side-channel attacks can track the sequence or patterns of operations such as the time, power, and memory address, and monitor trusted executions. To defend against tracking, data-oblivious algorithms relocate the memory address or obfuscate the path to access the memory address of the recently used value. Previous research [5] developed basic oblivious primitives such as comparisons;https://mofanv.github.io/papers/abstract_eurosys_2019.pdf;WCrLS3dCUHgJ
Wei, X., Li, Q., Liu, Y., Yu, H., Chen, T., & Yang, Q. (2019, September). Multi-Agent Visualization for Explaining Federated Learning. In IJCAI (pp. 6572-6574).;0_federated_learning_data_privacy;2019;Multi-Agent Visualization for Explaining Federated Learning;Xiguang Wei, Quan Li, Yang Liu, Han Yu, Tianjian Chen, Qiang Yang;IJCAI, 6572-6574, 2019;As an alternative decentralized training approach, Federated Learning enables distributed agents to collaboratively learn a machine learning model while keeping personal/private information on local devices. However, one significant issue of this framework is the lack of transparency, thus obscuring understanding of the working mechanism of Federated Learning systems. This paper proposes a multi-agent visualization system that illustrates what is Federated Learning and how it supports multi-agents coordination. To be specific, it allows users to participate in the Federated Learning empowered multi-agent coordination. The input and output of Federated Learning are visualized simultaneously, which provides an intuitive explanation of Federated Learning for users in order to help them gain deeper understanding of the technology.;https://www.ijcai.org/proceedings/2019/0960.pdf;OvEKWZY4aY8J
Liu, Y., Kang, Y., Li, L., Zhang, X., Cheng, Y., Chen, T., ... & Yang, Q. (2019). A communication efficient vertical federated learning framework. Scanning Electron Microsc Meet at.;0_federated_learning_data_privacy;2019;A communication efficient vertical federated learning framework;Yang Liu, Yan Kang, Liping Li, Xinwei Zhang, Yong Cheng, Tianjian Chen, Mingyi Hong, Qiang Yang;Scanning Electron Microsc Meet at, 2019;One critical challenge for applying today’s Artificial Intelligence (AI) technologies to real-world applications is the common existence of data silos across different organizations. Due to legal, privacy and other practical constraints, data from different organizations cannot be easily integrated. Federated learning (FL), especially the vertical FL (VFL), allows multiple parties having different sets of attributes about the same user collaboratively build models while preserving user privacy. However, communication overhead is a principal bottleneck since the …;;OaOcNQfrWSEJ
Bakopoulou, E., Tillman, B., & Markopoulou, A. (2021). Fedpacket: A federated learning approach to mobile packet classification. IEEE Transactions on Mobile Computing, 21(10), 3609-3628.;0_federated_learning_data_privacy;2021;Fedpacket: A federated learning approach to mobile packet classification;Evita Bakopoulou, Balint Tillman, Athina Markopoulou;IEEE Transactions on Mobile Computing 21 (10), 3609-3628, 2021;"In order to improve mobile data transparency, various approaches have been proposed to inspect network traffic generated by mobile devices and detect exposure of personally identifiable information (PII), ad requests, etc. State-of-the-art approaches use features extracted from HTTP packets and train classifiers in a centralized way: users collect and label network packets on their mobile devices, then upload data to a central server; the server uses the data contributed by all users to train a packet classifier. However, training datasets from network traffic collected on user devices may contain sensitive information that users may not want to upload. In this article, we propose a federated learning approach to mobile packet classification, which enables devices to collaboratively train a global model, without uploading the training data collected on devices. We apply our framework to two packet classification tasks (i.e., to predict PII exposure or ad requests in individual packets) and we demonstrate its effectiveness in terms of classification performance, communication and computation cost, using three real-world datasets. Methodological challenges we address in the process include model and feature selection, as well as tuning the federated learning parameters specifically for our packet classification tasks. We also discuss privacy limitations and mitigation approaches.";https://ieeexplore.ieee.org/abstract/document/9352526/;09eAmlXJWy4J
Ryffel, T., Trask, A., Dahl, M., Wagner, B., Mancuso, J., Rueckert, D., & Passerat-Palmbach, J. (2018). A generic framework for privacy preserving deep learning. arXiv preprint arXiv:1811.04017.;0_federated_learning_data_privacy;2018;A generic framework for privacy preserving deep learning;Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel Rueckert, Jonathan Passerat-Palmbach;arXiv preprint arXiv:1811.04017, 2018;We detail a new framework for privacy preserving deep learning and discuss its assets. The framework puts a premium on ownership and secure processing of data and introduces a valuable representation based on chains of commands and tensors. This abstraction allows one to implement complex privacy preserving constructs such as Federated Learning, Secure Multiparty Computation, and Differential Privacy while still exposing a familiar deep learning API to the end-user. We report early results on the Boston Housing and Pima Indian Diabetes datasets. While the privacy features apart from Differential Privacy do not impact the prediction accuracy, the current implementation of the framework introduces a significant overhead in performance, which will be addressed at a later stage of the development. We believe this work is an important milestone introducing the first reliable, general framework for privacy preserving deep learning.;https://arxiv.org/abs/1811.04017;7LNHwMBCucsJ
Chen, M., Yang, Z., Saad, W., Yin, C., Poor, H. V., & Cui, S. (2020). A joint learning and communications framework for federated learning over wireless networks. IEEE Transactions on Wireless Communications, 20(1), 269-283.;0_federated_learning_data_privacy;2020;A joint learning and communications framework for federated learning over wireless networks;Mingzhe Chen, Zhaohui Yang, Walid Saad, Changchuan Yin, H Vincent Poor, Shuguang Cui;IEEE Transactions on Wireless Communications 20 (1), 269-283, 2020;In this article, the problem of training federated learning (FL) algorithms over a realistic wireless network is studied. In the considered model, wireless users execute an FL algorithm while training their local FL models using their own data and transmitting the trained local FL models to a base station (BS) that generates a global FL model and sends the model back to the users. Since all training parameters are transmitted over wireless links, the quality of training is affected by wireless factors such as packet errors and the availability of wireless resources. Meanwhile, due to the limited wireless bandwidth, the BS needs to select an appropriate subset of users to execute the FL algorithm so as to build a global FL model accurately. This joint learning, wireless resource allocation, and user selection problem is formulated as an optimization problem whose goal is to minimize an FL loss function that captures the performance of the FL algorithm. To seek the solution, a closed-form expression for the expected convergence rate of the FL algorithm is first derived to quantify the impact of wireless factors on FL. Then, based on the expected convergence rate of the FL algorithm, the optimal transmit power for each user is derived, under a given user selection and uplink resource block (RB) allocation scheme. Finally, the user selection and uplink RB allocation is optimized so as to minimize the FL loss function. Simulation results show that the proposed joint federated learning and communication framework can improve the identification accuracy by up to 1.4%, 3.5% and 4.1%, respectively, compared to: 1) An optimal user selection algorithm with random resource allocation, 2) a standard FL algorithm with random user selection and resource allocation, and 3) a wireless optimization algorithm that minimizes the sum packet error rates of all users while being agnostic to the FL parameters.;https://ieeexplore.ieee.org/abstract/document/9210812/;mTM03elDYeIJ
Yang, K., Fan, T., Chen, T., Shi, Y., & Yang, Q. (2019). A quasi-newton method based vertical federated learning framework for logistic regression. arXiv preprint arXiv:1912.00513.;0_federated_learning_data_privacy;2019;A quasi-newton method based vertical federated learning framework for logistic regression;Kai Yang, Tao Fan, Tianjian Chen, Yuanming Shi, Qiang Yang;arXiv preprint arXiv:1912.00513, 2019;Data privacy and security becomes a major concern in building machine learning models from different data providers. Federated learning shows promise by leaving data at providers locally and exchanging encrypted information. This paper studies the vertical federated learning structure for logistic regression where the data sets at two parties have the same sample IDs but own disjoint subsets of features. Existing frameworks adopt the first-order stochastic gradient descent algorithm, which requires large number of communication rounds. To address the communication challenge, we propose a quasi-Newton method based vertical federated learning framework for logistic regression under the additively homomorphic encryption scheme. Our approach can considerably reduce the number of communication rounds with a little additional communication cost per round. Numerical results demonstrate the advantages of our approach over the first-order method.;https://arxiv.org/abs/1912.00513;u0PZsfdJF4wJ
Li, S., Cheng, Y., Liu, Y., Wang, W., & Chen, T. (2019). Abnormal client behavior detection in federated learning. arXiv preprint arXiv:1910.09933.;0_federated_learning_data_privacy;2019;Abnormal client behavior detection in federated learning;Suyi Li, Yong Cheng, Yang Liu, Wei Wang, Tianjian Chen;arXiv preprint arXiv:1910.09933, 2019;In federated learning systems, clients are autonomous in that their behaviors are not fully governed by the server. Consequently, a client may intentionally or unintentionally deviate from the prescribed course of federated model training, resulting in abnormal behaviors, such as turning into a malicious attacker or a malfunctioning client. Timely detecting those anomalous clients is therefore critical to minimize their adverse impacts. In this work, we propose to detect anomalous clients at the server side. In particular, we generate low-dimensional surrogates of model weight vectors and use them to perform anomaly detection. We evaluate our solution through experiments on image classification model training over the FEMNIST dataset. Experimental results show that the proposed detection-based approach significantly outperforms the conventional defense-based methods.;https://arxiv.org/abs/1910.09933;1Mu3wRyhiJQJ
Qian, J., Sengupta, S., & Hansen, L. K. (2019). Active learning solution on distributed edge computing. arXiv preprint arXiv:1906.10718.;7_edge_computing_deep_learning;2019;Active learning solution on distributed edge computing;Jia Qian, Sayantan Sengupta, Lars Kai Hansen;arXiv preprint arXiv:1906.10718, 2019;"Industry 4.0 becomes possible through the convergence between Operational and Information Technologies. All the requirements to realize the convergence is integrated on the Fog Platform. Fog Platform is introduced between the cloud server and edge devices when the unprecedented generation of data causes the burden of the cloud server, leading the ineligible latency. In this new paradigm, we divide the computation tasks and push it down to edge devices. Furthermore, local computing (at edge side) may improve privacy and trust. To address these problems, we present a new method, in which we decompose the data aggregation and processing, by dividing them between edge devices and fog nodes intelligently. We apply active learning on edge devices; and federated learning on the fog node which significantly reduces the data samples to train the model as well as the communication cost. To show the effectiveness of the proposed method, we implemented and evaluated its performance for an image classification task. In addition, we consider two settings: massively distributed and non-massively distributed and offer the corresponding solutions.";https://arxiv.org/abs/1906.10718;yO-deuF4lDgJ
Yang, H. H., Arafa, A., Quek, T. Q., & Poor, H. V. (2020, May). Age-based scheduling policy for federated learning in mobile edge networks. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 8743-8747). IEEE.;0_federated_learning_data_privacy;2020;Age-based scheduling policy for federated learning in mobile edge networks;Howard H Yang, Ahmed Arafa, Tony QS Quek, H Vincent Poor;ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 8743-8747, 2020;Federated learning (FL) is a machine learning model that preserves data privacy in the training process. Specifically, FL brings the model directly to the user equipments (UEs) for local training, where an edge server periodically collects the trained parameters to produce an improved model and sends it back to the UEs. However, since communication usually occurs through a limited spectrum, only a portion of the UEs can update their parameters upon each global aggregation. As such, new scheduling algorithms have to be engineered to facilitate the full implementation of FL. In this paper, based on a metric termed the age of update (AoU), we propose a scheduling policy by jointly accounting for the staleness of the received parameters and the instantaneous channel qualities to improve the running efficiency of FL. The proposed algorithm has low complexity and its effectiveness is demonstrated by Monte Carlo simulations.;https://ieeexplore.ieee.org/abstract/document/9053740/;6POduarC-zMJ
Mohri, M., Sivek, G., & Suresh, A. T. (2019, May). Agnostic federated learning. In International Conference on Machine Learning (pp. 4615-4625). PMLR.;0_federated_learning_data_privacy;2019;Agnostic federated learning;Mehryar Mohri, Gary Sivek, Ananda Theertha Suresh;International Conference on Machine Learning, 4615-4625, 2019;A key learning scenario in large-scale applications is that of federated learning, where a centralized model is trained based on data originating from a large number of clients. We argue that, with the existing training and inference, federated models can be biased towards different clients. Instead, we propose a new framework of agnostic federated learning, where the centralized model is optimized for any target distribution formed by a mixture of the client distributions. We further show that this framework naturally yields a notion of fairness. We present data-dependent Rademacher complexity guarantees for learning with this objective, which guide the definition of an algorithm for agnostic federated learning. We also give a fast stochastic optimization algorithm for solving the corresponding optimization problem, for which we prove convergence bounds, assuming a convex loss function and a convex hypothesis set. We further empirically demonstrate the benefits of our approach in several datasets. Beyond federated learning, our framework and algorithm can be of interest to other learning scenarios such as cloud computing, domain adaptation, drifting, and other contexts where the training and test distributions do not coincide.;https://proceedings.mlr.press/v97/mohri19a.html;KVKkGkeC3xUJ
Yang, T., Andrew, G., Eichner, H., Sun, H., Li, W., Kong, N., ... & Beaufays, F. (2018). Applied federated learning: Improving google keyboard query suggestions. arXiv preprint arXiv:1812.02903.;0_federated_learning_data_privacy;2018;Applied federated learning: Improving google keyboard query suggestions;Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage, FranÃ§oise Beaufays;arXiv preprint arXiv:1812.02903, 2018;Federated learning is a distributed form of machine learning where both the training data and model training are decentralized. In this paper, we use federated learning in a commercial, global-scale setting to train, evaluate and deploy a model to improve virtual keyboard search suggestion quality without direct access to the underlying user data. We describe our observations in federated training, compare metrics to live deployments, and present resulting quality increases. In whole, we demonstrate how federated learning can be applied end-to-end to both improve user experiences and enhance user privacy.;https://arxiv.org/abs/1812.02903;pES4tTpMc30J
Duan, M., Liu, D., Chen, X., Tan, Y., Ren, J., Qiao, L., & Liang, L. (2019, November). Astraea: Self-balancing federated learning for improving classification accuracy of mobile deep learning applications. In 2019 IEEE 37th international conference on computer design (ICCD) (pp. 246-254). IEEE.;0_federated_learning_data_privacy;2019;Astraea: Self-balancing federated learning for improving classification accuracy of mobile deep learning applications;Moming Duan, Duo Liu, Xianzhang Chen, Yujuan Tan, Jinting Ren, Lei Qiao, Liang Liang;2019 IEEE 37th international conference on computer design (ICCD), 246-254, 2019;Federated learning (FL) is a distributed deep learning method which enables multiple participants, such as mobile phones and IoT devices, to contribute a neural network model while their private training data remains in local devices. This distributed approach is promising in the edge computing system where have a large corpus of decentralized data and require high privacy. However, unlike the common training dataset, the data distribution of the edge computing system is imbalanced which will introduce biases in the model training and cause a decrease in accuracy of federated learning applications. In this paper, we demonstrate that the imbalanced distributed training data will cause accuracy degradation in FL. To counter this problem, we build a self-balancing federated learning framework call Astraea, which alleviates the imbalances by 1) Global data distribution based data augmentation, and 2) Mediator based multi-client rescheduling. The proposed framework relieves global imbalance by runtime data augmentation, and for averaging the local imbalance, it creates the mediator to reschedule the training of clients based on Kullback-Leibler divergence (KLD) of their data distribution. Compared with FedAvg, the state-of-the-art FL algorithm, Astraea shows +5.59% and +5.89% improvement of top-1 accuracy on the imbalanced EMNIST and imbalanced CINIC-10 datasets, respectively. Meanwhile, the communication traffic of Astraea can be 92% lower than that of FedAvg.;https://ieeexplore.ieee.org/abstract/document/8988732/;GwUhGjCgFGoJ
Xie, C., Koyejo, S., & Gupta, I. (2019). Asynchronous federated optimization. arXiv preprint arXiv:1903.03934.;0_federated_learning_data_privacy;2019;Asynchronous federated optimization;Cong Xie, Sanmi Koyejo, Indranil Gupta;arXiv preprint arXiv:1903.03934, 2019;Federated learning enables training on a massive number of edge devices. To improve flexibility and scalability, we propose a new asynchronous federated optimization algorithm. We prove that the proposed approach has near-linear convergence to a global optimum, for both strongly convex and a restricted family of non-convex problems. Empirical results show that the proposed algorithm converges quickly and tolerates staleness in various applications.;https://arxiv.org/abs/1903.03934;MQRoGJ5rXUEJ
Chen, Y., Ning, Y., Slawski, M., & Rangwala, H. (2020, December). Asynchronous online federated learning for edge devices with non-iid data. In 2020 IEEE International Conference on Big Data (Big Data) (pp. 15-24). IEEE.;0_federated_learning_data_privacy;2020;Asynchronous online federated learning for edge devices with non-iid data;Yujing Chen, Yue Ning, Martin Slawski, Huzefa Rangwala;2020 IEEE International Conference on Big Data (Big Data), 15-24, 2020;Federated learning (FL) is a machine learning paradigm where a shared central model is learned across distributed devices while the training data remains on these devices. Federated Averaging (FedAvg) is the leading optimization method for training non-convex models in this setting with a synchronized protocol. However, the assumptions made by FedAvg are not realistic given the heterogeneity of devices. First, the volume and distribution of collected data vary in the training process due to different sampling rates of edge devices. Second, the edge devices themselves also vary in latency and system configurations, such as memory, processor speed, and power requirements. This leads to vastly different computation times. Third, availability issues at edge devices can lead to a lack of contribution from specific edge devices to the federated model. In this paper, we present an Asynchronous Online Federated Learning (ASO-Fed) framework, where the edge devices perform online learning with continuous streaming local data and a central server aggregates model parameters from clients. Our framework updates the central model in an asynchronous manner to tackle the challenges associated with both varying computational loads at heterogeneous edge devices and edge devices that lag behind or dropout. We perform extensive experiments on a benchmark image dataset and three real-world datasets with non-IID streaming data. The results demonstrate ASO-Fed converging fast and maintaining good prediction performance.;https://ieeexplore.ieee.org/abstract/document/9378161/;8aTjlbcQNSwJ
Yurochkin, M., Agarwal, M., Ghosh, S., Greenewald, K., Hoang, N., & Khazaeni, Y. (2019, May). Bayesian nonparametric federated learning of neural networks. In International conference on machine learning (pp. 7252-7261). PMLR.;0_federated_learning_data_privacy;2019;Bayesian nonparametric federated learning of neural networks;Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, Yasaman Khazaeni;International conference on machine learning, 7252-7261, 2019;In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited. We develop a Bayesian nonparametric framework for federated learning with neural networks. Each data server is assumed to provide local neural network weights, which are modeled through our framework. We then develop an inference approach that allows us to synthesize a more expressive global network without additional supervision, data pooling and with as few as a single communication round. We then demonstrate the efficacy of our approach on federated learning problems simulated from two popular image classification datasets.;https://proceedings.mlr.press/v97/yurochkin19a.html;yZS8hKkNFckJ
Shayan, M., Fung, C., Yoon, C. J., & Beschastnikh, I. (2018). Biscotti: A ledger for private and secure peer-to-peer machine learning. arXiv preprint arXiv:1811.09904.;0_federated_learning_data_privacy;2018;Biscotti: A ledger for private and secure peer-to-peer machine learning;Muhammad Shayan, Clement Fung, Chris JM Yoon, Ivan Beschastnikh;arXiv preprint arXiv:1811.09904, 2018;Federated Learning is the current state of the art in supporting secure multi-party machine learning (ML): data is maintained on the owner's device and the updates to the model are aggregated through a secure protocol. However, this process assumes a trusted centralized infrastructure for coordination, and clients must trust that the central service does not use the byproducts of client data. In addition to this, a group of malicious clients could also harm the performance of the model by carrying out a poisoning attack. As a response, we propose Biscotti: a fully decentralized peer to peer (P2P) approach to multi-party ML, which uses blockchain and cryptographic primitives to coordinate a privacy-preserving ML process between peering clients. Our evaluation demonstrates that Biscotti is scalable, fault tolerant, and defends against known attacks. For example, Biscotti is able to protect the privacy of an individual client's update and the performance of the global model at scale when 30% of adversaries are trying to poison the model. The implementation can be found at: https://github.com/DistributedML/Biscotti;https://arxiv.org/abs/1811.09904;uEmvugEO_8QJ
Liu, Y., Ma, Z., Liu, X., Ma, S., Nepal, S., & Deng, R. (2019). Boosting privately: Privacy-preserving federated extreme boosting for mobile crowdsensing. arXiv preprint arXiv:1907.10218.;0_federated_learning_data_privacy;2019;Boosting privately: Privacy-preserving federated extreme boosting for mobile crowdsensing;Yang Liu, Zhuo Ma, Ximeng Liu, Siqi Ma, Surya Nepal, Robert Deng;arXiv preprint arXiv:1907.10218, 2019;Recently, Google and other 24 institutions proposed a series of open challenges towards federated learning (FL), which include application expansion and homomorphic encryption (HE). The former aims to expand the applicable machine learning models of FL. The latter focuses on who holds the secret key when applying HE to FL. For the naive HE scheme, the server is set to master the secret key. Such a setting causes a serious problem that if the server does not conduct aggregation before decryption, a chance is left for the server to access the user's update. Inspired by the two challenges, we propose FedXGB, a federated extreme gradient boosting (XGBoost) scheme supporting forced aggregation. FedXGB mainly achieves the following two breakthroughs. First, FedXGB involves a new HE based secure aggregation scheme for FL. By combining the advantages of secret sharing and homomorphic encryption, the algorithm can solve the second challenge mentioned above, and is robust to the user dropout. Then, FedXGB extends FL to a new machine learning model by applying the secure aggregation scheme to the classification and regression tree building of XGBoost. Moreover, we conduct a comprehensive theoretical analysis and extensive experiments to evaluate the security, effectiveness, and efficiency of FedXGB. The results indicate that FedXGB achieves less than 1% accuracy loss compared with the original XGBoost, and can provide about 23.9% runtime and 33.3% communication reduction for HE based model update aggregation of FL.;https://arxiv.org/abs/1907.10218;QbD8ngXHqjUJ
Roy, A. G., Siddiqui, S., Pölsterl, S., Navab, N., & Wachinger, C. (2019). Braintorrent: A peer-to-peer environment for decentralized federated learning. arXiv preprint arXiv:1905.06731.;0_federated_learning_data_privacy;2019;Braintorrent: A peer-to-peer environment for decentralized federated learning;Abhijit Guha Roy, Shayan Siddiqui, Sebastian PÃ¶lsterl, Nassir Navab, Christian Wachinger;arXiv preprint arXiv:1905.06731, 2019;Access to sufficient annotated data is a common challenge in training deep neural networks on medical images. As annotating data is expensive and time-consuming, it is difficult for an individual medical center to reach large enough sample sizes to build their own, personalized models. As an alternative, data from all centers could be pooled to train a centralized model that everyone can use. However, such a strategy is often infeasible due to the privacy-sensitive nature of medical data. Recently, federated learning (FL) has been introduced to collaboratively learn a shared prediction model across centers without the need for sharing data. In FL, clients are locally training models on site-specific datasets for a few epochs and then sharing their model weights with a central server, which orchestrates the overall training process. Importantly, the sharing of models does not compromise patient privacy. A disadvantage of FL is the dependence on a central server, which requires all clients to agree on one trusted central body, and whose failure would disrupt the training process of all clients. In this paper, we introduce BrainTorrent, a new FL framework without a central server, particularly targeted towards medical applications. BrainTorrent presents a highly dynamic peer-to-peer environment, where all centers directly interact with each other without depending on a central body. We demonstrate the overall effectiveness of FL for the challenging task of whole brain segmentation and observe that the proposed server-less BrainTorrent approach does not only outperform the traditional server-based one but reaches a similar performance to a model trained on pooled data.;https://arxiv.org/abs/1905.06731;0Tfj4903I7IJ
Vu, T. T., Ngo, D. T., Tran, N. H., Ngo, H. Q., Dao, M. N., & Middleton, R. H. (2020). Cell-free massive MIMO for wireless federated learning. IEEE Transactions on Wireless Communications, 19(10), 6377-6392.;0_federated_learning_data_privacy;2020;Cell-free massive MIMO for wireless federated learning;Tung Thanh Vu, Duy Trong Ngo, Nguyen H Tran, Hien Quoc Ngo, Minh Ngoc Dao, Richard H Middleton;IEEE Transactions on Wireless Communications 19 (10), 6377-6392, 2020;This paper proposes a novel scheme for cell-free massive multiple-input multiple-output (CFmMIMO) networks to support any federated learning (FL) framework. This scheme allows each instead of all the iterations of the FL framework to happen in a large-scale coherence time to guarantee a stable operation of an FL process. To show how to optimize the FL performance using this proposed scheme, we consider an existing FL framework as an example and target FL training time minimization for this framework. An optimization problem is then formulated to jointly optimize the local accuracy, transmit power, data rate, and users' processing frequency. This mixed-timescale stochastic nonconvex problem captures the complex interactions among the training time, and transmission and computation of training updates of one FL process. By employing the online successive convex approximation approach, we develop a new algorithm to solve the formulated problem with proven convergence to the neighbourhood of its stationary points. Our numerical results confirm that the presented joint design reduces the training time by up to 55% over baseline approaches. They also show that CFmMIMO here requires the lowest training time for FL processes compared with cell-free time-division multiple access massive MIMO and collocated massive MIMO.;https://ieeexplore.ieee.org/abstract/document/9124715/;ubTiio1bQSgJ
He, C., Tan, C., Tang, H., Qiu, S., & Liu, J. (2019). Central server free federated learning over single-sided trust social networks. arXiv preprint arXiv:1910.04956.;0_federated_learning_data_privacy;2019;Central server free federated learning over single-sided trust social networks;Chaoyang He, Conghui Tan, Hanlin Tang, Shuang Qiu, Ji Liu;arXiv preprint arXiv:1910.04956, 2019;Federated learning has become increasingly important for modern machine learning, especially for data privacy-sensitive scenarios. Existing federated learning mostly adopts the central server-based architecture or centralized architecture. However, in many social network scenarios, centralized federated learning is not applicable (e.g., a central agent or server connecting all users may not exist, or the communication cost to the central server is not affordable). In this paper, we consider a generic setting: 1) the central server may not exist, and 2) the social network is unidirectional or of single-sided trust (i.e., user A trusts user B but user B may not trust user A). We propose a central server free federated learning algorithm, named Online Push-Sum (OPS) method, to handle this challenging but generic scenario. A rigorous regret analysis is also provided, which shows very interesting results on how users can benefit from communication with trusted users in the federated learning scenario. This work builds upon the fundamental algorithm framework and theoretical guarantees for federated learning in the generic social network scenario.;https://arxiv.org/abs/1910.04956;5WKfH3BilkYJ
Liu, L., Zhang, J., Song, S. H., & Letaief, K. B. (2020, June). Client-edge-cloud hierarchical federated learning. In ICC 2020-2020 IEEE International Conference on Communications (ICC) (pp. 1-6). IEEE.;0_federated_learning_data_privacy;2020;Client-edge-cloud hierarchical federated learning;Lumin Liu, Jun Zhang, SH Song, Khaled B Letaief;ICC 2020-2020 IEEE International Conference on Communications (ICC), 1-6, 2020;Federated Learning is a collaborative machine learning framework to train a deep learning model without accessing clientsâ€™ private data. Previous works assume one central parameter server either at the cloud or at the edge. The cloud server can access more data but with excessive communication overhead and long latency, while the edge server enjoys more efficient communications with the clients. To combine their advantages, we propose a client-edge-cloud hierarchical Federated Learning system, supported with a HierFAVG algorithm that allows multiple edge servers to perform partial model aggregation. In this way, the model can be trained faster and better communication-computation trade-offs can be achieved. Convergence analysis is provided for HierFAVG and the effects of key parameters are also investigated, which lead to qualitative design guidelines. Empirical experiments verify the analysis and demonstrate the benefits of this hierarchical architecture in different data distribution scenarios. Particularly, it is shown that by introducing the intermediate edge servers, the model training time and the energy consumption of the end devices can be simultaneously reduced compared to cloud-based Federated Learning.;https://ieeexplore.ieee.org/abstract/document/9148862/;tZI2W8ZGIaIJ
Sattler, F., Müller, K. R., & Samek, W. (2020). Clustered federated learning: Model-agnostic distributed multitask optimization under privacy constraints. IEEE transactions on neural networks and learning systems, 32(8), 3710-3722.;0_federated_learning_data_privacy;2020;Clustered federated learning: Model-agnostic distributed multitask optimization under privacy constraints;Felix Sattler, Klaus-Robert MÃ¼ller, Wojciech Samek;IEEE transactions on neural networks and learning systems 32 (8), 3710-3722, 2020;Federated learning (FL) is currently the most widely adopted framework for collaborative training of (deep) machine learning models under privacy constraints. Albeit its popularity, it has been observed that FL yields suboptimal results if the local clients' data distributions diverge. To address this issue, we present clustered FL (CFL), a novel federated multitask learning (FMTL) framework, which exploits geometric properties of the FL loss surface to group the client population into clusters with jointly trainable data distributions. In contrast to existing FMTL approaches, CFL does not require any modifications to the FL communication protocol to be made, is applicable to general nonconvex objectives (in particular, deep neural networks), does not require the number of clusters to be known a priori, and comes with strong mathematical guarantees on the clustering quality. CFL is flexible enough to handle client populations that vary over time and can be implemented in a privacy-preserving way. As clustering is only performed after FL has converged to a stationary point, CFL can be viewed as a postprocessing method that will always achieve greater or equal performance than conventional FL by allowing clients to arrive at more specialized models. We verify our theoretical analysis in experiments with deep convolutional and recurrent neural networks on commonly used FL data sets.;https://ieeexplore.ieee.org/abstract/document/9174890/;7ReNgLbaLAwJ
McMahan, B., Moore, E., Ramage, D., Hampson, S., & y Arcas, B. A. (2017, April). Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics (pp. 1273-1282). PMLR.;0_federated_learning_data_privacy;2017;Communication-efficient learning of deep networks from decentralized data;Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Aguera y Arcas;Artificial intelligence and statistics, 1273-1282, 2017;Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.;https://proceedings.mlr.press/v54/mcmahan17a?ref=https://githubhelp.com;qw8WjTtZ6lwJ
Jeong, E., Oh, S., Kim, H., Park, J., Bennis, M., & Kim, S. L. (2018). Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data. arXiv preprint arXiv:1811.11479.;0_federated_learning_data_privacy;2018;Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data;Eunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park, Mehdi Bennis, Seong-Lyun Kim;arXiv preprint arXiv:1811.11479, 2018;On-device machine learning (ML) enables the training process to exploit a massive amount of user-generated private data samples. To enjoy this benefit, inter-device communication overhead should be minimized. With this end, we propose federated distillation (FD), a distributed model training algorithm whose communication payload size is much smaller than a benchmark scheme, federated learning (FL), particularly when the model size is large. Moreover, user-generated data samples are likely to become non-IID across devices, which commonly degrades the performance compared to the case with an IID dataset. To cope with this, we propose federated augmentation (FAug), where each device collectively trains a generative model, and thereby augments its local data towards yielding an IID dataset. Empirical studies demonstrate that FD with FAug yields around 26x less communication overhead while achieving 95-98% test accuracy compared to FL.;https://arxiv.org/abs/1811.11479;s7fHCXaEsa8J
Fung, C., Koerner, J., Grant, S., & Beschastnikh, I. (2018). Dancing in the dark: Private multi-party machine learning in an untrusted setting. arXiv preprint arXiv:1811.09712.;0_federated_learning_data_privacy;2018;Dancing in the dark: Private multi-party machine learning in an untrusted setting;Clement Fung, Jamie Koerner, Stewart Grant, Ivan Beschastnikh;arXiv preprint arXiv:1811.09712, 2018;Distributed machine learning (ML) systems today use an unsophisticated threat model: data sources must trust a central ML process. We propose a brokered learning abstraction that allows data sources to contribute towards a globally-shared model with provable privacy guarantees in an untrusted setting. We realize this abstraction by building on federated learning, the state of the art in multi-party ML, to construct TorMentor: an anonymous hidden service that supports private multi-party ML. We define a new threat model by characterizing, developing and evaluating new attacks in the brokered learning setting, along with new defenses for these attacks. We show that TorMentor effectively protects data providers against known ML attacks while providing them with a tunable trade-off between model accuracy and privacy. We evaluate TorMentor with local and geo-distributed deployments on Azure/Tor. In an experiment with 200 clients and 14 MB of data per client, our prototype trained a logistic regression model using stochastic gradient descent in 65s. Code is available at: https://github.com/DistributedML/TorML;https://arxiv.org/abs/1811.09712;RI0J7tS2RBwJ
Lalitha, A., Wang, X., Kilinc, O., Lu, Y., Javidi, T., & Koushanfar, F. (2019). Decentralized Bayesian learning over graphs. arXiv preprint arXiv:1905.10466.;0_federated_learning_data_privacy;2019;Decentralized Bayesian learning over graphs;Anusha Lalitha, Xinghan Wang, Osman Kilinc, Yongxi Lu, Tara Javidi, Farinaz Koushanfar;arXiv preprint arXiv:1905.10466, 2019;"We propose a decentralized learning algorithm over a general social network. The algorithm leaves the training data distributed on the mobile devices while utilizing a peer to peer model aggregation method. The proposed algorithm allows agents with local data to learn a shared model explaining the global training data in a decentralized fashion. The proposed algorithm can be viewed as a Bayesian and peer-to-peer variant of federated learning in which each agent keeps a ""posterior probability distribution"" over a global model parameters. The agent update its ""posterior"" based on 1) the local training data and 2) the asynchronous communication and model aggregation with their 1-hop neighbors. This Bayesian formulation allows for a systematic treatment of model aggregation over any arbitrary connected graph. Furthermore, it provides strong analytic guarantees on converge in the realizable case as well as a closed form characterization of the rate of convergence. We also show that our methodology can be combined with efficient Bayesian inference techniques to train Bayesian neural networks in a decentralized manner. By empirical studies we show that our theoretical analysis can guide the design of network/social interactions and data partitioning to achieve convergence.";https://arxiv.org/abs/1905.10466;xH4je93p2egJ
Hu, C., Jiang, J., & Wang, Z. (2019). Decentralized federated learning: A segmented gossip approach. arXiv preprint arXiv:1908.07782.;0_federated_learning_data_privacy;2019;Decentralized federated learning: A segmented gossip approach;Chenghao Hu, Jingyan Jiang, Zhi Wang;arXiv preprint arXiv:1908.07782, 2019;The emerging concern about data privacy and security has motivated the proposal of federated learning, which allows nodes to only synchronize the locally-trained models instead their own original data. Conventional federated learning architecture, inherited from the parameter server design, relies on highly centralized topologies and the assumption of large nodes-to-server bandwidths. However, in real-world federated learning scenarios the network capacities between nodes are highly uniformly distributed and smaller than that in a datacenter. It is of great challenges for conventional federated learning approaches to efficiently utilize network capacities between nodes. In this paper, we propose a model segment level decentralized federated learning to tackle this problem. In particular, we propose a segmented gossip approach, which not only makes full utilization of node-to-node bandwidth, but also has good training convergence. The experimental results show that even the training time can be highly reduced as compared to centralized federated learning.;https://arxiv.org/abs/1908.07782;Pi_857AZ07cJ
Shi, W., Zhou, S., & Niu, Z. (2020, June). Device scheduling with fast convergence for wireless federated learning. In ICC 2020-2020 IEEE International Conference on Communications (ICC) (pp. 1-6). IEEE.;0_federated_learning_data_privacy;2020;Device scheduling with fast convergence for wireless federated learning;Wenqi Shi, Sheng Zhou, Zhisheng Niu;ICC 2020-2020 IEEE International Conference on Communications (ICC), 1-6, 2020;Owing to the increasing need for massive data analysis and model training at the network edge, as well as the rising concerns about the data privacy, a new distributed training framework called federated learning (FL) has emerged. In each iteration of FL (called round), the edge devices update local models based on their own data and contribute to the global training by uploading the model updates via wireless channels. Due to the limited spectrum resources, only a portion of the devices can be scheduled in each round. While most of the existing work on scheduling focuses on the convergence of FL w.r.t. rounds, the convergence performance under a total training time budget is not yet explored. In this paper, a joint bandwidth allocation and scheduling problem is formulated to capture the long-term convergence performance of FL, and is solved by being decoupled into two sub-problems. For the bandwidth allocation sub-problem, the derived optimal solution suggests to allocate more bandwidth to the devices with worse channel conditions or weaker computation capabilities. For the device scheduling sub-problem, by revealing the trade-off between the number of rounds required to attain a certain model accuracy and the latency per round, a greedy policy is inspired, that continuously selects the device that consumes the least time in model updating until achieving a good trade-off between the learning efficiency and latency per round. The experiments show that the proposed policy outperforms other state-of-the-art scheduling policies, with the best achievable model accuracy under training time budgets.;https://ieeexplore.ieee.org/abstract/document/9149138/;vL_j3Ji_H8QJ
Choudhury, O., Gkoulalas-Divanis, A., Salonidis, T., Sylla, I., Park, Y., Hsu, G., & Das, A. (2019). Differential privacy-enabled federated learning for sensitive health data. arXiv preprint arXiv:1910.02578.;0_federated_learning_data_privacy;2019;Differential privacy-enabled federated learning for sensitive health data;Olivia Choudhury, Aris Gkoulalas-Divanis, Theodoros Salonidis, Issa Sylla, Yoonyoung Park, Grace Hsu, Amar Das;arXiv preprint arXiv:1910.02578, 2019;Leveraging real-world health data for machine learning tasks requires addressing many practical challenges, such as distributed data silos, privacy concerns with creating a centralized database from person-specific sensitive data, resource constraints for transferring and integrating data from multiple sites, and risk of a single point of failure. In this paper, we introduce a federated learning framework that can learn a global model from distributed health data held locally at different sites. The framework offers two levels of privacy protection. First, it does not move or share raw data across sites or with a centralized server during the model training process. Second, it uses a differential privacy mechanism to further protect the model from potential privacy attacks. We perform a comprehensive evaluation of our approach on two healthcare applications, using real-world electronic health data of 1 million patients. We demonstrate the feasibility and effectiveness of the federated learning framework in offering an elevated level of privacy and maintaining utility of the global model.;https://arxiv.org/abs/1910.02578;lN4oVGahprYJ
Geyer, R. C., Klein, T., & Nabi, M. (2017). Differentially private federated learning: A client level perspective. arXiv preprint arXiv:1712.07557.;0_federated_learning_data_privacy;2017;Differentially private federated learning: A client level perspective;Robin C Geyer, Tassilo Klein, Moin Nabi;arXiv preprint arXiv:1712.07557, 2017;Federated learning is a recent advance in privacy protection. In this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. However, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client's contribution during training and information about their data set is revealed through analyzing the distributed model. We tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients' contributions during training, balancing the trade-off between privacy loss and model performance. Empirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance.;https://arxiv.org/abs/1712.07557;Hlozhr4HhU0J
Li, J., Khodak, M., Caldas, S., & Talwalkar, A. (2019). Differentially private meta-learning. arXiv preprint arXiv:1909.05830.;0_federated_learning_data_privacy;2019;Differentially private meta-learning;Jeffrey Li, Mikhail Khodak, Sebastian Caldas, Ameet Talwalkar;arXiv preprint arXiv:1909.05830, 2019;Parameter-transfer is a well-known and versatile approach for meta-learning, with applications including few-shot learning, federated learning, and reinforcement learning. However, parameter-transfer algorithms often require sharing models that have been trained on the samples from specific tasks, thus leaving the task-owners susceptible to breaches of privacy. We conduct the first formal study of privacy in this setting and formalize the notion of task-global differential privacy as a practical relaxation of more commonly studied threat models. We then propose a new differentially private algorithm for gradient-based parameter transfer that not only satisfies this privacy requirement but also retains provable transfer learning guarantees in convex settings. Empirically, we apply our analysis to the problems of federated learning with personalization and few-shot classification, showing that allowing the relaxation to task-global privacy from the more commonly studied notion of local privacy leads to dramatically increased performance in recurrent neural language modeling and image classification.;https://arxiv.org/abs/1909.05830;_TB2VQHu-dQJ
Chen, X., Chen, T., Sun, H., Wu, S. Z., & Hong, M. (2020). Distributed training with heterogeneous data: Bridging median-and mean-based algorithms. Advances in Neural Information Processing Systems, 33, 21616-21626.;0_federated_learning_data_privacy;2020;Distributed training with heterogeneous data: Bridging median-and mean-based algorithms;Xiangyi Chen, Tiancong Chen, Haoran Sun, Steven Z Wu, Mingyi Hong;Advances in Neural Information Processing Systems 33, 21616-21626, 2020;Recently, there is a growing interest in the study of median-based algorithms for distributed non-convex optimization. Two prominent examples include signSGD with majority vote, an effective approach for communication reduction via 1-bit compression on the local gradients, and medianSGD, an algorithm recently proposed to ensure robustness against Byzantine workers. The convergence analyses for these algorithms critically rely on the assumption that all the distributed data are drawn iid from the same distribution. However, in applications such as Federated Learning, the data across different nodes or machines can be inherently heterogeneous, which violates such an iid assumption. This work analyzes signSGD and medianSGD in distributed settings with heterogeneous data. We show that these algorithms are non-convergent whenever there is some disparity between the expected median and mean over the local gradients. To overcome this gap, we provide a novel gradient correction mechanism that perturbs the local gradients with noise, which we show can provably close the gap between mean and median of the gradients. The proposed methods largely preserve nice properties of these median-based algorithms, such as the low per-iteration communication complexity of signSGD, and further enjoy global convergence to stationary solutions. Our perturbation technique can be of independent interest when one wishes to estimate mean through a median estimator.;https://proceedings.neurips.cc/paper/2020/hash/f629ed9325990b10543ab5946c1362fb-Abstract.html;TRneS7oPn_MJ
Saputra, Y. M., Hoang, D. T., Nguyen, D. N., Dutkiewicz, E., Mueck, M. D., & Srikanteswara, S. (2019, December). Energy demand prediction with federated learning for electric vehicle networks. In 2019 IEEE global communications conference (GLOBECOM) (pp. 1-6). IEEE.;0_federated_learning_data_privacy;2019;Energy demand prediction with federated learning for electric vehicle networks;Yuris Mulya Saputra, Dinh Thai Hoang, Diep N Nguyen, Eryk Dutkiewicz, Markus Dominik Mueck, Srikathyayani Srikanteswara;2019 IEEE global communications conference (GLOBECOM), 1-6, 2019;In this paper, we propose novel approaches using state-of-the-art machine learning techniques, aiming at predicting energy demand for electric vehicle (EV) networks. These methods can learn and find the correlation of complex hidden features to improve the prediction accuracy. First, we propose an energy demand learning (EDL)-based prediction solution in which a charging station provider (CSP) gathers information from all charging stations (CSs) and then performs the EDL algorithm to predict the energy demand for the considered area. However, this approach requires frequent data sharing between the CSs and the CSP, thereby driving communication overhead and privacy issues for the EVs and CSs. To address this problem, we propose a federated energy demand learning (FEDL) approach which allows the CSs sharing their information without revealing real datasets. Specifically, the CSs only need to send their trained models to the CSP for processing. In this case, we can significantly reduce the communication overhead and effectively protect data privacy for the EV users. To further improve the effectiveness of the FEDL, we then introduce a novel clustering- based EDL approach for EV networks by grouping the CSs into clusters before applying the EDL algorithms. Through experimental results, we show that our proposed approaches can improve the accuracy of energy demand prediction up to 24.63% and decrease communication overhead by 83.4% compared with other baseline machine learning algorithms.;https://ieeexplore.ieee.org/abstract/document/9013587/;nFCKrZ7sTW4J
Yang, Z., Chen, M., Saad, W., Hong, C. S., & Shikh-Bahaei, M. (2020). Energy efficient federated learning over wireless communication networks. IEEE Transactions on Wireless Communications, 20(3), 1935-1949.;0_federated_learning_data_privacy;2020;Energy efficient federated learning over wireless communication networks;Zhaohui Yang, Mingzhe Chen, Walid Saad, Choong Seon Hong, Mohammad Shikh-Bahaei;IEEE Transactions on Wireless Communications 20 (3), 1935-1949, 2020;In this paper, the problem of energy efficient transmission and computation resource allocation for federated learning (FL) over wireless communication networks is investigated. In the considered model, each user exploits limited local computational resources to train a local FL model with its collected data and, then, sends the trained FL model to a base station (BS) which aggregates the local FL model and broadcasts it back to all of the users. Since FL involves an exchange of a learning model between users and the BS, both computation and communication latencies are determined by the learning accuracy level. Meanwhile, due to the limited energy budget of the wireless users, both local computation energy and transmission energy must be considered during the FL process. This joint learning and communication problem is formulated as an optimization problem whose goal is to minimize the total energy consumption of the system under a latency constraint. To solve this problem, an iterative algorithm is proposed where, at every step, closed-form solutions for time allocation, bandwidth allocation, power control, computation frequency, and learning accuracy are derived. Since the iterative algorithm requires an initial feasible solution, we construct the completion time minimization problem and a bisection-based algorithm is proposed to obtain the optimal solution, which is a feasible solution to the original energy minimization problem. Numerical results show that the proposed algorithms can reduce up to 59.5% energy consumption compared to the conventional FL method.;https://ieeexplore.ieee.org/abstract/document/9264742/;cb5QWoRue2UJ
Sun, Y., Zhou, S., & Gündüz, D. (2020, June). Energy-aware analog aggregation for federated learning with redundant data. In ICC 2020-2020 IEEE International Conference on Communications (ICC) (pp. 1-7). IEEE.;0_federated_learning_data_privacy;2020;Energy-aware analog aggregation for federated learning with redundant data;Yuxuan Sun, Sheng Zhou, Deniz Gündüz;ICC 2020-2020 IEEE International Conference on Communications (ICC), 1-7, 2020;Federated learning (FL) enables workers to learn a model collaboratively by using their local data, with the help of a parameter server (PS) for global model aggregation. The high communication cost for periodic model updates and the nonindependent and identically distributed (i.i.d.) data become major bottlenecks for FL. In this work, we consider analog aggregation to scale down the communication cost with respect to the number of workers, and introduce data redundancy to the system to deal with non-i.i.d. data. We propose an online energy-aware dynamic worker scheduling policy, which maximizes the average number of workers scheduled for gradient update at each iteration under a long-term energy constraint, and analyze its performance based on Lyapunov optimization. Experiments using MNIST dataset show that, for non-i.i.d. data, doubling data storage can improve the accuracy by 9.8% under a stringent energy budget, while the proposed policy can achieve close-to-optimal accuracy without violating the energy constraint.;https://ieeexplore.ieee.org/abstract/document/9148853/;MqU9SVuXQFkJ
Nock, R., Hardy, S., Henecka, W., Ivey-Law, H., Patrini, G., Smith, G., & Thorne, B. (2018). Entity resolution and federated learning get a federated resolution. arXiv preprint arXiv:1803.04035.;0_federated_learning_data_privacy;2018;Entity resolution and federated learning get a federated resolution;Richard Nock, Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Giorgio Patrini, Guillaume Smith, Brian Thorne;arXiv preprint arXiv:1803.04035, 2018;Consider two data providers, each maintaining records of different feature sets about common entities. They aim to learn a linear model over the whole set of features. This problem of federated learning over vertically partitioned data includes a crucial upstream issue: entity resolution, i.e. finding the correspondence between the rows of the datasets. It is well known that entity resolution, just like learning, is mistake-prone in the real world. Despite the importance of the problem, there has been no formal assessment of how errors in entity resolution impact learning. In this paper, we provide a thorough answer to this question, answering how optimal classifiers, empirical losses, margins and generalisation abilities are affected. While our answer spans a wide set of losses --- going beyond proper, convex, or classification calibrated ---, it brings simple practical arguments to upgrade entity resolution as a preprocessing step to learning. One of these suggests that entity resolution should be aimed at controlling or minimizing the number of matching errors between examples of distinct classes. In our experiments, we modify a simple token-based entity resolution algorithm so that it indeed aims at avoiding matching rows belonging to different classes, and perform experiments in the setting where entity resolution relies on noisy data, which is very relevant to real world domains. Notably, our approach covers the case where one peer \textit{does not} have classes, or a noisy record of classes. Experiments display that using the class information during entity resolution can buy significant uplift for learning at little expense from the complexity standpoint.;https://arxiv.org/abs/1803.04035;qej3jl1paH8J
Caldas, S., Konečny, J., McMahan, H. B., & Talwalkar, A. (2018). Expanding the reach of federated learning by reducing client resource requirements. arXiv preprint arXiv:1812.07210.;0_federated_learning_data_privacy;2018;Expanding the reach of federated learning by reducing client resource requirements;Sebastian Caldas, Jakub Konečny, H Brendan McMahan, Ameet Talwalkar;arXiv preprint arXiv:1812.07210, 2018;"Communication on heterogeneous edge networks is a fundamental bottleneck in Federated Learning (FL), restricting both model capacity and user participation. To address this issue, we introduce two novel strategies to reduce communication costs: (1) the use of lossy compression on the global model sent server-to-client; and (2) Federated Dropout, which allows users to efficiently train locally on smaller subsets of the global model and also provides a reduction in both client-to-server communication and local computation. We empirically show that these strategies, combined with existing compression approaches for client-to-server communication, collectively provide up to a reduction in server-to-client communication, a reduction in local computation, and a reduction in upload communication, all without degrading the quality of the final model. We thus comprehensively reduce FL's impact on client device resources, allowing higher capacity models to be trained, and a more diverse set of users to be reached.";https://arxiv.org/abs/1812.07210;4xwsueWrhwAJ
Liu, D., Miller, T., Sayeed, R., & Mandl, K. D. (2018). FADL: Federated-autonomous deep learning for distributed electronic health record. arXiv preprint arXiv:1811.11400.;0_federated_learning_data_privacy;2018;FADL: Federated-autonomous deep learning for distributed electronic health record;Dianbo Liu, Timothy Miller, Raheel Sayeed, Kenneth D Mandl;arXiv preprint arXiv:1811.11400, 2018;Electronic health record (EHR) data is collected by individual institutions and often stored across locations in silos. Getting access to these data is difficult and slow due to security, privacy, regulatory, and operational issues. We show, using ICU data from 58 different hospitals, that machine learning models to predict patient mortality can be trained efficiently without moving health data out of their silos using a distributed machine learning strategy. We propose a new method, called Federated-Autonomous Deep Learning (FADL) that trains part of the model using all data sources in a distributed manner and other parts using data from specific data sources. We observed that FADL outperforms traditional federated learning strategy and conclude that balance between global and local training is an important factor to consider when design distributed machine learning methods , especially in healthcare.;https://arxiv.org/abs/1811.11400;oj2NidpCt6oJ
Li, T., Sanjabi, M., Beirami, A., & Smith, V. (2019). Fair resource allocation in federated learning. arXiv preprint arXiv:1905.10497.;0_federated_learning_data_privacy;2019;Fair resource allocation in federated learning;Tian Li, Maziar Sanjabi, Ahmad Beirami, Virginia Smith;arXiv preprint arXiv:1905.10497, 2019;Federated learning involves training statistical models in massive, heterogeneous networks. Naively minimizing an aggregate loss function in such a network may disproportionately advantage or disadvantage some of the devices. In this work, we propose q-Fair Federated Learning (q-FFL), a novel optimization objective inspired by fair resource allocation in wireless networks that encourages a more fair (specifically, a more uniform) accuracy distribution across devices in federated networks. To solve q-FFL, we devise a communication-efficient method, q-FedAvg, that is suited to federated networks. We validate both the effectiveness of q-FFL and the efficiency of q-FedAvg on a suite of federated datasets with both convex and non-convex models, and show that q-FFL (along with q-FedAvg) outperforms existing baselines in terms of the resulting fairness, flexibility, and efficiency.;https://arxiv.org/abs/1905.10497;Lnl0QFZEstwJ
Habachi, O., Adjif, M. A., & Cances, J. P. (2020). Fast uplink grant for NOMA: A federated learning based approach. In Ubiquitous Networking: 5th International Symposium, UNet 2019, Limoges, France, November 20–22, 2019, Revised Selected Papers 5 (pp. 96-109). Springer International Publishing.;0_federated_learning_data_privacy;2020;Fast uplink grant for NOMA: A federated learning based approach;Oussama Habachi, Mohamed-Ali Adjif, Jean-Pierre Cances;Ubiquitous Networking: 5th International Symposium, UNet 2019, Limoges, France, November 20â€“22, 2019, Revised Selected Papers 5, 96-109, 2020;Recently, non-orthogonal multiple access (NOMA) technique has emerged and is being considered as a building block of 5G systems and beyond. In this paper, we focus on the resource allocation for NOMA-based systems and we investigate how Machine Type Devices (MTDs) can be arranged into clusters. Specifically, we propose two allocation techniques to enable the integration of massive NOMA-based MTD in the 5G. Firstly, we propose a low-complexity schema where the base station (BS) assigns an MTD to a cluster based on its Channel State Information (CSI) and transmit power in order to ensure that the Successive Interference Cancellation (SIC) can be performed in the uplink as well as the downlink. The proposed technique enable us to allocate an optimal number of MTDs without inter-NOMA-interference (INI), while being of low complexity and communication overhead. In the second framework, we propose a Federated Learning (FL) based-technique using traffic model estimation at the MTD side in order to extend the capacity of the system. In fact, the BS take into account the traffic model of the MTDs in order to use time multiplexing in addition to the power multiplexing to separate MTDs. Then, we propose a synchronization method to allow contending MTDs synchronize their transmissions. Simulation results show that the proposed techniques outperform existing schemes in the literature.;https://link.springer.com/chapter/10.1007/978-3-030-58008-7_8;cH7Z5AktjxcJ
Peng, X., Huang, Z., Zhu, Y., & Saenko, K. (2019). Federated adversarial domain adaptation. arXiv preprint arXiv:1911.02054.;0_federated_learning_data_privacy;2019;Federated adversarial domain adaptation;Xingchao Peng, Zijun Huang, Yizhe Zhu, Kate Saenko;arXiv preprint arXiv:1911.02054, 2019;Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.;https://arxiv.org/abs/1911.02054;N66WIsdhAkIJ
Wang, K., Mathews, R., Kiddon, C., Eichner, H., Beaufays, F., & Ramage, D. (2019). Federated evaluation of on-device personalization. arXiv preprint arXiv:1910.10252.;0_federated_learning_data_privacy;2019;Federated evaluation of on-device personalization;Kangkang Wang, Rajiv Mathews, Chloé Kiddon, Hubert Eichner, Françoise Beaufays, Daniel Ramage;arXiv preprint arXiv:1910.10252, 2019;Federated learning is a distributed, on-device computation framework that enables training global models without exporting sensitive user data to servers. In this work, we describe methods to extend the federation framework to evaluate strategies for personalization of global models. We present tools to analyze the effects of personalization and evaluate conditions under which personalization yields desirable models. We report on our experiments personalizing a language model for a virtual keyboard for smartphones with a population of tens of millions of users. We show that a significant fraction of users benefit from personalization.;https://arxiv.org/abs/1910.10252;Zyb7chy2GeQJ
Triastcyn, A., & Faltings, B. (2020). Federated generative privacy. IEEE Intelligent Systems, 35(4), 50-57.;0_federated_learning_data_privacy;2020;Federated generative privacy;Aleksei Triastcyn, Boi Faltings;IEEE Intelligent Systems 35 (4), 50-57, 2020;We propose FedGP, a framework for privacy-preserving data release in the federated learning setting. We use generative adversarial networks, generator components of which are trained by FedAvg algorithm, to draw private artificial data samples and empirically assess the risk of information disclosure. Our experiments show that FedGP is able to generate labeled data of high quality to successfully train and validate supervised models. Finally, we demonstrate that our approach significantly reduces vulnerability of such models to model inversion attacks.;https://ieeexplore.ieee.org/abstract/document/9091604/;nA01bDoTDlQJ
Khan, L. U., Pandey, S. R., Tran, N. H., Saad, W., Han, Z., Nguyen, M. N., & Hong, C. S. (2020). Federated learning for edge networks: Resource optimization and incentive mechanism. IEEE Communications Magazine, 58(10), 88-93.;0_federated_learning_data_privacy;2020;Federated learning for edge networks: Resource optimization and incentive mechanism;Latif U Khan, Shashi Raj Pandey, Nguyen H Tran, Walid Saad, Zhu Han, Minh NH Nguyen, Choong Seon Hong;IEEE Communications Magazine 58 (10), 88-93, 2020;Recent years have witnessed a rapid proliferation of smart Internet of Things (IoT) devices. IoT devices with intelligence require the use of effective machine learning paradigms. Federated learning can be a promising solution for enabling IoT-based smart applications. In this article, we present the primary design aspects for enabling federated learning at the network edge. We model the incentive- based interaction between a global server and participating devices for federated learning via a Stackelberg game to motivate the participation of the devices in the federated learning process. We present several open research challenges with their possible solutions. Finally, we provide an outlook on future research.;https://ieeexplore.ieee.org/abstract/document/9247530/;SJJjQE21H4QJ
Ramaswamy, S., Mathews, R., Rao, K., & Beaufays, F. (2019). Federated learning for emoji prediction in a mobile keyboard. arXiv preprint arXiv:1906.04329.;0_federated_learning_data_privacy;2019;Federated learning for emoji prediction in a mobile keyboard;Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, FranÃ§oise Beaufays;arXiv preprint arXiv:1906.04329, 2019;We show that a word-level recurrent neural network can predict emoji from text typed on a mobile keyboard. We demonstrate the usefulness of transfer learning for predicting emoji by pretraining the model using a language modeling task. We also propose mechanisms to trigger emoji and tune the diversity of candidates. The model is trained using a distributed on-device learning framework called federated learning. The federated model is shown to achieve better performance than a server-trained model. This work demonstrates the feasibility of using federated learning to train production-quality models for natural language understanding tasks while keeping users' data on their devices.;https://arxiv.org/abs/1906.04329;3I8yWmVP074J
Hard, A., Rao, K., Mathews, R., Ramaswamy, S., Beaufays, F., Augenstein, S., ... & Ramage, D. (2018). Federated learning for mobile keyboard prediction. arXiv preprint arXiv:1811.03604.;0_federated_learning_data_privacy;2018;Federated learning for mobile keyboard prediction;Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Françoise Beaufays, Sean Augenstein, Hubert Eichner, Chloé Kiddon, Daniel Ramage;arXiv preprint arXiv:1811.03604, 2018;We train a recurrent neural network language model using a distributed, on-device learning framework called federated learning for the purpose of next-word prediction in a virtual keyboard for smartphones. Server-based training using stochastic gradient descent is compared with training on client devices using the Federated Averaging algorithm. The federated algorithm, which enables training on a higher-quality dataset for this use case, is shown to achieve better prediction recall. This work demonstrates the feasibility and benefit of training language models on client devices without exporting sensitive user data to servers. The federated learning environment gives users greater control over the use of their data and simplifies the task of incorporating privacy by default with distributed training and aggregation across a population of client devices.;https://arxiv.org/abs/1811.03604;nciS-AIVE3kJ
Chen, M., Suresh, A. T., Mathews, R., Wong, A., Allauzen, C., Beaufays, F., & Riley, M. (2019). Federated learning of n-gram language models. arXiv preprint arXiv:1910.03432.;0_federated_learning_data_privacy;2019;Federated learning of n-gram language models;Mingqing Chen, Ananda Theertha Suresh, Rajiv Mathews, Adeline Wong, Cyril Allauzen, FranÃ§oise Beaufays, Michael Riley;arXiv preprint arXiv:1910.03432, 2019;We propose algorithms to train production-quality n-gram language models using federated learning. Federated learning is a distributed computation platform that can be used to train global models for portable devices such as smart phones. Federated learning is especially relevant for applications handling privacy-sensitive data, such as virtual keyboards, because training is performed without the users' data ever leaving their devices. While the principles of federated learning are fairly generic, its methodology assumes that the underlying models are neural networks. However, virtual keyboards are typically powered by n-gram language models for latency reasons. We propose to train a recurrent neural network language model using the decentralized FederatedAveraging algorithm and to approximate this federated model server-side with an n-gram model that can be deployed to devices for fast inference. Our technical contributions include ways of handling large vocabularies, algorithms to correct capitalization errors in user data, and efficient finite state transducer algorithms to convert word language models to word-piece language models and vice versa. The n-gram language models trained with federated learning are compared to n-grams trained with traditional server-based algorithms using A/B tests on tens of millions of users of virtual keyboard. Results are presented for two languages, American English and Brazilian Portuguese. This work demonstrates that high-quality n-gram language models can be trained directly on client mobile devices without sensitive training data ever leaving the devices.;https://arxiv.org/abs/1910.03432;DYCkO83INNYJ
Chen, M., Mathews, R., Ouyang, T., & Beaufays, F. (2019). Federated learning of out-of-vocabulary words. arXiv preprint arXiv:1903.10635.;0_federated_learning_data_privacy;2019;Federated learning of out-of-vocabulary words;Mingqing Chen, Rajiv Mathews, Tom Ouyang, FranÃ§oise Beaufays;arXiv preprint arXiv:1903.10635, 2019;We demonstrate that a character-level recurrent neural network is able to learn out-of-vocabulary (OOV) words under federated learning settings, for the purpose of expanding the vocabulary of a virtual keyboard for smartphones without exporting sensitive text to servers. High-frequency words can be sampled from the trained generative model by drawing from the joint posterior directly. We study the feasibility of the approach in two settings: (1) using simulated federated learning on a publicly available non-IID per-user dataset from a popular social networking website, (2) using federated learning on data hosted on user mobile devices. The model achieves good recall and precision compared to ground-truth OOV words in setting (1). With (2) we demonstrate the practicality of this approach by showing that we can learn meaningful OOV words with good character-level prediction accuracy and cross entropy loss.;https://arxiv.org/abs/1903.10635;0hr5yCJJ0-YJ
Amiri, M. M., & Gündüz, D. (2020). Federated learning over wireless fading channels. IEEE Transactions on Wireless Communications, 19(5), 3546-3557.;0_federated_learning_data_privacy;2020;Federated learning over wireless fading channels;Mohammad Mohammadi Amiri, Deniz Gündüz;IEEE Transactions on Wireless Communications 19 (5), 3546-3557, 2020;"We study federated machine learning at the wireless network edge, where limited power wireless devices, each with its own dataset, build a joint model with the help of a remote parameter server (PS). We consider a bandwidth-limited fading multiple access channel (MAC) from the wireless devices to the PS, and propose various techniques to implement distributed stochastic gradient descent (DSGD) over this shared noisy wireless channel. We first propose a digital DSGD (D-DSGD) scheme, in which one device is selected opportunistically for transmission at each iteration based on the channel conditions; the scheduled device quantizes its gradient estimate to a finite number of bits imposed by the channel condition, and transmits these bits to the PS in a reliable manner. Next, motivated by the additive nature of the wireless MAC, we propose a novel analog communication scheme, referred to as the compressed analog DSGD (CA-DSGD), where the devices first sparsify their gradient estimates while accumulating error from previous iterations, and project the resultant sparse vector into a low-dimensional vector for bandwidth reduction. We also design a power allocation scheme to align the received gradient vectors at the PS in an efficient manner. Numerical results show that D-DSGD outperforms other digital approaches in the literature; however, in general the proposed CA-DSGD algorithm converges faster than the D-DSGD scheme, and reaches a higher level of accuracy. We have observed that the gap between the analog and digital schemes increases when the datasets of devices are not independent and identically distributed (i.i.d.). Furthermore, the performance of the CA-DSGD scheme is shown to be robust against imperfect channel state information (CSI) at the devices. Overall these results show clear advantages for the proposed analog over-the-air DSGD scheme, which suggests that learning and communication algorithms should be designed jointly to achieve the best end-to-end performance in machine learning applications at the wireless edge.";https://ieeexplore.ieee.org/abstract/document/9014530/;Ev8QtHxJr1QJ
Dinh, C. T., Tran, N. H., Nguyen, M. N., Hong, C. S., Bao, W., Zomaya, A. Y., & Gramoli, V. (2020). Federated learning over wireless networks: Convergence analysis and resource allocation. IEEE/ACM Transactions on Networking, 29(1), 398-409.;0_federated_learning_data_privacy;2020;Federated learning over wireless networks: Convergence analysis and resource allocation;Canh T Dinh, Nguyen H Tran, Minh NH Nguyen, Choong Seon Hong, Wei Bao, Albert Y Zomaya, Vincent Gramoli;IEEE/ACM Transactions on Networking 29 (1), 398-409, 2020;There is an increasing interest in a fast-growing machine learning technique called Federated Learning (FL), in which the model training is distributed over mobile user equipment (UEs), exploiting UEs' local computation and training data. Despite its advantages such as preserving data privacy, FL still has challenges of heterogeneity across UEs' data and physical resources. To address these challenges, we first propose FEDL, a FL algorithm which can handle heterogeneous UE data without further assumptions except strongly convex and smooth loss functions. We provide a convergence rate characterizing the trade-off between local computation rounds of each UE to update its local model and global communication rounds to update the FL global model. We then employ FEDL in wireless networks as a resource allocation optimization problem that captures the trade-off between FEDL convergence wall clock time and energy consumption of UEs with heterogeneous computing and power resources. Even though the wireless resource allocation problem of FEDL is non-convex, we exploit this problem's structure to decompose it into three sub-problems and analyze their closed-form solutions as well as insights into problem design. Finally, we empirically evaluate the convergence of FEDL with PyTorch experiments, and provide extensive numerical results for the wireless resource allocation sub-problems. Experimental results show that FEDL outperforms the vanilla FedAvg algorithm in terms of convergence rate and test accuracy in various settings.;https://ieeexplore.ieee.org/abstract/document/9261995/;6IOiBsxcTLQJ
Bonawitz, K., Salehi, F., Konečný, J., McMahan, B., & Gruteser, M. (2019, November). Federated learning with autotuned communication-efficient secure aggregation. In 2019 53rd Asilomar Conference on Signals, Systems, and Computers (pp. 1222-1226). IEEE.;0_federated_learning_data_privacy;2019;Federated learning with autotuned communication-efficient secure aggregation;Keith Bonawitz, Fariborz Salehi, Jakub Konečný, Brendan McMahan, Marco Gruteser;2019 53rd Asilomar Conference on Signals, Systems, and Computers, 1222-1226, 2019;Federated Learning enables mobile devices to collaboratively learn a shared inference model while keeping all the training data on a user's device, decoupling the ability to do machine learning from the need to store the data in the cloud. Existing work on federated learning with limited communication demonstrates how random rotation can enable users' model updates to be quantized much more efficiently, reducing the communication cost between users and the server. Meanwhile, secure aggregation enables the server to learn an aggregate of at least a threshold number of device's model contributions without observing any individual device's contribution in unaggregated form. In this paper, we highlight some of the challenges of setting the parameters for secure aggregation to achieve communication efficiency, especially in the context of the aggressively quantized inputs enabled by random rotation. We then develop a recipe for auto-tuning communication-efficient secure aggregation, based on specific properties of random rotation and secure aggregation - namely, the predictable distribution of vector entries post-rotation and the modular wrapping inherent in secure aggregation. We present both theoretical results and initial experiments.;https://ieeexplore.ieee.org/abstract/document/9049066/;GmU1V2MG2qQJ
Triastcyn, A., & Faltings, B. (2019, December). Federated learning with bayesian differential privacy. In 2019 IEEE International Conference on Big Data (Big Data) (pp. 2587-2596). IEEE.;0_federated_learning_data_privacy;2019;Federated learning with bayesian differential privacy;Aleksei Triastcyn, Boi Faltings;2019 IEEE International Conference on Big Data (Big Data), 2587-2596, 2019;We consider the problem of reinforcing federated learning with formal privacy guarantees. We propose to employ Bayesian differential privacy, a relaxation of differential privacy for similarly distributed data, to provide sharper privacy loss bounds. We adapt the Bayesian privacy accounting method to the federated setting and suggest multiple improvements for more efficient privacy budgeting at different levels. Our experiments show significant advantage over the state-of-the-art differential privacy bounds for federated learning on image classification tasks, including a medical application, bringing the privacy budget below ε = 1 at the client level, and below ε = 0.1 at the instance level. Lower amounts of noise also benefit the model accuracy and reduce the number of communication rounds.;https://ieeexplore.ieee.org/abstract/document/9005465/;aiH0pPpIoVYJ
Wei, K., Li, J., Ding, M., Ma, C., Yang, H. H., Farokhi, F., ... & Poor, H. V. (2020). Federated learning with differential privacy: Algorithms and performance analysis. IEEE Transactions on Information Forensics and Security, 15, 3454-3469.;0_federated_learning_data_privacy;2020;Federated learning with differential privacy: Algorithms and performance analysis;Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin, Tony QS Quek, H Vincent Poor;IEEE Transactions on Information Forensics and Security 15, 3454-3469, 2020;"Federated learning (FL), as a type of distributed machine learning, is capable of significantly preserving clients’ private data from being exposed to adversaries. Nevertheless, private information can still be divulged by analyzing uploaded parameters from clients, e.g., weights trained in deep neural networks. In this paper, to effectively prevent information leakage, we propose a novel framework based on the concept of differential privacy (DP), in which artificial noise is added to parameters at the clients’ side before aggregating, namely, noising before model aggregation FL (NbAFL). First, we prove that the NbAFL can satisfy DP under distinct protection levels by properly adapting different variances of artificial noise. Then we develop a theoretical convergence bound on the loss function of the trained FL model in the NbAFL. Specifically, the theoretical bound reveals the following three key properties: 1) there is a tradeoff between convergence performance and privacy protection levels, i.e., better convergence performance leads to a lower protection level; 2) given a fixed privacy protection level, increasing the number of overall clients participating in FL can improve the convergence performance; and 3) there is an optimal number aggregation times (communication rounds) in terms of convergence performance for a given protection level. Furthermore, we propose a -client random scheduling strategy, where ( ) clients are randomly selected from the overall clients to participate in each aggregation. We also develop a corresponding convergence bound for the loss function in this case and the -client random scheduling strategy also retains the above three properties. Moreover, we find that there is an optimal that achieves the best convergence performance at a fixed privacy level. Evaluations demonstrate that our theoretical results are consistent with simulations, thereby facilitating the design of various privacy-preserving FL algorithms with different tradeoff requirements on convergence performance and privacy levels.";https://ieeexplore.ieee.org/abstract/document/9069945/;kGEwc4WUHcsJ
Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., & Chandra, V. (2018). Federated learning with non-iid data. arXiv preprint arXiv:1806.00582.;0_federated_learning_data_privacy;2018;Federated learning with non-iid data;Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, Vikas Chandra;arXiv preprint arXiv:1806.00582, 2018;Federated learning enables resource-constrained edge compute devices, such as mobile phones and IoT devices, to learn a shared model for prediction, while keeping the training data local. This decentralized approach to train models provides privacy, security, regulatory and economic benefits. In this work, we focus on the statistical challenge of federated learning when local data is non-IID. We first show that the accuracy of federated learning reduces significantly, by up to 55% for neural networks trained for highly skewed non-IID data, where each client device trains only on a single class of data. We further show that this accuracy reduction can be explained by the weight divergence, which can be quantified by the earth mover's distance (EMD) between the distribution over classes on each device and the population distribution. As a solution, we propose a strategy to improve training on non-IID data by creating a small subset of data which is globally shared between all the edge devices. Experiments show that accuracy can be increased by 30% for the CIFAR-10 dataset with only 5% globally shared data.;https://arxiv.org/abs/1806.00582;NkUsPcaDEVEJ
Konečný, J., McMahan, H. B., Yu, F. X., Richtárik, P., Suresh, A. T., & Bacon, D. (2016). Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492.;0_federated_learning_data_privacy;2016;Federated learning: Strategies for improving communication efficiency;Jakub Konečný, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, Dave Bacon;arXiv preprint arXiv:1610.05492, 2016;"Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.";https://arxiv.org/abs/1610.05492;oL710FRqvyoJ
Skatchkovsky, N., Jang, H., & Simeone, O. (2020, May). Federated neuromorphic learning of spiking neural networks for low-power edge intelligence. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 8524-8528). IEEE.;0_federated_learning_data_privacy;2020;Federated neuromorphic learning of spiking neural networks for low-power edge intelligence;Nicolas Skatchkovsky, Hyeryung Jang, Osvaldo Simeone;ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 8524-8528, 2020;Spiking Neural Networks (SNNs) offer a promising alternative to conventional Artificial Neural Networks (ANNs) for the implementation of on-device low-power online learning and inference. On-device training is, however, constrained by the limited amount of data available at each device. In this paper, we propose to mitigate this problem via cooperative training through Federated Learning (FL). To this end, we introduce an online FL-based learning rule for networked on-device SNNs, which we refer to as FL-SNN. FL-SNN leverages local feedback signals within each SNN, in lieu of back-propagation, and global feedback through communication via a base station. The scheme demonstrates significant advantages over separate training and features a flexible trade-off between communication load and accuracy via the selective exchange of synaptic weights.;https://ieeexplore.ieee.org/abstract/document/9053861/;vCfcFBlpIuwJ
Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., & Smith, V. (2020). Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, 2, 429-450.;0_federated_learning_data_privacy;2020;Federated optimization in heterogeneous networks;Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, Virginia Smith;Proceedings of Machine learning and systems 2, 429-450, 2020;Federated Learning is a distributed learning paradigm with two key challenges that differentiate it from traditional distributed optimization:(1) significant variability in terms of the systems characteristics on each device in the network (systems heterogeneity), and (2) non-identically distributed data across the network (statistical heterogeneity). In this work, we introduce a framework, FedProx, to tackle heterogeneity in federated networks. FedProx can be viewed as a generalization and re-parametrization of FedAvg, the current state-of-the-art method for federated learning. While this re-parameterization makes only minor modifications to the method itself, these modifications have important ramifications both in theory and in practice. Theoretically, we provide convergence guarantees for our framework when learning over data from non-identical distributions (statistical heterogeneity), and while adhering to device-level systems constraints by allowing each participating device to perform a variable amount of work (systems heterogeneity). Practically, we demonstrate that FedProx allows for more robust convergence than FedAvg across a suite of realistic federated datasets. In particular, in highly heterogeneous settings, FedProx demonstrates significantly more stable and accurate convergence behavior relative to FedAvgâ€”improving absolute test accuracy by 18.8% on average.;https://proceedings.mlsys.org/paper_files/paper/2020/hash/1f5fe83998a09396ebe6477d9475ba0c-Abstract.html;t5s7fmKgfE0J
Chen, Y., Qin, X., Wang, J., Yu, C., & Gao, W. (2020). Fedhealth: A federated transfer learning framework for wearable healthcare. IEEE Intelligent Systems, 35(4), 83-93.;0_federated_learning_data_privacy;2020;Fedhealth: A federated transfer learning framework for wearable healthcare;Yiqiang Chen, Xin Qin, Jindong Wang, Chaohui Yu, Wen Gao;IEEE Intelligent Systems 35 (4), 83-93, 2020;With the rapid development of computing technology, wearable devices make it easy to get access to people's health information. Smart healthcare achieves great success by training machine learning models on a large quantity of user personal data. However, there are two critical challenges. First, user data often exist in the form of isolated islands, making it difficult to perform aggregation without compromising privacy security. Second, the models trained on the cloud fail on personalization. In this article, we propose FedHealth, the first federated transfer learning framework for wearable healthcare to tackle these challenges. FedHealth performs data aggregation through federated learning, and then builds relatively personalized models by transfer learning. Wearable activity recognition experiments and real Parkinson's disease auxiliary diagnosis application have evaluated that FedHealth is able to achieve accurate and personalized healthcare without compromising privacy and security. FedHealth is general and extensible in many healthcare applications.;https://ieeexplore.ieee.org/abstract/document/9076082/;uwILNL2kpUYJ
Li, D., & Wang, J. (2019). Fedmd: Heterogenous federated learning via model distillation. arXiv preprint arXiv:1910.03581.;0_federated_learning_data_privacy;2019;Fedmd: Heterogenous federated learning via model distillation;Daliang Li, Junpu Wang;arXiv preprint arXiv:1910.03581, 2019;Federated learning enables the creation of a powerful centralized model without compromising data privacy of multiple participants. While successful, it does not incorporate the case where each participant independently designs its own model. Due to intellectual property concerns and heterogeneous nature of tasks and data, this is a widespread requirement in applications of federated learning to areas such as health care and AI as a service. In this work, we use transfer learning and knowledge distillation to develop a universal framework that enables federated learning when each agent owns not only their private data, but also uniquely designed models. We test our framework on the MNIST/FEMNIST dataset and the CIFAR10/CIFAR100 dataset and observe fast improvement across all participating models. With 10 distinct participants, the final test accuracy of each model on average receives a 20% gain on top of what's possible without collaboration and is only a few percent lower than the performance each model would have obtained if all private datasets were pooled and made directly available for all participants.;https://arxiv.org/abs/1910.03581;FZmuoplvs5kJ
Reisizadeh, A., Mokhtari, A., Hassani, H., Jadbabaie, A., & Pedarsani, R. (2020, June). Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization. In International Conference on Artificial Intelligence and Statistics (pp. 2021-2031). PMLR.;0_federated_learning_data_privacy;2020;Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization;Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, Ramtin Pedarsani;International Conference on Artificial Intelligence and Statistics, 2021-2031, 2020;"Federated learning is a distributed framework according to which a model is trained over a set of devices, while keeping data localized. This framework faces several systems-oriented challenges which include (i) communication bottleneck since a large number of devices upload their local updates to a parameter server, and (ii) scalability as the federated network consists of millions of devices. Due to these systems challenges as well as issues related to statistical heterogeneity of data and privacy concerns, designing a provably efficient federated learning method is of significant importance yet it remains challenging. In this paper, we present FedPAQ, a communication-efficient Federated Learning method with Periodic Averaging and Quantization. FedPAQ relies on three key features:(1) periodic averaging where models are updated locally at devices and only periodically averaged at the server;(2) partial device participation where only a fraction of devices participate in each round of the training; and (3) quantized message-passing where the edge nodes quantize their updates before uploading to the parameter server. These features address the communications and scalability challenges in federated learning. We also show that FedPAQ achieves near-optimal theoretical guarantees for strongly convex and non-convex loss functions and empirically demonstrate the communication-computation tradeoff provided by our method.";https://proceedings.mlr.press/v108/reisizadeh20a.html;2v45WQ5D_CwJ
Augenstein, S., McMahan, H. B., Ramage, D., Ramaswamy, S., Kairouz, P., Chen, M., & Mathews, R. (2019). Generative models for effective ML on private, decentralized datasets. arXiv preprint arXiv:1911.06679.;0_federated_learning_data_privacy;2019;Generative models for effective ML on private, decentralized datasets;Sean Augenstein, H Brendan McMahan, Daniel Ramage, Swaroop Ramaswamy, Peter Kairouz, Mingqing Chen, Rajiv Mathews;arXiv preprint arXiv:1911.06679, 2019;To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data - of representative samples, of outliers, of misclassifications - is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses, and c) assigning or refining human-provided labels. However, manual data inspection is problematic for privacy sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. This paper demonstrates that generative models - trained using federated methods and with formal differential privacy guarantees - can be used effectively to debug many commonly occurring data issues even when the data cannot be directly inspected. We explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm for differentially private federated GANs.;https://arxiv.org/abs/1911.06679;RQ-de15CDIMJ
Abad, M. S. H., Ozfatura, E., Gunduz, D., & Ercetin, O. (2020, May). Hierarchical federated learning across heterogeneous cellular networks. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 8866-8870). IEEE.;0_federated_learning_data_privacy;2020;Hierarchical federated learning across heterogeneous cellular networks;Mehdi Salehi Heydar Abad, Emre Ozfatura, Deniz Gunduz, Ozgur Ercetin;ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 8866-8870, 2020;We consider federated edge learning (FEEL), where mobile users (MUs) collaboratively learn a global model by sharing local updates on the model parameters rather than their datasets, with the help of a mobile base station (MBS). We optimize the resource allocation among MUs to reduce the communication latency in learning iterations. Observing that the performance in this centralized setting is limited due to the distance of the cell-edge users to the MBS, we introduce small cell base stations (SBSs) orchestrating FEEL among MUs within their cells, and periodically exchanging model updates with the MBS for global consensus. We show that this hierarchical federated learning (HFL) scheme significantly reduces the communication latency without sacrificing the accuracy.;https://ieeexplore.ieee.org/abstract/document/9054634/;yu0A1Zug-hAJ
Jiang, Y., Konečný, J., Rush, K., & Kannan, S. (2019). Improving federated learning personalization via model agnostic meta learning. arXiv preprint arXiv:1909.12488.;0_federated_learning_data_privacy;2019;Improving federated learning personalization via model agnostic meta learning;Yihan Jiang, Jakub Konečný, Keith Rush, Sreeram Kannan;arXiv preprint arXiv:1909.12488, 2019;Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. We present FL as a natural source of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.;https://arxiv.org/abs/1909.12488;5ctLP-1JnzkJ
Wang, G. (2019). Interpret federated learning with shapley values. arXiv preprint arXiv:1905.04519.;0_federated_learning_data_privacy;2019;Interpret federated learning with shapley values;Guan Wang;arXiv preprint arXiv:1905.04519, 2019;Federated Learning is introduced to protect privacy by distributing training data into multiple parties. Each party trains its own model and a meta-model is constructed from the sub models. In this way the details of the data are not disclosed in between each party. In this paper we investigate the model interpretation methods for Federated Learning, specifically on the measurement of feature importance of vertical Federated Learning where feature space of the data is divided into two parties, namely host and guest. For host party to interpret a single prediction of vertical Federated Learning model, the interpretation results, namely the feature importance, are very likely to reveal the protected data from guest party. We propose a method to balance the model interpretability and data privacy in vertical Federated Learning by using Shapley values to reveal detailed feature importance for host features and a unified importance value for federated guest features. Our experiments indicate robust and informative results for interpreting Federated Learning models.;https://arxiv.org/abs/1905.04519;6SF6G1uxQ_YJ
Caldas, S., Duddu, S. M. K., Wu, P., Li, T., Konečný, J., McMahan, H. B., ... & Talwalkar, A. (2018). Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097.;0_federated_learning_data_privacy;2018;Leaf: A benchmark for federated settings;Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečný, H Brendan McMahan, Virginia Smith, Ameet Talwalkar;arXiv preprint arXiv:1812.01097, 2018;Modern federated networks, such as those comprised of wearable devices, mobile phones, or autonomous vehicles, generate massive amounts of data each day. This wealth of data can help to learn models that can improve the user experience on each device. However, the scale and heterogeneity of federated data presents new challenges in research areas such as federated learning, meta-learning, and multi-task learning. As the machine learning community begins to tackle these challenges, we are at a critical time to ensure that developments made in these areas are grounded with realistic benchmarks. To this end, we propose LEAF, a modular benchmarking framework for learning in federated settings. LEAF includes a suite of open-source federated datasets, a rigorous evaluation framework, and a set of reference implementations, all geared towards capturing the obstacles and intricacies of practical federated environments.;https://arxiv.org/abs/1812.01097;BSQT3iJRYG4J
Koskela, A., & Honkela, A. (2018). Learning rate adaptation for federated and differentially private learning. arXiv preprint arXiv:1809.03832.;0_federated_learning_data_privacy;2018;Learning rate adaptation for federated and differentially private learning;Antti Koskela, Antti Honkela;arXiv preprint arXiv:1809.03832, 2018;We propose an algorithm for the adaptation of the learning rate for stochastic gradient descent (SGD) that avoids the need for validation set use. The idea for the adaptiveness comes from the technique of extrapolation: to get an estimate for the error against the gradient flow which underlies SGD, we compare the result obtained by one full step and two half-steps. The algorithm is applied in two separate frameworks: federated and differentially private learning. Using examples of deep neural networks we empirically show that the adaptive algorithm is competitive with manually tuned commonly used optimisation methods for differentially privately training. We also show that it works robustly in the case of federated learning unlike commonly used optimisation methods.;https://arxiv.org/abs/1809.03832;ixFm2UU8nxQJ
Huang, L., Yin, Y., Fu, Z., Zhang, S., Deng, H., & Liu, D. (2020). LoAdaBoost: Loss-based AdaBoost federated machine learning with reduced computational complexity on IID and non-IID intensive care data. Plos one, 15(4), e0230706.;0_federated_learning_data_privacy;2020;LoAdaBoost: Loss-based AdaBoost federated machine learning with reduced computational complexity on IID and non-IID intensive care data;Li Huang, Yifeng Yin, Zeng Fu, Shifa Zhang, Hao Deng, Dianbo Liu;Plos one 15 (4), e0230706, 2020;Intensive care data are valuable for improvement of health care, policy making and many other purposes. Vast amount of such data are stored in different locations, on many different devices and in different data silos. Sharing data among different sources is a big challenge due to regulatory, operational and security reasons. One potential solution is federated machine learning, which is a method that sends machine learning algorithms simultaneously to all data sources, trains models in each source and aggregates the learned models. This strategy allows utilization of valuable data without moving them. One challenge in applying federated machine learning is the possibly different distributions of data from diverse sources. To tackle this problem, we proposed an adaptive boosting method named LoAdaBoost that increases the efficiency of federated machine learning. Using intensive care unit data from hospitals, we investigated the performance of learning in IID and non-IID data distribution scenarios, and showed that the proposed LoAdaBoost method achieved higher predictive accuracy with lower computational complexity than the baseline method.;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0230706;VmYDU91HtDoJ
Wang, G., Dang, C. X., & Zhou, Z. (2019, December). Measure contribution of participants in federated learning. In 2019 IEEE international conference on big data (Big Data) (pp. 2597-2604). IEEE.;0_federated_learning_data_privacy;2019;Measure contribution of participants in federated learning;Guan Wang, Charlie Xiaoqian Dang, Ziye Zhou;2019 IEEE international conference on big data (Big Data), 2597-2604, 2019;Federated Machine Learnig (FML) creates an ecosystem for multiple parties to collaborate on building models while protecting data privacy for the participants. A measure of the contribution for each party in FML enables fair credits allocation. In this paper we develop simple but powerful techniques to fairly calculate the contributions of multiple parties in FML, in the context of both horizontal FML and vertical FML. For Horizontal FML we use deletion method to calculate the grouped instance influence. For Vertical FML we use Shapley Values to calculate the grouped feature importance. Our methods open the door for research in model contribution and credit allocation in the context of federated machine learning.;https://ieeexplore.ieee.org/abstract/document/9006179/;DSSvXAFetsUJ
Hsu, T. M. H., Qi, H., & Brown, M. (2019). Measuring the effects of non-identical data distribution for federated visual classification. arXiv preprint arXiv:1909.06335.;0_federated_learning_data_privacy;2019;Measuring the effects of non-identical data distribution for federated visual classification;Tzu-Ming Harry Hsu, Hang Qi, Matthew Brown;arXiv preprint arXiv:1909.06335, 2019;Federated Learning enables visual models to be trained in a privacy-preserving way using real-world data from mobile devices. Given their distributed nature, the statistics of the data across these devices is likely to differ significantly. In this work, we look at the effect such non-identical data distributions has on visual classification via Federated Learning. We propose a way to synthesize datasets with a continuous range of identicalness and provide performance measures for the Federated Averaging algorithm. We show that performance degrades as distributions differ more, and propose a mitigation strategy via server momentum. Experiments on CIFAR-10 demonstrate improved classification performance over a range of non-identicalness, with classification accuracy improved from 30.1% to 76.9% in the most skewed settings.;https://arxiv.org/abs/1909.06335;lmTUUssDYisJ
Fung, C., Yoon, C. J., & Beschastnikh, I. (2018). Mitigating sybils in federated learning poisoning. arXiv preprint arXiv:1808.04866.;0_federated_learning_data_privacy;2018;Mitigating sybils in federated learning poisoning;Clement Fung, Chris JM Yoon, Ivan Beschastnikh;arXiv preprint arXiv:1808.04866, 2018;Machine learning (ML) over distributed multi-party data is required for a variety of domains. Existing approaches, such as federated learning, collect the outputs computed by a group of devices at a central aggregator and run iterative algorithms to train a globally shared model. Unfortunately, such approaches are susceptible to a variety of attacks, including model poisoning, which is made substantially worse in the presence of sybils. In this paper we first evaluate the vulnerability of federated learning to sybil-based poisoning attacks. We then describe \emph{FoolsGold}, a novel defense to this problem that identifies poisoning sybils based on the diversity of client updates in the distributed learning process. Unlike prior work, our system does not bound the expected number of attackers, requires no auxiliary information outside of the learning process, and makes fewer assumptions about clients and their data. In our evaluation we show that FoolsGold exceeds the capabilities of existing state of the art approaches to countering sybil-based label-flipping and backdoor poisoning attacks. Our results hold for different distributions of client data, varying poisoning targets, and various sybil strategies. Code can be found at: https://github.com/DistributedML/FoolsGold;https://arxiv.org/abs/1808.04866;ZqS4N8JfU8kJ
Zhao, Y., Zhao, J., Jiang, L., Tan, R., & Niyato, D. (2019). Mobile edge computing, blockchain and reputation-based crowdsourcing iot federated learning: A secure, decentralized and privacy-preserving system. arXiv preprint arXiv:1906.10893, 2327-4662.;0_federated_learning_data_privacy;2019;Mobile edge computing, blockchain and reputation-based crowdsourcing iot federated learning: A secure, decentralized and privacy-preserving system;Yang Zhao, Jun Zhao, Linshan Jiang, Rui Tan, Dusit Niyato;Arxiv;Internet-of-Things (IoT) companies strive to get feedback from users to improve their products and services. However, traditional surveys cannot reflect the actual conditions of customers’ due to the limited questions. Besides, survey results are affected by various subjective factors. In contrast, the recorded usages of IoT devices reflect customers’ behaviours more comprehensively and accurately. We design an intelligent system to help IoT device manufacturers to take advantage of customers’ data and build a machine learning model to predict customers’ requirements and possible consumption behaviours with federated learning (FL) technology. The FL consists of two stages: in the first stage, customers train the initial model using the phone and the edge computing server collaboratively. The mobile edge computing server’s high computation power can assist customers’ training locally. Customers first collect data from various IoT devices using phones, and then download and train the initial model with their data. During the training, customers first extract features using their mobiles, and then add the Laplacian noise to the extracted features based on differential privacy, a formal and popular notion to quantify privacy. After achieving the local model, customers sign on their models respectively and send them to the blockchain. We use the blockchain to replace the centralized aggregator which belongs to the third party in FL. In the second stage, miners calculate the averaged model using the collected models sent from customers. By the end of the crowdsourcing job, one of the miners, who is selected as the temporary leader, uploads the model to the blockchain. Besides, to attract more customers to participate in the crowdsourcing FL, we design an incentive mechanism, which awards participants with coins that can be used to purchase other services provided by the company.;https://d1wqtxts1xzle7.cloudfront.net/84198528/1906.10893v1-libre.pdf?1650020828=&response-content-disposition=inline%3B+filename%3DMobile_Edge_Computing_Blockchain_and_Rep.pdf&Expires=1698161416&Signature=atwDmhXhf4~qT6idT4SaqE3-05uKRuTWTsFKogz7p6~xrQmXUWPml4C94tTs1sBCgwGJsk3D3RODPcGHmYHj5daAR~eblz3QfxWUozR0MgZuh8FNiIgqMUEOZgOMUYCtU7VPwiRHeUyG7niHb0a1e8pJIsa-lor0MPF-FU-MdK-scRPC6E-b2YW5HLzCWx6-3L81HI9DPbRKQGPQGBJgYZBnLKebQ6voAo93upcN4DDZKElTCtozU7BwgLbCx8Z3218YCXe02Xf-p22itoiNIRrcNTxV5ZB4DLUpZo3ivPDXk81VJ5hmAbWN1ges5kjcCpttHOYLilE6Qe~oziIO7g__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA;Todo
Jiang, Y., Wang, S., Valls, V., Ko, B. J., Lee, W. H., Leung, K. K., & Tassiulas, L. (2022). Model pruning enables efficient federated learning on edge devices. IEEE Transactions on Neural Networks and Learning Systems.;0_federated_learning_data_privacy;2022;Model pruning enables efficient federated learning on edge devices;Yuang Jiang, Shiqiang Wang, Victor Valls, Bong Jun Ko, Wei-Han Lee, Kin K Leung, Leandros Tassiulas;IEEE Transactions on Neural Networks and Learning Systems, 2022;Federated learning (FL) allows model training from local data collected by edge/mobile devices while preserving data privacy, which has wide applicability to image and vision applications. A challenge is that client devices in FL usually have much more limited computation and communication resources compared to servers in a data center. To overcome this challenge, we propose PruneFL--a novel FL approach with adaptive and distributed parameter pruning, which adapts the model size during FL to reduce both communication and computation overhead and minimize the overall training time, while maintaining a similar accuracy as the original model. PruneFL includes initial pruning at a selected client and further pruning as part of the FL process. The model size is adapted during this process, which includes maximizing the approximate empirical risk reduction divided by the time of one FL round. Our experiments with various datasets on edge devices (e.g., Raspberry Pi) show that: 1) we significantly reduce the training time compared to conventional FL and various other pruning-based methods and 2) the pruned model with automatically determined size converges to an accuracy that is very similar to the original model, and it is also a lottery ticket of the original model.;https://ieeexplore.ieee.org/abstract/document/9762360/;ewTMyfi1RhgJ
Guha, N., Talwalkar, A., & Smith, V. (2019). One-shot federated learning. arXiv preprint arXiv:1902.11175.;0_federated_learning_data_privacy;2019;One-shot federated learning;Neel Guha, Ameet Talwalkar, Virginia Smith;arXiv preprint arXiv:1902.11175, 2019;We present one-shot federated learning, where a central server learns a global model over a network of federated devices in a single round of communication. Our approach - drawing on ensemble learning and knowledge aggregation - achieves an average relative gain of 51.5% in AUC over local baselines and comes within 90.1% of the (unattainable) global ideal. We discuss these methods and identify several promising directions of future work.;https://arxiv.org/abs/1902.11175;-FlH2mqM8m8J
Yang, S., Ren, B., Zhou, X., & Liu, L. (2019). Parallel distributed logistic regression for vertical federated learning without third-party coordinator. arXiv preprint arXiv:1911.09824.;0_federated_learning_data_privacy;2019;Parallel distributed logistic regression for vertical federated learning without third-party coordinator;Shengwen Yang, Bing Ren, Xuhui Zhou, Liping Liu;arXiv preprint arXiv:1911.09824, 2019;Federated Learning is a new distributed learning mechanism which allows model training on a large corpus of decentralized data owned by different data providers, without sharing or leakage of raw data. According to the characteristics of data dis-tribution, it could be usually classified into three categories: horizontal federated learning, vertical federated learning, and federated transfer learning. In this paper we present a solution for parallel dis-tributed logistic regression for vertical federated learning. As compared with existing works, the role of third-party coordinator is removed in our proposed solution. The system is built on the pa-rameter server architecture and aims to speed up the model training via utilizing a cluster of servers in case of large volume of training data. We also evaluate the performance of the parallel distributed model training and the experimental results show the great scalability of the system.;https://arxiv.org/abs/1911.09824;lgaH_CdzcDAJ
Lalitha, A., Kilinc, O. C., Javidi, T., & Koushanfar, F. (2019). Peer-to-peer federated learning on graphs. arXiv preprint arXiv:1901.11173.;0_federated_learning_data_privacy;2019;Peer-to-peer federated learning on graphs;Anusha Lalitha, Osman Cihan Kilinc, Tara Javidi, Farinaz Koushanfar;arXiv preprint arXiv:1901.11173, 2019;We consider the problem of training a machine learning model over a network of nodes in a fully decentralized framework. The nodes take a Bayesian-like approach via the introduction of a belief over the model parameter space. We propose a distributed learning algorithm in which nodes update their belief by aggregate information from their one-hop neighbors to learn a model that best fits the observations over the entire network. In addition, we also obtain sufficient conditions to ensure that the probability of error is small for every node in the network. We discuss approximations required for applying this algorithm to train Deep Neural Networks (DNNs). Experiments on training linear regression model and on training a DNN show that the proposed learning rule algorithm provides a significant improvement in the accuracy compared to the case where nodes learn without cooperation.;https://arxiv.org/abs/1901.11173;nOZn3TxNVJkJ
Bonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H. B., Patel, S., ... & Seth, K. (2016). Practical secure aggregation for federated learning on user-held data. arXiv preprint arXiv:1611.04482.;0_federated_learning_data_privacy;2016;Practical secure aggregation for federated learning on user-held data;Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, Karn Seth;arXiv preprint arXiv:1611.04482, 2016;Secure Aggregation protocols allow a collection of mutually distrust parties, each holding a private value, to collaboratively compute the sum of those values without revealing the values themselves. We consider training a deep neural network in the Federated Learning model, using distributed stochastic gradient descent across user-held training data on mobile devices, wherein Secure Aggregation protects each user's model gradient. We design a novel, communication-efficient Secure Aggregation protocol for high-dimensional data that tolerates up to 1/3 users failing to complete the protocol. For 16-bit input values, our protocol offers 1.73x communication expansion for users and -dimensional vectors, and 1.98x expansion for users and dimensional vectors.;https://arxiv.org/abs/1611.04482;SAy3yEe7rvoJ
Hardy, S., Henecka, W., Ivey-Law, H., Nock, R., Patrini, G., Smith, G., & Thorne, B. (2017). Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption. arXiv preprint arXiv:1711.10677.;0_federated_learning_data_privacy;2017;Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption;Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, Guillaume Smith, Brian Thorne;arXiv preprint arXiv:1711.10677, 2017;Consider two data providers, each maintaining private records of different feature sets about common entities. They aim to learn a linear model jointly in a federated setting, namely, data is local and a shared model is trained from locally computed updates. In contrast with most work on distributed learning, in this scenario (i) data is split vertically, i.e. by features, (ii) only one data provider knows the target variable and (iii) entities are not linked across the data providers. Hence, to the challenge of private learning, we add the potentially negative consequences of mistakes in entity resolution. Our contribution is twofold. First, we describe a three-party end-to-end solution in two phases ---privacy-preserving entity resolution and federated logistic regression over messages encrypted with an additively homomorphic scheme---, secure against a honest-but-curious adversary. The system allows learning without either exposing data in the clear or sharing which entities the data providers have in common. Our implementation is as accurate as a naive non-private solution that brings all data in one place, and scales to problems with millions of entities with hundreds of features. Second, we provide what is to our knowledge the first formal analysis of the impact of entity resolution's mistakes on learning, with results on how optimal classifiers, empirical losses, margins and generalisation abilities are affected. Our results bring a clear and strong support for federated learning: under reasonable assumptions on the number and magnitude of entity resolution's mistakes, it can be extremely beneficial to carry out federated learning in the setting where each peer's data provides a significant uplift to the other.;https://arxiv.org/abs/1711.10677;Vc_QaAQiot4J
Peterson, D., Kanani, P., & Marathe, V. J. (2019). Private federated learning with domain adaptation. arXiv preprint arXiv:1912.06733.;0_federated_learning_data_privacy;2019;Private federated learning with domain adaptation;Daniel Peterson, Pallika Kanani, Virendra J Marathe;arXiv preprint arXiv:1912.06733, 2019;Federated Learning (FL) is a distributed machine learning (ML) paradigm that enables multiple parties to jointly re-train a shared model without sharing their data with any other parties, offering advantages in both scale and privacy. We propose a framework to augment this collaborative model-building with per-user domain adaptation. We show that this technique improves model accuracy for all users, using both real and synthetic data, and that this improvement is much more pronounced when differential privacy bounds are imposed on the FL model.;https://arxiv.org/abs/1912.06733;EW6W4PxGHkMJ
Bhowmick, A., Duchi, J., Freudiger, J., Kapoor, G., & Rogers, R. (2018). Protection against reconstruction and its applications in private federated learning. arXiv preprint arXiv:1812.00984.;0_federated_learning_data_privacy;2018;Protection against reconstruction and its applications in private federated learning;Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, Ryan Rogers;arXiv preprint arXiv:1812.00984, 2018;In large-scale statistical learning, data collection and model fitting are moving increasingly toward peripheral devices---phones, watches, fitness trackers---away from centralized data collection. Concomitant with this rise in decentralized data are increasing challenges of maintaining privacy while allowing enough information to fit accurate, useful statistical models. This motivates local notions of privacy---most significantly, local differential privacy, which provides strong protections against sensitive data disclosures---where data is obfuscated before a statistician or learner can even observe it, providing strong protections to individuals' data. Yet local privacy as traditionally employed may prove too stringent for practical use, especially in modern high-dimensional statistical and machine learning problems. Consequently, we revisit the types of disclosures and adversaries against which we provide protections, considering adversaries with limited prior information and ensuring that with high probability, ensuring they cannot reconstruct an individual's data within useful tolerances. By reconceptualizing these protections, we allow more useful data release---large privacy parameters in local differential privacy---and we design new (minimax) optimal locally differentially private mechanisms for statistical learning problems for \emph{all} privacy levels. We thus present practicable approaches to large-scale locally private model training that were previously impossible, showing theoretically and empirically that we can fit large-scale image classification and language models with little degradation in utility.;https://arxiv.org/abs/1812.00984;7iRk4cvWkq0J
Kang, J., Xiong, Z., Niyato, D., Zou, Y., Zhang, Y., & Guizani, M. (2020). Reliable federated learning for mobile networks. IEEE Wireless Communications, 27(2), 72-80.;0_federated_learning_data_privacy;2020;Reliable federated learning for mobile networks;Jiawen Kang, Zehui Xiong, Dusit Niyato, Yuze Zou, Yang Zhang, Mohsen Guizani;IEEE Wireless Communications 27 (2), 72-80, 2020;Federated learning, as a promising machine learning approach, has emerged to leverage a distributed personalized dataset from a number of nodes, for example, mobile devices, to improve performance while simultaneously providing privacy preservation for mobile users. In federated learning, training data is widely distributed and maintained on the mobile devices as workers. A central aggregator updates a global model by collecting local updates from mobile devices using their local training data to train the global model in each iteration. However, unreliable data may be uploaded by the mobile devices (i.e., workers), leading to frauds in tasks of federated learning. The workers may perform unreliable updates intentionally, for example, the data poisoning attack, or unintentionally, for example, low-quality data caused by energy constraints or high-speed mobility. Therefore, finding out trusted and reliable workers in federated learning tasks becomes critical. In this article, the concept of reputation is introduced as a metric. Based on this metric, a reliable worker selection scheme is proposed for federated learning tasks. Consortium blockchain is leveraged as a decentralized approach for achieving efficient reputation management of the workers without repudiation and tampering. By numerical analysis, the proposed approach is demonstrated to improve the reliability of federated learning tasks in mobile networks.;https://ieeexplore.ieee.org/abstract/document/8994206/;HarRbpYV0TAJ
Pillutla, K., Kakade, S. M., & Harchaoui, Z. (2022). Robust aggregation for federated learning. IEEE Transactions on Signal Processing, 70, 1142-1154.;0_federated_learning_data_privacy;2022;Robust aggregation for federated learning;Krishna Pillutla, Sham M Kakade, Zaid Harchaoui;IEEE Transactions on Signal Processing 70, 1142-1154, 2022;We present a novel approach to federated learning that endows its aggregation process with greater robustness to potential poisoning of local data or model parameters of participating devices. The proposed approach, Robust Federated Aggregation (RFA), relies on the aggregation of updates using the geometric median, which can be computed efficiently using a Weiszfeld-type algorithm. RFA is agnostic to the level of corruption and aggregates model updates without revealing each deviceâ€™s individual contribution. We establish the convergence of the robust federated learning algorithm for the stochastic learning of additive models with least squares. We also offer two variants of RFA: a faster one with one-step robust aggregation, and another one with on-device personalization. We present experimental results with additive models and deep networks for three tasks in computer vision and natural language processing. The experiments show that RFA is competitive with the classical aggregation when the level of corruption is low, while demonstrating greater robustness under high corruption.;https://ieeexplore.ieee.org/abstract/document/9721118/;LRzRDSJFcwsJ
Ghosh, A., Hong, J., Yin, D., & Ramchandran, K. (2019). Robust federated learning in a heterogeneous environment. arXiv preprint arXiv:1906.06629.;0_federated_learning_data_privacy;2019;Robust federated learning in a heterogeneous environment;Avishek Ghosh, Justin Hong, Dong Yin, Kannan Ramchandran;arXiv preprint arXiv:1906.06629, 2019;"We study a recently proposed large-scale distributed learning paradigm, namely Federated Learning, where the worker machines are end users' own devices. Statistical and computational challenges arise in Federated Learning particularly in the presence of heterogeneous data distribution (i.e., data points on different devices belong to different distributions signifying different clusters) and Byzantine machines (i.e., machines that may behave abnormally, or even exhibit arbitrary and potentially adversarial behavior). To address the aforementioned challenges, first we propose a general statistical model for this problem which takes both the cluster structure of the users and the Byzantine machines into account. Then, leveraging the statistical model, we solve the robust heterogeneous Federated Learning problem \emph{optimally}; in particular our algorithm matches the lower bound on the estimation error in dimension and the number of data points. Furthermore, as a by-product, we prove statistical guarantees for an outlier-robust clustering algorithm, which can be considered as the Lloyd algorithm with robust estimation. Finally, we show via synthetic as well as real data experiments that the estimation error obtained by our proposed algorithm is significantly better than the non-Byzantine-robust algorithms; in particular, we gain at least by 53\% and 33\% for synthetic and real data experiments, respectively, in typical settings.";https://arxiv.org/abs/1906.06629;ox5tDa-DTtgJ
Han, Y., & Zhang, X. (2019). Robust federated training via collaborative machine teaching using trusted instances. arXiv preprint arXiv:1905.02941.;0_federated_learning_data_privacy;2019;Robust federated training via collaborative machine teaching using trusted instances;Yufei Han, Xiangliang Zhang;arXiv preprint arXiv:1905.02941, 2019;Federated learning performs distributed model training using local data hosted by agents. It shares only model parameter updates for iterative aggregation at the server. Although it is privacy-preserving by design, federated learning is vulnerable to noise corruption of local agents, as demonstrated in the previous study on adversarial data poisoning threat against federated learning systems. Even a single noise-corrupted agent can bias the model training. In our work, we propose a collaborative and privacy-preserving machine teaching paradigm with multiple distributed teachers, to improve robustness of the federated training process against local data corruption. We assume that each local agent (teacher) have the resources to verify a small portions of trusted instances, which may not by itself be adequate for learning. In the proposed collaborative machine teaching method, these trusted instances guide the distributed agents to jointly select a compact while informative training subset from data hosted by their own. Simultaneously, the agents learn to add changes of limited magnitudes into the selected data instances, in order to improve the testing performances of the federally trained model despite of the training data corruption. Experiments on toy and real data demonstrate that our approach can identify training set bugs effectively and suggest appropriate changes to the labels. Our algorithm is a step toward trustworthy machine learning.;https://arxiv.org/abs/1905.02941;5vliqgBNB4UJ
Wu, W., He, L., Lin, W., Mao, R., Maple, C., & Jarvis, S. (2020). SAFA: A semi-asynchronous protocol for fast federated learning with low overhead. IEEE Transactions on Computers, 70(5), 655-668.;0_federated_learning_data_privacy;2020;SAFA: A semi-asynchronous protocol for fast federated learning with low overhead;Wentai Wu, Ligang He, Weiwei Lin, Rui Mao, Carsten Maple, Stephen Jarvis;IEEE Transactions on Computers 70 (5), 655-668, 2020;Federated learning (FL) has attracted increasing attention as a promising approach to driving a vast number of end devices with artificial intelligence. However, it is very challenging to guarantee the efficiency of FL considering the unreliable nature of end devices while the cost of device-server communication cannot be neglected. In this article, we propose SAFA, a semi-asynchronous FL protocol, to address the problems in federated learning such as low round efficiency and poor convergence rate in extreme conditions (e.g., clients dropping offline frequently). We introduce novel designs in the steps of model distribution, client selection and global aggregation to mitigate the impacts of stragglers, crashes and model staleness in order to boost efficiency and improve the quality of the global model. We have conducted extensive experiments with typical machine learning tasks. The results demonstrate that the proposed protocol is effective in terms of shortening federated round duration, reducing local resource wastage, and improving the accuracy of the global model at an acceptable communication cost.;https://ieeexplore.ieee.org/abstract/document/9093123/;Et-dUZKtmDgJ
Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S., Stich, S., & Suresh, A. T. (2020, November). Scaffold: Stochastic controlled averaging for federated learning. In International conference on machine learning (pp. 5132-5143). PMLR.;0_federated_learning_data_privacy;2020;Scaffold: Stochastic controlled averaging for federated learning;Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, Ananda Theertha Suresh;International conference on machine learning, 5132-5143, 2020;Federated learning is a key scenario in modern large-scale machine learning where the data remains distributed over a large number of clients and the task is to learn a centralized model without transmitting the client data. The standard optimization algorithm used in this setting is Federated Averaging (FedAvg) due to its low communication cost. We obtain a tight characterization of the convergence of FedAvg and prove that heterogeneity (non-iid-ness) in the client’s data results in a ‘drift’in the local updates resulting in poor performance. As a solution, we propose a new algorithm (SCAFFOLD) which uses control variates (variance reduction) to correct for the ‘client drift’. We prove that SCAFFOLD requires significantly fewer communication rounds and is not affected by data heterogeneity or client sampling. Further, we show that (for quadratics) SCAFFOLD can take advantage of similarity in the client’s data yielding even faster convergence. The latter is the first result to quantify the usefulness of local-steps in distributed optimization.;http://proceedings.mlr.press/v119/karimireddy20a.html;iRw3cjUxQSAJ
Ghazi, B., Pagh, R., & Velingker, A. (2019). Scalable and differentially private distributed aggregation in the shuffled model. arXiv preprint arXiv:1906.08320.;0_federated_learning_data_privacy;2019;Scalable and differentially private distributed aggregation in the shuffled model;Badih Ghazi, Rasmus Pagh, Ameya Velingker;arXiv preprint arXiv:1906.08320, 2019;"Federated learning promises to make machine learning feasible on distributed, private datasets by implementing gradient descent using secure aggregation methods. The idea is to compute a global weight update without revealing the contributions of individual users. Current practical protocols for secure aggregation work in an ""honest but curious"" setting where a curious adversary observing all communication to and from the server cannot learn any private information assuming the server is honest and follows the protocol. A more scalable and robust primitive for privacy-preserving protocols is shuffling of user data, so as to hide the origin of each data item. Highly scalable and secure protocols for shuffling, so-called mixnets, have been proposed as a primitive for privacy-preserving analytics in the Encode-Shuffle-Analyze framework by Bittau et al., which was later analytically studied by Erlingsson et al. and Cheu et al.. The recent papers by Cheu et al., and Balle et al. have given protocols for secure aggregation that achieve differential privacy guarantees in this ""shuffled model"". Their protocols come at a cost, though: Either the expected aggregation error or the amount of communication per user scales as a polynomial in the number of users . In this paper we propose simple and more efficient protocol for aggregation in the shuffled model, where communication as well as error increases only polylogarithmically in . Our new technique is a conceptual ""invisibility cloak"" that makes users' data almost indistinguishable from random noise while introducing zero distortion on the sum.";https://arxiv.org/abs/1906.08320;9BiSUyuGRekJ
Chai, D., Wang, L., Chen, K., & Yang, Q. (2020). Secure federated matrix factorization. IEEE Intelligent Systems, 36(5), 11-20.;0_federated_learning_data_privacy;2020;Secure federated matrix factorization;Di Chai, Leye Wang, Kai Chen, Qiang Yang;IEEE Intelligent Systems 36 (5), 11-20, 2020;To protect user privacy and meet law regulations, federated (machine) learning is obtaining vast interests in recent years. The key principle of federated learning is training a machine learning model without needing to know each user’s personal raw private data. In this article, we propose a secure matrix factorization framework under the federated learning setting, called FedMF. First, we design a user-level distributed matrix factorization framework where the model can be learned when each user only uploads the gradient information (instead of the raw preference data) to the server. While gradient information seems secure, we prove that it could still leak users’ raw data. To this end, we enhance the distributed matrix factorization framework with homomorphic encryption. We implement the prototype of FedMF and test it with a real movie rating dataset. Results verify the feasibility of FedMF. We also discuss the challenges for applying FedMF in practice for future research.;https://ieeexplore.ieee.org/abstract/document/9162459/;s2gMxhQT2wwJ
Niu, C., Wu, F., Tang, S., Hua, L., Jia, R., Lv, C., ... & Chen, G. (2019). Secure federated submodel learning. arXiv preprint arXiv:1911.02254.;0_federated_learning_data_privacy;2019;Secure federated submodel learning;Chaoyue Niu, Fan Wu, Shaojie Tang, Lifeng Hua, Rongfei Jia, Chengfei Lv, Zhihua Wu, Guihai Chen;arXiv preprint arXiv:1911.02254, 2019;"Federated learning was proposed with an intriguing vision of achieving collaborative machine learning among numerous clients without uploading their private data to a cloud server. However, the conventional framework requires each client to leverage the full model for learning, which can be prohibitively inefficient for resource-constrained clients and large-scale deep learning tasks. We thus propose a new framework, called federated submodel learning, where clients download only the needed parts of the full model, namely submodels, and then upload the submodel updates. Nevertheless, the ""position"" of a client's truly required submodel corresponds to her private data, and its disclosure to the cloud server during interactions inevitably breaks the tenet of federated learning. To integrate efficiency and privacy, we have designed a secure federated submodel learning scheme coupled with a private set union protocol as a cornerstone. Our secure scheme features the properties of randomized response, secure aggregation, and Bloom filter, and endows each client with a customized plausible deniability, in terms of local differential privacy, against the position of her desired submodel, thus protecting her private data. We further instantiated our scheme with the e-commerce recommendation scenario in Alibaba, implemented a prototype system, and extensively evaluated its performance over 30-day Taobao user data. The analysis and evaluation results demonstrate the feasibility and scalability of our scheme from model accuracy and convergency, practical communication, computation, and storage overheads, as well as manifest its remarkable advantages over the conventional federated learning framework.";https://arxiv.org/abs/1911.02254;JcCgdKfekPgJ
Cheng, K., Fan, T., Jin, Y., Liu, Y., Chen, T., Papadopoulos, D., & Yang, Q. (2021). Secureboost: A lossless federated learning framework. IEEE Intelligent Systems, 36(6), 87-98.;0_federated_learning_data_privacy;2021;Secureboost: A lossless federated learning framework;Kewei Cheng, Tao Fan, Yilun Jin, Yang Liu, Tianjian Chen, Dimitrios Papadopoulos, Qiang Yang;IEEE Intelligent Systems 36 (6), 87-98, 2021;The protection of user privacy is an important concern in machine learning, as evidenced by the rolling out of the General Data Protection Regulation (GDPR) in the European Union (EU) in May 2018. The GDPR is designed to give users more control over their personal data, which motivates us to explore machine learning frameworks for data sharing that do not violate user privacy. To meet this goal, in this article, we propose a novel lossless privacy-preserving tree-boosting system known as SecureBoost in the setting of federated learning. SecureBoost first conducts entity alignment under a privacy-preserving protocol and then constructs boosting trees across multiple parties with a carefully designed encryption strategy. This federated learning system allows the learning process to be jointly conducted over multiple parties with common user samples but different feature sets, which corresponds to a vertically partitioned dataset. An advantage of SecureBoost is that it provides the same level of accuracy as the non -privacy-preserving approach while at the same time, reveals no information of each private data provider. We show that the SecureBoost framework is as accurate as other nonfederated gradient tree-boosting algorithms that require centralized data, and thus, it is highly scalable and practical for industrial applications such as credit risk analysis. To this end, we discuss information leakage during the protocol execution and propose ways to provably reduce it.;https://ieeexplore.ieee.org/abstract/document/9440789/;ul5Ij_rcNAQJ
Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V., ... & Roselander, J. (2019). Towards federated learning at scale: System design. Proceedings of machine learning and systems, 1, 374-388.;0_federated_learning_data_privacy;2019;Towards federated learning at scale: System design;Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konečný, Stefano Mazzocchi, Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage, Jason Roselander;Proceedings of machine learning and systems 1, 374-388, 2019;Federated Learning is a distributed machine learning approach which enables model training on a large corpus of decentralized data. We have built a scalable production system for Federated Learning in the domain of mobile devices, based on TensorFlow. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions.;https://proceedings.mlsys.org/paper_files/paper/2019/hash/7b770da633baf74895be22a8807f1a8f-Abstract.html;azbn-NEouKQJ
Liu, D., Dligach, D., & Miller, T. (2019, August). Two-stage federated phenotyping and patient representation learning. In Proceedings of the conference. Association for Computational Linguistics. Meeting (Vol. 2019, p. 283). NIH Public Access.;0_federated_learning_data_privacy;2019;Two-stage federated phenotyping and patient representation learning;Dianbo Liu, Dmitriy Dligach, Timothy Miller;Proceedings of the conference. Association for Computational Linguistics. Meeting 2019, 283, 2019;A large percentage of medical information is in unstructured text format in electronic medical record systems. Manual extraction of information from clinical notes is extremely time consuming. Natural language processing has been widely used in recent years for automatic information extraction from medical texts. However, algorithms trained on data from a single healthcare provider are not generalizable and error-prone due to the heterogeneity and uniqueness of medical documents. We develop a two-stage federated natural language processing method that enables utilization of clinical notes from different hospitals or clinics without moving the data, and demonstrate its performance using obesity and comorbities phenotyping as medical task. This approach not only improves the quality of a specific clinical task but also facilitates knowledge progression in the whole healthcare system, which is an essential part of learning health system. To the best of our knowledge, this is the first application of federated machine learning in clinical NLP.;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8072229/;cRn2_wftio0J
Amiri, M. M., Gündüz, D., Kulkarni, S. R., & Poor, H. V. (2020, June). Update aware device scheduling for federated learning at the wireless edge. In 2020 IEEE International Symposium on Information Theory (ISIT) (pp. 2598-2603). IEEE.;0_federated_learning_data_privacy;2020;Update aware device scheduling for federated learning at the wireless edge;Mohammad Mohammadi Amiri, Deniz Gündüz, Sanjeev R Kulkarni, H Vincent Poor;2020 IEEE International Symposium on Information Theory (ISIT), 2598-2603, 2020;We study federated learning (FL) at the wireless edge, where power-limited devices with local datasets train a joint model with the help of a remote parameter server (PS). We assume that the devices are connected to the PS through a bandwidth-limited shared wireless channel. At each iteration of FL, a subset of the devices are scheduled to transmit their local model updates to the PS over orthogonal channel resources. We design novel scheduling policies, that decide on the subset of devices to transmit at each round not only based on their channel conditions, but also on the significance of their local model updates. Numerical results show that the proposed scheduling policy provides a better long-term performance than scheduling policies based only on either of the two metrics individually. We also observe that when the data is independent and identically distributed (i.i.d.) across devices, selecting a single device at each round provides the best performance, while when the data distribution is non-i.i.d., more devices should be scheduled.;https://ieeexplore.ieee.org/abstract/document/9173960/;B-0ycac315kJ
Corinzia, L., Beuret, A., & Buhmann, J. M. (2019). Variational federated multi-task learning. arXiv preprint arXiv:1906.06268.;0_federated_learning_data_privacy;2019;Variational federated multi-task learning;Luca Corinzia, Ami Beuret, Joachim M Buhmann;arXiv preprint arXiv:1906.06268, 2019;In federated learning, a central server coordinates the training of a single model on a massively distributed network of devices. This setting can be naturally extended to a multi-task learning framework, to handle real-world federated datasets that typically show strong statistical heterogeneity among devices. Despite federated multi-task learning being shown to be an effective paradigm for real-world datasets, it has been applied only on convex models. In this work, we introduce VIRTUAL, an algorithm for federated multi-task learning for general non-convex models. In VIRTUAL the federated network of the server and the clients is treated as a star-shaped Bayesian network, and learning is performed on the network using approximated variational inference. We show that this method is effective on real-world federated datasets, outperforming the current state-of-the-art for federated learning, and concurrently allowing sparser gradient updates.;https://arxiv.org/abs/1906.06268;WZTss6nypX4J
Pandey, S. R., Tran, N. H., Bennis, M., Tun, Y. K., Manzoor, A., & Hong, C. S. (2020). A crowdsourcing framework for on-device federated learning. IEEE Transactions on Wireless Communications, 19(5), 3241-3256.;0_federated_learning_data_privacy;2020;A crowdsourcing framework for on-device federated learning;Shashi Raj Pandey, Nguyen H Tran, Mehdi Bennis, Yan Kyaw Tun, Aunas Manzoor, Choong Seon Hong;IEEE Transactions on Wireless Communications 19 (5), 3241-3256, 2020;Federated learning (FL) rests on the notion of training a global model in a decentralized manner. Under this setting, mobile devices perform computations on their local data before uploading the required updates to improve the global model. However, when the participating clients implement an uncoordinated computation strategy, the difficulty is to handle the communication efficiency (i.e., the number of communications per iteration) while exchanging the model parameters during aggregation. Therefore, a key challenge in FL is how users participate to build a high-quality global model with communication efficiency. We tackle this issue by formulating a utility maximization problem, and propose a novel crowdsourcing framework to leverage FL that considers the communication efficiency during parameters exchange. First, we show an incentive-based interaction between the crowdsourcing platform and the participating client's independent strategies for training a global learning model, where each side maximizes its own benefit. We formulate a two-stage Stackelberg game to analyze such scenario and find the game's equilibria. Second, we formalize an admission control scheme for participating clients to ensure a level of local accuracy. Simulated results demonstrate the efficacy of our proposed solution with up to 22% gain in the offered reward.;https://ieeexplore.ieee.org/abstract/document/8995775/;B_FnaQ959xEJ
Wang, X., Han, Y., Wang, C., Zhao, Q., Chen, X., & Chen, M. (2019). In-edge ai: Intelligentizing mobile edge computing, caching and communication by federated learning. Ieee Network, 33(5), 156-165.;7_edge_computing_deep_learning;2019;In-edge ai: Intelligentizing mobile edge computing, caching and communication by federated learning;Xiaofei Wang, Yiwen Han, Chenyang Wang, Qiyang Zhao, Xu Chen, Min Chen;Ieee Network 33 (5), 156-165, 2019;Recently, along with the rapid development of mobile communication technology, edge computing theory and techniques have been attracting more and more attention from global researchers and engineers, which can significantly bridge the capacity of cloud and requirement of devices by the network edges, and thus can accelerate content delivery and improve the quality of mobile services. In order to bring more intelligence to edge systems, compared to traditional optimization methodology, and driven by the current deep learning techniques, we propose to integrate the Deep Reinforcement Learning techniques and Federated Learning framework with mobile edge systems, for optimizing mobile edge computing, caching and communication. And thus, we design the “In-Edge AI” framework in order to intelligently utilize the collaboration among devices and edge nodes to exchange the learning parameters for a better training and inference of the models, and thus to carry out dynamic system-level optimization and application-level enhancement while reducing the unnecessary system communication load. “In-Edge AI” is evaluated and proved to have near-optimal performance but relatively low overhead of learning, while the system is cognitive and adaptive to mobile communication systems. Finally, we discuss several related challenges and opportunities for unveiling a promising upcoming future of “In-Edge AI.”;https://ieeexplore.ieee.org/abstract/document/8770530/;vpvs0vUSdKwJ
Liu, B., Wang, L., Liu, M., & Xu, C. Z. (2020). Federated imitation learning: A novel framework for cloud robotic systems with heterogeneous sensor data. IEEE Robotics and Automation Letters, 5(2), 3509-3516.;0_federated_learning_data_privacy;2020;Federated imitation learning: A novel framework for cloud robotic systems with heterogeneous sensor data;Boyi Liu, Lujia Wang, Ming Liu, Cheng-Zhong Xu;IEEE Robotics and Automation Letters 5 (2), 3509-3516, 2020;Humans are capable of learning a new behavior by observing others to perform the skill. Similarly, robots can also implement this by imitation learning. Furthermore, if with external guidance, humans can master the new behavior more efficiently. So, how can robots achieve this? To address the issue, we present a novel framework named FIL. It provides a heterogeneous knowledge fusion mechanism for cloud robotic systems. Then, a knowledge fusion algorithm in FIL is proposed. It enables the cloud to fuse heterogeneous knowledge from local robots and generate guide models for robots with service requests. After that, we introduce a knowledge transfer scheme to facilitate local robots acquiring knowledge from the cloud. With FIL, a robot is capable of utilizing knowledge from other robots to increase its imitation learning in accuracy and efficiency. Compared with transfer learning and meta-learning, FIL is more suitable to be deployed in cloud robotic systems. Finally, we conduct experiments of a self-driving task for robots (cars). The experimental results demonstrate that the shared model generated by FIL increases imitation learning efficiency of local robots in cloud robotic systems.;https://ieeexplore.ieee.org/abstract/document/9013081/;v853PWciFH4J
He, X., Ling, Q., & Chen, T. (2019, June). Byzantine-robust stochastic gradient descent for distributed low-rank matrix completion. In 2019 IEEE Data Science Workshop (DSW) (pp. 322-326). IEEE.;0_federated_learning_data_privacy;2019;Byzantine-robust stochastic gradient descent for distributed low-rank matrix completion;Xuechao He, Qing Ling, Tianyi Chen;2019 IEEE Data Science Workshop (DSW), 322-326, 2019;To overcome the growing privacy concerns of centralized machine learning, federated learning has been proposed to enable collaboratively training a model with data stored locally in the owners' devices. However, adversarial attacks (e.g., Byzantine attacks in the worst case) still exist in the federated learning systems so that the information shared by the data owners are unreliable. Byzantine-robust aggregation methods, such as median, geometric median and Krum, have been found to perform well in eliminating the negative effects caused by the Byzantine attacks. In this paper, we study the distributed low-rank matrix completion problem in a federated learning setting, where some data owners are malicious. We combine the Byzantine-robust aggregation rules with stochastic gradient descent (SGD) to solve this problem. Numerical experiments on the Netflix dataset demonstrate that the proposed methods are able to achieve comparable performance relative to SGD without attacks.;https://ieeexplore.ieee.org/abstract/document/8755575/;l6sh25lelIkJ
Chen, Y., Su, L., & Xu, J. (2017). Distributed statistical machine learning in adversarial settings: Byzantine gradient descent. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 1(2), 1-25.;0_federated_learning_data_privacy;2017;Distributed statistical machine learning in adversarial settings: Byzantine gradient descent;Yudong Chen, Lili Su, Jiaming Xu;Proceedings of the ACM on Measurement and Analysis of Computing Systems 1 (2), 1-25, 2017;"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server and m working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of the m working machines suffer Byzantine faults -- a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks.In this paper, based on the geometric median of means of the gradients, we propose a simple variant of the classical gradient descent method. We show that our method can tolerate q Byzantine failures up to 2(1+ε)q ≤ for an arbitrarily small but fixed constant ε > 0. The parameter estimate converges in O(log N) rounds with an estimation error on the order of max{√dq/N, √d/N, which is larger than the minimax-optimal error rate √d/N in the centralized and failure-free setting by at most a factor of √q. The total computational complexity of our algorithm is of O((Nd/m) log N) at each working machine and O(md + kd log3 N) at the central server, and the total communication cost is of O(m d log N). We further provide an application of our general results to the linear regression problem.A key challenge arises in the above problem is that Byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients. To handle this issue in the analysis, we prove that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function.";https://dl.acm.org/doi/abs/10.1145/3154503;Zu9UQmss7b4J
Smith, V., Chiang, C. K., Sanjabi, M., & Talwalkar, A. S. (2017). Federated multi-task learning. Advances in neural information processing systems, 30.;0_federated_learning_data_privacy;2017;Federated multi-task learning;Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, Ameet S Talwalkar;Advances in neural information processing systems 30, 2017;Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets.;https://proceedings.neurips.cc/paper/7029-federated-multi-task-learning;s7ppoyzf4iUJ
Han, P., Wang, S., & Leung, K. K. (2020, November). Adaptive gradient sparsification for efficient federated learning: An online learning approach. In 2020 IEEE 40th international conference on distributed computing systems (ICDCS) (pp. 300-310). IEEE.;0_federated_learning_data_privacy;2020;Adaptive gradient sparsification for efficient federated learning: An online learning approach;Pengchao Han, Shiqiang Wang, Kin K Leung;2020 IEEE 40th international conference on distributed computing systems (ICDCS), 300-310, 2020;Federated learning (FL) is an emerging technique for training machine learning models using geographically dispersed data collected by local entities. It includes local computation and synchronization steps. To reduce the communication overhead and improve the overall efficiency of FL, gradient sparsification (GS) can be applied, where instead of the full gradient, only a small subset of important elements of the gradient is communicated. Existing work on GS uses a fixed degree of gradient sparsity for i.i.d.-distributed data within a datacenter. In this paper, we consider adaptive degree of sparsity and non-i.i.d. local datasets. We first present a fairness-aware GS method which ensures that different clients provide a similar amount of updates. Then, with the goal of minimizing the overall training time, we propose a novel online learning formulation and algorithm for automatically determining the near-optimal communication and computation trade-off that is controlled by the degree of gradient sparsity. The online learning algorithm uses an estimated sign of the derivative of the objective function, which gives a regret bound that is asymptotically equal to the case where exact derivative is available. Experiments with real datasets confirm the benefits of our proposed approaches, showing up to 40% improvement in model accuracy for a finite training time.;https://ieeexplore.ieee.org/abstract/document/9355797/;1eEuN6L447oJ
Li, H., & Han, T. (2019). An end-to-end encrypted neural network for gradient updates transmission in federated learning. arXiv preprint arXiv:1908.08340.;0_federated_learning_data_privacy;2019;An end-to-end encrypted neural network for gradient updates transmission in federated learning;Hongyu Li, Tianqi Han;arXiv preprint arXiv:1908.08340, 2019;Federated learning is a distributed learning method to train a shared model by aggregating the locally-computed gradient updates. In federated learning, bandwidth and privacy are two main concerns of gradient updates transmission. This paper proposes an end-to-end encrypted neural network for gradient updates transmission. This network first encodes the input gradient updates to a lower-dimension space in each client, which significantly mitigates the pressure of data communication in federated learning. The encoded gradient updates are directly recovered as a whole, i.e. the aggregated gradient updates of the trained model, in the decoding layers of the network on the server. In this way, gradient updates encrypted in each client are not only prevented from interception during communication, but also unknown to the server. Based on the encrypted neural network, a novel federated learning framework is designed in real applications. Experimental results show that the proposed network can effectively achieve two goals, privacy protection and data compression, under a little sacrifice of the model accuracy in federated learning.;https://arxiv.org/abs/1908.08340;nqp90iz3pQUJ
Chen, F., Luo, M., Dong, Z., Li, Z., & He, X. (2018). Federated meta-learning with fast convergence and efficient communication. arXiv preprint arXiv:1802.07876.;0_federated_learning_data_privacy;2018;Federated meta-learning with fast convergence and efficient communication;Fei Chen, Mi Luo, Zhenhua Dong, Zhenguo Li, Xiuqiang He;arXiv preprint arXiv:1802.07876, 2018;Statistical and systematic challenges in collaboratively training machine learning models across distributed networks of mobile devices have been the bottlenecks in the real-world application of federated learning. In this work, we show that meta-learning is a natural choice to handle these issues, and propose a federated meta-learning framework FedMeta, where a parameterized algorithm (or meta-learner) is shared, instead of a global model in previous approaches. We conduct an extensive empirical evaluation on LEAF datasets and a real-world production dataset, and demonstrate that FedMeta achieves a reduction in required communication cost by 2.82-4.33 times with faster convergence, and an increase in accuracy by 3.23%-14.84% as compared to Federated Averaging (FedAvg) which is a leading optimization algorithm in federated learning. Moreover, FedMeta preserves user privacy since only the parameterized algorithm is transmitted between mobile devices and central servers, and no raw data is collected onto the servers.;https://arxiv.org/abs/1802.07876;HTKHmE7ZyBAJ
Orekondy, T., Oh, S. J., Zhang, Y., Schiele, B., & Fritz, M. (2018). Gradient-leaks: Understanding and controlling deanonymization in federated learning. arXiv preprint arXiv:1805.05838.;0_federated_learning_data_privacy;2018;Gradient-leaks: Understanding and controlling deanonymization in federated learning;Tribhuvanesh Orekondy, Seong Joon Oh, Yang Zhang, Bernt Schiele, Mario Fritz;arXiv preprint arXiv:1805.05838, 2018;Federated Learning (FL) systems are gaining popularity as a solution to training Machine Learning (ML) models from large-scale user data collected on personal devices (e.g., smartphones) without their raw data leaving the device. At the core of FL is a network of anonymous user devices sharing training information (model parameter updates) computed locally on personal data. However, the type and degree to which user-specific information is encoded in the model updates is poorly understood. In this paper, we identify model updates encode subtle variations in which users capture and generate data. The variations provide a strong statistical signal, allowing an adversary to effectively deanonymize participating devices using a limited set of auxiliary data. We analyze resulting deanonymization attacks on diverse tasks on real-world (anonymized) user-generated data across a range of closed- and open-world scenarios. We study various strategies to mitigate the risks of deanonymization. As random perturbation methods do not offer convincing operating points, we propose data-augmentation strategies which introduces adversarial biases in device data and thereby, offer substantial protection against deanonymization threats with little effect on utility.;https://arxiv.org/abs/1805.05838;LP-lkum1H4EJ
Ren, J., Yu, G., & Ding, G. (2020). Accelerating DNN training in wireless federated edge learning systems. IEEE Journal on Selected Areas in Communications, 39(1), 219-232.;0_federated_learning_data_privacy;2020;Accelerating DNN training in wireless federated edge learning systems;Jinke Ren, Guanding Yu, Guangyao Ding;IEEE Journal on Selected Areas in Communications 39 (1), 219-232, 2020;Training task in classical machine learning models, such as deep neural networks, is generally implemented at a remote cloud center for centralized learning, which is typically time-consuming and resource-hungry. It also incurs serious privacy issue and long communication latency since a large amount of data are transmitted to the centralized node. To overcome these shortcomings, we consider a newly-emerged framework, namely federated edge learning, to aggregate local learning updates at the network edge in lieu of users' raw data. Aiming at accelerating the training process, we first define a novel performance evaluation criterion, called learning efficiency. We then formulate a training acceleration optimization problem in the CPU scenario, where each user device is equipped with CPU. The closed-form expressions for joint batchsize selection and communication resource allocation are developed and some insightful results are highlighted. Further, we extend our learning framework to the GPU scenario. The optimal solution in this scenario is manifested to have the similar structure as that of the CPU scenario, recommending that our proposed algorithm is applicable in more general systems. Finally, extensive experiments validate the theoretical analysis and demonstrate that the proposed algorithm can reduce the training time and improve the learning accuracy simultaneously.;https://ieeexplore.ieee.org/abstract/document/9252924/;gATFT3fy9fAJ
Zeng, Q., Du, Y., Huang, K., & Leung, K. K. (2020, June). Energy-efficient radio resource allocation for federated edge learning. In 2020 IEEE International Conference on Communications Workshops (ICC Workshops) (pp. 1-6). IEEE.;0_federated_learning_data_privacy;2020;Energy-efficient radio resource allocation for federated edge learning;Qunsong Zeng, Yuqing Du, Kaibin Huang, Kin K Leung;2020 IEEE International Conference on Communications Workshops (ICC Workshops), 1-6, 2020;Edge machine learning involves the development of learning algorithms at the network edge to leverage massive distributed data and computation resources. Among others, the framework of federated edge learning (FEEL) is particularly promising for its data-privacy preservation. FEEL coordinates global model training at a server and local model training at edge devices over wireless links. In this work, we explore the new direction of energy-efficient radio resource management (RRM) for FEEL. To reduce devices' energy consumption, we propose energy-efficient strategies for bandwidth allocation and scheduling. They adapt to devices' channel states and computation capacities so as to reduce their sum energy consumption while warranting learning performance. In contrast with the traditional rate-maximization designs, the derived optimal policies allocate more bandwidth to those scheduled devices with weaker channels or poorer computation capacities, which are the bottlenecks of synchronized model updates in FEEL. On the other hand, the scheduling priority function derived in closed form gives preferences to devices with better channels and computation capacities. Substantial energy reduction contributed by the proposed strategies is demonstrated in learning experiments.;https://ieeexplore.ieee.org/abstract/document/9145118/;qC9hOI7tx80J
Konečný, J., McMahan, H. B., Ramage, D., & Richtárik, P. (2016). Federated optimization: Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527.;0_federated_learning_data_privacy;2016;Federated optimization: Distributed machine learning for on-device intelligence;Jakub Konečný, H Brendan McMahan, Daniel Ramage, Peter Richtárik;arXiv preprint arXiv:1610.02527, 2016;We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are unevenly distributed over an extremely large number of nodes. The goal is to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of the utmost importance and minimizing the number of rounds of communication is the principal goal. A motivating example arises when we keep the training data locally on users' mobile devices instead of logging it to a data center for training. In federated optimziation, the devices are used as compute nodes performing computation on their local data in order to update a global model. We suppose that we have extremely large number of devices in the network --- as many as the number of users of a given service, each of which has only a tiny fraction of the total data available. In particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, it is reasonable to assume that no device has a representative sample of the overall distribution. We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results for sparse convex problems. This work also sets a path for future research needed in the context of \federated optimization.;https://arxiv.org/abs/1610.02527;tef64Vm4Z6EJ
Zhuo, H. H., Feng, W., Lin, Y., Xu, Q., & Yang, Q. (2019). Federated deep reinforcement learning. arXiv preprint arXiv:1901.08277.;0_federated_learning_data_privacy;2019;Federated deep reinforcement learning;Hankz Hankui Zhuo, Wenfeng Feng, Yufeng Lin, Qian Xu, Qiang Yang;arXiv preprint arXiv:1901.08277, 2019;In deep reinforcement learning, building policies of high-quality is challenging when the feature space of states is small and the training data is limited. Despite the success of previous transfer learning approaches in deep reinforcement learning, directly transferring data or models from an agent to another agent is often not allowed due to the privacy of data and/or models in many privacy-aware applications. In this paper, we propose a novel deep reinforcement learning framework to federatively build models of high-quality for agents with consideration of their privacies, namely Federated deep Reinforcement Learning (FedRL). To protect the privacy of data and models, we exploit Gausian differentials on the information shared with each other when updating their local models. In the experiment, we evaluate our FedRL framework in two diverse domains, Grid-world and Text2Action domains, by comparing to various baselines.;https://arxiv.org/abs/1901.08277;3fj9l4sIy18J
Liang, X., Liu, Y., Chen, T., Liu, M., & Yang, Q. (2022). Federated transfer reinforcement learning for autonomous driving. In Federated and Transfer Learning (pp. 357-371). Cham: Springer International Publishing.;2_safety_system_autonomous_vehicle;2022;Federated transfer reinforcement learning for autonomous driving;Xinle Liang, Yang Liu, Tianjian Chen, Ming Liu, Qiang Yang;Federated and Transfer Learning, 357-371, 2022;Reinforcement learning (RL) is widely used in autonomous driving tasks and training RL models typically involves in a multi-step process: pre-training RL models on simulators, uploading the pre-trained model to real-life robots, and fine-tuning the weight parameters on robot vehicles. This sequential process is extremely time-consuming and more importantly, knowledge from the fine-tuned model stays local and can not be re-used or leveraged collaboratively. To tackle this problem, we present an online federated RL transfer process for real-time knowledge extraction where all the participant agents make corresponding actions with the knowledge learned by others, even when they are acting in very different environments. To validate the effectiveness of the proposed approach, we constructed a real-life collision avoidance system with Microsoft Airsim simulator and NVIDIA JetsonTX2 car agents, which cooperatively learn from scratch to avoid collisions in indoor environment with obstacle objects. We demonstrate that with the proposed framework, the simulator car agents can transfer knowledge to the RC cars in real-time, with 27% increase in the average distance with obstacles and 42% decrease in the collision counts.;https://link.springer.com/chapter/10.1007/978-3-031-11748-0_15;7ff2ZJxfe-gJ
Zhu, G., Du, Y., Gündüz, D., & Huang, K. (2020). One-bit over-the-air aggregation for communication-efficient federated edge learning: Design and convergence analysis. IEEE Transactions on Wireless Communications, 20(3), 2120-2135.;0_federated_learning_data_privacy;2020;One-bit over-the-air aggregation for communication-efficient federated edge learning: Design and convergence analysis;Guangxu Zhu, Yuqing Du, Deniz Gündüz, Kaibin Huang;IEEE Transactions on Wireless Communications 20 (3), 2120-2135, 2020;Federated edge learning (FEEL) is a popular framework for model training at an edge server using data distributed at edge devices (e.g., smart-phones and sensors) without compromising their privacy. In the FEEL framework, edge devices periodically transmit high-dimensional stochastic gradients to the edge server, where these gradients are aggregated and used to update a global model. When the edge devices share the same communication medium, the multiple access channel (MAC) from the devices to the edge server induces a communication bottleneck. To overcome this bottleneck, an efficient broadband analog transmission scheme has been recently proposed, featuring the aggregation of analog modulated gradients (or local models) via the waveform-superposition property of the wireless medium. However, the assumed linear analog modulation makes it difficult to deploy this technique in modern wireless systems that exclusively use digital modulation. To address this issue, we propose in this work a novel digital version of broadband over-the-air aggregation, called one-bit broadband digital aggregation (OBDA). The new scheme features one-bit gradient quantization followed by digital quadrature amplitude modulation (QAM) at edge devices and over-the-air majority-voting based decoding at edge server. We provide a comprehensive analysis of the effects of wireless channel hostilities (channel noise, fading, and channel estimation errors) on the convergence rate of the proposed FEEL scheme. The analysis shows that the hostilities slow down the convergence of the learning process by introducing a scaling factor and a bias term into the gradient norm. However, we show that all the negative effects vanish as the number of participating devices grows, but at a different rate for each type of channel hostility.;https://ieeexplore.ieee.org/abstract/document/9272666/;9XcTF2UpEuQJ
Liu, Y., Kang, Y., Xing, C., Chen, T., & Yang, Q. (2020). A secure federated transfer learning framework. IEEE Intelligent Systems, 35(4), 70-82.;0_federated_learning_data_privacy;2020;A secure federated transfer learning framework;Yang Liu, Yan Kang, Chaoping Xing, Tianjian Chen, Qiang Yang;IEEE Intelligent Systems 35 (4), 70-82, 2020;Machine learning relies on the availability of vast amounts of data for training. However, in reality, data are mostly scattered across different organizations and cannot be easily integrated due to many legal and practical constraints. To address this important challenge in the field of machine learning, we introduce a new technique and framework, known as federated transfer learning (FTL), to improve statistical modeling under a data federation. FTL allows knowledge to be shared without compromising user privacy and enables complementary knowledge to be transferred across domains in a data federation, thereby enabling a target-domain party to build flexible and effective models by leveraging rich labels from a source domain. This framework requires minimal modifications to the existing model structure and provides the same level of accuracy as the nonprivacy-preserving transfer learning. It is flexible and can be effectively adapted to various secure multiparty machine learning tasks.;https://ieeexplore.ieee.org/abstract/document/9076003/;qMOXUx_gEVsJ
Liu, Y., Liu, Y., Liu, Z., Liang, Y., Meng, C., Zhang, J., & Zheng, Y. (2020). Federated forest. IEEE Transactions on Big Data, 8(3), 843-854.;0_federated_learning_data_privacy;2020;Federated forest;Yang Liu, Yingting Liu, Zhijie Liu, Yuxuan Liang, Chuishi Meng, Junbo Zhang, Yu Zheng;IEEE Transactions on Big Data 8 (3), 843-854, 2020;Most real-world data are scattered across different companies or government organizations, and cannot be easily integrated under data privacy and related regulations such as the European Union’s General Data Protection Regulation (GDPR) and China’ Cyber Security Law. Such data islands situation and data privacy & security are two major challenges for applications of artificial intelligence. In this article, we tackle these challenges and propose a privacy-preserving machine learning model, called Federated Forest , which is a lossless learning model of the traditional random forest method, i.e., achieving the same level of accuracy as the non-privacy-preserving approach. Based on it, we developed a secure cross-regional machine learning system that allows a learning process to be jointly trained over different regions’ clients with the same user samples but different attribute sets, processing the data stored in each of them without exchanging their raw data. A novel prediction algorithm was also proposed which could largely reduce the communication overhead. Experiments on both real-world and UCI data sets demonstrate the performance of the Federated Forest is as accurate as of the non-federated version. The efficiency and robustness of our proposed system had been verified. Overall, our model is practical, scalable and extensible for real-life tasks.;https://ieeexplore.ieee.org/abstract/document/9088965/;0tII4iumNagJ
Zhu, W., Kairouz, P., McMahan, B., Sun, H., & Li, W. (2020, June). Federated heavy hitters discovery with differential privacy. In International Conference on Artificial Intelligence and Statistics (pp. 3837-3847). PMLR.;0_federated_learning_data_privacy;2020;Federated heavy hitters discovery with differential privacy;Wennan Zhu, Peter Kairouz, Brendan McMahan, Haicheng Sun, Wei Li;International Conference on Artificial Intelligence and Statistics, 3837-3847, 2020;The discovery of heavy hitters (most frequent items) in user-generated data streams drives improvements in the app and web ecosystems, but can incur substantial privacy risks if not done with care. To address these risks, we propose a distributed and privacy-preserving algorithm for discovering the heavy hitters in a population of user-generated data streams. We leverage the sampling and thresholding properties of our distributed algorithm to prove that it is inherently differentially private, without requiring additional noise. We also examine the trade-off between privacy and utility, and show that our algorithm provides excellent utility while also achieving strong privacy guarantees. A significant advantage of this approach is that it eliminates the need to centralize raw data while also avoiding the significant loss in utility incurred by local differential privacy. We validate our findings both theoretically, using worst-case analyses, and practically, using a Twitter dataset with 1.6 M tweets and over 650k users. Finally, we carefully compare our approach to Appleâ€™s local differential privacy method for discovering heavy hitters.;http://proceedings.mlr.press/v108/zhu20a.html;iTbbfn1YXlsJ
Jeong, E., Oh, S., Park, J., Kim, H., Bennis, M., & Kim, S. L. (2019). Multi-hop federated private data augmentation with sample compression. arXiv preprint arXiv:1907.06426.;0_federated_learning_data_privacy;2019;Multi-hop federated private data augmentation with sample compression;Eunjeong Jeong, Seungeun Oh, Jihong Park, Hyesung Kim, Mehdi Bennis, Seong-Lyun Kim;arXiv preprint arXiv:1907.06426, 2019;On-device machine learning (ML) has brought about the accessibility to a tremendous amount of data from the users while keeping their local data private instead of storing it in a central entity. However, for privacy guarantee, it is inevitable at each device to compensate for the quality of data or learning performance, especially when it has a non-IID training dataset. In this paper, we propose a data augmentation framework using a generative model: multi-hop federated augmentation with sample compression (MultFAug). A multi-hop protocol speeds up the end-to-end over-the-air transmission of seed samples by enhancing the transport capacity. The relaying devices guarantee stronger privacy preservation as well since the origin of each seed sample is hidden in those participants. For further privatization on the individual sample level, the devices compress their data samples. The devices sparsify their data samples prior to transmissions to reduce the sample size, which impacts the communication payload. This preprocessing also strengthens the privacy of each sample, which corresponds to the input perturbation for preserving sample privacy. The numerical evaluations show that the proposed framework significantly improves privacy guarantee, transmission delay, and local training performance with adjustment to the number of hops and compression rate.;https://arxiv.org/abs/1907.06426;OHwAyrfkduwJ
Pandey, S. R., Tran, N. H., Bennis, M., Tun, Y. K., Han, Z., & Hong, C. S. (2019, December). Incentivize to Build: A Crowdsourcing Framework for Federated Learning. In GLOBECOM (pp. 1-6).;0_federated_learning_data_privacy;2019;Incentivize to Build: A Crowdsourcing Framework for Federated Learning;Shashi Raj Pandey, Nguyen H. Tran, Mehdi Bennis, Xan Kyaw Tun, Zhu Han, Choong Seon Hong;GLOBECOM, 2019;Federated learning (FL) rests on the notion of training a global model in a decentralized manner. Under this setting, mobile devices perform computations on their local data before uploading the required updates to the central aggregator for improving the global model. However, a key challenge is to maintain communication efficiency (ie, the number of communications per iteration) when participating clients implement uncoordinated computation strategy during aggregation of model parameters. We formulate a utility maximization problem to tackle this difficulty, and propose a novel crowdsourcing framework, involving a number of participating clients with local training data to leverage FL. We show the incentivebased interaction between the crowdsourcing platform and the participating client’s independent strategies for training a global learning model, where each side maximizes its own benefit. We formulate a two-stage Stackelberg game to analyze such scenario and find the game’s equilibria. Further, we illustrate the efficacy of our proposed framework with simulation results. Results show that the proposed mechanism outperforms the heuristic approach with up to 22% gain in the offered reward to attain a level of target accuracy.;https://d1wqtxts1xzle7.cloudfront.net/105973772/nbnfi-fe2020050525025-libre.pdf?1695743181=&response-content-disposition=inline%3B+filename%3DIncentivize_to_Build_A_Crowdsourcing_Fra.pdf&Expires=1698161541&Signature=crOuixVXoZgu8LPoDgoA9GPuzWEpXtAZ2bCi0~FDSuG8VMiKj5ogoWnDqLTOsSMmyhDQPVem3um1N9Gkqe7SqIqpDjLzEu9dD4T5~Ch96m1p6ZT~IqHmPpZmz9UaRLJ9EOgsKLeV9IxY~zvw8KGq2XTJK19Jk6X4KDMYsjHM5dazGiDHPTaKVa0wb1LQavo3CVz9BmyUI148mZw0wnKiwzw7qeSZWVVClipoVnH08hi-u3WPOdLrylKUatrUzTNDn5uDQjyBixuLxN5PfMwjK-wK23ksVJUnN9KayKj9FnMCgrMDtFOxQ5yv~TBkvcOWJz40KU4qfXvzLtz7AqjsPg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA;Todo
Yin, B., Yin, H., Wu, Y., & Jiang, Z. (2020). FDC: A secure federated deep learning mechanism for data collaborations in the Internet of Things. IEEE Internet of Things Journal, 7(7), 6348-6359.;0_federated_learning_data_privacy;2020;FDC: A secure federated deep learning mechanism for data collaborations in the Internet of Things;Bo Yin, Hao Yin, Yulei Wu, Zexun Jiang;IEEE Internet of Things Journal 7 (7), 6348-6359, 2020;With the explosive network data due to the advanced development of the Internet of Things (IoT), the demand for multiparty computation is increasing. In addition, with the advent of future digital society, data have been gradually evolving into an effective virtual asset for sharing and usage. With the nature of the sensitivity, massiveness, fragmentation, and security of multiparty data computation in the IoT environment, we propose a secure data collaboration framework (FDC) based on federated deep-learning technology. The proposed framework can realize the secure collaboration of multiparty data computation on the premise that the data do not need to be transmitted out of their private data center. This framework is empowered by public data center, private data center, and the blockchain technology. The private data center is responsible for data governance, data registration, and data management. The public data center is used for multiparty secure computation. The blockchain paradigm is responsible for ensuring secure data usage and transmissions. A real IoT scenario is used to validate the effectiveness of the proposed framework.;https://ieeexplore.ieee.org/abstract/document/8960348/;O2GVsrA0ucYJ
Li, L., Xiong, H., Guo, Z., Wang, J., & Xu, C. Z. (2019, December). SmartPC: Hierarchical pace control in real-time federated learning system. In 2019 IEEE Real-Time Systems Symposium (RTSS) (pp. 406-418). IEEE.;0_federated_learning_data_privacy;2019;SmartPC: Hierarchical pace control in real-time federated learning system;Li Li, Haoyi Xiong, Zhishan Guo, Jun Wang, Cheng-Zhong Xu;2019 IEEE Real-Time Systems Symposium (RTSS), 406-418, 2019;Federated Learning is a technique for learning AI models through the collaboration of a large number of resourceconstrained mobile devices, while preserving data privacy. Instead of aggregating the training data from devices, Federated Learning uses multiple rounds of parameter aggregation to train a model, wherein the participating devices are coordinated to incrementally update a shared model with their own parameters locally learned. To efficiently deploy Federated Learning system over mobile devices, several critical issues including realtimeliness and energy efficiency should be well addressed. This paper proposes SmartPC, a hierarchical online pace control framework for Federated Learning that balances the training time and model accuracy in an energy-efficient manner. SmartPC consists of two layers of pace control: global and local. Prior to every training round, the global controller first oversees the status (e.g., connectivity, availability, and energy/resource remained) of every participating device, then selects qualified devices and assigns them a well-estimated virtual deadline for task completion. Within such virtual deadline, a statistically significant proportion (e.g., 60%) of the devices are expected to complete one round of their local training and model updates, while the overall progress of multi-round training procedure is kept up adaptively. On each device, a local pace controller then dynamically adjusts device settings such as CPU frequency so that the learning task is able to meet the deadline with the least amount of energy consumption. We performed extensive experiments to evaluate SmartPC on both Android smartphones and simulation platforms using well-known datasets. The experiment results show that SmartPC reduces up to 32:8% energy consumption on mobile devices and achieves a speedup of 2.27 in training time without model accuracy degradation.;https://ieeexplore.ieee.org/abstract/document/9052206/;dg-vGrmccbkJ
Northrop, L., Ozkaya, I., Fairbanks, G., & Keeling, M. (2019). Designing the software systems of the future. ACM SIGSOFT Software Engineering Notes, 43(4), 28-30.;1_ml_machine_data_learning;2019;Designing the software systems of the future;Linda Northrop, Ipek Ozkaya, George Fairbanks, Michael Keeling;ACM SIGSOFT Software Engineering Notes 43 (4), 28-30, 2019;We report here on the Future of Software DesignWorkshop that was held on Jan 12-14, 2018 in Pittsburgh, PA under the sponsorship of the Carnegie Mellon University Software Engineering Institute. The software industry is awash in modern trends that involve artificial intelligence (AI), autonomy, data everywhere, etc. These trends affect the structure of software-intensive systems and their designs. The goal of the workshop was to bring together participants from diverse backgrounds to formulate ideas for software design of future systems and related research opportunities and challenges. In this report we summarize the outcomes of the workshop.;https://dl.acm.org/doi/abs/10.1145/3282517.3302397;NvfgXpWrXvkJ
Liu, H., Eksmo, S., Risberg, J., & Hebig, R. (2020, June). Emerging and changing tasks in the development process for machine learning systems. In Proceedings of the international conference on software and system processes (pp. 125-134).;1_ml_machine_data_learning;2020;Emerging and changing tasks in the development process for machine learning systems;Hanyan Liu, Samuel Eksmo, Johan Risberg, Regina Hebig;Proceedings of the international conference on software and system processes, 125-134, 2020;Integrating machine learning components in software systems is a task more and more companies are confronted with. However, there is not much knowledge today on how the software development process needs to change, when such components are integrated into a software system. We performed an interview study with 16 participants, focusing on emerging and changing task. The results uncover a set of 25 tasks associated to different software development phases, such as requirements engineering or deployment. We are just starting to understand the implications of using machine-learning components on the software development process. This study allows some first insights into how widespread the required process changes are.;https://dl.acm.org/doi/abs/10.1145/3379177.3388905;UHez0q7BLSAJ
Aniculaesei, A., Grieser, J., Rausch, A., Rehfeldt, K., & Warnecke, T. (2018, May). Towards a holistic software systems engineering approach for dependable autonomous systems. In Proceedings of the 1st International Workshop on Software Engineering for AI in Autonomous Systems (pp. 23-30).;2_safety_system_autonomous_vehicle;2018;Towards a holistic software systems engineering approach for dependable autonomous systems;Adina Aniculaesei, JÃ¶rg Grieser, Andreas Rausch, Karina Rehfeldt, Tim Warnecke;Proceedings of the 1st International Workshop on Software Engineering for AI in Autonomous Systems, 23-30, 2018;Autonomous systems are gaining momentum in various application domains, such as autonomous vehicles, autonomous transport robotics and self-adaptation in smart homes. Product liability regulations impose high standards on manufacturers of such systems with respect to dependability (safety, security and privacy). Today's conventional engineering methods are not adequate for providing guarantees with respect to dependability requirements in a cost-efficient manner, e.g. road tests in the automotive industry sum up millions of miles before a system can be considered sufficiently safe. System engineers will no longer be able to test and respectively formally verify autonomous systems during development time in order to guarantee the dependability requirements in advance. In this vision paper, we introduce a new holistic software systems engineering approach for autonomous systems, which integrates development time methods as well as operation time techniques. With this approach, we aim to give the users a transparent view of the confidence level of the autonomous system under use with respect to the dependability requirements. We present already obtained results and point out research goals to be addressed in the future.;https://dl.acm.org/doi/abs/10.1145/3194085.3194091;a09O3ci9zsAJ
Serban, A. C. (2019, March). Designing safety critical software systems to manage inherent uncertainty. In 2019 IEEE International Conference on Software Architecture Companion (ICSA-C) (pp. 246-249). IEEE.;2_safety_system_autonomous_vehicle;2019;Designing safety critical software systems to manage inherent uncertainty;Alexandru Constantin Serban;2019 IEEE International Conference on Software Architecture Companion (ICSA-C), 246-249, 2019;Deploying machine learning algorithms in safety critical systems raises new challenges for system designers. The opaque nature of some algorithms together with the potentially large input space makes reasoning or formally proving safety difficult. In this paper, we argue that the inherent uncertainty that comes from using certain classes of machine learning algorithms can be mitigated through the development of software architecture design patterns. New or adapted patterns will allow faster roll out time for new technologies and decrease the negative impact machine learning components can have on safety critical systems. We outline the important safety challenges that machine learning algorithms raise and define three important directions for the development of new architectural patterns.;https://ieeexplore.ieee.org/abstract/document/8712362/;O0hafv30rUIJ
Belani, H., Vukovic, M., & Car, Ž. (2019, September). Requirements engineering challenges in building AI-based complex systems. In 2019 IEEE 27th International Requirements Engineering Conference Workshops (REW) (pp. 252-255). IEEE.;1_ml_machine_data_learning;2019;Requirements engineering challenges in building AI-based complex systems;Hrvoje Belani, Marin Vukovic, Å½eljka Car;2019 IEEE 27th International Requirements Engineering Conference Workshops (REW), 252-255, 2019;This paper identifies and tackles the challenges of the requirements engineering discipline when applied to development of AI-based complex systems. Due to their complex behaviour, there is an immanent need for a tailored development process for such systems. However, there is still no widely used and specifically tailored process in place to effectively and efficiently deal with requirements suitable for specifying a software solution that uses machine learning. By analysing the related work from software engineering and artificial intelligence fields, potential contributions have been recognized from agent-based software engineering and goal-oriented requirements engineering research, as well as examples from large product development companies. The challenges have been discussed, with proposals given how and when to tackle them. RE4AI taxonomy has also been outlined, to inform the tailoring of development process.;https://ieeexplore.ieee.org/abstract/document/8933653/;6X04qkoLux4J
McGraw, G., Bonett, R., Figueroa, H., & Shepardson, V. (2019). Security engineering for machine learning. Computer, 52(8), 54-57.;5_adversarial_attack_example_model;2019;Security engineering for machine learning;Gary McGraw, Richie Bonett, Harold Figueroa, Victor Shepardson;Computer 52 (8), 54-57, 2019;Artificial intelligence is in the midst of a popular resurgence in the guise of machine learning (ML). Neural networks and deep learning architectures have been shown empirically to solve many real-world problems. We ask what kinds of risks ML systems pose in terms of security engineering and software security.;https://ieeexplore.ieee.org/abstract/document/8779838/;eNHslhIoYHMJ
Chechik, M. (2019, September). Uncertain requirements, assurance and machine learning. In 2019 IEEE 27th International Requirements Engineering Conference (RE) (pp. 2-3). IEEE.;2_safety_system_autonomous_vehicle;2019;Uncertain requirements, assurance and machine learning;Marsha Chechik;2019 IEEE 27th International Requirements Engineering Conference (RE), 2-3, 2019;"From financial services platforms to social networks to vehicle control, software has come to mediate many activities of daily life. Governing bodies and standards organizations have responded to this trend by creating regulations and standards to address issues such as safety, security and privacy. In this environment, the compliance of software development to standards and regulations has emerged as a key requirement. Compliance claims and arguments are often captured in assurance cases, with linked evidence of compliance. Evidence can come from testcases, verification proofs, human judgement, or a combination of these. That is, we try to build (safety-critical) systems carefully according to well justified methods and articulate these justifications in an assurance case that is ultimately judged by a human. Yet software is deeply rooted in uncertainty making pragmatic assurance more inductive than deductive: most of complex open-world functionality is either not completely specifiable (due to uncertainty) or it is not cost-effective to do so, and deductive verification cannot happen without specification. Inductive assurance, achieved by sampling or testing, is easier but generalization from finite set of examples cannot be formally justified. And of course the recent popularity of constructing software via machine learning only worsens the problem - rather than being specified by predefined requirements, machine-learned components learn existing patterns from the available training data, and make predictions for unseen data when deployed. On the surface, this ability is extremely useful for hard-to specify concepts, e.g., the definition of a pedestrian in a pedestrian detection component of a vehicle. On the other, safety assessment and assurance of such components becomes very challenging. In this talk, I focus on two specific approaches to arguing about safety and security of software under uncertainty. The first one is a framework for managing uncertainty in assurance cases (for ""conventional"" and ""machine-learned"" systems) by systematically identifying, assessing and addressing it. The second is recent work on supporting development of requirements for machine-learned components in safety-critical domains.";https://ieeexplore.ieee.org/abstract/document/8920485/;GaFJmi9eNgcJ
Lalitha, A., Shekhar, S., Javidi, T., & Koushanfar, F. (2018, December). Fully decentralized federated learning. In Third workshop on bayesian deep learning (NeurIPS) (Vol. 2).;0_federated_learning_data_privacy;2018;Fully decentralized federated learning;Anusha Lalitha, Shubhanshu Shekhar, Tara Javidi, Farinaz Koushanfar;Third workshop on bayesian deep learning (NeurIPS) 2, 2018;We consider the problem of training a machine learning model over a network of users in a fully decentralized framework. The users take a Bayesian-like approach via the introduction of a belief over the model parameter space. We propose a distributed learning algorithm in which users update their belief by aggregate information from their one-hop neighbors to learn a model that best fits the observations over the entire network. In addition, we also obtain sufficient conditions to ensure that the probability of error is small for every user in the network. Finally, we discuss approximations required for applying this algorithm for training Neural Networks.;http://bayesiandeeplearning.org/2018/papers/140.pdf;2J8jWte1DkEJ
Batyuk, A., Voityshyn, V., & Verhun, V. (2018, August). Software architecture design of the real-time processes monitoring platform. In 2018 IEEE Second International Conference on Data Stream Mining & Processing (DSMP) (pp. 98-101). IEEE.;1_ml_machine_data_learning;2018;Software architecture design of the real-time processes monitoring platform;Anatoliy Batyuk, Volodymyr Voityshyn, Volodymyr Verhun;2018 IEEE Second International Conference on Data Stream Mining & Processing (DSMP), 98-101, 2018;Understanding of how business processes are executed in real-life is vitally important for a company. Any process leaves a digital footprint that can be transformed into so-called event logs and analyzed with process mining techniques. A software platform with the purpose of near realtime processes monitoring is implemented. Design of the represented platform is based on the lambda architecture combining online and offline process mining algorithms with advanced analytics based on machine learning.;https://ieeexplore.ieee.org/abstract/document/8478589/;zyoEvQEmT8cJ
Woods, E. (2016). Software architecture in a changing world. IEEE Software, 33(6), 94-97.;1_ml_machine_data_learning;2016;Software architecture in a changing world;Eoin Woods;IEEE Software 33 (6), 94-97, 2016;As software systems have evolved, so has software architecture, with practices growing to meet each era's new challenges. The next phase of evolution--intelligent connected systems--promises to be an exciting time for software architects.;https://ieeexplore.ieee.org/abstract/document/7725217/;7wQiFPe9U1sJ
National Science and Technology Council (US). Select Committee on Artificial Intelligence. (2019). The national artificial intelligence research and development strategic plan: 2019 update (p. 0050). National Science and Technology Council (US), Select Committee on Artificial Intelligence.;12_ai_ethical_ethic_intelligence;2019;Select Committee on Artificial Intelligence;National Science and Technology Council (US);"National Science and Technology Council (US). Select Committee on Artificial Intelligence
2019";Artificial intelligence AI holds tremendous promise to benefit nearly all aspects of society, including the economy, healthcare, security, the law, transportation, even technology itself. On February 11, 2019, the President signed Executive Order 13859, Maintaining American Leadership in Artificial Intelligence. 1 This order launched the American AI Initiative, a concerted effort to promote and protect AI technology and innovation in the United States. The Initiative implements a whole-of-government strategy in collaboration and engagement with the private sector, academia, the public, and like-minded international partners. Among other actions, key directives in the Initiative call for Federal agencies to prioritize AI research and development R and D investments, enhance access to high-quality cyberinfrastructure and data, ensure that the Nation leads in the development of technical standards for AI, and provide education and training opportunities to prepare the American workforce for the new era of AI. In support of the American AI Initiative, this National AI R and D Strategic Plan 2019 Update defines the priority areas for Federal investments in AI R and D. This 2019 update builds upon the first National AI R and D Strategic Plan released in 2016, accounting for new research, technical innovations, and other considerations that have emerged over the past three years. This update has been developed by leading AI researchers and research administrators from across the Federal Government, with input from the broader civil society, including from many of Americas leading academic research institutions, nonprofit organizations, and private sector technology companies. Feedback from these key stakeholders affirmed the continued relevance of each part of the 2016 Strategic Plan while also calling for greater attention to making AI trustworthy, to partnering with the private sector, and other imperatives;https://apps.dtic.mil/sti/citations/AD1079707;TODO
Washizaki, H., Uchida, H., Khomh, F., & Guéhéneuc, Y. G. (2020). Machine learning architecture and design patterns. IEEE Software, 8.;1_ml_machine_data_learning;2020;Machine learning architecture and design patterns;Hironori Washizaki, Hiromu Uchida, Foutse Khomh, Yann-Gaël Guéhéneuc;IEEE Software 8, 2020;Researchers and practitioners studying best practices strive to design Machine Learning (ML) application systems and software that address software complexity and quality issues. Such design practices are often formalized as architecture and design patterns by encapsulating reusable solutions to common problems within given contexts. In this paper, software-engineering architecture and design (anti-) patterns for ML application systems are analyzed to bridge the gap between traditional software systems and ML application systems with respect to architecture and design. Specifically, a systematic literature review confirms that ML application systems are popular due to the promotion of artificial intelligence. We identified 32 scholarly documents and 48 gray documents out of which 38 documents discuss 33 patterns: 12 architecture patterns, 13 design patterns, and 8 anti-patterns. Additionally, a survey of developers reveals that there are 7 major architecture patterns and 5 major design patterns. Then the relationships among patterns are identified in a pattern map.;http://www.washi.cs.waseda.ac.jp/wp-content/uploads/2019/12/IEEE_Software_19__ML_Patterns.pdf;IU3OouzT7YAJ
Kaur, D., Uslu, S., & Durresi, A. (2021). Requirements for trustworthy artificial intelligence–a review. In Advances in Networked-Based Information Systems: The 23rd International Conference on Network-Based Information Systems (NBiS-2020) 23 (pp. 105-115). Springer International Publishing.;1_ml_machine_data_learning;2021;Requirements for trustworthy artificial intelligence–a review;Davinder Kaur, Suleyman Uslu, Arjan Durresi;Advances in Networked-Based Information Systems: The 23rd International Conference on Network-Based Information Systems (NBiS-2020) 23, 105-115, 2021;The field of algorithmic decision-making, particularly Artificial Intelligence (AI), has been drastically changing. With the availability of a massive amount of data and an increase in the processing power, AI systems have been used in a vast number of high-stake applications. So, it becomes vital to make these systems reliable and trustworthy. Different approaches have been proposed to make theses systems trustworthy. In this paper, we have reviewed these approaches and summarized them based on the principles proposed by the European Union for trustworthy AI. This review provides an overview of different principles that are important to make AI trustworthy.;https://link.springer.com/chapter/10.1007/978-3-030-57811-4_11;lUDD_QQYvR8J
Zhang, X., Yang, Y., Feng, Y., & Chen, Z. (2019). Software engineering practice in the development of deep learning applications. arXiv preprint arXiv:1910.03156.;4_dl_testing_deep_network;2019;Software engineering practice in the development of deep learning applications;Xufan Zhang, Yilin Yang, Yang Feng, Zhenyu Chen;arXiv preprint arXiv:1910.03156, 2019;Deep-Learning(DL) applications have been widely employed to assist in various tasks. They are constructed based on a data-driven programming paradigm that is different from conventional software applications. Given the increasing popularity and importance of DL applications, software engineering practitioners have some techniques specifically for them. However, little research is conducted to identify the challenges and lacks in practice. To fill this gap, in this paper, we surveyed 195 practitioners to understand their insight and experience in the software engineering practice of DL applications. Specifically, we asked the respondents to identify lacks and challenges in the practice of the development life cycle of DL applications. The results present 13 findings that provide us with a better understanding of software engineering practice of DL applications. Further, we distil these findings into 7 actionable recommendations for software engineering researchers and practitioners to improve the development of DL applications.;https://arxiv.org/abs/1910.03156;LBfzaHUnCc8J
Horneman, A., Mellinger, A., & Ozkaya, I. (2019). AI engineering: 11 foundational practices. Pittsburgh: Carnegie Mellon University Software Engineering Institute.;1_ml_machine_data_learning;2019;AI engineering: 11 foundational practices;Angela Horneman, Andrew Mellinger, Ipek Ozkaya;Pittsburgh: Carnegie Mellon University Software Engineering Institute, 2019;THE US DEPARTMENT OF DEFENSE DOD is increasingly interested in taking full advantage of the improved capabilities of machine learning ML algorithms and building artificial intelligence AI-enabled systems that speed up timeliness and accuracy of the decisions made to support DoD missions. With the increased availability of computing resources, applying ML algorithms to models of thousands of parametersand terabytes and petabytes of datais now possible.Descriptors:;https://apps.dtic.mil/sti/citations/AD1099280;xFsXQy-7O-cJ
Wu, C. J., Brooks, D., Chen, K., Chen, D., Choudhury, S., Dukhan, M., ... & Zhang, P. (2019, February). Machine learning at facebook: Understanding inference at the edge. In 2019 IEEE international symposium on high performance computer architecture (HPCA) (pp. 331-344). IEEE.;7_edge_computing_deep_learning;2019;Machine learning at facebook: Understanding inference at the edge;Carole-Jean Wu, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury, Marat Dukhan, Kim Hazelwood, Eldad Isaac, Yangqing Jia, Bill Jia, Tommer Leyvand, Hao Lu, Yang Lu, Lin Qiao, Brandon Reagen, Joe Spisak, Fei Sun, Andrew Tulloch, Peter Vajda, Xiaodong Wang, Yanghan Wang, Bram Wasti, Yiming Wu, Ran Xian, Sungjoo Yoo, Peizhao Zhang;2019 IEEE international symposium on high performance computer architecture (HPCA), 331-344, 2019;At Facebook, machine learning provides a wide range of capabilities that drive many aspects of user experience including ranking posts, content understanding, object detection and tracking for augmented and virtual reality, speech and text translations. While machine learning models are currently trained on customized data-center infrastructure, Facebook is working to bring machine learning inference to the edge. By doing so, user experience is improved with reduced latency (inference time) and becomes less dependent on network connectivity. Furthermore, this also enables many more applications of deep learning with important features only made available at the edge. This paper takes a data-driven approach to present the opportunities and design challenges faced by Facebook in order to enable machine learning inference locally on smart phones and other edge platforms.;https://ieeexplore.ieee.org/abstract/document/8675201/;3rqOijPPGrIJ
Rios, G. (2008). Patterns (and Anti-Patterns) for Developing Machine Learning Systems.;1_ml_machine_data_learning;2008;Patterns (and Anti-Patterns) for Developing Machine Learning Systems.;Gordon Rios;-;• An older model or early approach needs to be replaced but has entrenched support• Use as an input to new approach (presumably based on ML)• Can be technically challenging but frequently can be converted to an input in conjunction with Pipeline• Related: Chop Shop, Tiers, Shadow• Advanced: Champion/Challenger;https://www.usenix.org/events/sysml08/tech/rios_talk.pdf;nLaWZEEydQUJ
Serban, A., Poll, E., & Visser, J. (2020). Towards using probabilistic models to design software systems with inherent uncertainty. In Software Architecture: 14th European Conference, ECSA 2020, L'Aquila, Italy, September 14–18, 2020, Proceedings 14 (pp. 89-97). Springer International Publishing.;1_ml_machine_data_learning;2020;Towards using probabilistic models to design software systems with inherent uncertainty;Alex Serban, Erik Poll, Joost Visser;Software Architecture: 14th European Conference, ECSA 2020, L'Aquila, Italy, September 14â€“18, 2020, Proceedings 14, 89-97, 2020;The adoption of machine learning (ML) components in software systems raises new engineering challenges. In particular, the inherent uncertainty regarding functional suitability and the operation environment makes architecture evaluation and trade-off analysis difficult. We propose a software architecture evaluation method called Modeling Uncertainty During Design (MUDD) that explicitly models the uncertainty associated to ML components and evaluates how it propagates through a system. The method supports reasoning over how architectural patterns can mitigate uncertainty and enables comparison of different architectures focused on the interplay between ML and classical software components. While our approach is domain-agnostic and suitable for any system where uncertainty plays a central role, we demonstrate our approach using as example a perception system for autonomous driving.;https://link.springer.com/chapter/10.1007/978-3-030-58923-3_6;zTyJePCEBcAJ
Melchor, F., Rodriguez-Echeverria, R., Conejero, J. M., Prieto, Á. E., & Gutiérrez, J. D. (2022, June). A Model-Driven Approach for Systematic Reproducibility and Replicability of Data Science Projects. In International Conference on Advanced Information Systems Engineering (pp. 147-163). Cham: Springer International Publishing.;1_ml_machine_data_learning;2022;A Model-Driven Approach for Systematic Reproducibility and Replicability of Data Science Projects;Fran Melchor, Roberto Rodriguez-Echeverria, José M Conejero, Álvaro E Prieto, Juan D Gutiérrez;International Conference on Advanced Information Systems Engineering, 147-163, 2022;"In the last few years, there has been an important increase in the number of tools and approaches to define pipelines that allow the development of data science projects. They allow not only the pipeline definition but also the code generation needed to execute the project providing an easy way to carry out the projects even for non-expert users. However, there are still some challenges that these tools do not address yet, e.g. the possibility of executing pipelines defined by using different tools or execute them in different environments (reproducibility and replicability) or models validation and verification by identifying inconsistent operations (intentionality). In order to alleviate these problems, this paper presents a Model-Driven framework for the definition of data science pipelines independent of the particular execution platform and tools. The framework relies on the separation of the pipeline definition into two different modelling layers: conceptual, where the data scientist may specify all the data and models operations to be carried out by the pipeline; operational, where the data engineer may describe the execution environment details where the operations (defined in the conceptual part) will be implemented. Based on this abstract definition and layers separation, the approach allows: the usage of different tools improving, thus, process replicability; the automation of the process execution, enhancing process reproducibility; and the definition of model verification rules, providing intentionality restrictions.";https://link.springer.com/chapter/10.1007/978-3-031-07472-1_9;PkXvD4c3BRgJ
Khalajzadeh, H., Abdelrazek, M., Grundy, J., Hosking, J., & He, Q. (2018, July). A survey of current end-user data analytics tool support. In 2018 IEEE International Congress on Big Data (BigData Congress) (pp. 41-48). IEEE.;9_data_science_software_process;2018;A survey of current end-user data analytics tool support;Hourieh Khalajzadeh, Mohamed Abdelrazek, John Grundy, John Hosking, Qiang He;2018 IEEE International Congress on Big Data (BigData Congress), 41-48, 2018;There is a large growth in interest in big data analytics to discover unknown patterns and insights. A major challenge in this domain is the need to combine domain knowledge - what the data means (semantics) and what it is used for - with data analytics and visualization techniques to mine and communicate important information from huge volumes of raw data. Many data analytics tools have been developed for both research and practice to assist in specifying, integrating and deploying data analytics and visualization applications. However, delivering such big data analytics application requires a capable team with different skillsets including data scientists, software engineers and domain experts. Such teams and skillset usually take a long time to build and have high running costs. An alternative is to provide domain experts and data scientists with tools they can use to do the exploration and analysis directly with less technical skills required. We present an overview and analysis of several current approaches to supporting the data analytics for endusers, identifying key strengths, weaknesses and opportunities for future research.;https://ieeexplore.ieee.org/abstract/document/8457729/;uYBl6-jWv4wJ
Chowdary, M. N., Sankeerth, B., Chowdary, C. K., & Gupta, M. (2022, August). Accelerating the Machine Learning Model Deployment using MLOps. In Journal of Physics: Conference Series (Vol. 2327, No. 1, p. 012027). IOP Publishing.;1_ml_machine_data_learning;2022;Accelerating the Machine Learning Model Deployment using MLOps;Mandepudi Nobel Chowdary, Bussa Sankeerth, Chennupati Kumar Chowdary, Manu Gupta;Journal of Physics: Conference Series 2327 (1), 012027, 2022;Machine Learning projects acquire a large amount of data to make predictions on the data. Deploying machine learning models is a difficult task as it is involved a lot factors such as continuous builds to improve efficiency and different libraries used for prediction. As the data grow for better predictions with proper accuracy some of the parameters are to be dynamically changed for the sake of performance tuning. To enhance the variation in the parameters it takes effort and time from the developers to make the changes in the code and deploy to the production environment. In proposed work, an end to end automation cycle to enhance the timely deployment of machine learning models with better performance is presented. This automation cycle converts the changed code with varied parameters into the container image and uploads to the container registry. Later, when the new image is uploaded it auto detects and updates the production environment with the new image data by launching a new container and shutting down the previous old one. The proposed automated model will display the updated features which are developed. The users can utilize the updated model simultaneously without any downtime to obtain the new features. The main contribution of proposed work is that it can build and deploy the machine learning models automatically, without any manual intervention. The models can also be retrained for better performance of the predictions.;https://iopscience.iop.org/article/10.1088/1742-6596/2327/1/012027/meta;wqU0nxRigLQJ
Laato, S., Birkstedt, T., Mäantymäki, M., Minkkinen, M., & Mikkonen, T. (2022, May). AI governance in the system development life cycle: Insights on responsible machine learning engineering. In Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI (pp. 113-123).;1_ml_machine_data_learning;2022;AI governance in the system development life cycle: Insights on responsible machine learning engineering;Samuli Laato, Teemu Birkstedt, Matti Mäantymäki, Matti Minkkinen, Tommi Mikkonen;Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI, 113-123, 2022;In this study we explore the incorporation of artificial intelligence (AI) governance to system development life cycle (SDLC) models. We conducted expert interviews among AI and SDLC professionals and analyzed the interview data using qualitative coding and clustering to extract AI governance concepts. Subsequently, we mapped these concepts onto three stages in the machine learning (ML) system development process: (1) design, (2) development, and (3) operation. We discovered 20 governance concepts, some of which are relevant to more than one of the three stages. Our analysis highlights AI governance as a complex process that involves multiple activities and stakeholders. As development projects are unique, the governance requirements and processes also vary. This study is a step towards understanding how AI governance is conceptually connected to ML systems' management processes through the project life cycle.;https://dl.acm.org/doi/abs/10.1145/3522664.3528598;5WIgnQYCYdAJ
Nigenda, D., Karnin, Z., Zafar, M. B., Ramesha, R., Tan, A., Donini, M., & Kenthapadi, K. (2022, August). Amazon sagemaker model monitor: A system for real-time insights into deployed machine learning models. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (pp. 3671-3681).;1_ml_machine_data_learning;2022;Amazon sagemaker model monitor: A system for real-time insights into deployed machine learning models;David Nigenda, Zohar Karnin, Muhammad Bilal Zafar, Raghu Ramesha, Alan Tan, Michele Donini, Krishnaram Kenthapadi;Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 3671-3681, 2022;With the increasing adoption of machine learning (ML) models and systems in high-stakes settings across different industries, guaranteeing a model's performance after deployment has become crucial. Monitoring models in production is a critical aspect of ensuring their continued performance and reliability. We present Amazon SageMaker Model Monitor, a fully managed service that continuously monitors the quality of machine learning models hosted on Amazon SageMaker. Our system automatically detects data, concept, bias, and feature attribution drift in models in real-time and provides alerts so that model owners can take corrective actions and thereby maintain high quality models. We describe the key requirements obtained from customers, system design and architecture, and methodology for detecting different types of drift. Further, we provide quantitative evaluations followed by use cases, insights, and lessons learned from more than two years of production deployment.;https://dl.acm.org/doi/abs/10.1145/3534678.3539145;XLeddGW0YPoJ
Khalajzadeh, H., Simmons, A. J., Abdelrazek, M., Grundy, J., Hosking, J., & He, Q. (2020). An end-to-end model-based approach to support big data analytics development. Journal of Computer Languages, 58, 100964.;9_data_science_software_process;2020;An end-to-end model-based approach to support big data analytics development;Hourieh Khalajzadeh, Andrew J Simmons, Mohamed Abdelrazek, John Grundy, John Hosking, Qiang He;Journal of Computer Languages 58, 100964, 2020;We present BiDaML 2.0, an integrated suite of visual languages and supporting tool to help multidisciplinary teams with the design of big data analytics solutions. BiDaML tool support provides a platform for efficiently producing BiDaML diagrams and facilitating their design, creation, report and code generation. We evaluated BiDaML using two types of evaluations, a theoretical analysis using the “physics of notations”, and an empirical study with 1) a group of 12 target end-users and 2) five individual end-users. Participants mostly agreed that …;https://www.sciencedirect.com/science/article/pii/S2590118420300241;GRdb5M2sVEMJ
Chen, K. H., Su, H. P., Chuang, W. C., Hsiao, H. C., Tan, W., Tang, Z., ... & Wang, C. M. (2022, April). Apache submarine: a unified machine learning platform made simple. In Proceedings of the 2nd European Workshop on Machine Learning and Systems (pp. 101-108).;1_ml_machine_data_learning;2022;Apache submarine: a unified machine learning platform made simple;Kai-Hsun Chen, Huan-Ping Su, Wei-Chiu Chuang, Hung-Chang Hsiao, Wangda Tan, Zhankun Tang, Xun Liu, Yanbo Liang, Wen-Chih Lo, Wanqiang Ji, Byron Hsu, Keqiu Hu, HuiYang Jian, Quan Zhou, Chien-Min Wang;Proceedings of the 2nd European Workshop on Machine Learning and Systems, 101-108, 2022;"As machine learning is applied more widely, it is necessary to have a machine-learning platform for both infrastructure administrators and users including expert data scientists and citizen data scientists [24] to improve their productivity. However, existing machine-learning platforms are ill-equipped to address the ""Machine Learning tech debts"" [36] such as glue code, reproducibility, and portability. Furthermore, existing platforms only take expert data scientists into consideration, and thus they are inflexible for infrastructure administrators and non-user-friendly for citizen data scientists. We propose Submarine, a unified machine-learning platform, and takes all infrastructure administrators, expert data scientists, and citizen data scientists into consideration. Submarine has been widely used in many technology companies, including Ke.com and LinkedIn. We present two use cases in Section 5.";https://dl.acm.org/doi/abs/10.1145/3517207.3526984;WIgCd6nsADgJ
Berezsky, O., Pitsun, O., Melnyk, G., Batko, Y., Derysh, B., & Liashchynskyi, P. (2022). Application Of MLOps Practices For Biomedical Image Classification. In IDDM (pp. 69-77).;1_ml_machine_data_learning;2022;Application Of MLOps Practices For Biomedical Image Classification;O Berezsky, O Pitsun, G Melnyk, Y Batko, B Derysh;Citation;Citation – no abstract;no URL available;TODO
Ruf, P., Reich, C., & Ould-Abdeslam, D. (2022, June). Aspects of Module Placement in Machine Learning Operations for Cyber Physical Systems. In 2022 11th Mediterranean Conference on Embedded Computing (MECO) (pp. 1-6). IEEE.;1_ml_machine_data_learning;2022;Aspects of Module Placement in Machine Learning Operations for Cyber Physical Systems;Philipp Ruf, Christoph Reich, Djaffar Ould-Abdeslam;2022 11th Mediterranean Conference on Embedded Computing (MECO), 1-6, 2022;The traditional field of industrial manufacturing is in the process of being revolutionized as machines become smart and processes are translated to and perfected by digital systems. The application of Machine Learning (ML) has established itself as a smart technology in the manufacturing industry. The optimal operation and training of ML applications requires a flexible, adaptable, and modular service infrastructure for industry 4.0. The objective of this work is the elaboration of requirements for ML integration in Cyber Physical System (CPS) and the added value of modularization in CPS-related ML tasks. As the paradigm of Machine Learning Operations (MLOps) be-comes firmly established, principles for determining an optimized placement of the different ML pipeline modules in distributed systems are required. A collaborative cross-company use-case management framework KOSMoS is used to discuss selected benefits of using MLOps in industrial scenarios.;https://ieeexplore.ieee.org/abstract/document/9797080/;yZMwVMh7Nq4J
Yang, D., Wang, D., Yang, D., Dong, Q., Wang, Y., Zhou, H., & Hong, D. (2020). DevOps in practice for education management information system at ECNU. Procedia Computer Science, 176, 1382-1391.;9_data_science_software_process;2020;DevOps in practice for education management information system at ECNU;Dawei Yang, Daojiang Wang, Dongming Yang, Qiwen Dong, Ye Wang, Huan Zhou, Daocheng Hong;Procedia Computer Science 176, 1382-1391, 2020;With the rapid development of the Internet, the education information systems have become more prevalent aligning with better management to produce better education. However, the limitations of prior education systems development are gradually exposed, which ignore the changing requirements, the high concurrency bottlenecks and lean development of education information systems. Therefore, we develop and build a novel education information system at ECNU based on DevOps and related techniques. This paper reveals the practice of DevOps for new education information system from four aspects: CI (Continuous Integration), CD (Continuous Deployment), log management, and code quality. Meanwhile, brief technical explanations include Git, Jenkins, Kubernetes, ELK, SonarQube, etc. Through our continuous engineering practice, the new education information system has been developed and implemented at ECNU. The DevOps practice for information system establishes that it is so convenient for developing, testing and release of education information systems, and it also improves reliability, availability and scalability of information platform especially considering the guarantee of efficiency.;https://www.sciencedirect.com/science/article/pii/S1877050920320482;ihcIdqj5L6UJ
Baumann, N., Kusmenko, E., Ritz, J., Rumpe, B., & Weber, M. B. (2022, October). Dynamic data management for continuous retraining. In Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings (pp. 359-366).;1_ml_machine_data_learning;2022;Dynamic data management for continuous retraining;Nils Baumann, Evgeny Kusmenko, Jonas Ritz, Bernhard Rumpe, Moritz Benedikt Weber;Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings, 359-366, 2022;Managing dynamic datasets intended to serve as training data for a Machine Learning (ML) model often emerges as very challenging, especially when data is often altered iteratively and already existing ML models should pertain to the data. For example, this applies when new data versions arise from either a generated or aggregated extension of an existing dataset a model has already been trained on. In this work, it is investigated on how a model-based approach for these training data concerns can be provided as well as how the complete process, including the resulting training and retraining process of the ML model, can therein be integrated. Hence, model-based concepts and the implementation are devised to cope with the complexity of iterative data management as an enabler for the integration of continuous retraining routines. With Deep Learning techniques becoming technically feasible and massively being developed further over the last decade, MLOps, aiming to establish DevOps tailored to ML projects, gained crucial relevance. Unfortunately, data-management concepts for iteratively growing datasets with retraining capabilities embedded in a model-driven ML development methodology are unexplored to the best of our knowledge. To fill in this gap, this contribution provides such agile data management concepts and integrates them and continuous retraining into the model-driven ML Framework MontiAnna [18]. The new functionality is evaluated in the context of a research project where ML is exploited for the optimal design of lattice structures for crash applications.;https://dl.acm.org/doi/abs/10.1145/3550356.3561568;4IIwoYDmlcQJ
Zahid, H., Mahmood, T., & Ikram, N. (2018). Enhancing dependability in big data analytics enterprise pipelines. In Security, Privacy, and Anonymity in Computation, Communication, and Storage: 11th International Conference and Satellite Workshops, SpaCCS 2018, Melbourne, NSW, Australia, December 11-13, 2018, Proceedings 11 (pp. 272-281). Springer International Publishing.;9_data_science_software_process;2018;Enhancing dependability in big data analytics enterprise pipelines;Hira Zahid, Tariq Mahmood, Nassar Ikram;Security, Privacy, and Anonymity in Computation, Communication, and Storage: 11th International Conference and Satellite Workshops, SpaCCS 2018, Melbourne, NSW, Australia …, 2018;Big Data Analytics (BDA) brings extensive opportunities to enterprises to extract valuable information from high volume, velocity and variety data streams. However, the BDA dynamics can lead to significant project failures due to high-risk factors in terms of data availability, reliability, integrity, security and resilience which are the key components of a dependable system and are strongly linked to BDA process execution. Specifically, the heterogeneity of big data sources, diverse set of challenges related to big data integration and processing, along with a rapidly-expanding landscape warrant the need to make dependable big data systems capable of providing standard analytical solutions. In this paper, we propose the first dependable pipeline architecture for the BDA process which has a layered front-end and back-end implementation, employs the standard lambda architecture in a DataOps analytical cycle, incorporates state-of-the-art tools which are all open-source, and is coded entirely in the standard Python language to remove cross-platform implementation dependencies. We have implemented this architecture in five enterprise BDA projects but we are unable to present implementation details and results due to space limitations.;https://link.springer.com/chapter/10.1007/978-3-030-05345-1_23;LkhSlOZfQz0J
Mahapatra, T., & Banoo, S. N. (2022). Flow-based programming for machine learning. Future Internet, 14(2), 58.;1_ml_machine_data_learning;2022;Flow-based programming for machine learning;Tanmaya Mahapatra, Syeeda Nilofer Banoo;Future Internet 14 (2), 58, 2022;"Machine Learning (ML) has gained prominence and has tremendous applications in fields like medicine, biology, geography and astrophysics, to name a few. Arguably, in such areas, it is used by domain experts, who are not necessarily skilled-programmers. Thus, it presents a steep learning curve for such domain experts in programming ML applications. To overcome this and foster widespread adoption of ML techniques, we propose to equip them with domain-specific graphical tools. Such tools, based on the principles of flow-based programming paradigm, would support the graphical composition of ML applications at a higher level of abstraction and auto-generation of target code. Accordingly, (i) we have modelled ML algorithms as composable components; (ii) described an approach to parse a flow created by connecting several such composable components and use an API-based code generation technique to generate the ML application. To demonstrate the feasibility of our conceptual approach, we have modelled the APIs of Apache Spark ML as composable components and validated it in three use-cases. The use-cases are designed to capture the ease of program specification at a higher abstraction level, easy parametrisation of ML APIs, auto-generation of the ML application and auto-validation of the generated model for better prediction accuracy.";https://www.mdpi.com/1999-5903/14/2/58;BXP52Ir8LJMJ
Capizzi, A., Distefano, S., & Mazzara, M. (2020). From devops to devdataops: Data management in devops processes. In Software Engineering Aspects of Continuous Development and New Paradigms of Software Production and Deployment: Second International Workshop, DEVOPS 2019, Château de Villebrumier, France, May 6–8, 2019, Revised Selected Papers 2 (pp. 52-62). Springer International Publishing.;9_data_science_software_process;2020;From devops to devdataops: Data management in devops processes;Antonio Capizzi, Salvatore Distefano, Manuel Mazzara;Software Engineering Aspects of Continuous Development and New Paradigms of Software Production and Deployment: Second International Workshop, DEVOPS 2019, Château de …, 2020;DevOps is a quite effective approach for managing software development and operation, as confirmed by plenty of success stories in real applications and case studies. DevOps is now becoming the main-stream solution adopted by the software industry in development, able to reduce the time to market and costs while improving quality and ensuring evolvability and adaptability of the resulting software architecture. Among the aspects to take into account in a DevOps process, data is assuming strategic importance, since it allows to gain insights from the operation directly into the development, the main objective of a DevOps approach. Data can be therefore considered as the fuel of the DevOps process, requiring proper solutions for its management. Based on the amount of data generated, its variety, velocity, variability, value and other relevant features, DevOps data management can be mainly framed into the BigData category. This allows exploiting BigData solutions for the management of DevOps data generated throughout the process, including artefacts, code, documentation, logs and so on. This paper aims at investigating data management in DevOps processes, identifying related issues, challenges and potential solutions taken from the BigData world as well as from new trends adopting and adapting DevOps approaches in data management, i.e. DataOps.;https://link.springer.com/chapter/10.1007/978-3-030-39306-9_4;qwyLirUHt2IJ
Hegeman, T., Jansen, M., Iosup, A., & Trivedi, A. (2021, April). GradeML: Towards holistic performance analysis for machine learning workflows. In Companion of the ACM/SPEC International Conference on Performance Engineering (pp. 57-63).;1_ml_machine_data_learning;2021;GradeML: Towards holistic performance analysis for machine learning workflows;Tim Hegeman, Matthijs Jansen, Alexandru Iosup, Animesh Trivedi;Companion of the ACM/SPEC International Conference on Performance Engineering, 57-63, 2021;Today, machine learning (ML) workloads are nearly ubiquitous. Over the past decade, much effort has been put into making ML model-training fast and efficient, e.g., by proposing new ML frameworks (such as TensorFlow, PyTorch), leveraging hardware support (TPUs, GPUs, FPGAs), and implementing new execution models (pipelines, distributed training). Matching this trend, considerable effort has also been put into performance analysis tools focusing on ML model-training. However, as we identify in this work, ML model training rarely happens in isolation and is instead one step in a larger ML workflow. Therefore, it is surprising that there exists no performance analysis tool that covers the entire life-cycle of ML workflows. Addressing this large conceptual gap, we envision in this work a holistic performance analysis tool for ML workflows. We analyze the state-of-practice and the state-of-the-art, presenting quantitative evidence about the performance of existing performance tools. We formulate our vision for holistic performance analysis of ML workflows along four design pillars: a unified execution model, lightweight collection of performance data, efficient data aggregation and presentation, and close integration in ML systems. Finally, we propose first steps towards implementing our vision as GradeML, a holistic performance analysis tool for ML workflows. Our preliminary work and experiments are open source at https://github.com/atlarge-research/grademl.;https://dl.acm.org/doi/abs/10.1145/3447545.3451185;o0NlxUlgbhEJ
Langenkamp, M., & Yue, D. N. (2022, July). How open source machine learning software shapes ai. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (pp. 385-395).;12_ai_ethical_ethic_intelligence;2022;How open source machine learning software shapes ai;Max Langenkamp, Daniel N Yue;Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society, 385-395, 2022;If we want a future where AI serves a plurality of interests, then we should pay attention to the factors that drive its success. While others have studied the importance of data, hardware, and models in directing the trajectory of AI, we argue that open source software is a neglected factor shaping AI as a discipline. We start with the observation that almost all AI research and applications are built on machine learning open source software (MLOSS). This paper presents three contributions. First, it quantifies the outsized impact of MLOSS by using Github contributions data. By contrasting the costs of MLOSS and its economic benefits, we find that the average dollar of MLOSS investment corresponds to at least \$100 of global economic value created, corresponding to \$30B of economic value created this year. Second, we leverage interviews with AI researchers and developers to develop a causal model of the effect of open sourcing on economic value. We argue that open sourcing creates value through three primary mechanisms: standardization of MLOSS tools, increased experimentation in AI research, and creation of communities. Finally, we consider the incentives for developing MLOSS and the broader implications of these effects. We intend this paper to be useful for technologists and academics who want to analyze and critique AI, and policymakers who want to better understand and regulate AI systems.;https://dl.acm.org/doi/abs/10.1145/3514094.3534167;jwWkjHupw9YJ
Rosa, L., Cruz, T., de Freitas, M. B., Quitério, P., Henriques, J., Caldeira, F., ... & Simões, P. (2021). Intrusion and anomaly detection for the next-generation of industrial automation and control systems. Future Generation Computer Systems, 119, 50-67.;5_adversarial_attack_example_model;2021;Intrusion and anomaly detection for the next-generation of industrial automation and control systems;Luis Rosa, Tiago Cruz, Miguel Borges de Freitas, Pedro Quitério, João Henriques, Filipe Caldeira, Edmundo Monteiro, Paulo Simões;Future Generation Computer Systems 119, 50-67, 2021;The next-generation of Industrial Automation and Control Systems (IACS) and Supervisory Control and Data Acquisition (SCADA) systems pose numerous challenges in terms of cybersecurity monitoring. We have been witnessing the convergence of OT/IT networks, combined with massively distributed metering and control scenarios such as smart grids. Larger and geographically widespread attack surfaces, and inherently more data to analyse, will become the norm.Despite several advances in recent years, domain-specific security tools have been facing the challenges of trying to catch up with all the existing security flaws from the past, while also accounting for the specific needs of the next-generation of IACS. Moreover, the aggregation of multiple techniques and sources of information into a comprehensive approach has not been explored in depth. Such a holistic perspective is paramount since it enables a global and enhanced analysis enabled by the usage, combination and aggregation of the outputs from multiple sources and techniques.This paper starts by providing a review of the more recent anomaly detection techniques for SCADA systems, focused on both theoretical machine learning approaches and complete frameworks. Afterwards, it proposes a complete framework for an Intrusion and Anomaly Detection System (IADS) composed of specific detection probes, an event processing layer and a core anomaly detection component, amongst others. Finally, the paper presents an evaluation of the framework within a large-scale hybrid testbed, and a comparison of different anomaly detection scenarios based on various machine learning techniques.;https://www.sciencedirect.com/science/article/pii/S0167739X21000431;B6k_LBB8Q10J
Niranjan, D. R. (2022, October). Jenkins Pipelines: A Novel Approach to Machine Learning Operations (MLOps). In 2022 International Conference on Edge Computing and Applications (ICECAA) (pp. 1292-1297). IEEE.;1_ml_machine_data_learning;2022;Jenkins Pipelines: A Novel Approach to Machine Learning Operations (MLOps;DR Niranjan;2022 International Conference on Edge Computing and Applications (ICECAA), 1292-1297, 2022;Machine Learning is a widely popular field that is being used in an increasingly large number of projects worldwide. This necessitates the use of certain practices to create a structured framework for such projects. These practices, processes and pipelines are termed as Machine Learning Operations (MLOps). The software development lifecycle for a machine learning is indeed very complex and sequential nature of it results in several repetitive tasks for developers. Automation in this aspect would greatly reduce time and manual effort required. Jenkins is an open-source continuous integration tool that can be used to build pipelines to define and automate workflows in MLOps domain. This paper proposes the design and implementation of pipelines for various stages of MLOps that are data analysis, data preparation, training, testing and deployment on a single platform.;https://ieeexplore.ieee.org/abstract/document/9936252/;sU3qG7saJi4J
Zárate, G., Miñón, R., Díaz-de-Arcaya, J., & Torre-Bastida, A. I. (2022, March). K2E: Building MLOps Environments for Governing Data and Models Catalogues while Tracking Versions. In 2022 IEEE 19th International Conference on Software Architecture Companion (ICSA-C) (pp. 206-209). IEEE.;9_data_science_software_process;2022;K2E: Building MLOps Environments for Governing Data and Models Catalogues while Tracking Versions;Gorka Zárate, Raúl Miñón, Josu Díaz-de-Arcaya, Ana I Torre-Bastida;2022 IEEE 19th International Conference on Software Architecture Companion (ICSA-C), 206-209, 2022;Nowadays, there are a variety of problems associated with the process of extracting value and information from data such as: data heterogeneity, data distribution, model versioning, and the vast variety of techniques and approaches. Due to all this, the data management process becomes hard to implement in real world scenarios. In this context, the catalogue tools for data and Artificial Intelligence models alleviate the burden of dealing with versioning tasks. Thus, the automation of the data and models’ management processes is facilitated, complying with DataOps and MLOps good practices. This work in progress enumerates key challenges to address when creating these types of catalogues: on the one hand, the management of the diversity of data and models’ internal nature and their different versions, and on the other hand, the provision of adequate meta-information and Governance tools such as access control and auditing. In this paper, the Knowledge to Environment (K2E) platform is presented, whose architecture aims to define the necessary components for the creation of environments that allow working with data and model catalogues. By environment creation, we mean providing a workspace populated with the datasets and models of an organization, while tracking their distinct versions by using specialised catalogues. In addition, this workspace will incorporate added-value tools for governance and auditing. Finally, an approach for implementing K2E is detailed.;https://ieeexplore.ieee.org/abstract/document/9779801/;tBk1_MDNIDYJ
Markov, I. L., Wang, H., Kasturi, N. S., Singh, S., Garrard, M. R., Huang, Y., ... & Zhou, N. (2022, August). Looper: An end-to-end ML platform for product decisions. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (pp. 3513-3523).;1_ml_machine_data_learning;2022;Looper: An end-to-end ML platform for product decisions;Igor L Markov, Hanson Wang, Nitya S Kasturi, Shaun Singh, Mia R Garrard, Yin Huang, Sze Wai Celeste Yuen, Sarah Tran, Zehui Wang, Igor Glotov, Tanvi Gupta, Peng Chen, Boshuang Huang, Xiaowen Xie, Michael Belkin, Sal Uryasev, Sam Howie, Eytan Bakshy, Norm Zhou;Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 3513-3523, 2022;Modern software systems and products increasingly rely on machine learning models to make data-driven decisions based on interactions with users, infrastructure and other systems. For broader adoption, this practice must (i) accommodate product engineers without ML backgrounds, (ii) support finegrain product-metric evaluation and (iii) optimize for product goals. To address shortcomings of prior platforms, we introduce general principles for and the architecture of an ML platform, Looper, with simple APIs for decision-making and feedback collection. Looper covers the end-to-end ML lifecycle from collecting training data and model training to deployment and inference, and extends support to personalization, causal evaluation with heterogenous treatment effects, and Bayesian tuning for product goals. During the 2021 production deployment, Looper simultaneously hosted 440-1,000 ML models that made 4-6 million real-time decisions per second. We sum up experiences of platform adopters and describe their learning curve.;https://dl.acm.org/doi/abs/10.1145/3534678.3539059;8MzBBATv9OsJ
Tomaszewski, P., Yu, S., Borg, M., & Rönnols, J. (2021, December). Machine learning-assisted analysis of small angle x-ray scattering. In 2021 Swedish Workshop on Data Science (SweDS) (pp. 1-6). IEEE.;1_ml_machine_data_learning;2021;Machine learning-assisted analysis of small angle x-ray scattering;Piotr Tomaszewski, Shun Yu, Markus Borg, Jerk Rönnols;2021 Swedish Workshop on Data Science (SweDS), 2021;Small angle X-ray scattering (SAXS) is extensively used in materials science as a way of examining nanostructures. The analysis of experimental SAXS data involves mapping a rather simple data format to a vast amount of structural models. Despite various scientific computing tools to assist the model selection, the activity heavily relies on the SAXS analysts’ experience, which is recognized as an efficiency bottleneck by the community. To cope with this decision-making problem, we develop and evaluate the open-source, Machine Learning-based tool SCAN (SCattering Ai aNalysis) to provide recommendations on model selection. SCAN exploits multiple machine learning algorithms and uses models and a simulation tool implemented in the SasView package for generating a well defined set of datasets. Our evaluation shows that SCAN delivers an overall accuracy of 95%-97%. The XGBoost Classifier has been identified as the most accurate method with a good balance between accuracy and training time. With eleven predefined structural models for common nanostructures and an easy draw-drop function to expand the number and types training models, SCAN can accelerate the SAXS data analysis workflow.;https://ieeexplore.ieee.org/abstract/document/9638297;TODO
Derakhshan, B., Rezaei Mahdiraji, A., Kaoudi, Z., Rabl, T., & Markl, V. (2022, June). Materialization and reuse optimizations for production data science pipelines. In Proceedings of the 2022 International Conference on Management of Data (pp. 1962-1976).;1_ml_machine_data_learning;2022;Materialization and reuse optimizations for production data science pipelines;Behrouz Derakhshan, Alireza Rezaei Mahdiraji, Zoi Kaoudi, Tilmann Rabl, Volker Markl;Proceedings of the 2022 International Conference on Management of Data, 1962-1976, 2022;Many companies and businesses train and deploy machine learning (ML) pipelines to answer prediction queries. In many applications, new training data continuously becomes available. A typical approach to ensure that ML models are up-to-date is to retrain the ML pipelines following a schedule, e.g., every day on the last seven days of data. Several use cases, such as A/B testing and ensemble learning, require many pipelines to be deployed in parallel. Existing solutions train each pipeline separately, which generates redundant data processing. Our goal is to eliminate redundant data processing in such scenarios using materialization and reuse optimizations. Our solution comprises of two main parts. First, we propose a materialization algorithm that given a storage budget, materializes the subset of the artifacts to minimize the run time of the subsequent executions. Second, we design a reuse algorithm to generate an execution plan by combining the pipelines into a directed acyclic graph (DAG) and reusing the materialized artifacts when appropriate. Our experiments show that our solution can reduce the training time by up to an order of magnitude for different deployment scenarios.;https://dl.acm.org/doi/abs/10.1145/3514221.3526186;IFtlaev8rtIJ
Symeonidis, G., Nerantzis, E., Kazakis, A., & Papakostas, G. A. (2022, January). Mlops-definitions, tools and challenges. In 2022 IEEE 12th Annual Computing and Communication Workshop and Conference (CCWC) (pp. 0453-0460). IEEE.;1_ml_machine_data_learning;2022;Mlops-definitions, tools and challenges;Georgios Symeonidis, Evangelos Nerantzis, Apostolos Kazakis, George A Papakostas;2022 IEEE 12th Annual Computing and Communication Workshop and Conference (CCWC), 0453-0460, 2022;This paper is an concentrated overview of the Machine Learning Operations (MLOps) area. Our aim is to define the operation and the components of such systems by highlighting the current problems and trends. In this context we present the different tools and their usefulness in order to provide the corresponding guidelines. Moreover, the connection between MLOps and AutoML (Automated Machine Learning) is identified and how this combination could work is proposed. The novelty of our approach relies on the combination of state-of-the-art topics such as AutoML, exlainability and sustain-ability in order to overcome the current challenges in MLOps identifying them not only as the answer for the incorporation of ML models in production but also as a possible tool for efficient, robust and accurate machine learning models.;https://ieeexplore.ieee.org/abstract/document/9720902/;7OjQjPGg-skJ
Testi, M., Ballabio, M., Frontoni, E., Iannello, G., Moccia, S., Soda, P., & Vessio, G. (2022). MLOps: A taxonomy and a methodology. IEEE Access, 10, 63606-63618.;1_ml_machine_data_learning;2022;MLOps: A taxonomy and a methodology;Matteo Testi, Matteo Ballabio, Emanuele Frontoni, Giulio Iannello, Sara Moccia, Paolo Soda, Gennaro Vessio;IEEE Access 10, 63606-63618, 2022;Over the past few decades, the substantial growth in enterprise-data availability and the advancements in Artificial Intelligence (AI) have allowed companies to solve real-world problems using Machine Learning (ML). ML Operations (MLOps) represents an effective strategy for bringing ML models from academic resources to useful tools for solving problems in the corporate world. The current literature on MLOps is still mostly disconnected and sporadic. In this work, we review the existing scientific literature and we propose a taxonomy for clustering research papers on MLOps. In addition, we present methodologies and operations aimed at defining an ML pipeline to simplify the release of ML applications in the industry. The pipeline is based on ten steps: business problem understanding, data acquisition, ML methodology, ML training & testing, continuous integration, continuous delivery, continuous training, continuous monitoring, explainability, and sustainability. The scientific and business interest and the impact of MLOps have grown significantly over the past years: the definition of a clear and standardized methodology for conducting MLOps projects is the main contribution of this paper.;https://ieeexplore.ieee.org/abstract/document/9792270/;qm1dR1f8qLIJ
Miñón, R., Díaz-de-Arcaya, J., Torre-Bastida, A. I., Zarate, G., & Moreno-Fernandez-de-Leceta, A. (2022, July). MLPacker: A Unified Software Tool for Packaging and Deploying Atomic and Distributed Analytic Pipelines. In 2022 7th International Conference on Smart and Sustainable Technologies (SpliTech) (pp. 1-6). IEEE.;1_ml_machine_data_learning;2022;MLPacker: A Unified Software Tool for Packaging and Deploying Atomic and Distributed Analytic Pipelines;Raúl Miñón, Josu Díaz-de-Arcaya, Ana I Torre-Bastida, Gorka Zarate, Aitor Moreno-Fernandez-de-Leceta;2022 7th International Conference on Smart and Sustainable Technologies (SpliTech), 1-6, 2022;In the last years, MLOps (Machine Learning Operations) paradigm is attracting the attention from the community, extrapolating the DevOps (Development and Operations) paradigm to the artificial intelligence (AI) development life-cycle. In this area, some challenges must be addressed to successfully deliver solutions since there are specific nuances when dealing with AI operationalization such as the model packaging or monitoring. Fortunately, interesting and helpful approaches, both from the research community and industry have emerged. However, further research is still necessary to fulfil key gaps. This paper presents a tool, MLPacker, for addressing some of them. Concretely, this tool provides mechanisms to package and deploy analytic pipelines both in REST APIs and in streaming mode. In addition, the analytic pipelines can be deployed atomically (i.e., the whole pipeline in the same machine) or in a distributed fashion (i.e., deploying each stage of the pipeline in distinct machines). In this way, users can take advantage from the cloud continuum paradigm considering edge-fog-cloud computing layers. Finally, the tool is decoupled from the training stage to avoid data scientists the integration of blocks of code in their experiments for the operationalization. Besides the package mode (REST API or streaming), the tool can be configured to perform the deployments in local or in remote machines and by using or not containers. For this aim, this paper describes the gaps this tool addresses, the detailed components and flows supported, as well as an scenario with three different case studies to better explain the research conducted.;https://ieeexplore.ieee.org/abstract/document/9854211/;H9oU7Cm2Y3cJ
Lee, H., Jang, Y., Song, J., & Yeon, H. (2021, December). O-RAN AI/ML workflow implementation of personalized network optimization via reinforcement learning. In 2021 IEEE Globecom Workshops (GC Wkshps) (pp. 1-6). IEEE.;0_federated_learning_data_privacy;2021;O-RAN AI/ML workflow implementation of personalized network optimization via reinforcement learning;Hoejoo Lee, Youngcheol Jang, Juhwan Song, Hunje Yeon;2021 IEEE Globecom Workshops (GC Wkshps), 1-6, 2021;In this paper, we study AI-based RAN technology for 5G and 6G networks that are more complex and difficult to analyze than previous generations to make the network more intelligent. We implement a reference AI/ML workflow using RAN Intelligent Controller (RIC) by referring to the AI/ML workflow architecture of O-RAN. We focus on the establishment of an online training environment based on the RIC platform. We use various open-source platforms to serve the ML model as an inference service and to build a Machine Learning (ML) training pipeline for online training. We train our own Reinforcement Learning (RL) model which controls function parameters in Distributed Unit (DU) to maximize total cell throughput. After training the model with data from a specific cell, it is deployed in a different environment. We demonstrate the effectiveness of our proposal by optimizing the model performance and executing the training pipeline for retraining the model using online workflow. As compared to the model before retraining, the total cell throughput has increased by 19.4% when controlled using the retrained model.;https://ieeexplore.ieee.org/abstract/document/9681936/;LU_nM0PuYJwJ
Garg, S., Pundir, P., Rathee, G., Gupta, P. K., Garg, S., & Ahlawat, S. (2021, December). On continuous integration/continuous delivery for automated deployment of machine learning models using mlops. In 2021 IEEE fourth international conference on artificial intelligence and knowledge engineering (AIKE) (pp. 25-28). IEEE.;1_ml_machine_data_learning;2021;On continuous integration/continuous delivery for automated deployment of machine learning models using mlops;Satvik Garg, Pradyumn Pundir, Geetanjali Rathee, PK Gupta, Somya Garg, Saransh Ahlawat;2021 IEEE fourth international conference on artificial intelligence and knowledge engineering (AIKE), 25-28, 2021;In recent years, model deployment in machine learning is observed to be an interesting area of study. It can be seen as a process similar to the one established for traditional software development. Development and operations (DevOps) incorporating Continuous Integration and Continuous Delivery (CI/CD) have demonstrated to smooth out software advancement and speed up organizations. Nonetheless, employing CI/CD pipelines in an application that incorporates components of Machine Learning Operations (MLOps) has challenging issues, and pioneers in the field settle them with the utilization of exclusive tooling, frequently presented by cloud suppliers. This study gives a higher perspective on the machine learning lifecycle and the vital differences between DevOps and MLOps. We talk about tools and techniques to execute the CI/CD pipeline of machine learning frameworks in the MLOps approach. Subsequently, we deep dive into push and pull-based deployments in Github Operations (GitOps). Open exploration challenges are additionally distinguished and added that can direct future research.;https://ieeexplore.ieee.org/abstract/document/9723793/;_Kbaw4jd5rAJ
Díaz-de-Arcaya, J., Torre-Bastida, A. I., Miñón, R., & Almeida, A. (2023). Orfeon: An AIOps framework for the goal-driven operationalization of distributed analytical pipelines. Future Generation Computer Systems, 140, 18-35.;1_ml_machine_data_learning;2023;Orfeon: An AIOps framework for the goal-driven operationalization of distributed analytical pipelines;Josu Díaz-de-Arcaya, Ana I Torre-Bastida, Raúl Miñón, Aitor Almeida;Future Generation Computer Systems 140, 18-35, 2023;The use of Artificial Intelligence solutions keeps raising in the business domain. However, this adoption has not brought the expected results to companies so far. There are several reasons that make Artificial Intelligence solutions particularly complicated to adopt by businesses, such as the knowledge gap between the data science and operations teams. In this paper, we tackle the operationalization of distributed analytical pipelines in heterogeneous production environments, which span across different computational layers. In particular, we present a system called Orfeon, which can leverage different objectives and yields an optimized deployment for these pipelines. In addition, we offer the mathematical formulation of the problem alongside the objectives in hand (i.e. resilience, performance, and cost). Next, we propose a scenario utilizing cloud and edge infrastructural devices, in which we demonstrate how the system can optimize these objectives, without incurring scalability issues in terms of time nor memory. Finally, we compare the usefulness of Orfeon with a variety of tools in the field of machine learning operationalization and conclude that it is able to outperform these tools under the analyzed criteria, making it an appropriate system for the operationalization of machine learning pipelines.;https://www.sciencedirect.com/science/article/pii/S0167739X22003223;JDrH-0TYMzkJ
Miñón, R., Diaz-de-Arcaya, J., Torre-Bastida, A. I., & Hartlieb, P. (2022). Pangea: an MLOps tool for automatically generating infrastructure and deploying analytic pipelines in edge, fog and cloud layers. Sensors, 22(12), 4425.;1_ml_machine_data_learning;2022;Pangea: an MLOps tool for automatically generating infrastructure and deploying analytic pipelines in edge, fog and cloud layers;Raúl Miñón, Josu Diaz-de-Arcaya, Ana I Torre-Bastida, Philipp Hartlieb;Sensors 22 (12), 4425, 2022;"Development and operations (DevOps), artificial intelligence (AI), big data and edge–fog–cloud are disruptive technologies that may produce a radical transformation of the industry. Nevertheless, there are still major challenges to efficiently applying them in order to optimise productivity. Some of them are addressed in this article, concretely, with respect to the adequate management of information technology (IT) infrastructures for automated analysis processes in critical fields such as the mining industry. In this area, this paper presents a tool called Pangea aimed at automatically generating suitable execution environments for deploying analytic pipelines. These pipelines are decomposed into various steps to execute each one in the most suitable environment (edge, fog, cloud or on-premise) minimising latency and optimising the use of both hardware and software resources. Pangea is focused in three distinct objectives: (1) generating the required infrastructure if it does not previously exist; (2) provisioning it with the necessary requirements to run the pipelines (i.e., configuring each host operative system and software, install dependencies and download the code to execute); and (3) deploying the pipelines. In order to facilitate the use of the architecture, a representational state transfer application programming interface (REST API) is defined to interact with it. Therefore, in turn, a web client is proposed. Finally, it is worth noting that in addition to the production mode, a local development environment can be generated for testing and benchmarking purposes.";https://www.mdpi.com/1424-8220/22/12/4425;PvHMQhUNMogJ
Quattrocchi, G., & Tamburri, D. A. (2022). Predictive maintenance of infrastructure code using “fluid” datasets: An exploratory study on Ansible defect proneness. Journal of Software: Evolution and Process, 34(11), e2480.;1_ml_machine_data_learning;2022;Predictive maintenance of infrastructure code using “fluid” datasets: An exploratory study on Ansible defect proneness;Giovanni Quattrocchi, Damian Andrew Tamburri;Journal of Software: Evolution and Process 34 (11), e2480, 2022;This work consolidates and compounds previous investigations in recognizing defects for infrastructure‐as‐code (IaC) scripts using general software development quality metrics with a focus on defect severity but adding to previous work an explorative look at creating datasets, which may boost the predictive power of provided models—we call this notion a fluid dataset. More specifically, we experiment with 50 different metrics harnessing a multiple dataset creation process whereby different versions of the same datasets are rigged with auto‐training facilities for model retraining and redeployment in a DataOps fashion. At this point, with a focus on the Ansible infrastructure code language—as a de facto standard for industrial‐strength infrastructure code—we build defect prediction models and manage to improve on the state of the art by finding an F1 score of 0.52 and a recall of 0.57 using a Naive–Bayes classifier. On the one hand, by improving state‐of‐the‐art defect prediction models using metrics generalizable for different IaC languages, we provide interesting leads for the future of infrastructure‐as‐code. On the other hand, we have barely scratched the surface on the novel approach of fluid‐datasets creation and automated retraining of Machine Learning (ML) defect prediction models, warranting for more research on the same direction in the future.;https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2480;Z3eJwu7pgEkJ
Rauschmayr, N., Kama, S., Kim, M., Choi, M., & Kenthapadi, K. (2022, August). Profiling Deep Learning Workloads at Scale using Amazon SageMaker. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (pp. 3801-3809).;1_ml_machine_data_learning;2022;Profiling Deep Learning Workloads at Scale using Amazon SageMaker;Nathalie Rauschmayr, Sami Kama, Muhyun Kim, Miyoung Choi, Krishnaram Kenthapadi;Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 3801-3809, 2022;With the rise of deep learning (DL), machine learning (ML) has become compute and data intensive, typically requiring multi-node multi-GPU clusters. As state-of-the-art models grow in size in the order of trillions of parameters, their computational complexity and cost also increase rapidly. Since 2012, the cost of deep learning doubled roughly every quarter, and this trend is likely to continue. ML practitioners have to cope with common challenges of efficient resource utilization when training such large models. In this paper, we propose a new profiling tool that cross-correlates relevant system utilization metrics and framework operations. The tool supports profiling DL models at scale, identifies performance bottlenecks, and provides insights with recommendations. We deployed the profiling functionality as an add-on to Amazon SageMaker Debugger, a fully-managed service that leverages an on-the-fly analysis system (called rules) to automatically identify complex issues in DL training jobs. By presenting deployment results and customer case studies, we show that it enables users to identify and fix issues caused by inefficient hardware resource usage, thereby reducing training time and cost.;https://dl.acm.org/doi/abs/10.1145/3534678.3539036;ayDXvVoZ2GEJ
Zeydan, E., & Mangues-Bafalluy, J. (2022). Recent advances in data engineering for networking. IEEE Access, 10, 34449-34496.;9_data_science_software_process;2022;Recent advances in data engineering for networking;Engin Zeydan, Josep Mangues-Bafalluy;IEEE Access 10, 34449-34496, 2022;This tutorial paper examines recent advances in data engineering, focusing on aspects of network management and orchestration. We provide a comprehensive analysis of standardization efforts as well as platform development activities related to data engineering driven network design. We then focus on the integration aspects of the data engineering ecosystem and telecommunication networks. The results of our tutorial investigation show that despite various efforts towards standardization and network management and orchestration platforms, there is still a significant gap in applying recent developments in the evolving data engineering world to the telecommunication domain. New advanced functionalities in data engineering as well as clear separations between the building blocks of data engineering pipelines within the proposed standardized architectures have been overlooked or not explored in detail by the standardization or platform development bodies in the telecommunication domain. Therefore, at the end of the paper, we discuss these gaps and research challenges in the context of future development processes for data engineering-driven network design and applications of data engineering concepts in telecommunication networks. We also propose several recommendations for early adoption of these technologies and frameworks in telecommunication infrastructures and platforms.;https://ieeexplore.ieee.org/abstract/document/9743922/;ihAdQOLN8KUJ
Hagos, D. H., Kakantousis, T., Sheikholeslami, S., Wang, T., Vlassov, V., Payberah, A. H., ... & Dowling, J. (2022). Scalable Artificial Intelligence for Earth Observation Data Using Hopsworks. Remote Sensing, 14(8), 1889.;1_ml_machine_data_learning;2022;Scalable Artificial Intelligence for Earth Observation Data Using Hopsworks;Desta Haileselassie Hagos, Theofilos Kakantousis, Sina Sheikholeslami, Tianze Wang, Vladimir Vlassov, Amir Hossein Payberah, Moritz Meister, Robin Andersson, Jim Dowling;Remote Sensing 14 (8), 1889, 2022;This paper introduces the Hopsworks platform to the entire Earth Observation (EO) data community and the Copernicus programme. Hopsworks is a scalable data-intensive open-source Artificial Intelligence (AI) platform that was jointly developed by Logical Clocks and the KTH Royal Institute of Technology for building end-to-end Machine Learning (ML)/Deep Learning (DL) pipelines for EO data. It provides the full stack of services needed to manage the entire life cycle of data in ML. In particular, Hopsworks supports the development of horizontally scalable DL applications in notebooks and the operation of workflows to support those applications, including parallel data processing, model training, and model deployment at scale. To the best of our knowledge, this is the first work that demonstrates the services and features of the Hopsworks platform, which provide users with the means to build scalable end-to-end ML/DL pipelines for EO data, as well as support for the discovery and search for EO metadata. This paper serves as a demonstration and walkthrough of the stages of building a production-level model that includes data ingestion, data preparation, feature extraction, model training, model serving, and monitoring. To this end, we provide a practical example that demonstrates the aforementioned stages with real-world EO data and includes source code that implements the functionality of the platform. We also perform an experimental evaluation of two frameworks built on top of Hopsworks, namely Maggy and AutoAblation. We show that using Maggy for hyperparameter tuning results in roughly half the wall-clock time required to execute the same number of hyperparameter tuning trials using Spark while providing linear scalability as more workers are added. Furthermore, we demonstrate how AutoAblation facilitates the definition of ablation studies and enables the asynchronous parallel execution of ablation trials.;https://www.mdpi.com/2072-4292/14/8/1889;zzhbIigV8XIJ
Dessalk, Y. D., Nikolov, N., Matskin, M., Soylu, A., & Roman, D. (2020, November). Scalable execution of big data workflows using software containers. In Proceedings of the 12th International Conference on Management of Digital EcoSystems (pp. 76-83).;9_data_science_software_process;2020;Scalable execution of big data workflows using software containers;Yared Dejene Dessalk, Nikolay Nikolov, Mihhail Matskin, Ahmet Soylu, Dumitru Roman;Proceedings of the 12th International Conference on Management of Digital EcoSystems, 76-83, 2020;Big Data processing involves handling large and complex data sets, incorporating different tools and frameworks as well as other processes that help organisations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, require taking advantage of the elasticity of cloud infrastructures for scalability. In this paper, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies and message-oriented middleware (MOM) to enable highly scalable workflow execution. The approach is demonstrated in a use case together with a set of experiments that demonstrate the practical applicability of the proposed approach for the scalable execution of Big Data workflows. Furthermore, we present a scalability comparison of our proposed approach with that of Argo Workflows - one of the most prominent tools in the area of Big Data workflows.;https://dl.acm.org/doi/abs/10.1145/3415958.3433082;JYK21wIw_4gJ
Patel, D., Shrivastava, S., Gifford, W., Siegel, S., Kalagnanam, J., & Reddy, C. (2020, December). Smart-ml: A system for machine learning model exploration using pipeline graph. In 2020 IEEE International Conference on Big Data (Big Data) (pp. 1604-1613). IEEE.;1_ml_machine_data_learning;2020;Smart-ml: A system for machine learning model exploration using pipeline graph;Dhaval Patel, Shrey Shrivastava, Wesley Gifford, Stuart Siegel, Jayant Kalagnanam, Chandra Reddy;2020 IEEE International Conference on Big Data (Big Data), 1604-1613, 2020;In this paper, we describe an overarching ML system with a simple programming interface that leverages existing AI and ML frameworks to make the task of model exploration easier. The proposed system introduces a new programming construct namely pipeline graph (a directed acyclic graph) consisting of multiple machine learning operations provided by different ML repositories. End user uses the pipeline graph as a common interface for modeling different ML tasks such as classification, regression, and timeseries prediction, while enabling efficient execution on different environments (Spark, Celery and Cloud). We further annotated the pipeline graph with a hyper-parameter grid and an option to try-out a wide range of optimization strategies (i.e., Random, Bayesian, Bandit, AutoLearn, etc). Given a large pre-defined pipeline graph along with its hyper-parameters, we provided a general-purpose, scalable and efficient pipeline-graph exploration technique to provide the automated solutions to a variety of ML tasks. We compare our automated approach to several state-of-the-art automated AI systems and find that we achieve performance comparable to the best results, while often producing simpler pipelines using off the shelf components. Our evaluation suite consists of experiments on 60+ classifications and regressions datasets.;https://ieeexplore.ieee.org/abstract/document/9378082/;yM0trr7mxkMJ
Openja, M., Majidi, F., Khomh, F., Chembakottu, B., & Li, H. (2022, June). Studying the practices of deploying machine learning projects on docker. In Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering (pp. 190-200).;1_ml_machine_data_learning;2022;Studying the practices of deploying machine learning projects on docker;Moses Openja, Forough Majidi, Foutse Khomh, Bhagya Chembakottu, Heng Li;Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering, 190-200, 2022;Docker is a containerization service that allows for convenient deployment of websites, databases, applicationsâ€™ APIs, and machine learning (ML) models with a few lines of code. Studies have recently explored the use of Docker for deploying general software projects with no specific focus on how Docker is used to deploy ML-based projects. In this study, we conducted an exploratory study to understand how Docker is being used to deploy ML-based projects. As the initial step, we examined the categories of ML-based projects that use Docker. We then examined why and how these projects use Docker, and the characteristics of the resulting Docker images. Our results indicate that six categories of ML-based projects use Docker for deployment, including ML Applications, MLOps/ AIOps, Toolkits, DL Frameworks, Models, and Documentation. We derived the taxonomy of 21 major categories representing the purposes of using Docker, including those specific to models such as model management tasks (e.g., testing, training). We then showed that ML engineers use Docker images mostly to help with the platform portability, such as transferring the software across the operating systems, runtimes such as GPU, and language constraints. However, we also found that more resources may be required to run the Docker images for building ML-based software projects due to the large number of files contained in the image layers with deeply nested directories. We hope to shed light on the emerging practices of deploying ML software projects using containers and highlight aspects that should be improved.;https://dl.acm.org/doi/abs/10.1145/3530019.3530039;zdigUTCybcoJ
Wu, W., & Zhang, C. (2021, June). Towards understanding end-to-end learning in the context of data: machine learning dancing over semirings & Codd's table. In Proceedings of the Fifth Workshop on Data Management for End-To-End Machine Learning (pp. 1-4).;1_ml_machine_data_learning;2021;Towards understanding end-to-end learning in the context of data: machine learning dancing over semirings & Codd's table;Wentao Wu, Ce Zhang;Proceedings of the Fifth Workshop on Data Management for End-To-End Machine Learning, 1-4, 2021;"Recent advances in machine learning (ML) systems have made it incredibly easier to train ML models given a training set. However, our understanding of the behavior of the model training process has not been improving at the same pace. Consequently, a number of key questions remain: How can we systematically assign importance or value to training data with respect to the utility of the trained models, may it be accuracy, fairness, or robustness? How does noise in the training data, either injected by noisy data acquisition processes or adversarial parties, have an impact on the trained models? How can we find the right data that can be cleaned and labeled to improve the utility of the trained models? Just when we start to understand these important questions for ML models in isolation recently, we now have to face the reality that most real-world ML applications are way more complex than a single ML model.In this article---an extended abstract for an invited talk at the DEEM workshop---we will discuss our current efforts in revisiting these questions for an end-to-end ML pipeline, which consists of a noise model for data and a feature extraction pipeline, followed by the training of an ML model. In our opinion, this poses a unique challenge on the joint analysis of data processing and learning. Although we will describe some of our recent results towards understanding this interesting problem, this article is more of a ""confession"" on our technical struggles and a ""cry for help"" to our data management community.";https://dl.acm.org/doi/abs/10.1145/3462462.3468878;tI87pMngrf0J
Atwal, H. (2019). Practical DataOps: Delivering agile data science at scale. Apress.;9_data_science_software_process;2019;Practical DataOps: Delivering agile data science at scale;Harvinder Atwal;Apress, 2019;It is an amazing time to be working with data. Exponential growth in data collection, advances in machine learning (ML) and artificial intelligence (AI) algorithms, explosion in software libraries for working with much bigger quantities of data than was possible even a decade ago, and advances in big data technologies for storing and processing data have ushered in a transformative period for business, science, and government. Data science aims to aid in better decision-making, leading to beneficial actions than we otherwise could achieve by extracting knowledge from data. Data science does this by applying the scientific method, algorithms, and processes to data in various forms. Data science cannot exist on its own and is part of an ecosystem of skills that includes data engineering and the broader field of data analytics. Although there is hype associated with any technological change, and data science is no exception, many industries and fields are still at the beginning of the data-driven digital transformation. Over the next decade, machine learning, deep learning, and other data science techniques will transform every aspect of our lives from personalized healthcare to financial management to how we interface with machines, whether selfdriving cars or virtual assistants.Just as we are at the beginning of data-driven transformation, we are also only at the start of the journey to understand the best processes required to deliver our desired outcomes from raw data. Modern data science is still in the comparable transition phase between bespoke hand-crafted production and mechanized automation that manufacturing was confronting during the early 19th century.;https://link.springer.com/content/pdf/10.1007/978-1-4842-5104-1.pdf;9Yx3bYWqCoYJ
Popp, M. (2019). Comprehensive support of the lifecycle of machine learning models in model management systems (Master's thesis).;1_ml_machine_data_learning;2019;Comprehensive support of the lifecycle of machine learning models in model management systems (Master's thesis).;Matthias Popp;-;Today, Machine Learning (ML) is entering many economic and scientific fields. The lifecycle of ML models includes data pre-processing to transform raw data into features, training a model with the features, and providing the model to answer predictive queries. The challenge is to ensure accurate predictions by continuously updating the model with automatic or manual retraining. To be aware of all changes, e.g. datasets and parameters, it is required to store metadata over the entire ML lifecycle. In this thesis we present a concept and system for comprehensive support of the ML lifecycle. The concept includes a metadata schema, as well as a solution to collect and enrich the metadata. The metadata schema contains information about the experiment, runs, executions, executables and common artifacts in ML such as datasets, models, and metrics. The stored information can be used for comparisons, re-iterations, and backtracking of ML experiments. We achieve this by tracking the lineage of ML pipeline steps and collecting metadata such as hyperparameters. Furthermore, a prototype is implemented to demonstrate and evaluate the concept. A case study, based on a selected scenario, serves as the basis for a qualitative assessment. The case study shows that the concept meets all the requirements and is therefore a suitable approach to comprehensively support ML model lifecycle.;http://elib.uni-stuttgart.de/handle/11682/10707;mcdy5DC7CiEJ
Takeuchi, H., Doi, T., Washizaki, H., Okuda, S., & Yoshioka, N. (2021, October). Enterprise architecture based representation of architecture and design patterns for machine learning systems. In 2021 IEEE 25th International Enterprise Distributed Object Computing Workshop (EDOCW) (pp. 245-250). IEEE.;1_ml_machine_data_learning;2021;Enterprise architecture based representation of architecture and design patterns for machine learning systems;Hironori Takeuchi, Takuo Doi, Hironori Washizaki, Satoshi Okuda, Nobukazu Yoshioka;2021 IEEE 25th International Enterprise Distributed Object Computing Workshop (EDOCW), 2021;In this study, we consider projects for the development of machine learning (ML) service systems that apply ML techniques to enterprise functions, and propose a method of representing the architecture and design patterns for ML service systems. Based on the proposed method, we represent the items described in the pattern documents as elements in the enterprise architecture modeling and derived a generic model for ML architecture and designed patterns. By applying the proposed method and the generic pattern model, we analyze an existing ML design pattern and represent it as a model. Through modeling practice, we confirm that an effective use scenario occurs when using the represented model during the project activities, and we can revise or enhance the pattern documents consistently by applying the model.;https://ieeexplore.ieee.org/abstract/document/9626310;TODO
Sharma, R., & Davuluri, K. (2019, March). Design patterns for machine learning applications. In 2019 3rd International Conference on Computing Methodologies and Communication (ICCMC) (pp. 818-821). IEEE.;1_ml_machine_data_learning;2019;Design patterns for machine learning applications;Ruchi Sharma, Kiran Davuluri;2019 3rd International Conference on Computing Methodologies and Communication (ICCMC), 2019;The aim of this paper is detecting and analyzing design patterns and architectural patterns for two software applications that use Machine Learning (ML) and Deep Learning techniques respectively. The classification is done based on the design principles that need to be adhered for a standard design and architectural patterns. ML based applications generally have ubiquitous modules to some extent. However, modeling their components through varied design patterns bring out positive changes to the performance of the systems as well as mitigates many of the computational shortcomings faced otherwise. Although it is still a novel approach for systems implementing machine learning algorithms, the paper aims to bring a new paradigm in analyzing system models.;https://ieeexplore.ieee.org/abstract/document/8819692;TODO
Lu, Q., Zhu, L., Xu, X., Whittle, J., Douglas, D., & Sanderson, C. (2022, May). Software engineering for responsible AI: An empirical study and operationalised patterns. In Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice (pp. 241-242).;12_ai_ethical_ethic_intelligence;2022;Software engineering for responsible AI: An empirical study and operationalised patterns;Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, David Douglas, Conrad Sanderson;Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice, 241-242, 2022;"AI ethics principles and guidelines are typically high-level and do not provide concrete guidance on how to develop responsible AI systems. To address this shortcoming, we perform an empirical study involving interviews with 21 scientists and engineers to understand the practitioners' views on AI ethics principles and their implementation. Our major findings are: (1) the current practice is often a done-once-and-forget type of ethical risk assessment at a particular development step, which is not sufficient for highly uncertain and continual learning AI systems; (2) ethical requirements are either omitted or mostly stated as high-level objectives, and not specified explicitly in verifiable way as system outputs or outcomes; (3) although ethical requirements have the characteristics of cross-cutting quality and non-functional requirements amenable to architecture and design analysis, system-level architecture and design are under-explored; (4) there is a strong desire for continuously monitoring and validating AI systems post deployment for ethical requirements but current operation practices provide limited guidance. To address these findings, we suggest a preliminary list of patterns to provide operationalised guidance for developing responsible AI systems.";https://dl.acm.org/doi/abs/10.1145/3510457.3513063;Zb7FENmNsPgJ
Washizaki, H., Takeuchi, H., Khomh, F., Natori, N., Doi, T., & Okuda, S. (2020, September). Practitioners’ insights on machine-learning software engineering design patterns: a preliminary study. In 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME) (pp. 797-799). IEEE.;1_ml_machine_data_learning;2020;Practitioners’ insights on machine-learning software engineering design patterns: a preliminary study;Hironori Washizaki, Hironori Takeuchi, Foutse Khomh, Naotake Natori, Takuo Doi, Satoshi Okuda;2020 IEEE International Conference on Software Maintenance and Evolution (ICSME), 797-799, 2020;Machine-learning (ML) software engineering design patterns encapsulate reusable solutions to commonly occurring problems within the given contexts of ML systems and software design. These ML patterns should help develop and maintain ML systems and software from the design perspective. However, to the best of our knowledge, there is no study on the practitioners' insights on the use of ML patterns for design of their ML systems and software. Herein we report the preliminary results of a literature review and a questionnaire-based survey on ML system developers' state-of-practices with concrete ML patterns.;https://ieeexplore.ieee.org/abstract/document/9240692/;FH6wzUoTjQYJ
Tkachuk, R. V., Ilie, D., & Tutschku, K. (2020, November). Towards a secure proxy-based architecture for collaborative AI engineering. In 2020 Eighth International Symposium on Computing and Networking Workshops (CANDARW) (pp. 373-379). IEEE.;5_adversarial_attack_example_model;2020;Towards a secure proxy-based architecture for collaborative AI engineering;Roman-Valentyn Tkachuk, Dragos Ilie, Kurt Tutschku;2020 Eighth International Symposium on Computing and Networking Workshops (CANDARW), 373-379, 2020;In this paper, we investigate how to design a security architecture of a Platform-as-a-Service (PaaS) solution, denoted as Secure Virtual Premise (SVP), for collaborative and distributed AI engineering using AI artifacts and Machine Learning (ML) pipelines. Artifacts are re-usable software objects which are a) tradeable in marketplaces, b) implemented by containers, c) offer AI functions as microservices, and, d) can form service chains, denoted as AI pipelines. Collaborative engineering is facilitated by the trading and (re-)using artifacts and, thus, accelerating the AI application design. The security architecture of the SVP is built around the security needs of collaborative AI engineering and uses a proxy concept for microservices. The proxy shields the AI artifact and pipelines from outside adversaries as well as from misbehaving users, thus building trust among the collaborating parties. We identify the security needs of collaborative AI engineering, derive the security challenges, outline the SVP's architecture, and describe its security capabilities and its implementation, which is currently in use with several AI developer communities. Furthermore, we evaluate the SVP's Technology Readiness Level (TRL) with regard to collaborative AI engineering and data security.;https://ieeexplore.ieee.org/abstract/document/9355887/;2BOPVCWZeVoJ
Yasser, A., & Abu-Elkhier, M. (2021, November). Towards fluid software architectures: bidirectional human-AI interaction. In 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) (pp. 1368-1372). IEEE.;1_ml_machine_data_learning;2021;Towards fluid software architectures: bidirectional human-AI interaction;Ammar Yasser, Mervat Abu-Elkhier;2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE), 1368-1372, 2021;The research on engineering software applications that employ artificial intelligence (AI) and machine learning (ML) is at an all-time peak. However, most of the research in this area is focused on the interactions between humans and AI which, in turn, is predominantly concerned with either building immersive interfaces and user experiences that allow for increased telemetry or on handling AI and ML applications in production (MLOps). Nonetheless, the research on fundamental architectural differences between AI-powered applications and traditional ones did not receive its fair share of attention. To that end, we believe that a new take on the fundamental architecture of building software applications is needed. With the ever increasing prominence of content-driven AI-powered applications, it is our conviction that 1) content could be served by servers without clients requesting, 2) servers could (should) request data from clients without waiting for their requests, and 3) interfaces should dynamically adapt to updates that happen to the intelligence driving the application. Hence, in this paper, we propose the fluid architecture that facilitates the bidirectional interaction between clients and servers as well as accommodates the co-dependent evolution of interfaces and back-end intelligence in AI-powered systems.;https://ieeexplore.ieee.org/abstract/document/9678647/;piui-tQE9VkJ
Kum, S., Kim, Y., Siracusa, D., & Moon, J. (2020, November). Artificial intelligence service architecture for edge device. In 2020 IEEE 10th International Conference on Consumer Electronics (ICCE-Berlin) (pp. 1-3). IEEE.;7_edge_computing_deep_learning;2020;Artificial intelligence service architecture for edge device;Seungwoo Kum, Youngkee Kim, Domenico Siracusa, Jaewon Moon;2020 IEEE 10th International Conference on Consumer Electronics (ICCE-Berlin), 1-3, 2020;Edge computing is getting more focused recently due to high demand of Artificial Intelligence application, for example, detection of wearing masks from a video stream. In edge computing, the AI applications are placed near data source to improve quality of service, and there are several researches to bring AI service onto edge device such as TensorFlow Serving. However, existing researches focus on providing accessibility of the trained model itself and require additional preprocessing and postprocessing of data to build an end-to-end service. In this paper, an AI Service Architecture for an Edge Device is proposed to provide accessibility to the AI service itself. The proposed architecture provides AI as a service, which means it includes pre-processing and postprocessing, as well as the model itself. Since it includes all the methods which consists an AI service, the proposed architecture provides more intuitive ways to bring an AI method to edge device. Moreover, it defines interfaces to configure and access the AI service, which makes it suitable apply microservice architecture.;https://ieeexplore.ieee.org/abstract/document/9352184/;8bRQDKkYtAsJ
Cha, B., Cha, Y., An, S., Jeon, E., Park, S., & Kim, J. (2021, October). Experimental Design for Multi-task Deep Learning toward Intelligence Augmented Visual AI. In 2021 International Conference on Information and Communication Technology Convergence (ICTC) (pp. 1735-1737). IEEE.;7_edge_computing_deep_learning;2021;Experimental Design for Multi-task Deep Learning toward Intelligence Augmented Visual AI;ByungRae Cha, YoonSeok Cha, SeongYeol An, EunJin Jeon, Sun Park, JongWon Kim;2021 International Conference on Information and Communication Technology Convergence (ICTC), 1735-1737, 2021;Gartner's top ten strategic technologies for 2020 and 2021 include Democratization and Intelligent composable business. It predicts the democratization of AI technology and the acceleration of DT in the analog business aspect. In this study, we propose a task-based deep learning architecture for computer vision as the first step towards AGI (Artificial General Intelligence) from the perspective of narrow AI. Multi-task deep learning is composed of Guide-task model, Sub-task model, Glue code, and Integration module, and the functions of the components are defined.;https://ieeexplore.ieee.org/abstract/document/9620883/;HU4ROANXifkJ
Zhou, J. (2020, October). Design of AI-based self-learning platform for college English listening. In 2020 2nd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI) (pp. 544-547). IEEE.;1_ml_machine_data_learning;2020;Design of AI-based self-learning platform for college English listening;Junping Zhou;2020 2nd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI), 544-547, 2020;Due to the current ineffectiveness of College English listening class, this paper puts forward a design of AI-based self-learning platform aimed at improving students' listening ability with AI assistance. The design concept and operation flow of this AI-based learning platform derive from the current application problems of AI-aided listening platforms for English learning. A survey is conducted to analyze the application characteristics of present AI-aided listening platforms for college English listening. The data analysis of the survey shows that the intelligent listening products have the merits such as rich listening resources, big exercise database and various learning channels. However, there are also some demerits like lack of specific and personalized exercise guidance, too wide material accumulation without logical classification, poor supervision and evaluation system. Based on the application problems, this paper introduces an AI-based self-learning platform for college English listening, which adopts three layer design concept e.g. service layer, technical layer and data layer. The three layers help form the integrated learning circle to improve students' daily self-learning efficiency. This AI-based self-learning platform can provide individual tailored teaching services, such as spontaneous learning help, learning suggestions and evaluations based on core technologies of artificial intelligence. It will create an effective self-learning environment and improve students' English listening proficiency in a better and more effective way.;https://ieeexplore.ieee.org/abstract/document/9361038/;75G7t_kyHWkJ
Bi, J., Zhang, G., Yang, C., Jin, L., & Zhang, W. (2021, December). Architecture Design of Typical Target Detection and Tracking System in Battlefield Environment. In 2021 3rd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI) (pp. 473-477). IEEE.;2_safety_system_autonomous_vehicle;2021;Architecture Design of Typical Target Detection and Tracking System in Battlefield Environment;Jianauan Bi, Guohui Zhang, Chaohong Yang, Liya Jin, Wei Zhang;2021 3rd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI), 473-477, 2021;Aiming at the characteristics of small targets, few samples, many occlusions, and strong camouflage in the battlefield environment, a target detection and tracking system based on multi-source sensor data is designed. The system is mainly composed of three parts: the unmanned aerial vehicle (UAV) platform, the ground display control platform (GDCP), and the offline algorithm training platform. The offline-trained target detection and tracking algorithm is deployed in the UAV-borne AI processing chip, which can conduct the real-time detection and tracking of typical targets in the battlefield and send the results and attitude information of UAVs to the GDCP. The system can realize real-time detection of typical battlefield targets and tracking control through the GDCP.;https://ieeexplore.ieee.org/abstract/document/9730973/;wChh5G6XOXQJ
Haindl, P., Buchgeher, G., Khan, M., & Moser, B. (2022, May). Towards a reference software architecture for human-AI teaming in smart manufacturing. In Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results (pp. 96-100).;1_ml_machine_data_learning;2022;Towards a reference software architecture for human-AI teaming in smart manufacturing;Philipp Haindl, Georg Buchgeher, Maqbool Khan, Bernhard Moser;Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results, 96-100, 2022;With the proliferation of AI-enabled software systems in smart manufacturing, the role of such systems moves away from a reactive to a proactive role that provides context-specific support to manufacturing operators. In the frame of the EU funded Teaming.AI project, we identified the monitoring of teaming aspects in human-AI collaboration, the runtime monitoring and validation of ethical policies, and the support for experimentation with data and machine learning algorithms as the most relevant challenges for human-AI teaming in smart manufacturing. Based on these challenges, we developed a reference software architecture based on knowledge graphs, tracking and scene analysis, and components for relational machine learning with a particular focus on its scalability. Our approach uses knowledge graphs to capture product-and process specific knowledge in the manufacturing process and to utilize it for relational machine learning. This allows for context-specific recommendations for actions in the manufacturing process for the optimization of product quality and the prevention of physical harm. The empirical validation of this software architecture will be conducted in cooperation with three large-scale companies in the automotive, energy systems, and precision machining domain. In this paper we discuss the identified challenges for such a reference software architecture, present its preliminary status, and sketch our further research vision in this project.;https://dl.acm.org/doi/abs/10.1145/3510455.3512788;JlN4E4HCMJwJ
Wang, Z. (2021, September). Research on Feature and Architecture Design of AI Firewall. In 2021 5th Annual International Conference on Data Science and Business Analytics (ICDSBA) (pp. 75-78). IEEE.;5_adversarial_attack_example_model;2021;Research on Feature and Architecture Design of AI Firewall;Zhijia Wang;2021 5th Annual International Conference on Data Science and Business Analytics (ICDSBA), 75-78, 2021;This paper analyzes the current network security environment and the shortcomings of firewall products in the current network environment, and expounds the benefits that can be brought by the application of AI technology in threat detection. Then, the paper analyzes the advantages of AI firewall and expounds the characteristics that AI firewall can build learning models and realize the independent evolution of threat detection capability. Finally, the intelligent defense system and logical architecture of AI firewall are designed. The intelligent defense system has three components, including cloud intelligence, security center and firewall. The logical architecture has six layers of architecture, including hardware layer, data layer, algorithm layer, detection layer, analysis layer and interaction layer. These designs provide some reference for the design and development of AI firewall.;https://ieeexplore.ieee.org/abstract/document/9693884/;bOzqx0izAwoJ
He, H., & Shi, L. (2010, June). The application of design pattern on intellectualized selecting the doors and windows pattern system. In 2010 International Conference On Computer Design and Applications (Vol. 3, pp. V3-545). IEEE.;1_ml_machine_data_learning;2010;The application of design pattern on intellectualized selecting the doors and windows pattern system;Haihui He, Linxiang Shi;2010 International Conference On Computer Design and Applications 3, V3-545-V3-547, 2010;The paper introduces the basic principle of the MVC design pattern, and how to apply to the intellectualized selecting the doors and windows pattern system, which improves the system scalability, maintainability and code reuse rate. At the same time, the paper introduces the realization of the intelligent system, and gives a flexible method of deploying the doors and windows pattern system.;https://ieeexplore.ieee.org/abstract/document/5541359/;-gA3jcXdF4wJ
Lu, Q., Zhu, L., Xu, X., & Whittle, J. (2023). Responsible-AI-by-design: A pattern collection for designing responsible AI systems. IEEE Software.;1_ml_machine_data_learning;2023;Responsible-AI-by-design: A pattern collection for designing responsible AI systems;Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle;IEEE Software, 2023;Responsible artificial intelligence (AI) issues often occur at the system level, crosscutting many system components and the entire software engineering lifecycle. We summarize design patterns that can be embedded into AI systems as product features to contribute to responsible-AI-by-design.;https://ieeexplore.ieee.org/abstract/document/10007631/;s4LdmLyajHIJ
Smith, D. (2017). Exploring development patterns in data science.;1_ml_machine_data_learning;2017;Exploring development patterns in data science.;only citation on google scholar;only citation on google scholar;only citation on google scholar;;q1Eqspd8vUAJ
Savolainen, J., & Myllarniemi, V. (2009, September). Layered architecture revisited—Comparison of research and practice. In 2009 Joint Working IEEE/IFIP Conference on Software Architecture & European Conference on Software Architecture (pp. 317-320). IEEE.;1_ml_machine_data_learning;2009;Layered architecture revisited—Comparison of research and practice;Juha Savolainen, Varvana Myllarniemi;2009 Joint Working IEEE/IFIP Conference on Software Architecture & European Conference on Software Architecture, 317-320, 2009;"Organizing a software architecture into layers has been one of the earliest architectural styles ever used. Even today layered structure is a very common architectural style used in various industrial systems. However, we have observed that the usage of layered architectural style varies greatly in different contexts. This paper aims to compare the notion of software architecture layers in research literature as well as in industrial practice. Firstly, we performed a systematic literature review of research articles on layered software architectures; we also reviewed selected books of software architecture. Secondly, to understand the practice, we investigated a number different recent architecture documents to cover the current usage of layered architectures. Our results indicate that there is very little actual research done on layered architectures. The current usage of layered structures seems to be more complex than reported before. This gap between the research and practice needs to be bridged by researchers.";https://ieeexplore.ieee.org/abstract/document/5290685/;OHa9b_C6hMsJ
Washizaki, H., Yoshioka, N., Hazeyama, A., Kato, T., Kaiya, H., Ogata, S., ... & Fernandez, E. B. (2019, May). Landscape of IoT patterns. In 2019 IEEE/ACM 1st International Workshop on Software Engineering Research & Practices for the Internet of Things (SERP4IoT) (pp. 57-60). IEEE.;1_ml_machine_data_learning;2019;Landscape of IoT patterns;Hironori Washizaki, Nobukazu Yoshioka, Atsuo Hazeyama, Takehisa Kato, Haruhiko Kaiya, Shinpei Ogata, Takao Okubo, Eduardo B Fernandez;2019 IEEE/ACM 1st International Workshop on Software Engineering Research & Practices for the Internet of Things (SERP4IoT), 57-60, 2019;Patterns are encapsulations of problems and solutions under specific contexts. As the industry is realizing many successes (and failures) in IoT systems development and operations, many IoT patterns have been published such as IoT design patterns and IoT architecture patterns. Because these patterns are not well classified, their adoption does not live up to their potential. To understand the reasons, this paper analyzes an extensive set of published IoT architecture and design patterns according to several dimensions and outlines directions for improvements in publishing and adopting IoT patterns.;https://ieeexplore.ieee.org/abstract/document/8900688/;tXGbgPo8l5kJ
Renggli, C., Karlaš, B., Ding, B., Liu, F., Schawinski, K., Wu, W., & Zhang, C. (2019). Continuous integration of machine learning models with ease. ml/ci: Towards a rigorous yet practical treatment. Proceedings of Machine Learning and Systems, 1, 322-333.;1_ml_machine_data_learning;2019;Continuous integration of machine learning models with ease;Cedric Renggli, Bojan Karlaš, Bolin Ding, Feng Liu, Kevin Schawinski, Wentao Wu, Ce Zhang;Proceedings of Machine Learning and Systems 1, 322-333, 2019;Continuous integration is an indispensable step of modern software engineering practices to systematically manage the life cycles of system development. Developing a machine learning model is no difference—it is an engineering process with a life cycle, including design, implementation, tuning, testing, and deployment. However, most, if not all, existing continuous integration engines do not support machine learning as first-class citizens.;https://proceedings.mlsys.org/paper_files/paper/2019/hash/4284d31e68c0a4a39dcdad167ac4bd72-Abstract.html;AwOvWQsL1zcJ
Ahmed, A., Abdullah, U., & Sawar, M. J. (2010, June). Software architecture of a learning apprentice system in medical billing. In Proceedings of the World Congress on Engineering (Vol. 1, pp. 52-56).;1_ml_machine_data_learning;2010;Software architecture of a learning apprentice system in medical billing;Aftab Ahmed, Umair Abdullah, Mohammad J Sawar;Proceedings of the World Congress on Engineering 1, 52-56, 2010;Machine learning is an emerging field of computer science concerned with the learning of knowledge from exploration of already stored data. However, effective utilization of extracted knowledge is an important issue. Extracted knowledge may be best utilized via feeding to knowledge based system. To this end, the work reported in this paper is based on a novel idea to enhance the productivity of the previously developed systems. This paper presents the proposed architecture of a Learning Apprentice System in Medical Billing system being developed for medical claim processing. A new dimension is added whereby, the process of extracting and utilization of knowledge are implemented in relational database environment for improved performance. It opens enormous application areas as most business data is in relational format managed by some relational database management server. The major components of the proposed system include knowledge base, rule engine, knowledge editor, and data mining module. Knowledge base consists of rules, meta rules and logical variables defined in the form of SQL queries stored in relational tables. Rule engine has been successfully developed and deployed in the form of SQL stored procedures. Knowledge editor and data mining modules are under development. Given architecture depicts over all business process of medical billing along with major components of the system. The proposed architecture effectively integrates all three pertinent components given by data mining (production rule discovery), rule based systems technology and database systems environment.;https://www.researchgate.net/profile/Umair-Abdullah-3/publication/45534356_Software_Architecture_of_a_Learning_Apprentice_System_in_Medical_Billing/links/0046353b3b745e20e7000000/Software-Architecture-of-a-Learning-Apprentice-System-in-Medical-Billing.pdf;SxhL9j9m6eYJ
Serban, A., & Visser, J. (2022, March). Adapting software architectures to machine learning challenges. In 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER) (pp. 152-163). IEEE.;1_ml_machine_data_learning;2022;Adapting software architectures to machine learning challenges;Alex Serban, Joost Visser;2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER), 152-163, 2022;"Unique developmental and operational characteristics of machine learning (ML) components as well as their inherent uncertainty demand robust engineering principles are used to ensure their quality. We aim to determine how software systems can be (re-) architected to enable robust integration of ML components. Towards this goal, we conducted a mixed-methods empirical study consisting of (i) a systematic literature review to identify the challenges and their solutions in software architecture for ML, (ii) semi-structured interviews with practitioners to qualitatively complement the initial findings and (iii) a survey to quantitatively validate the challenges and their solutions. We compiled and validated twenty challenges and solutions for (re-) architecting systems with ML components. Our results indicate, for example, that traditional software architecture challenges (e.g., component coupling) also play an important role when using ML components; along with new ML specific challenges (e.g., the need for continuous retraining). Moreover, the results indicate that ML heightened decision drivers, such as privacy, play a marginal role compared to traditional decision drivers, such as scalability. Using the survey we were able to establish a link between architectural solutions and software quality attributes, which enabled us to provide twenty architectural tactics used to satisfy individual quality requirements of systems with ML components. Altogether, the results of the study can be interpreted as an empirical framework that supports the process of (re-) architecting software systems with ML components.";https://ieeexplore.ieee.org/abstract/document/9825909/;1vdfWhfL2pwJ
Take, M., Alpers, S., Becker, C., Schreiber, C., & Oberweis, A. (2021). Software Design Patterns for AI-Systems. In EMISA (pp. 30-35).;1_ml_machine_data_learning;2021;Software Design Patterns for AI-Systems;Marius Take, Sascha Alpers, Christoph Becker, Clemens Schreiber, Andreas Oberweis;EMISA, 30-35, 2021;Well-established design patterns offer the possibility of standardized construction of software systems and can be used in various ways. The systematic use of design patterns in the field of Artificial Intelligence (AI) Systems however, has received little attention so far, despite AI being a popular research area in recent years. AI systems can be used for a wide variety of applications and play an increasingly important role in business and everyday life. AI systems are becoming more complex however, the actual machine learning (ML) task comprises only a small part of the total source code of a system. In order to maintain a clear and structured architecture for such systems and to allow easy maintenance, standardized elements should be reused in the design. This paper describes possible applications of well-known design patterns in AI systems to improve traceability of the system design.;https://www.researchgate.net/profile/Marius-Take/publication/354005672_Software_Design_Patterns_for_AI-Systems/links/611e5be71ca20f6f8633e7e4/Software-Design-Patterns-for-AI-Systems.pdf;fMpukxl1_6EJ
Washizaki, H., Khomh, F., Guéhéneuc, Y. G., Takeuchi, H., Natori, N., Doi, T., & Okuda, S. (2022). Software-engineering design patterns for machine learning applications. Computer, 55(3), 30-39.;1_ml_machine_data_learning;2022;Software-engineering design patterns for machine learning applications;Hironori Washizaki, Foutse Khomh, Yann-Gaël Guéhéneuc, Hironori Takeuchi, Naotake Natori, Takuo Doi, Satoshi Okuda;Computer 55 (3), 30-39, 2022;In this study, a multivocal literature review identified 15 software-engineering design patterns for machine learning applications. Findings suggest that there are opportunities to increase the patterns’ adoption in practice by raising awareness of such patterns within the community.;https://ieeexplore.ieee.org/abstract/document/9734272/;vevP7RagIc0J
Horkoff, J. (2019, September). Non-functional requirements for machine learning: Challenges and new directions. In 2019 IEEE 27th international requirements engineering conference (RE) (pp. 386-391). IEEE.;1_ml_machine_data_learning;2019;Non-functional requirements for machine learning: Challenges and new directions;Jennifer Horkoff;2019 IEEE 27th international requirements engineering conference (RE), 2019;"Machine Learning (ML) provides approaches which use big data to enable algorithms to ""learn"", producing outputs which would be difficult to obtain otherwise. Despite the advances allowed by ML, much recent attention has been paid to certain qualities of ML solutions, particularly fairness and transparency, but also qualities such as privacy, security, and testability. From a requirements engineering (RE) perspective, such qualities are also known as non-functional requirements (NFRs). In RE, the meaning of certain NFRs, how to refine those NFRs, and how to use NFRs for design and runtime decision making over traditional software is relatively well established and understood. However, in a context where the solution involves ML, much of our knowledge about NFRs no longer applies. First, the types of NFRs we are concerned with undergo a shift: NFRs like fairness and transparency become prominent, whereas other NFRs such as modularity may become less relevant. The meanings and interpretations of NFRs in an ML context (e.g., maintainability, interoperability, and usability) must be rethought, including how these qualities are decomposed into sub-qualities. Trade-offs between NFRs in an ML context must be re-examined. Beyond the changing landscape of NFRs, we can ask if our known approaches to understanding, formalizing, modeling, and reasoning over NFRs at design and runtime must also be adjusted, or can be applied as-is to this new area? Given these questions, this work outlines challenges and a proposed research agenda for the exploration of NFRs for ML-based solutions.";https://ieeexplore.ieee.org/abstract/document/8920538/;Todo
Bauer, E., & Kohavi, R. (1999). An empirical comparison of voting classification algorithms: Bagging, boosting, and variants. Machine learning, 36, 105-139.;1_ml_machine_data_learning;1999;An empirical comparison of voting classification algorithms: Bagging, boosting, and variants;Eric Bauer, Ron Kohavi;Machine learning 36, 105-139, 1999;Methods for voting classification algorithms, such as Bagging and AdaBoost, have been shown to be very successful in improving the accuracy of certain classifiers for artificial and real-world datasets. We review these algorithms and describe a large empirical study comparing several variants in conjunction with a decision tree inducer (three variants) and a Naive-Bayes inducer. The purpose of the study is to improve our understanding of why and when these algorithms, which use perturbation, reweighting, and combination techniques, affect classification error. We provide a bias and variance decomposition of the error to show how different methods and variants influence these two terms. This allowed us to determine that Bagging reduced variance of unstable methods, while boosting methods (AdaBoost and Arc-x4) reduced both the bias and variance of unstable methods but increased the variance for Naive-Bayes, which was very stable. We observed that Arc-x4 behaves differently than AdaBoost if reweighting is used instead of resampling, indicating a fundamental difference. Voting variants, some of which are introduced in this paper, include: pruning versus no pruning, use of probabilistic estimates, weight perturbations (Wagging), and backfitting of data. We found that Bagging improves when probabilistic estimates in conjunction with no-pruning are used, as well as when the data was backfit. We measure tree sizes and show an interesting positive correlation between the increase in the average tree size in AdaBoost trials and its success in reducing the error. We compare the mean-squared error of voting methods to non-voting methods and show that the voting methods lead to large and significant reductions in the mean-squared errors. Practical problems that arise in implementing boosting algorithms are explored, including numerical instabilities and underflows. We use scatterplots that graphically show how AdaBoost reweights instances, emphasizing not only “hard” areas but also outliers and noise.;https://link.springer.com/article/10.1023/A:1007515423169;6Xp11inGTvQJ
Chu, X., Ilyas, I. F., Krishnan, S., & Wang, J. (2016, June). Data cleaning: Overview and emerging challenges. In Proceedings of the 2016 international conference on management of data (pp. 2201-2206).;1_ml_machine_data_learning;2016;Data cleaning: Overview and emerging challenges;Xu Chu, Ihab F Ilyas, Sanjay Krishnan, Jiannan Wang;Proceedings of the 2016 international conference on management of data, 2201-2206, 2016;Detecting and repairing dirty data is one of the perennial challenges in data analytics, and failure to do so can result in inaccurate analytics and unreliable decisions. Over the past few years, there has been a surge of interest from both industry and academia on data cleaning problems including new abstractions, interfaces, approaches for scalability, and statistical techniques. To better understand the new advances in the field, we will first present a taxonomy of the data cleaning literature in which we highlight the recent interest in techniques that use constraints, rules, or patterns to detect errors, which we call qualitative data cleaning. We will describe the state-of-the-art techniques and also highlight their limitations with a series of illustrative examples. While traditionally such approaches are distinct from quantitative approaches such as outlier detection, we also discuss recent work that casts such approaches into a statistical estimation framework including: using Machine Learning to improve the efficiency and accuracy of data cleaning and considering the effects of data cleaning on statistical analysis.;https://dl.acm.org/doi/abs/10.1145/2882903.2912574;9eKr_scCTG4J
Crankshaw, D., Bailis, P., Gonzalez, J. E., Li, H., Zhang, Z., Franklin, M. J., ... & Jordan, M. I. (2014). The missing piece in complex analytics: Low latency, scalable model management and serving with velox. arXiv preprint arXiv:1409.3809.;1_ml_machine_data_learning;2014;The missing piece in complex analytics: Low latency, scalable model management and serving with velox;Daniel Crankshaw, Peter Bailis, Joseph E Gonzalez, Haoyuan Li, Zhao Zhang, Michael J Franklin, Ali Ghodsi, Michael I Jordan;arXiv preprint arXiv:1409.3809, 2014;"To support complex data-intensive applications such as personalized recommendations, targeted advertising, and intelligent services, the data management community has focused heavily on the design of systems to support training complex models on large datasets. Unfortunately, the design of these systems largely ignores a critical component of the overall analytics process: the deployment and serving of models at scale. In this work, we present Velox, a new component of the Berkeley Data Analytics Stack. Velox is a data management system for facilitating the next steps in real-world, large-scale analytics pipelines: online model management, maintenance, and serving. Velox provides end-user applications and services with a low-latency, intuitive interface to models, transforming the raw statistical models currently trained using existing offline large-scale compute frameworks into full-blown, end-to-end data products capable of recommending products, targeting advertisements, and personalizing web content. To provide up-to-date results for these complex models, Velox also facilitates lightweight online model maintenance and selection (i.e., dynamic weighting). In this paper, we describe the challenges and architectural considerations required to achieve this functionality, including the abilities to span online and offline systems, to adaptively adjust model materialization strategies, and to exploit inherent statistical properties such as model error tolerance, all while operating at ""Big Data"" scale.";https://arxiv.org/abs/1409.3809;kQzIFh951IgJ
Cuzzocrea, A., Song, I. Y., & Davis, K. C. (2011, October). Analytics over large-scale multidimensional data: the big data revolution!. In Proceedings of the ACM 14th international workshop on Data Warehousing and OLAP (pp. 101-104).;9_data_science_software_process;2011;Analytics over large-scale multidimensional data: the big data revolution!;Alfredo Cuzzocrea, Il-Yeol Song, Karen C Davis;Proceedings of the ACM 14th international workshop on Data Warehousing and OLAP, 101-104, 2011;In this paper, we provide an overview of state-of-the-art research issues and achievements in the field of analytics over big data, and we extend the discussion to analytics over big multidimensional data as well, by highlighting open problems and actual research trends. Our analytical contribution is finally completed by several novel research directions arising in this field, which plays a leading role in next-generation Data Warehousing and OLAP research.;https://dl.acm.org/doi/abs/10.1145/2064676.2064695;gnK8WoGNFjAJ
Tang, J., Alelyani, S., & Liu, H. (2014). Feature selection for classification: A review. Data classification: Algorithms and applications, 37.;1_ml_machine_data_learning;2014;Feature selection for classification: A review;Jiliang Tang, Salem Alelyani, Huan Liu;Data classification: Algorithms and applications, 37, 2014;Nowadays, the growth of the high-throughput technologies has resulted in exponential growth in the harvested data with respect to both dimensionality and sample size. The trend of this growth of the UCI machine learning repository is shown in Figure 1. Efficient and effective management of these data becomes increasing challenging. Traditionally manual management of these datasets to be impractical. Therefore, data mining and machine learning techniques were developed to automatically discover knowledge and recognize patterns from these data.However, these collected data is usually associated with a high level of noise. There are many reasons causing noise in these data, among which imperfection in the technologies that collected the data and the source of the data itself are two major reasons. For example, in the medical images domain, any deficiency in the imaging device will be reflected as noise for the later process. This kind of noise is caused by the device itself. The development of social media changes the role of online users from traditional content consumers to both content creators and consumers. The quality of social media data varies from excellent data to spam or abuse content by nature. Meanwhile, social media data is usually informally written and suffer from grammatical mistakes, misspelling, and improper punctuation. Undoubtedly, extracting useful knowledge and patterns from such huge and noisy data is a challenging task.;https://www.cvs.edu.in/upload/feature_selection_for_classification.pdf;dzFyVfJ6GSQJ
Dayal, U., Castellanos, M., Simitsis, A., & Wilkinson, K. (2009, March). Data integration flows for business intelligence. In Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology (pp. 1-11).;9_data_science_software_process;2009;Data integration flows for business intelligence;Umeshwar Dayal, Malu Castellanos, Alkis Simitsis, Kevin Wilkinson;Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology, 1-11, 2009;"Business Intelligence (BI) refers to technologies, tools, and practices for collecting, integrating, analyzing, and presenting large volumes of information to enable better decision making. Today's BI architecture typically consists of a data warehouse (or one or more data marts), which consolidates data from several operational databases, and serves a variety of front-end querying, reporting, and analytic tools. The back-end of the architecture is a data integration pipeline for populating the data warehouse by extracting data from distributed and usually heterogeneous operational sources; cleansing, integrating and transforming the data; and loading it into the data warehouse. Since BI systems have been used primarily for off-line, strategic decision making, the traditional data integration pipeline is a oneway, batch process, usually implemented by extract-transform-load (ETL) tools. The design and implementation of the ETL pipeline is largely a labor-intensive activity, and typically consumes a large fraction of the effort in data warehousing projects. Increasingly, as enterprises become more automated, data-driven, and real-time, the BI architecture is evolving to support operational decision making. This imposes additional requirements and tradeoffs, resulting in even more complexity in the design of data integration flows. These include reducing the latency so that near real-time data can be delivered to the data warehouse, extracting information from a wider variety of data sources, extending the rigidly serial ETL pipeline to more general data flows, and considering alternative physical implementations. We describe the requirements for data integration flows in this next generation of operational BI system, the limitations of current technologies, the research challenges in meeting these requirements, and a framework for addressing these challenges. The goal is to facilitate the design and implementation of optimal flows to meet business requirements.";https://dl.acm.org/doi/abs/10.1145/1516360.1516362;d_nijvrAM7kJ
Domingos, P. (2012). A few useful things to know about machine learning. Communications of the ACM, 55(10), 78-87.;1_ml_machine_data_learning;2012;A few useful things to know about machine learning;Pedro Domingos;Communications of the ACM 55 (10), 78-87, 2012;"Tapping into the ""folk knowledge"" needed to advance machine learning applications.";https://dl.acm.org/doi/pdf/10.1145/2347736.2347755;hbP9gXSyID0J
Hernández, M. A., & Stolfo, S. J. (1998). Real-world data is dirty: Data cleansing and the merge/purge problem. Data mining and knowledge discovery, 2, 9-37.;1_ml_machine_data_learning;1998;Real-world data is dirty: Data cleansing and the merge/purge problem;Mauricio A Hernández, Salvatore J Stolfo;Data mining and knowledge discovery 2, 9-37, 1998;The problem of merging multiple databases of information about common entities is frequently encountered in KDD and decision support applications in large commercial and government organizations. The problem we study is often called the Merge/Purge problem and is difficult to solve both in scale and accuracy. Large repositories of data typically have numerous duplicate information entries about the same entities that are difficult to cull together without an intelligent “equational theory” that identifies equivalent items by a complex, domain-dependent matching process. We have developed a system for accomplishing this Data Cleansing task and demonstrate its use for cleansing lists of names of potential customers in a direct marketing-type application. Our results for statistically generated data are shown to be accurate and effective when processing the data multiple times using different keys for sorting on each successive pass. Combing results of individual passes using transitive closure over the independent results, produces far more accurate results at lower cost. The system provides a rule programming module that is easy to program and quite good at finding duplicates especially in an environment with massive amounts of data. This paper details improvements in our system, and reports on the successful implementation for a real-world database that conclusively validates our results previously achieved for statistically generated data.;https://link.springer.com/article/10.1023/A:1009761603038;7IjuYUPYRUUJ
Jordan, M. I., & Mitchell, T. M. (2015). Machine learning: Trends, perspectives, and prospects. Science, 349(6245), 255-260.;1_ml_machine_data_learning;2015;Machine learning: Trends, perspectives, and prospects;Michael I Jordan, Tom M Mitchell;Science 349 (6245), 255-260, 2015;Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of todayâ€™s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.;https://www.science.org/doi/abs/10.1126/science.aaa8415;pdcI9r5sCJcJ
Khayyat, Z., Ilyas, I. F., Jindal, A., Madden, S., Ouzzani, M., Papotti, P., ... & Yin, S. (2015, May). Bigdansing: A system for big data cleansing. In Proceedings of the 2015 ACM SIGMOD international conference on management of data (pp. 1215-1230).;1_ml_machine_data_learning;2015;Bigdansing: A system for big data cleansing;Zuhair Khayyat, Ihab F Ilyas, Alekh Jindal, Samuel Madden, Mourad Ouzzani, Paolo Papotti, Jorge-Arnulfo QuianÃ©-Ruiz, Nan Tang, Si Yin;Proceedings of the 2015 ACM SIGMOD international conference on management of data, 1215-1230, 2015;Data cleansing approaches have usually focused on detecting and fixing errors with little attention to scaling to big datasets. This presents a serious impediment since data cleansing often involves costly computations such as enumerating pairs of tuples, handling inequality joins, and dealing with user-defined functions. In this paper, we present BigDansing, a Big Data Cleansing system to tackle efficiency, scalability, and ease-of-use issues in data cleansing. The system can run on top of most common general purpose data processing platforms, ranging from DBMSs to MapReduce-like frameworks. A user-friendly programming interface allows users to express data quality rules both declaratively and procedurally, with no requirement of being aware of the underlying distributed platform. BigDansing takes these rules into a series of transformations that enable distributed computations and several optimizations, such as shared scans and specialized joins operators. Experimental results on both synthetic and real datasets show that BigDansing outperforms existing baseline systems up to more than two orders of magnitude without sacrificing the quality provided by the repair algorithms.;https://dl.acm.org/doi/abs/10.1145/2723372.2747646;ofHBTLPcByoJ
Lin, J., & Kolcz, A. (2012, May). Large-scale machine learning at twitter. In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data (pp. 793-804).;1_ml_machine_data_learning;2012;Large-scale machine learning at twitter;Jimmy Lin, Alek Kolcz;Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, 793-804, 2012;"The success of data-driven solutions to difficult problems, along with the dropping costs of storing and processing massive amounts of data, has led to growing interest in large-scale machine learning. This paper presents a case study of Twitter's integration of machine learning tools into its existing Hadoop-based, Pig-centric analytics platform. We begin with an overview of this platform, which handles ""traditional"" data warehousing and business intelligence tasks for the organization. The core of this work lies in recent Pig extensions to provide predictive analytics capabilities that incorporate machine learning, focused specifically on supervised classification. In particular, we have identified stochastic gradient descent techniques for online learning and ensemble methods as being highly amenable to scaling out to large amounts of data. In our deployed solution, common machine learning tasks such as data sampling, feature generation, training, and testing can be accomplished directly in Pig, via carefully crafted loaders, storage functions, and user-defined functions. This means that machine learning is just another Pig script, which allows seamless integration with existing infrastructure for data management, scheduling, and monitoring in a production environment, as well as access to rich libraries of user-defined functions and the materialized output of other scripts.";https://dl.acm.org/doi/abs/10.1145/2213836.2213958;12jsGmaQgzcJ
Rajaram, S., Mishra, K., & O'mara, M. (2023). U.S. Patent No. 11,550,809. Washington, DC: U.S. Patent and Trademark Office.;7_edge_computing_deep_learning;2023;U.S;TODO;TODO;TODO;Todo;
Sparks, E. R., Venkataraman, S., Kaftan, T., Franklin, M. J., & Recht, B. (2017, April). Keystoneml: Optimizing pipelines for large-scale advanced analytics. In 2017 IEEE 33rd international conference on data engineering (ICDE) (pp. 535-546). IEEE.;1_ml_machine_data_learning;2017;Keystoneml: Optimizing pipelines for large-scale advanced analytics;Evan R Sparks, Shivaram Venkataraman, Tomer Kaftan, Michael J Franklin, Benjamin Recht;2017 IEEE 33rd international conference on data engineering (ICDE), 535-546, 2017;Modern advanced analytics applications make use of machine learning techniques and contain multiple steps of domain-specific and general-purpose processing with high resource requirements. We present KeystoneML, a system that captures and optimizes the end-to-end large-scale machine learning applications for high-throughput training in a distributed environment with a high-level API. This approach offers increased ease of use and higher performance over existing systems for large scale learning. We demonstrate the effectiveness of KeystoneML in achieving high quality statistical accuracy and scalable training using real world datasets in several domains.;https://ieeexplore.ieee.org/abstract/document/7930005/;IV3j6fd0jFQJ
Vassiliadis, P. (2009). A survey of extract–transform–load technology. International Journal of Data Warehousing and Mining (IJDWM), 5(3), 1-27.;9_data_science_software_process;2009;A survey of extract–transform–load technology;Panos Vassiliadis;International Journal of Data Warehousing and Mining (IJDWM) 5 (3), 1-27, 2009;The software processes that facilitate the original loading and the periodic refreshment of the data warehouse contents are commonly known as Extraction-Transformation-Loading (ETL) processes. The intention of this survey is to present the research work in the field of ETL technology in a structured way. To this end, we organize the coverage of the field as follows:(a) first, we cover the conceptual and logical modeling of ETL processes, along with some design methods,(b) we visit each stage of the ETL triplet, and examine problems that fall within each of these stages,(c) we discuss problems that pertain to the entirety of an ETL process, and,(d) we review some research prototypes of academic origin.;https://www.igi-global.com/article/survey-extract-transform-load-technology/3894;3IU_tDZVNYcJ
Vassiliadis, P., & Simitsis, A. (2009). Extraction, Transformation, and Loading. Encyclopedia of Database Systems, 10.;9_data_science_software_process;2009;Extraction, Transformation, and Loading;Panos Vassiliadis, Alkis Simitsis;Encyclopedia of Database Systems 10, 2009;DEFINITION Extraction, Transformation, and Loading (ETL) processes are responsible for the operations taking place in the back stage of a data warehouse architecture. In a high level description of an ETL process, first, the data are extracted from the source datastores, which can be in a relational and/or a semi-structured format. In typical cases, the source datastores can be On-Line Transactional Processing (OLTP) or legacy systems, files under any format, web pages, various kinds of documents (eg, spreadsheets and text documents) or even data coming in a streaming fashion. Typically, only the data that are different from the previous execution of an ETL process (newly inserted, updated, and deleted information) should be extracted from the sources. After this phase, the extracted data are propagated to a special-purpose area of the warehouse, called Data Staging Area (DSA), where their transformation, homogenization, and cleansing take place. The most frequently used transformations include filters and checks to ensure that the data propagated to the warehouse respect business rules and integrity constraints, as well as schema transformations that ensure that data fit the target data warehouse schema. Finally, the data are loaded to the central data warehouse (DW) and all its counterparts (eg, data marts and views). In a traditional data warehouse setting, the ETL process periodically refreshes the data warehouse during idle or low-load, periods of its operation (eg, every night) and has a specific time-window to complete. Nowadays, business necessities and demands require near real-time data warehouse refreshment and significant attention is drawn to this kind of technological advancement.HISTORICAL BACKGROUND Despite the fact that ETL took its name and separate existence during the first decade of the 21st century, ETL processes have been a companion to database technology for a lengthier period of timeâ€“in fact, from the beginning of its existence. During that period, ETL software was just silently hidden as a routine programming task without any particular name or individual importance. ETL was born on the first day that a programmer constructed a program that takes records from a certain persistent file and populates or enriches another file with this information. Since then, any kind of data processing software that reshapes or filters records, calculates new values, and populates another data store than the original one is a form of an ETL program. Apart from this low-profile programming task, research efforts have long hidden ETL tasks, although not much attention was paid to them.;https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=891ff75edc19889cbba9d90722104e2489d64902;gScQoJLf4ZwJ
Ahmed, M. S., Ishikawa, F., & Sugiyama, M. (2020, November). Testing machine learning code using polyhedral region. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 1533-1536).;10_testing_test_machine_metamorphic;2020;Testing machine learning code using polyhedral region;Md Sohel Ahmed, Fuyuki Ishikawa, Mahito Sugiyama;Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 1533-1536, 2020;To date, although machine learning has been successful in various practical applications, generic methods of testing machine learning code have not been established yet. Here we present a new approach to test machine learning code using the possible input region obtained as a polyhedron. If an ML system generates different output for multiple input in the polyhedron, it is ensured that there exists a bug in the code. This property is known as one of theoretical fundamentals in statistical inference, for example, sparse regression models such as the lasso, and a wide range of machine learning algorithms satisfy this polyhedral condition, to which our testing procedure can be applied. We empirically show that the existence of bugs in lasso code can be effectively detected by our method in the mutation testing framework.;https://dl.acm.org/doi/abs/10.1145/3368089.3417043;BayJ9SZLtVUJ
Alvarez‐Rodríguez, J. M., Zuñiga, R. M., Pelayo, V. M., & Llorens, J. (2019, July). Challenges and opportunities in the integration of the Systems Engineering process and the AI/ML model lifecycle. In INCOSE International Symposium (Vol. 29, No. 1, pp. 560-575).;1_ml_machine_data_learning;2019;Challenges and opportunities in the integration of the Systems Engineering process and the AI/ML model lifecycle;Jose María Alvarez-Rodríguez, Roy Mendieta Zuñiga, Valentín Moreno Pelayo, Juan Llorens;INCOSE International Symposium, 2019;The Digital Age, the “Society 5.0” or the “4th Industrial Revolution” has created a challenging and evolving environment in which more up‐to‐date, secure, safer, cost‐efficient and personalized products and services must be timely delivered. Furthermore, the growing interest in equipping systems with intelligence implies that the engineering process must be adapted to consider the specific characteristics of Artificial Intelligent (AI) and Machine Learning (ML) systems. From the AI/ML point of view, the possibility of following an engineering process must also imply an improvement to overcome their “hidden” technical debt. To pave the way to the development of the next generation of smart systems, a retrospective in the current engineering practice and, in the development of AI/ML systems, is presented. Afterwards, the main challenges to harmonize both disciplines are outlined to finally describe the main opportunities and expected impacts.;https://incose.onlinelibrary.wiley.com/doi/full/10.1002/j.2334-5837.2019.00621.x;TODO
Benton, W. C. (2020). Machine learning systems and intelligent applications. IEEE Software, 37(4), 43-49.;1_ml_machine_data_learning;2020;Machine learning systems and intelligent applications;William C Benton;IEEE Software 37 (4), 43-49, 2020;Machine learning techniques are useful in a wide range of contexts, but techniques alone are insufficient to solve real business problems. We introduce the intelligent applications concept, which characterizes the structure and responsibilities of contemporary machine learning systems.;https://ieeexplore.ieee.org/abstract/document/9052717/;jm6HgpH2YUIJ
Byun, T., & Rayadurgam, S. (2020, June). Manifold for machine learning assurance. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results (pp. 97-100).;1_ml_machine_data_learning;2020;Manifold for machine learning assurance;Taejoon Byun, Sanjai Rayadurgam;Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results, 97-100, 2020;The increasing use of machine-learning (ML) enabled systems in critical tasks fuels the quest for novel verification and validation techniques yet grounded in accepted system assurance principles. In traditional system development, model-based techniques have been widely adopted, where the central premise is that abstract models of the required system provide a sound basis for judging its implementation. We posit an analogous approach for ML systems using an ML technique that extracts from the high-dimensional training data implicitly describing the required system, a low-dimensional underlying structure---a manifold. It is then harnessed for a range of quality assurance tasks such as test adequacy measurement, test input generation, and runtime monitoring of the target ML system. The approach is built on variational autoencoder, an unsupervised method for learning a pair of mutually near-inverse functions between a given high-dimensional dataset and a low-dimensional representation. Preliminary experiments establish that the proposed manifold-based approach, for test adequacy drives diversity in test data, for test generation yields fault-revealing yet realistic test cases, and for run-time monitoring provides an independent means to assess trustability of the target system's output.;https://dl.acm.org/doi/abs/10.1145/3377816.3381734;rg6D4dMAIeoJ
Chakraborty, J., Majumder, S., Yu, Z., & Menzies, T. (2020, November). Fairway: a way to build fair ML software. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 654-665).;6_fairness_discrimination_bias_decision;2020;Fairway: a way to build fair ML software;Joymallya Chakraborty, Suvodeep Majumder, Zhe Yu, Tim Menzies;Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 654-665, 2020;"Machine learning software is increasingly being used to make decisions that affect people's lives. But sometimes, the core part of this software (the learned model), behaves in a biased manner that gives undue advantages to a specific group of people (where those groups are determined by sex, race, etc.). This ""algorithmic discrimination"" in the AI software systems has become a matter of serious concern in the machine learning and software engineering community. There have been works done to find ""algorithmic bias"" or ""ethical bias"" in the software system. Once the bias is detected in the AI software system, the mitigation of bias is extremely important. In this work, we a)explain how ground-truth bias in training data affects machine learning model fairness and how to find that bias in AI software,b)propose a method Fairway which combines pre-processing and in-processing approach to remove ethical bias from training data and trained model. Our results show that we can find bias and mitigate bias in a learned model, without much damaging the predictive performance of that model. We propose that (1) testing for bias and (2) bias mitigation should be a routine part of the machine learning software development life cycle. Fairway offers much support for these two purposes.";https://dl.acm.org/doi/abs/10.1145/3368089.3409697;4_Z4879Jds0J
Chakraborty, J., Peng, K., & Menzies, T. (2020, December). Making fair ML software using trustworthy explanation. In Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering (pp. 1229-1233).;6_fairness_discrimination_bias_decision;2020;Making fair ML software using trustworthy explanation;Joymallya Chakraborty, Kewen Peng, Tim Menzies;Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering, 1229-1233, 2020;Machine learning software is being used in many applications (finance, hiring, admissions, criminal justice) having huge social impact. But sometimes the behavior of this software is biased and it shows discrimination based on some sensitive attributes such as sex, race etc. Prior works concentrated on finding and mitigating bias in ML models. A recent trend is using instance-based model-agnostic explanation methods such as LIME[36] to find out bias in the model prediction. Our work concentrates on finding shortcomings of current bias measures and explanation methods. We show how our proposed method based on K nearest neighbors can overcome those shortcomings and find the underlying bias of black box models. Our results are more trustworthy and helpful for the practitioners. Finally, We describe our future framework combining explanation and planning to build fair software.;https://dl.acm.org/doi/abs/10.1145/3324884.3418932;cfxxTFRHAPIJ
Challa, H., Niu, N., & Johnson, R. (2020, September). Faulty requirements made valuable: On the role of data quality in deep learning. In 2020 IEEE Seventh International Workshop on Artificial Intelligence for Requirements Engineering (AIRE) (pp. 61-69). IEEE.;4_dl_testing_deep_network;2020;Faulty requirements made valuable: On the role of data quality in deep learning;Harshitha Challa, Nan Niu, Reese Johnson;2020 IEEE Seventh International Workshop on Artificial Intelligence for Requirements Engineering (AIRE), 61-69, 2020;Large collections of data help evolve deep learning into the state-of-the-art in solving many artificial intelligence problems. However, the requirements engineering (RE) community has yet to adapt to such sweeping changes caused exclusively by data. One reason is that the traditional requirements quality like unambiguity becomes less applicable to data, and so do requirements fault detection techniques like inspections. In this paper, we view deep learning as a class of machines whose effects must be evaluated with direct consideration of inherent data quality attributes: accuracy, consistency, currentness, etc. We substantiate this view by altering stationarity of the multivariate time-series data, and by further analyzing how the stationarity changes affect the behavior of a recurrent neural network in the context of predicting combined sewer overflow. Our work sheds light on the active role RE plays in deep learning.;https://ieeexplore.ieee.org/abstract/document/9233033/;Ec1kc0rzPaUJ
Chechik, M., Salay, R., Viger, T., Kokaly, S., & Rahimi, M. (2019). Software assurance in an uncertain world. In Fundamental Approaches to Software Engineering: 22nd International Conference, FASE 2019, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2019, Prague, Czech Republic, April 6–11, 2019, Proceedings 22 (pp. 3-21). Springer International Publishing.;2_safety_system_autonomous_vehicle;2019;Software assurance in an uncertain world;Marsha Chechik, Rick Salay, Torin Viger, Sahar Kokaly, Mona Rahimi;Fundamental Approaches to Software Engineering: 22nd International Conference, FASE 2019, Held as Part of the European Joint Conferences on Theory and Practice of Software …, 2019;"From financial services platforms to social networks to vehicle control, software has come to mediate many activities of daily life. Governing bodies and standards organizations have responded to this trend by creating regulations and standards to address issues such as safety, security and privacy. In this environment, the compliance of software development to standards and regulations has emerged as a key requirement. Compliance claims and arguments are often captured in assurance cases, with linked evidence of compliance. Evidence can come from testcases, verification proofs, human judgment, or a combination of these. That is, experts try to build (safety-critical) systems carefully according to well justified methods and articulate these justifications in an assurance case that is ultimately judged by a human. Yet software is deeply rooted in uncertainty; most complex open-world functionality (eg, perception of the state of the world by a self-driving vehicle), is either not completely specifiable or it is not cost-effective to do so; software systems are often to be placed into uncertain environments, and there can be uncertainties that need to be We argue that the role of assurance cases is to be the grand unifier for software development, focusing on capturing and managing uncertainty. We discuss three approaches for arguing about safety and security of software under uncertainty, in the absence of fully sound and complete methods: assurance argument rigor, semantic evidence composition and applicability to new kinds of systems, specifically those relying on ML.";https://library.oapen.org/bitstream/handle/20.500.12657/23333/1006822.pdf?sequence=1#page=16;TLfNlKeX3XgJ
Chen, Y., Wang, Z., Wang, D., Fang, C., & Chen, Z. (2019, April). Variable strength combinatorial testing for deep neural networks. In 2019 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW) (pp. 281-284). IEEE.;4_dl_testing_deep_network;2019;Variable strength combinatorial testing for deep neural networks;Yanshan Chen, Ziyuan Wang, Dong Wang, Chunrong Fang, Zhenyu Chen;2019 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW), 281-284, 2019;In deep neural networks (DNNs), each neuron in the post-layer receives the influence of all the neurons in the pre-layer. As we known, different connections in a DNN model have different weights. It means that, different combinations of pre-layer neurons have different effects on the post-layer neurons. Therefore, the variable strength combinatorial testing can reflect the effect of combination interaction of neurons in the pre-layer on the neurons in the post-layer. In this paper, we propose to adopt variable strength combinatorial testing technique on DNNs testing. In order to modeling the effect of combinatorial interaction of pre-layer neurons on the post-layer neurons, we propose three methods to construct variable strength combinatorial interaction relationship for DNNs. The experimental results show that, 1) variable strength combinatorial coverage criteria are discriminating to measure the adequacy of DNNs testing, and 2) there is correlation between variable strength combinatorial coverage and adversarial detection.;https://ieeexplore.ieee.org/abstract/document/8728959/;J9x4cP-Q8iUJ
Chen, J., Wu, Z., Wang, Z., You, H., Zhang, L., & Yan, M. (2020). Practical accuracy estimation for efficient deep neural network testing. ACM Transactions on Software Engineering and Methodology (TOSEM), 29(4), 1-35.;4_dl_testing_deep_network;2020;Practical accuracy estimation for efficient deep neural network testing;Junjie Chen, Zhuo Wu, Zan Wang, Hanmo You, Lingming Zhang, Ming Yan;ACM Transactions on Software Engineering and Methodology (TOSEM) 29 (4), 1-35, 2020;Deep neural network (DNN) has become increasingly popular and DNN testing is very critical to guarantee the correctness of DNN, i.e., the accuracy of DNN in this work. However, DNN testing suffers from a serious efficiency problem, i.e., it is costly to label each test input to know the DNN accuracy for the testing set, since labeling each test input involves multiple persons (even with domain-specific knowledge) in a manual way and the testing set is large-scale. To relieve this problem, we propose a novel and practical approach, called PACE (which is short for Practical ACcuracy Estimation), which selects a small set of test inputs that can precisely estimate the accuracy of the whole testing set. In this way, the labeling costs can be largely reduced by just labeling this small set of selected test inputs. Besides achieving a precise accuracy estimation, to make PACE more practical it is also required that it is interpretable, deterministic, and as efficient as possible. Therefore, PACE first incorporates clustering to interpretably divide test inputs with different testing capabilities (i.e., testing different functionalities of a DNN model) into different groups. Then, PACE utilizes the MMD-critic algorithm, a state-of-the-art example-based explanation algorithm, to select prototypes (i.e., the most representative test inputs) from each group, according to the group sizes, which can reduce the impact of noise due to clustering. Meanwhile, PACE also borrows the idea of adaptive random testing to select test inputs from the minority space (i.e., the test inputs that are not clustered into any group) to achieve great diversity under the required number of test inputs. The two parallel selection processes (i.e., selection from both groups and the minority space) compose the final small set of selected test inputs. We conducted an extensive study to evaluate the performance of PACE based on a comprehensive benchmark (i.e., 24 pairs of DNN models and testing sets) by considering different types of models (i.e., classification and regression models, high-accuracy and low-accuracy models, and CNN and RNN models) and different types of test inputs (i.e., original, mutated, and automatically generated test inputs). The results demonstrate that PACE is able to precisely estimate the accuracy of the whole testing set with only 1.181%âˆ¼2.302% deviations, on average, significantly outperforming the state-of-the-art approaches.;https://dl.acm.org/doi/abs/10.1145/3394112;M2ZU_KfAg9IJ
Cheng, D., Cao, C., Xu, C., & Ma, X. (2018, July). Manifesting bugs in machine learning code: An explorative study with mutation testing. In 2018 IEEE International Conference on Software Quality, Reliability and Security (QRS) (pp. 313-324). IEEE.;10_testing_test_machine_metamorphic;2018;Manifesting bugs in machine learning code: An explorative study with mutation testing;Dawei Cheng, Chun Cao, Chang Xu, Xiaoxing Ma;2018 IEEE International Conference on Software Quality, Reliability and Security (QRS), 313-324, 2018;Nowadays statistical machine learning is widely adopted in various domains such as data mining, image recognition and automated driving. However, software quality assurance for machine learning is still in its infancy. While recent efforts have been put into improving the quality of training data and trained models, this paper focuses on code-level bugs in the implementations of machine learning algorithms. In this explorative study we simulated program bugs by mutating Weka implementations of several classification algorithms. We observed that 8%-40% of the logically non-equivalent executable mutants were statistically indistinguishable from their golden versions. Moreover, other 15%-36% of the mutants were stubborn, as they performed not significantly worse than a reference classifier on at least one natural data set. We also experimented with several approaches to killing those stubborn mutants. Preliminary results indicate that bugs in machine learning code may have negative impacts on statistical properties such as robustness and learning curves, but they could be very difficult to detect, due to the lack of effective oracles.;https://ieeexplore.ieee.org/abstract/document/8424982/;yC-jkQWEveMJ
Du, X., Li, Y., Xie, X., Ma, L., Liu, Y., & Zhao, J. (2020, December). Marble: Model-based robustness analysis of stateful deep learning systems. In Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering (pp. 423-435).;4_dl_testing_deep_network;2020;Marble: Model-based robustness analysis of stateful deep learning systems;Xiaoning Du, Yi Li, Xiaofei Xie, Lei Ma, Yang Liu, Jianjun Zhao;Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering, 423-435, 2020;State-of-the-art deep learning (DL) systems are vulnerable to adversarial examples, which hinders their potential adoption in safety-and security-critical scenarios. While some recent progress has been made in analyzing the robustness of feed-forward neural networks, the robustness analysis for stateful DL systems, such as recurrent neural networks (RNNs), still remains largely uncharted. In this paper, we propose Marble, a model-based approach for quantitative robustness analysis of real-world RNN-based DL systems. Marble builds a probabilistic model to compactly characterize the robustness of RNNs through abstraction. Furthermore, we propose an iterative refinement algorithm to derive a precise abstraction, which enables accurate quantification of the robustness measurement. We evaluate the effectiveness of Marble on both LSTM and GRU models trained separately with three popular natural language datasets. The results demonstrate that (1) our refinement algorithm is more efficient in deriving an accurate abstraction than the random strategy, and (2) Marble enables quantitative robustness analysis, in rendering better efficiency, accuracy, and scalability than the state-of-the-art techniques.;https://dl.acm.org/doi/abs/10.1145/3324884.3416564;SDiusOKTUWsJ
Dutta, S., Shi, A., Choudhary, R., Zhang, Z., Jain, A., & Misailovic, S. (2020, July). Detecting flaky tests in probabilistic and machine learning applications. In Proceedings of the 29th ACM SIGSOFT international symposium on software testing and analysis (pp. 211-224).;10_testing_test_machine_metamorphic;2020;Detecting flaky tests in probabilistic and machine learning applications;Saikat Dutta, August Shi, Rutvik Choudhary, Zhekun Zhang, Aryaman Jain, Sasa Misailovic;Proceedings of the 29th ACM SIGSOFT international symposium on software testing and analysis, 211-224, 2020;Probabilistic programming systems and machine learning frameworks like Pyro, PyMC3, TensorFlow, and PyTorch provide scalable and efficient primitives for inference and training. However, such operations are non-deterministic. Hence, it is challenging for developers to write tests for applications that depend on such frameworks, often resulting in flaky tests â€“ tests which fail non-deterministically when run on the same version of code.In this paper, we conduct the first extensive study of flaky tests in this domain. In particular, we study the projects that depend on four frameworks: Pyro, PyMC3, TensorFlow-Probability, and PyTorch. We identify 75 bug reports/commits that deal with flaky tests, and we categorize the common causes and fixes for them. This study provides developers with useful insights on dealing with flaky tests in this domain.Motivated by our study, we develop a technique, FLASH, to systematically detect flaky tests due to assertions passing and failing in different runs on the same code. These assertions fail due to differences in the sequence of random numbers in different runs of the same test. FLASH exposes such failures, and our evaluation on 20 projects results in 11 previously-unknown flaky tests that we reported to developers.;https://dl.acm.org/doi/abs/10.1145/3395363.3397366;t1XvGwtKGx8J
Dwarakanath, A., Ahuja, M., Sikand, S., Rao, R. M., Bose, R. J. C., Dubash, N., & Podder, S. (2018, July). Identifying implementation bugs in machine learning based image classifiers using metamorphic testing. In Proceedings of the 27th ACM SIGSOFT international symposium on software testing and analysis (pp. 118-128).;10_testing_test_machine_metamorphic;2018;Identifying implementation bugs in machine learning based image classifiers using metamorphic testing;Anurag Dwarakanath, Manish Ahuja, Samarth Sikand, Raghotham M Rao, RP Jagadeesh Chandra Bose, Neville Dubash, Sanjay Podder;Proceedings of the 27th ACM SIGSOFT international symposium on software testing and analysis, 118-128, 2018;We have recently witnessed tremendous success of Machine Learning (ML) in practical applications. Computer vision, speech recognition and language translation have all seen a near human level performance. We expect, in the near future, most business applications will have some form of ML. However, testing such applications is extremely challenging and would be very expensive if we follow today's methodologies. In this work, we present an articulation of the challenges in testing ML based applications. We then present our solution approach, based on the concept of Metamorphic Testing, which aims to identify implementation bugs in ML based image classifiers. We have developed metamorphic relations for an application based on Support Vector Machine and a Deep Learning based application. Empirical validation showed that our approach was able to catch 71% of the implementation bugs in the ML applications.;https://dl.acm.org/doi/abs/10.1145/3213846.3213858;s8KucO3wtN0J
Eniser, H. F., Gerasimou, S., & Sen, A. (2019, April). Deepfault: Fault localization for deep neural networks. In International Conference on Fundamental Approaches to Software Engineering (pp. 171-191). Cham: Springer International Publishing.;4_dl_testing_deep_network;2019;Deepfault: Fault localization for deep neural networks;Hasan Ferit Eniser, Simos Gerasimou, Alper Sen;International Conference on Fundamental Approaches to Software Engineering, 171-191, 2019;Deep Neural Networks (DNNs) are increasingly deployed in safety-critical applications including autonomous vehicles and medical diagnostics. To reduce the residual risk for unexpected DNN behaviour and provide evidence for their trustworthy operation, DNNs should be thoroughly tested. The DeepFault whitebox DNN testing approach presented in our paper addresses this challenge by employing suspiciousness measures inspired by fault localization to establish the hit spectrum of neurons and identify suspicious neurons whose weights have not been calibrated correctly and thus are considered responsible for inadequate DNN performance. DeepFault also uses a suspiciousness-guided algorithm to synthesize new inputs, from correctly classified inputs, that increase the activation values of suspicious neurons. Our empirical evaluation on several DNN instances trained on MNIST and CIFAR-10 datasets shows that DeepFault is effective in identifying suspicious neurons. Also, the inputs synthesized by DeepFault closely resemble the original inputs, exercise the identified suspicious neurons and are highly adversarial.;https://link.springer.com/chapter/10.1007/978-3-030-16722-6_10;mSFQqkCEozIJ
Feng, Y., Shi, Q., Gao, X., Wan, J., Fang, C., & Chen, Z. (2020, July). Deepgini: prioritizing massive tests to enhance the robustness of deep neural networks. In Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis (pp. 177-188).;4_dl_testing_deep_network;2020;Deepgini: prioritizing massive tests to enhance the robustness of deep neural networks;Yang Feng, Qingkai Shi, Xinyu Gao, Jun Wan, Chunrong Fang, Zhenyu Chen;Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis, 177-188, 2020;Deep neural networks (DNN) have been deployed in many software systems to assist in various classification tasks. In company with the fantastic effectiveness in classification, DNNs could also exhibit incorrect behaviors and result in accidents and losses. Therefore, testing techniques that can detect incorrect DNN behaviors and improve DNN quality are extremely necessary and critical. However, the testing oracle, which defines the correct output for a given input, is often not available in the automated testing. To obtain the oracle information, the testing tasks of DNN-based systems usually require expensive human efforts to label the testing data, which significantly slows down the process of quality assurance.To mitigate this problem, we propose DeepGini, a test prioritization technique designed based on a statistical perspective of DNN. Such a statistical perspective allows us to reduce the problem of measuring misclassification probability to the problem of measuring set impurity, which allows us to quickly identify possibly-misclassified tests. To evaluate, we conduct an extensive empirical study on popular datasets and prevalent DNN models. The experimental results demonstrate that DeepGini outperforms existing coverage-based techniques in prioritizing tests regarding both effectiveness and efficiency. Meanwhile, we observe that the tests prioritized at the front by DeepGini are more effective in improving the DNN quality in comparison with the coverage-based techniques.;https://dl.acm.org/doi/abs/10.1145/3395363.3397357;HjBLZMyqXgUJ
Fredriksson, T., Mattos, D. I., Bosch, J., & Olsson, H. H. (2020, November). Data labeling: An empirical investigation into industrial challenges and mitigation strategies. In International Conference on Product-Focused Software Process Improvement (pp. 202-216). Cham: Springer International Publishing.;1_ml_machine_data_learning;2020;Data labeling: An empirical investigation into industrial challenges and mitigation strategies;Teodor Fredriksson, David Issa Mattos, Jan Bosch, Helena HolmstrÃ¶m Olsson;International Conference on Product-Focused Software Process Improvement, 202-216, 2020;Labeling is a cornerstone of supervised machine learning. However, in industrial applications, data is often not labeled, which complicates using this data for machine learning. Although there are well-established labeling techniques such as crowdsourcing, active learning, and semi-supervised learning, these still do not provide accurate and reliable labels for every machine learning use case in the industry. In this context, the industry still relies heavily on manually annotating and labeling their data. This study investigates the challenges that companies experience when annotating and labeling their data. We performed a case study using a semi-structured interview with data scientists at two companies to explore their problems when labeling and annotating their data. This paper provides two contributions. We identify industry challenges in the labeling process, and then we propose mitigation strategies for these challenges.;https://link.springer.com/chapter/10.1007/978-3-030-64148-1_13;MzdOiumKGC8J
Fujii, G., Hamada, K., Ishikawa, F., Masuda, S., Matsuya, M., Myojin, T., ... & Ujita, Y. (2020). Guidelines for quality assurance of machine learning-based artificial intelligence. International journal of software engineering and knowledge engineering, 30(11n12), 1589-1606.;1_ml_machine_data_learning;2020;Guidelines for quality assurance of machine learning-based artificial intelligence;Gaku Fujii, Koichi Hamada, Fuyuki Ishikawa, Satoshi Masuda, Mineo Matsuya, Tomoyuki Myojin, Yasuharu Nishi, Hideto Ogawa, Takahiro Toku, Susumu Tokumoto, Kazunori Tsuchiya, Yasuhiro Ujita;International journal of software engineering and knowledge engineering 30 (11n12), 1589-1606, 2020;Significant effort is being put into developing industrial applications for artificial intelligence (AI), especially those using machine learning (ML) techniques. Despite the intensive support for building ML applications, there are still challenges when it comes to evaluating, assuring, and improving the quality or dependability. The difficulty stems from the unique nature of ML, namely, system behavior is derived from training data not from logical design by human engineers. This leads to black-box and intrinsically imperfect implementations that invalidate many principles and techniques in traditional software engineering. In light of this situation, the Japanese industry has jointly worked on a set of guidelines for the quality assurance of AI systems (in the Consortium of Quality Assurance for AI-based Products and Services) from the viewpoint of traditional quality-assurance engineers and test engineers. We report on the second version of these guidelines, which cover a list of quality evaluation aspects, catalogue of current state-of-the-art techniques, and domain-specific discussions in five representative domains. The guidelines provide significant insights for engineers in terms of methodologies and designs for tests driven by application-specific requirements.;https://www.worldscientific.com/doi/abs/10.1142/S0218194020400227;YfbPucbYV6UJ
Gambi, A., Huynh, T., & Fraser, G. (2019, August). Generating effective test cases for self-driving cars from police reports. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 257-267).;2_safety_system_autonomous_vehicle;2019;Generating effective test cases for self-driving cars from police reports;Alessio Gambi, Tri Huynh, Gordon Fraser;Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 257-267, 2019;"Autonomous driving carries the promise to drastically reduce the number of car accidents; however, recently reported fatal crashes involving self-driving cars show that such an important goal is not yet achieved. This calls for better testing of the software controlling self-driving cars, which is difficult because it requires producing challenging driving scenarios. To better test self-driving car soft- ware, we propose to specifically test car crash scenarios, which are critical par excellence. Since real car crashes are difficult to test in field operation, we recreate them as physically accurate simulations in an environment that can be used for testing self-driving car software. To cope with the scarcity of sensory data collected during real car crashes which does not enable a full reproduction, we extract the information to recreate real car crashes from the police reports which document them. Our extensive evaluation, consisting of a user study involving 34 participants and a quantitative analysis of the quality of the generated tests, shows that we can generate accurate simulations of car crashes in a matter of minutes. Compared to tests which implement non critical driving scenarios, our tests effectively stressed the test subject in different ways and exposed several shortcomings in its implementation.";https://dl.acm.org/doi/abs/10.1145/3338906.3338942;Vn8ZKU7m5KwJ
Gao, Y., Liu, Y., Zhang, H., Li, Z., Zhu, Y., Lin, H., & Yang, M. (2020, November). Estimating gpu memory consumption of deep learning models. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 1342-1352).;1_ml_machine_data_learning;2020;Estimating gpu memory consumption of deep learning models;Yanjie Gao, Yu Liu, Hongyu Zhang, Zhengxian Li, Yonghao Zhu, Haoxiang Lin, Mao Yang;Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 1342-1352, 2020;Deep learning (DL) has been increasingly adopted by a variety of software-intensive systems. Developers mainly use GPUs to accelerate the training, testing, and deployment of DL models. However, the GPU memory consumed by a DL model is often unknown to them before the DL job executes. Therefore, an improper choice of neural architecture or hyperparameters can cause such a job to run out of the limited GPU memory and fail. Our recent empirical study has found that many DL job failures are due to the exhaustion of GPU memory. This leads to a horrendous waste of computing resources and a significant reduction in development productivity. In this paper, we propose DNNMem, an accurate estimation tool for GPU memory consumption of DL models. DNNMem employs an analytic estimation approach to systematically calculate the memory consumption of both the computation graph and the DL framework runtime. We have evaluated DNNMem on 5 real-world representative models with different hyperparameters under 3 mainstream frameworks (TensorFlow, PyTorch, and MXNet). Our extensive experiments show that DNNMem is effective in estimating GPU memory consumption.;https://dl.acm.org/doi/abs/10.1145/3368089.3417050;Q3gqDOLjWM4J
Gao, X., Saha, R. K., Prasad, M. R., & Roychoudhury, A. (2020, June). Fuzz testing based data augmentation to improve robustness of deep neural networks. In Proceedings of the acm/ieee 42nd international conference on software engineering (pp. 1147-1158).;4_dl_testing_deep_network;2020;Fuzz testing based data augmentation to improve robustness of deep neural networks;Xiang Gao, Ripon K Saha, Mukul R Prasad, Abhik Roychoudhury;Proceedings of the acm/ieee 42nd international conference on software engineering, 1147-1158, 2020;Deep neural networks (DNN) have been shown to be notoriously brittle to small perturbations in their input data. This problem is analogous to the over-fitting problem in test-based program synthesis and automatic program repair, which is a consequence of the incomplete specification, i.e., the limited tests or training examples, that the program synthesis or repair algorithm has to learn from. Recently, test generation techniques have been successfully employed to augment existing specifications of intended program behavior, to improve the generalizability of program synthesis and repair. Inspired by these approaches, in this paper, we propose a technique that re-purposes software testing methods, specifically mutation-based fuzzing, to augment the training data of DNNs, with the objective of enhancing their robustness. Our technique casts the DNN data augmentation problem as an optimization problem. It uses genetic search to generate the most suitable variant of an input data to use for training the DNN, while simultaneously identifying opportunities to accelerate training by skipping augmentation in many instances. We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets. Our evaluation shows that Sensei can improve the robust accuracy of the DNN, compared to the state of the art, on each of the 15 models, by upto 11.9% and 5.5% on average. Further, Sensei-SA can reduce the average DNN training time by 25%, while still improving robust accuracy.;https://dl.acm.org/doi/abs/10.1145/3377811.3380415;jKYCaiUkUEgJ
Garcia, J., Feng, Y., Shen, J., Almanee, S., Xia, Y., & Chen, A. Q. A. (2020, June). A comprehensive study of autonomous vehicle bugs. In Proceedings of the ACM/IEEE 42nd international conference on software engineering (pp. 385-396).;10_testing_test_machine_metamorphic;2020;A comprehensive study of autonomous vehicle bugs;Joshua Garcia, Yang Feng, Junjie Shen, Sumaya Almanee, Yuan Xia, and Qi Alfred Chen;Proceedings of the ACM/IEEE 42nd international conference on software engineering, 385-396, 2020;"Self-driving cars, or Autonomous Vehicles (AVs), are increasingly becoming an integral part of our daily life. About 50 corporations are actively working on AVs, including large companies such as Google, Ford, and Intel. Some AVs are already operating on public roads, with at least one unfortunate fatality recently on record. As a result, understanding bugs in AVs is critical for ensuring their security, safety, robustness, and correctness. While previous studies have focused on a variety of domains (e.g., numerical software; machine learning; and error-handling, concurrency, and performance bugs) to investigate bug characteristics, AVs have not been studied in a similar manner. Recently, two software systems for AVs, Baidu Apollo and Autoware, have emerged as frontrunners in the open-source community and have been used by large companies and governments (e.g., Lincoln, Volvo, Ford, Intel, Hitachi, LG, and the US Department of Transportation). From these two leading AV software systems, this paper describes our investigation of 16,851 commits and 499 AV bugs and introduces our classification of those bugs into 13 root causes, 20 bug symptoms, and 18 categories of software components those bugs often affect. We identify 16 major findings from our study and draw broader lessons from them to guide the research community towards future directions in software bug detection, localization, and repair.";https://dl.acm.org/doi/abs/10.1145/3377811.3380397;Smzo3Tk2sz0J
Ghamizi, S., Cordy, M., Papadakis, M., & Traon, Y. L. (2020, June). FeatureNET: diversity-driven generation of deep learning models. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings (pp. 41-44).;4_dl_testing_deep_network;2020;FeatureNET: diversity-driven generation of deep learning models;Salah Ghamizi, Maxime Cordy, Mike Papadakis, Yves Le Traon;Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings, 41-44, 2020;We present FeatureNET, an open-source Neural Architecture Search (NAS) tool1 that generates diverse sets of Deep Learning (DL) models. FeatureNET relies on a meta-model of deep neural networks, consisting of generic configurable entities. Then, it uses tools developed in the context of software product lines to generate diverse (maximize the differences between the generated) DL models. The models are translated to Keras and can be integrated into typical machine learning pipelines. FeatureNET allows researchers to generate seamlessly a large variety of models. Thereby, it helps choosing appropriate DL models and performing experiments with diverse models (mitigating potential threats to validity). As a NAS method, FeatureNET successfully generates models performing equally well with handcrafted models.;https://dl.acm.org/doi/abs/10.1145/3377812.3382153;QYWjtbBjvJ4J
Groce, A., Kulesza, T., Zhang, C., Shamasunder, S., Burnett, M., Wong, W. K., ... & McIntosh, K. (2013). You are the only possible oracle: Effective test selection for end users of interactive machine learning systems. IEEE Transactions on Software Engineering, 40(3), 307-323.;10_testing_test_machine_metamorphic;2013;You are the only possible oracle: Effective test selection for end users of interactive machine learning systems;Alex Groce, Todd Kulesza, Chaoqiang Zhang, Shalini Shamasunder, Margaret Burnett, Weng-Keen Wong, Simone Stumpf, Shubhomoy Das, Amber Shinsel, Forrest Bice, Kevin McIntosh;IEEE Transactions on Software Engineering 40 (3), 307-323, 2013;How do you test a program when only a single user, with no expertise in software testing, is able to determine if the program is performing correctly? Such programs are common today in the form of machine-learned classifiers. We consider the problem of testing this common kind of machine-generated program when the only oracle is an end user: e.g., only you can determine if your email is properly filed. We present test selection methods that provide very good failure rates even for small test suites, and show that these methods work in both large-scale random experiments using a “gold standard” and in studies with real users. Our methods are inexpensive and largely algorithm-independent. Key to our methods is an exploitation of properties of classifiers that is not possible in traditional software testing. Our results suggest that it is plausible for time-pressured end users to interactively detect failures-even very hard-to-find failures-without wading through a large number of successful (and thus less useful) tests. We additionally show that some methods are able to find the arguably most difficult-to-detect faults of classifiers: cases where machine learning algorithms have high confidence in an incorrect result.;https://ieeexplore.ieee.org/abstract/document/6682887/;oMK1GTsomokJ
Guo, J., Jiang, Y., Zhao, Y., Chen, Q., & Sun, J. (2018, October). Dlfuzz: Differential fuzzing testing of deep learning systems. In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 739-743).;4_dl_testing_deep_network;2018;Dlfuzz: Differential fuzzing testing of deep learning systems;Jianmin Guo, Yu Jiang, Yue Zhao, Quan Chen, Jiaguang Sun;Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 739-743, 2018;Deep learning (DL) systems are increasingly applied to safety-critical domains such as autonomous driving cars. It is of significant importance to ensure the reliability and robustness of DL systems. Existing testing methodologies always fail to include rare inputs in the testing dataset and exhibit low neuron coverage. In this paper, we propose DLFuzz, the first differential fuzzing testing framework to guide DL systems exposing incorrect behaviors. DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality. We present empirical evaluations on two well-known datasets to demonstrate its efficiency. Compared with DeepXplore, the state-of-the-art DL whitebox testing framework, DLFuzz does not require extra efforts to find similar functional DL systems for cross-referencing check, but could generate 338.59% more adversarial inputs with 89.82% smaller perturbations, averagely obtain 2.86% higher neuron coverage, and save 20.11% time consumption.;https://dl.acm.org/doi/abs/10.1145/3236024.3264835;_bwSObNA6b0J
Guo, Q., Xie, X., Li, Y., Zhang, X., Liu, Y., Li, X., & Shen, C. (2020, December). Audee: Automated testing for deep learning frameworks. In Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering (pp. 486-498).;4_dl_testing_deep_network;2020;Audee: Automated testing for deep learning frameworks;Qianyu Guo, Xiaofei Xie, Yi Li, Xiaoyu Zhang, Yang Liu, Xiaohong Li, Chao Shen;Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering, 486-498, 2020;Deep learning (DL) has been applied widely, and the quality of DL system becomes crucial, especially for safety-critical applications. Existing work mainly focuses on the quality analysis of DL models, but lacks attention to the underlying frameworks on which all DL models depend. In this work, we propose Audee, a novel approach for testing DL frameworks and localizing bugs. Audee adopts a search-based approach and implements three different mutation strategies to generate diverse test cases by exploring combinations of model structures, parameters, weights and inputs. Audee is able to detect three types of bugs: logical bugs, crashes and Not-a-Number (NaN) errors. In particular, for logical bugs, Audee adopts a cross-reference check to detect behavioural inconsistencies across multiple frameworks (e.g., TensorFlow and PyTorch), which may indicate potential bugs in their implementations. For NaN errors, Audee adopts a heuristic-based approach to generate DNNs that tend to output outliers (i.e., too large or small values), and these values are likely to produce NaN. Furthermore, Audee leverages a causal-testing based technique to localize layers as well as parameters that cause inconsistencies or bugs. To evaluate the effectiveness of our approach, we applied Audee on testing four DL frameworks, i.e., TensorFlow, PyTorch, CNTK, and Theano. We generate a large number of DNNs which cover 25 widely-used APIs in the four frameworks. The results demonstrate that Audee is effective in detecting inconsistencies, crashes and NaN errors. In total, 26 unique unknown bugs were discovered, and 7 of them have already been confirmed or fixed by the developers.;https://dl.acm.org/doi/abs/10.1145/3324884.3416571;kE1fGO2ojT4J
Han, J., Deng, S., Lo, D., Zhi, C., Yin, J., & Xia, X. (2020, September). An empirical study of the dependency networks of deep learning libraries. In 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME) (pp. 868-878). IEEE.;4_dl_testing_deep_network;2020;An empirical study of the dependency networks of deep learning libraries;Junxiao Han, Shuiguang Deng, David Lo, Chen Zhi, Jianwei Yin, Xin Xia;2020 IEEE International Conference on Software Maintenance and Evolution (ICSME), 868-878, 2020;Deep Learning techniques have been prevalent in various domains, and more and more open source projects in GitHub rely on deep learning libraries to implement their algorithms. To that end, they should always keep pace with the latest versions of deep learning libraries to make the best use of deep learning libraries. Aptly managing the versions of deep learning libraries can help projects avoid crashes or security issues caused by deep learning libraries. Unfortunately, very few studies have been done on the dependency networks of deep learning libraries. In this paper, we take the first step to perform an exploratory study on the dependency networks of deep learning libraries, namely, Tensorflow, PyTorch, and Theano. We study the project purposes, application domains, dependency degrees, update behaviors and reasons as well as version distributions of deep learning projects that depend on Tensorflow, PyTorch, and Theano. Our study unveils some commonalities in various aspects (e.g., purposes, application domains, dependency degrees) of deep learning libraries and reveals some discrepancies as for the update behaviors, update reasons, and the version distributions. Our findings highlight some directions for researchers and also provide suggestions for deep learning developers and users.;https://ieeexplore.ieee.org/abstract/document/9240645/;CEaUsVu9Dg4J
Haq, F. U., Shin, D., Nejati, S., & Briand, L. C. (2020, October). Comparing offline and online testing of deep neural networks: An autonomous car case study. In 2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST) (pp. 85-95). IEEE.;4_dl_testing_deep_network;2020;Comparing offline and online testing of deep neural networks: An autonomous car case study;Fitash Ul Haq, Donghwan Shin, Shiva Nejati, Lionel C Briand;2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST), 85-95, 2020;There is a growing body of research on developing testing techniques for Deep Neural Networks (DNNs). We distinguish two general modes of testing for DNNs: Offline testing where DNNs are tested as individual units based on test datasets obtained independently from the DNNs under test, and online testing where DNNs are embedded into a specific application and tested in a close-loop mode in interaction with the application environment. In addition, we identify two sources for generating test datasets for DNNs: Datasets obtained from real-life and datasets generated by simulators. While offline testing can be used with datasets obtained from either sources, online testing is largely confined to using simulators since online testing within real-life applications can be time consuming, expensive and dangerous. In this paper, we study the following two important questions aiming to compare test datasets and testing modes for DNNs: First, can we use simulator-generated data as a reliable substitute to real-world data for the purpose of DNN testing? Second, how do online and offline testing results differ and complement each other? Though these questions are generally relevant to all autonomous systems, we study them in the context of automated driving systems where, as study subjects, we use DNNs automating end-to-end control of cars' steering actuators. Our results show that simulator-generated datasets are able to yield DNN prediction errors that are similar to those obtained by testing DNNs with real-life datasets. Further, offline testing is more optimistic than online testing as many safety violations identified by online testing could not be identified by offline testing, while large prediction errors generated by offline testing always led to severe safety violations detectable by online testing.;https://ieeexplore.ieee.org/abstract/document/9159088/;V_4F2PywKVIJ
Harel-Canada, F., Wang, L., Gulzar, M. A., Gu, Q., & Kim, M. (2020, November). Is neuron coverage a meaningful measure for testing deep neural networks?. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 851-862).;4_dl_testing_deep_network;2020;Is neuron coverage a meaningful measure for testing deep neural networks?;Fabrice Harel-Canada, Lingxiao Wang, Muhammad Ali Gulzar, Quanquan Gu, Miryung Kim;Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 851-862, 2020;Recent effort to test deep learning systems has produced an intuitive and compelling test criterion called neuron coverage (NC), which resembles the notion of traditional code coverage. NC measures the proportion of neurons activated in a neural network and it is implicitly assumed that increasing NC improves the quality of a test suite. In an attempt to automatically generate a test suite that increases NC, we design a novel diversity promoting regularizer that can be plugged into existing adversarial attack algorithms. We then assess whether such attempts to increase NC could generate a test suite that (1) detects adversarial attacks successfully, (2) produces natural inputs, and (3) is unbiased to particular class predictions. Contrary to expectation, our extensive evaluation finds that increasing NC actually makes it harder to generate an effective test suite: higher neuron coverage leads to fewer defects detected, less natural inputs, and more biased prediction preferences. Our results invoke skepticism that increasing neuron coverage may not be a meaningful objective for generating tests for deep neural networks and call for a new test generation technique that considers defect detection, naturalness, and output impartiality in tandem.;https://dl.acm.org/doi/abs/10.1145/3368089.3409754;sHWqQiFHf80J
Nishi, Y., Masuda, S., Ogawa, H., & Uetsuki, K. (2018, April). A test architecture for machine learning product. In 2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW) (pp. 273-278). IEEE.;1_ml_machine_data_learning;2018;A test architecture for machine learning product;Yasuharu Nishi, Satoshi Masuda, Hideto Ogawa, Keiji Uetsuki;2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW), 273-278, 2018;As machine learning (ML) technology continues to spread by rapid evolution, the system or service using Machine Learning technology, called ML product, makes big impact on our life, society and economy. Meanwhile, Quality Assurance (QA) for ML product is quite more difficult than hardware, non-ML software and service because performance of ML technology is much better than non-ML technology in exchange for the characteristics of ML product, e.g. low explainability. We must keep rapid evolution and reduce quality risk of ML product simultaneously. In this paper, we show a Quality Assurance Framework for Machine Learning product. Scope of QA in this paper is limited to product evaluation. First, a policy of QA for ML Product is proposed. General principles of product evaluation is introduced and applied to ML product evaluation as a part of the policy. They are composed of A-ARAI: Allowability, Achievability, Robustness, Avoidability and Improvability. A strategy of ML Product Evaluation is constructed as another part of the policy. Quality Integrity Level for ML product is also modelled. Second, we propose a test architecture of ML product testing. It consists of test levels and fundamental test types of ML product testing, including snapshot testing, learning testing and confrontation testing. Finally, we defines QA activity levels for ML product.;https://ieeexplore.ieee.org/abstract/document/8411763/;Q4V4NvWlY90J
Washizaki, H., Khomh, F., Guéhéneuc, Y. G., Takeuchi, H., Okuda, S., Natori, N., & Shioura, N. (2020, October). Software engineering patterns for machine learning applications (sep4mla) part 2. In Proceedings of the 27th conference on pattern languages of programs (pp. 1-10).;1_ml_machine_data_learning;2020;Software engineering patterns for machine learning applications (sep4mla) part 2;Hironori Washizaki, Foutse Khomh, Yann-Gaël Guéhéneuc, Hironori Takeuchi, Satoshi Okuda, Naotake Natori, Naohisa Shioura;Proceedings of the 27th conference on pattern languages of programs, 1-10, 2020;"Practitioners and researchers study best practices to develop and maintain ML application systems and software to address quality and constraint problems. Such practices are often formalized as software patterns. We discovered software-engineering design patterns for machine-learning applications by doing a thorough search of the literature available on the subject. Among the ML patterns found, we describe three ML patterns in the standard pattern format so that practitioners can (re)use them in their contexts: ""Different Workloads in Different Computing Environments"", ""Encapsulate ML Models Within Rule-base Safeguards"", and ""Data Flows Up, Model Flows Down""";https://dl.acm.org/doi/abs/10.5555/3511065.3511077;2OtAt06bqAYJ
Hu, B. C., Salay, R., Czarnecki, K., Rahimi, M., Selim, G., & Chechik, M. (2020, September). Towards requirements specification for machine-learned perception based on human performance. In 2020 IEEE Seventh International Workshop on Artificial Intelligence for Requirements Engineering (AIRE) (pp. 48-51). IEEE.;2_safety_system_autonomous_vehicle;2020;Towards requirements specification for machine-learned perception based on human performance;Boyue Caroline Hu, Rick Salay, Krzysztof Czarnecki, Mona Rahimi, Gehan Selim, Marsha Chechik;2020 IEEE Seventh International Workshop on Artificial Intelligence for Requirements Engineering (AIRE), 48-51, 2020;The application of machine learning (ML) based perception algorithms in safety-critical systems such as autonomous vehicles have raised major safety concerns due to the apparent risks to human lives. Yet assuring the safety of such systems is a challenging task, in a large part because ML components (MLCs) rarely have clearly specified requirements. Instead, they learn their intended tasks from the training data. One of the most well-studied properties that ensure the safety of MLCs is the robustness against small changes in images. But the range of changes considered small has not been systematically defined. In this paper, we propose an approach for specifying and testing requirements for robustness based on human perception. With this approach, the MLCs are required to be robust to changes that fall within the range defined based on human perception performance studies. We demonstrate the approach on a state-of-the-art object detector.;https://ieeexplore.ieee.org/abstract/document/9233007/;0_KtO-UURZIJ
Humbatova, N., Jahangirova, G., Bavota, G., Riccio, V., Stocco, A., & Tonella, P. (2020, June). Taxonomy of real faults in deep learning systems. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (pp. 1110-1121).;4_dl_testing_deep_network;2020;Taxonomy of real faults in deep learning systems;Nargiz Humbatova, Gunel Jahangirova, Gabriele Bavota, Vincenzo Riccio, Andrea Stocco, Paolo Tonella;Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, 1110-1121, 2020;The growing application of deep neural networks in safety-critical domains makes the analysis of faults that occur in such systems of enormous importance. In this paper we introduce a large taxonomy of faults in deep learning (DL) systems. We have manually analysed 1059 artefacts gathered from GitHub commits and issues of projects that use the most popular DL frameworks (TensorFlow, Keras and PyTorch) and from related Stack Overflow posts. Structured interviews with 20 researchers and practitioners describing the problems they have encountered in their experience have enriched our taxonomy with a variety of additional faults that did not emerge from the other two sources. Our final taxonomy was validated with a survey involving an additional set of 21 developers, confirming that almost all fault categories (13/15) were experienced by at least 50% of the survey participants.;https://dl.acm.org/doi/abs/10.1145/3377811.3380395;W8uhXQ6jQccJ
Ishikawa, F. (2018). Concepts in quality assessment for machine learning-from test data to arguments. In Conceptual Modeling: 37th International Conference, ER 2018, Xi'an, China, October 22–25, 2018, Proceedings 37 (pp. 536-544). Springer International Publishing.;1_ml_machine_data_learning;2018;Concepts in quality assessment for machine learning-from test data to arguments;Fuyuki Ishikawa;Conceptual Modeling: 37th International Conference, ER 2018, Xi'an, China, October 22â€“25, 2018, Proceedings 37, 536-544, 2018;There have been active efforts to use machine learning (ML) techniques for the development of smart systems, e.g., driving support systems with image recognition. However, the behavior of ML components, e.g., neural networks, is inductively derived from training data and thus uncertain and imperfect. Quality assessment heavily depends on and is restricted by a test data set or what has been tried among an enormous number of possibilities. Given this unique nature, we propose a MLQ framework for assessing the quality of ML components and ML-based systems. We introduce concepts to capture activities and evidences for the assessment and support the construction of arguments.;https://link.springer.com/chapter/10.1007/978-3-030-00847-5_39;vLeeSvquWKQJ
Hashemi, Y., Nayebi, M., & Antoniol, G. (2020, February). Documentation of machine learning software. In 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER) (pp. 666-667). IEEE.;1_ml_machine_data_learning;2020;Documentation of machine learning software;Yalda Hashemi, Maleknaz Nayebi, Giuliano Antoniol;2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER), 666-667, 2020;Machine Learning software documentation is different from most of the documentations that were studied in software engineering research. Often, the users of these documentations are not software experts. The increasing interest in using data science and in particular, machine learning in different fields attracted scientists and engineers with various levels of knowledge about programming and software engineering. Our ultimate goal is automated generation and adaptation of machine learning software documents for users with different levels of expertise. We are interested in understanding the nature and triggers of the problems and the impact of the users' levels of expertise in the process of documentation evolution. We will investigate the Stack Overflow Q&As and classify the documentation related Q/As within the machine learning domain to understand the types and triggers of the problems as well as the potential change requests to the documentation. We intend to use the results for building on top of the state of the art techniques for automatic documentation generation and extending on the adoption, summarization, and explanation of software functionalities.;https://ieeexplore.ieee.org/abstract/document/9054844/;m8F5aNfkKAwJ
Jahić, J., & Roitsch, R. (2020, September). State of the practice survey: Predicting the influence of ai adoption on system software architecture in traditional embedded systems. In European Conference on Software Architecture (pp. 155-169). Cham: Springer International Publishing.;1_ml_machine_data_learning;2020;State of the practice survey: Predicting the influence of ai adoption on system software architecture in traditional embedded systems;Jasmin JahiÄ‡, Robin Roitsch;European Conference on Software Architecture, 155-169, 2020;Artificial intelligence (AI) is a very disruptive technology. When adopted by a software system, AI influences and significantly changes its architecture due to its complexity, as well as due to a need to adjust the existing system to use AI (e.g., adopt accelerators). This is particularly critical in traditional embedded systems as they focus on a tight coupling of software and hardware. In this paper, we present results of a survey on how well companies in embedded software domain understand AI, how they perceive its possible benefits, and how they discuss the adoption of AI and its influence on their software system architecture. The goal of this survey is to evaluate architectural techniques that companies currently use when trying to assess the influence of adopting AI and to discuss the adequacy of these techniques for this task.;https://link.springer.com/chapter/10.1007/978-3-030-59155-7_12;0jXUMX__FQEJ
John, M. M., Olsson, H. H., & Bosch, J. (2020, August). Ai on the edge: Architectural alternatives. In 2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA) (pp. 21-28). IEEE.;7_edge_computing_deep_learning;2020;Ai on the edge: Architectural alternatives;Meenu Mary John, Helena HolmstrÃ¶m Olsson, Jan Bosch;2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA), 21-28, 2020;Since the advent of mobile computing and IoT, a large amount of data is distributed around the world. Companies are increasingly experimenting with innovative ways of implementing edge/cloud (re)training of AI systems to exploit large quantities of data to optimize their business value. Despite the obvious benefits, companies face challenges as the decision on how to implement edge/cloud (re)training depends on factors such as the task intent, the amount of data needed for (re)training, edge-to-cloud data transfer, the available computing and memory resources. Based on action research in a software-intensive embedded systems company where we study multiple use cases as well as insights from our previous collaborations with industry, we develop a generic framework consisting of five architectural alternatives to deploy AI on the edge utilizing transfer learning. We validate the framework in four additional case companies and present the challenges they face in selecting the optimal architecture. The contribution of the paper is threefold. First, we develop a generic framework consisting of five architectural alternatives ranging from a centralized architecture where cloud (re)training is given priority to a decentralized architecture where edge (re)training is instead given priority. Second, we validate the framework in a qualitative interview study with four additional case companies. As an outcome of validation study, we present two variants to the architectural alternatives identified as part of the framework. Finally, we identify the key challenges that experts face in selecting an ideal architectural alternative.;https://ieeexplore.ieee.org/abstract/document/9226348/;9TJ2D9IO9swJ
Kaindl, H., & Ferdigg, J. (2020, September). Towards an extended requirements problem formulation for superintelligence safety. In 2020 IEEE Seventh International Workshop on Artificial Intelligence for Requirements Engineering (AIRE) (pp. 33-38). IEEE.;1_ml_machine_data_learning;2020;Towards an extended requirements problem formulation for superintelligence safety;Hermann Kaindl, Jonas Ferdigg;2020 IEEE Seventh International Workshop on Artificial Intelligence for Requirements Engineering (AIRE), 33-38, 2020;"Under the headline ""AI safety"", a wide-reaching issue is being discussed, whether in the future some ""superhuman artificial intelligence"" / ""superintelligence"" could pose a threat to humanity. In addition, the late Steven Hawking warned that the rise of robots may be disastrous for mankind. A major concern is that even benevolent superhuman artificial intelligence (AI) may become seriously harmful if its given goals are not exactly aligned with ours, or if we cannot specify precisely its objective function. Metaphorically, this is compared to king Midas in Greek mythology, who expressed the wish that everything he touched should turn to gold, but obviously this wish was not specified precisely enough. In our view, this sounds like requirements problems and the challenge of their precise formulation. Hence, we take a new perspective on the problem by exploring it using insights from requirements engineering (RE). In addition, the overall issue calls for a major RE endeavor, figuring out the wishes and the needs with regard to a superintelligence, which will in our opinion most likely be a very complex softwareintensive system based on AI. In this paper, we introduce the idea of developing a new theoretical formulation of an extended requirements problem applicable to it, since it involves goals of both stakeholders and of the AI-based system-to-be-built.";https://ieeexplore.ieee.org/abstract/document/9233037/;YsqcUU6cLjMJ
Kang, S., Feldt, R., & Yoo, S. (2020, June). Sinvad: Search-based image space navigation for dnn image classifier test input generation. In Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops (pp. 521-528).;4_dl_testing_deep_network;2020;Sinvad: Search-based image space navigation for dnn image classifier test input generation;Sungmin Kang, Robert Feldt, Shin Yoo;Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops, 521-528, 2020;The testing of Deep Neural Networks (DNNs) has become increasingly important as DNNs are widely adopted by safety critical systems. While many test adequacy criteria have been suggested, automated test input generation for many types of DNNs remains a challenge because the raw input space is too large to randomly sample or to navigate and search for plausible inputs. Consequently, current testing techniques for DNNs depend on small local perturbations to existing inputs, based on the metamorphic testing principle. We propose new ways to search not over the entire image space, but rather over a plausible input space that resembles the true training distribution. This space is constructed using Variational Autoencoders (VAEs), and navigated through their latent vector space. We show that this space helps efficiently produce test inputs that can reveal information about the robustness of DNNs when dealing with realistic tests, opening the field to meaningful exploration through the space of highly structured images.;https://dl.acm.org/doi/abs/10.1145/3387940.3391456;_y9nUWFvYM8J
Kawamoto, Y. (2019, September). Towards logical specification of statistical machine learning. In International Conference on Software Engineering and Formal Methods (pp. 293-311). Cham: Springer International Publishing.;6_fairness_discrimination_bias_decision;2019;Towards logical specification of statistical machine learning;Yusuke Kawamoto;International Conference on Software Engineering and Formal Methods, 293-311, 2019;We introduce a logical approach to formalizing statistical properties of machine learning. Specifically, we propose a formal model for statistical classification based on a Kripke model, and formalize various notions of classification performance, robustness, and fairness of classifiers by using epistemic logic. Then we show some relationships among properties of classifiers and those between classification performance and robustness, which suggests robustness-related properties that have not been formalized in the literature as far as we know. To formalize fairness properties, we define a notion of counterfactual knowledge and show techniques to formalize conditional indistinguishability by using counterfactual epistemic operators. As far as we know, this is the first work that uses logical formulas to express statistical properties of machine learning, and that provides epistemic (resp. counterfactually epistemic) views on robustness (resp. fairness) of classifiers.;https://link.springer.com/chapter/10.1007/978-3-030-30446-1_16;npzFX0b8xBIJ
Klampfl, L., Chetouane, N., & Wotawa, F. (2020, December). Mutation testing for artificial neural networks: An empirical evaluation. In 2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS) (pp. 356-365). IEEE.;4_dl_testing_deep_network;2020;Mutation testing for artificial neural networks: An empirical evaluation;Lorenz Klampfl, Nour Chetouane, Franz Wotawa;2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS), 2020;Testing AI-based systems and especially when they rely on machine learning is considered a challenging task. In this paper, we contribute to this challenge considering testing neural networks utilizing mutation testing. A former paper focused on applying mutation testing to the configuration of neural networks leading to the conclusion that mutation testing can be effectively used. In this paper, we discuss a substantially extended empirical evaluation where we considered different test data and the source code of neural network implementations. In particular, we discuss whether a mutated neural network can be distinguished from the original one after learning, only considering a test evaluation. Unfortunately, this is rarely the case leading to a low mutation score. As a consequence, we see that the testing method, which works well at the configuration level of a neural network, is not sufficient to test neural network libraries requiring substantially more testing effort for assuring quality.;https://ieeexplore.ieee.org/abstract/document/9282748;TODO
Knauss, A., Schroder, J., Berger, C., & Eriksson, H. (2017, May). Software-related challenges of testing automated vehicles. In 2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C) (pp. 328-330). IEEE.;2_safety_system_autonomous_vehicle;2017;Software-related challenges of testing automated vehicles;Alessia Knauss, Jan Schroder, Christian Berger, Henrik Eriksson;2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C), 328-330, 2017;Automated vehicles are not supposed to fail at any time or in any situations during driving. Thus, vehicle manufactures and proving ground operators are challenged to complement existing test procedures with means to systematically evaluate automated driving. In this paper, we explore software related challenges from testing the safety of automated vehicles. We report on findings from conducting focus groups and interviews including 26 participants (e.g., vehicle manufacturers, suppliers, and researchers) from five countries.;https://ieeexplore.ieee.org/abstract/document/7965347/;lr7arva3680J
Li, Z., Ma, X., Xu, C., & Cao, C. (2019, May). Structural coverage criteria for neural networks could be misleading. In 2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER) (pp. 89-92). IEEE.;4_dl_testing_deep_network;2019;Structural coverage criteria for neural networks could be misleading;Zenan Li, Xiaoxing Ma, Chang Xu, Chun Cao;2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER), 89-92, 2019;"There is a dramatically increasing interest in the quality assurance for DNN-based systems in the software engineering community. An emerging hot topic in this direction is structural coverage criteria for testing neural networks, which are inspired by coverage metrics used in conventional software testing. In this short paper, we argue that these criteria could be misleading because of the fundamental differences between neural networks and human written programs. Our preliminary exploration shows that (1) adversarial examples are pervasively distributed in the finely divided space defined by such coverage criteria, while available natural samples are very sparse, and as a consequence, (2) previously reported fault-detection ""capabilities"" conjectured from high coverage testing are more likely due to the adversary-oriented search but not the real ""high"" coverage.";https://ieeexplore.ieee.org/abstract/document/8805667/;zE9eiGEM4WoJ
Li, Z., Ma, X., Xu, C., Xu, J., Cao, C., & Lü, J. (2020, November). Operational calibration: Debugging confidence errors for dnns in the field. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 901-913).;4_dl_testing_deep_network;2020;Operational calibration: Debugging confidence errors for dnns in the field;Zenan Li, Xiaoxing Ma, Chang Xu, Jingwei Xu, Chun Cao, Jian LÃ¼;Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 901-913, 2020;Trained DNN models are increasingly adopted as integral parts of software systems, but they often perform deficiently in the field. A particularly damaging problem is that DNN models often give false predictions with high confidence, due to the unavoidable slight divergences between operation data and training data. To minimize the loss caused by inaccurate confidence, operational calibration, i.e., calibrating the confidence function of a DNN classifier against its operation domain, becomes a necessary debugging step in the engineering of the whole system.Operational calibration is difficult considering the limited budget of labeling operation data and the weak interpretability of DNN models. We propose a Bayesian approach to operational calibration that gradually corrects the confidence given by the model under calibration with a small number of labeled operation data deliberately selected from a larger set of unlabeled operation data. The approach is made effective and efficient by leveraging the locality of the learned representation of the DNN model and modeling the calibration as Gaussian Process Regression. Comprehensive experiments with various practical datasets and DNN models show that it significantly outperformed alternative methods, and in some difficult tasks it eliminated about 71% to 97% high-confidence (>0.9) errors with only about 10% of the minimal amount of labeled operation data needed for practical learning techniques to barely work;https://dl.acm.org/doi/abs/10.1145/3368089.3409696;P-PfgO5Tg2cJ
Liem, C. C., & Panichella, A. (2020, June). Oracle issues in machine learning and where to find them. In Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops (pp. 483-488).;10_testing_test_machine_metamorphic;2020;Oracle issues in machine learning and where to find them;Cynthia CS Liem, Annibale Panichella;Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops, 483-488, 2020;The rise in popularity of machine learning (ML), and deep learning in particular, has both led to optimism about achievements of artificial intelligence, as well as concerns about possible weaknesses and vulnerabilities of ML pipelines. Within the software engineering community, this has led to a considerable body of work on ML testing techniques, including white- and black-box testing for ML models. This means the oracle problem needs to be addressed. For supervised ML applications, oracle information is indeed available in the form of dataset 'ground truth', that encodes input data with corresponding desired output labels. However, while ground truth forms a gold standard, there still is no guarantee it is truly correct. Indeed, syntactic, semantic, and conceptual framing issues in the oracle may negatively affect the ML system's integrity. While syntactic issues may automatically be verified and corrected, the higher-level issues traditionally need human judgment and manual analysis. In this paper, we employ two heuristics based on information entropy and semantic analysis on well-known computer vision models and benchmark data from ImageNet. The heuristics are used to semi-automatically uncover potential higher-level issues in (i) the label taxonomy used to define the ground truth oracle (labels), and (ii) data encoding and representation. In doing this, beyond existing ML testing efforts, we illustrate the need for software engineering strategies that especially target and assess the oracle.;https://dl.acm.org/doi/abs/10.1145/3387940.3391490;KGpTlgrn4BgJ
Liu, J., Huang, Q., Xia, X., Shihab, E., Lo, D., & Li, S. (2020, June). Is using deep learning frameworks free? characterizing technical debt in deep learning frameworks. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Society (pp. 1-10).;4_dl_testing_deep_network;2020;Is using deep learning frameworks free? characterizing technical debt in deep learning frameworks;Jiakun Liu, Qiao Huang, Xin Xia, Emad Shihab, David Lo, Shanping Li;Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Society, 1-10, 2020;Developers of deep learning applications (shortened as application developers) commonly use deep learning frameworks in their projects. However, due to time pressure, market competition, and cost reduction, developers of deep learning frameworks (shortened as framework developers) often have to sacrifice software quality to satisfy a shorter completion time. This practice leads to technical debt in deep learning frameworks, which results in the increasing burden to both the application developers and the framework developers in future development.In this paper, we analyze the comments indicating technical debt (self-admitted technical debt) in 7 of the most popular open-source deep learning frameworks. Although framework developers are aware of such technical debt, typically the application developers are not. We find that: 1) there is a significant number of technical debt in all the studied deep learning frameworks. 2) there is design debt, defect debt, documentation debt, test debt, requirement debt, compatibility debt, and algorithm debt in deep learning frameworks. 3) the majority of the technical debt in deep learning framework is design debt (24.07% - 65.27%), followed by requirement debt (7.09% - 31.48%) and algorithm debt (5.62% - 20.67%). In some projects, compatibility debt accounts for more than 10%. These findings illustrate that technical debt is common in deep learning frameworks, and many types of technical debt also impact the deep learning applications. Based on our findings, we highlight future research directions and provide recommendations for practitioners.;https://dl.acm.org/doi/abs/10.1145/3377815.3381377;c04IViooRvEJ
Ma, S., Aafer, Y., Xu, Z., Lee, W. C., Zhai, J., Liu, Y., & Zhang, X. (2017, August). LAMP: data provenance for graph based machine learning algorithms through derivative computation. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering (pp. 786-797).;1_ml_machine_data_learning;2017;LAMP: data provenance for graph based machine learning algorithms through derivative computation;Shiqing Ma, Yousra Aafer, Zhaogui Xu, Wen-Chuan Lee, Juan Zhai, Yingqi Liu, Xiangyu Zhang;Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering, 786-797, 2017;Data provenance tracking determines the set of inputs related to a given output. It enables quality control and problem diagnosis in data engineering. Most existing techniques work by tracking program dependencies. They cannot quantitatively assess the importance of related inputs, which is critical to machine learning algorithms, in which an output tends to depend on a huge set of inputs while only some of them are of importance. In this paper, we propose LAMP, a provenance computation system for machine learning algorithms. Inspired by automatic differentiation (AD), LAMP quantifies the importance of an input for an output by computing the partial derivative. LAMP separates the original data processing and the more expensive derivative computation to different processes to achieve cost-effectiveness. In addition, it allows quantifying importance for inputs related to discrete behavior, such as control flow selection. The evaluation on a set of real world programs and data sets illustrates that LAMP produces more precise and succinct provenance than program dependence based techniques, with much less overhead. Our case studies demonstrate the potential of LAMP in problem diagnosis in data engineering.;https://dl.acm.org/doi/abs/10.1145/3106237.3106291;OGGFs8xMG_cJ
Moreb, M., Mohammed, T. A., & Bayat, O. (2020). A novel software engineering approach toward using machine learning for improving the efficiency of health systems. IEEE Access, 8, 23169-23178.;1_ml_machine_data_learning;2020;A novel software engineering approach toward using machine learning for improving the efficiency of health systems;Mohammed Moreb, Tareq Abed Mohammed, Oguz Bayat;IEEE Access 8, 23169-23178, 2020;"Recently, machine learning has become a hot research topic. Therefore, this study investigates the interaction between software engineering and machine learning within the context of health systems. We proposed a novel framework for health informatics: the framework and methodology of software engineering for machine learning in health informatics (SEMLHI). The SEMLHI framework includes four modules (software, machine learning, machine learning algorithms, and health informatics data) that organize the tasks in the framework using a SEMLHI methodology, thereby enabling researchers and developers to analyze health informatics software from an engineering perspective and providing developers with a new road map for designing health applications with system functions and software implementations. Our novel approach sheds light on its features and allows users to study and analyze the user requirements and determine both the function of objects related to the system and the machine learning algorithms that must be applied to the dataset. Our dataset used in this research consists of real data and was originally collected from a hospital run by the Palestine government covering the last three years. The SEMLHI methodology includes seven phases: designing, implementing, maintaining and defining workflows; structuring information; ensuring security and privacy; performance testing and evaluation; and releasing the software applications.";https://ieeexplore.ieee.org/abstract/document/8974224/;Jy_wrXtyKlQJ
Munappy, A. R., Bosch, J., & Olsson, H. H. (2020). Data pipeline management in practice: Challenges and opportunities. In Product-Focused Software Process Improvement: 21st International Conference, PROFES 2020, Turin, Italy, November 25–27, 2020, Proceedings 21 (pp. 168-184). Springer International Publishing.;9_data_science_software_process;2020;Data pipeline management in practice: Challenges and opportunities;Aiswarya Raj Munappy, Jan Bosch, Helena Homström Olsson;Product-Focused Software Process Improvement: 21st International Conference, PROFES 2020, Turin, Italy, November 25–27, 2020, Proceedings 21, 168-184, 2020;Data pipelines involve a complex chain of interconnected activities that starts with a data source and ends in a data sink. Data pipelines are important for data-driven organizations since a data pipeline can process data in multiple formats from distributed data sources with minimal human intervention, accelerate data life cycle activities, and enhance productivity in data-driven enterprises. However, there are challenges and opportunities in implementing data pipelines but practical industry experiences are seldom reported. The findings of this study are derived by conducting a qualitative multiple-case study and interviews with the representatives of three companies. The challenges include data quality issues, infrastructure maintenance problems, and organizational barriers. On the other hand, data pipelines are implemented to enable traceability, fault-tolerance, and reduce human errors through maximizing automation thereby producing high-quality data. Based on multiple-case study research with five use cases from three case companies, this paper identifies the key challenges and benefits associated with the implementation and use of data pipelines.;https://link.springer.com/chapter/10.1007/978-3-030-64148-1_11;U9uEM2CW6wUJ
Murphy, C., Kaiser, G. E., & Arias, M. (2007). An approach to software testing of machine learning applications.;10_testing_test_machine_metamorphic;2007;An approach to software testing of machine learning applications.;Christian Murphy, Gail E Kaiser, Marta Arias;-;Some machine learning applications are intended to learn properties of data sets where the correct answers are not already known to human users. It is challenging to test such ML software, because there is no reliable test oracle. We describe a software testing approach aimed at addressing this problem. We present our findings from testing implementations of two different ML ranking algorithms: Support Vector Machines and MartiRank.;https://academiccommons.columbia.edu/doi/10.7916/D8TH8VG9/download;6UXCXqB7464J
Nakajima, S. (2019). Dataset diversity for metamorphic testing of machine learning software. In Structured Object-Oriented Formal Language and Method: 8th International Workshop, SOFL+ MSVL 2018, Gold Coast, QLD, Australia, November 16, 2018, Revised Selected Papers 8 (pp. 21-38). Springer International Publishing.;10_testing_test_machine_metamorphic;2019;Dataset diversity for metamorphic testing of machine learning software;Shin Nakajima;Structured Object-Oriented Formal Language and Method: 8th International Workshop, SOFL+ MSVL 2018, Gold Coast, QLD, Australia, November 16, 2018, Revised Selected Papers 8, 21-38, 2019;Machine learning software is non-testable in that training results are not available in advance. The metamorphic testing, using pseudo oracle, is promising for software testing of such machine learning programs. Machine learning software, indeed, works on a collection of a large number of data, and thus slight changes in the input training dataset have a large impact on training results. This paper proposes a new metamorphic testing method applicable to neural network learning models. Key ideas are dataset diversity as well as behavioral oracle. Dataset diversity takes into account the dataset dependency of training results, and provides a new way of generating follow-up test inputs. Behavioral oracle monitors changes of certain statistical indicators as training processes proceed and is a basis of metamorphic relations to be checked. The proposed method is illustrated with a case of software testing of neural network programs to classify handwritten numbers.;https://link.springer.com/chapter/10.1007/978-3-030-13651-2_2;feXc0iMdAFIJ
Nguyen-Duc, A., Sundbø, I., Nascimento, E., Conte, T., Ahmed, I., & Abrahamsson, P. (2020, April). A multiple case study of artificial intelligent system development in industry. In Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering (pp. 1-10).;1_ml_machine_data_learning;2020;A multiple case study of artificial intelligent system development in industry;Anh Nguyen-Duc, Ingrid SundbÃ¸, Elizamary Nascimento, Tayana Conte, Iftekhar Ahmed, Pekka Abrahamsson;Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering, 1-10, 2020;There is a rapidly increasing amount of Artificial Intelligence (AI) systems developed in recent years, with much expectation on its capacity of innovation and business value generation. However, the promised value of AI systems in specific business contexts might not be understood, and further integrated into the development processes. We wanted to understand how software engineering processes and practices can be applied to develop AI systems in a fast-faced, business-driven manner. As the first step, we explored contextual factors of AI development and the connections between AI developments to business opportunities. We conducted 12 semi-structured interviews in seven companies in Brazil, Norway and Southeast Asia. Our investigation revealed different types of AI systems and different AI development approaches. However, it is common that business opportunities involving with AI systems are not validated and there is lack of business-driven metrics that guide the development of AI systems. The findings have implications for future research on business-driven AI development and supporting tools and practices.;https://dl.acm.org/doi/abs/10.1145/3383219.3383220;ETy6L5F25d4J
Ozkaya, I. (2020). What is really different in engineering AI-enabled systems?. IEEE software, 37(4), 3-6.;1_ml_machine_data_learning;2020;What is really different in engineering AI-enabled systems?;Ipek Ozkaya;IEEE software 37 (4), 3-6, 2020;"Advances in machine learning (ML) algorithms and increasing availability of computational power have resulted in huge investments in systems that aspire to exploit artificial intelligence (AI), in particular ML. AIenabled systems, software-reliant systems that include data and components that implement algorithms mimicking learning and problem solving, have inherently different characteristics than software systems alone.1 However, the development and sustainment of such systems also have many parallels with building, deploying, and sustaining software systems. A common observation is that although software systems are deterministic and you can build and test to a specification, AI-enabled systems, in particular those that include ML components, are generally probabilistic. Systems with ML components can have a high margin of error due to the uncertainty that often follows predictive algorithms. The margin of error can be related to the inability to predict the result in advance or the same result cannot be reproduced. This characteristic makes AI-enabled systems hard to test and verify.2 Consequently, it is easy to assume that what we know about designing and reasoning about software systems does not immediately apply in AI engineering. AI-enabled systems are software systems. The sneaky part about engineering AI systems is they are ""just like"" conventional software systems we can design and reason about until they?re not.";https://ieeexplore.ieee.org/abstract/document/9121629/;Ief1I2pWe0wJ
Pan, R. (2020, June). Does fixing bug increase robustness in deep learning?. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings (pp. 146-148).;4_dl_testing_deep_network;2020;Does fixing bug increase robustness in deep learning?;Rangeet Pan;Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings, 146-148, 2020;Deep Learning (DL) based systems are utilized vastly. Developers update the code to fix the bugs in the system. How these code fixing techniques impacts the robustness of these systems has not been clear. Does fixing code increase the robustness? Do they deteriorate the learning capability of the DL based systems? To answer these questions, we studied 321 Stack Overflow posts based on a published dataset. In this study, we built a classification scheme to analyze how bug-fixes changed the robustness of the DL model and found that most of the bug-fixes can increase the robustness. We also found evidence of bug-fixing that decrease the robustness. Our preliminary result suggests that 12.5% and 2.4% of the bug-fixes in Stack Overflow posts caused the increase and the decrease of the robustness of DL models, respectively.;https://dl.acm.org/doi/abs/10.1145/3377812.3382175;4z3yzQD96rIJ
Paulsen, B., Wang, J., Wang, J., & Wang, C. (2020, December). Neurodiff: scalable differential verification of neural networks using fine-grained approximation. In Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering (pp. 784-796).;4_dl_testing_deep_network;2020;Neurodiff: scalable differential verification of neural networks using fine-grained approximation;Brandon Paulsen, Jingbo Wang, Jiawei Wang, Chao Wang;Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering, 784-796, 2020;As neural networks make their way into safety-critical systems, where misbehavior can lead to catastrophes, there is a growing interest in certifying the equivalence of two structurally similar neural networks - a problem known as differential verification. For example, compression techniques are often used in practice for deploying trained neural networks on computationally- and energy-constrained devices, which raises the question of how faithfully the compressed network mimics the original network. Unfortunately, existing methods either focus on verifying a single network or rely on loose approximations to prove the equivalence of two networks. Due to overly conservative approximation, differential verification lacks scalability in terms of both accuracy and computational cost. To overcome these problems, we propose NeuroDiff, a symbolic and fine-grained approximation technique that drastically increases the accuracy of differential verification on feed-forward ReLU networks while achieving many orders-of-magnitude speedup. NeuroDiff has two key contributions. The first one is new convex approximations that more accurately bound the difference of two networks under all possible inputs. The second one is judicious use of symbolic variables to represent neurons whose difference bounds have accumulated significant error. We find that these two techniques are complementary, i.e., when combined, the benefit is greater than the sum of their individual benefits. We have evaluated NeuroDiff on a variety of differential verification tasks. Our results show that NeuroDiff is up to 1000X faster and 5X more accurate than the state-of-the-art tool.;https://dl.acm.org/doi/abs/10.1145/3324884.3416560;FgcrE2Pi3AUJ
Peng, Z., Yang, J., Chen, T. H., & Ma, L. (2020, November). A first look at the integration of machine learning models in complex autonomous driving systems: a case study on Apollo. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 1240-1250).;2_safety_system_autonomous_vehicle;2020;A first look at the integration of machine learning models in complex autonomous driving systems: a case study on Apollo;Zi Peng, Jinqiu Yang, Tse-Hsun Chen, Lei Ma;Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 1240-1250, 2020;Autonomous Driving System (ADS) is one of the most promising and valuable large-scale machine learning (ML) powered systems. Hence, ADS has attracted much attention from academia and practitioners in recent years. Despite extensive study on ML models, it still lacks a comprehensive empirical study towards understanding the ML model roles, peculiar architecture, and complexity of ADS (i.e., various ML models and their relationship with non-trivial code logic). In this paper, we conduct an in-depth case study on Apollo, which is one of the state-of-the-art ADS, widely adopted by major automakers worldwide. We took the first step to reveal the integration of the underlying ML models and code logic in Apollo. In particular, we study the Apollo source code and present the underlying ML model system architecture. We present our findings on how the ML models interact with each other, and how the ML models are integrated with code logic to form a complex system. Finally, we inspect Apollo in a dynamic view and notice the heavy use of model-relevant components and the lack of adequate tests in general. Our study reveals potential maintenance challenges of complex ML-powered systems and identifies future directions to improve the quality assurance of ADS and general ML systems.;https://dl.acm.org/doi/abs/10.1145/3368089.3417063;WrmZEVFfrV4J
Pham, H. V., Lutellier, T., Qi, W., & Tan, L. (2019, May). CRADLE: cross-backend validation to detect and localize bugs in deep learning libraries. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) (pp. 1027-1038). IEEE.;4_dl_testing_deep_network;2019;CRADLE: cross-backend validation to detect and localize bugs in deep learning libraries;Hung Viet Pham, Thibaud Lutellier, Weizhen Qi, Lin Tan;2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 1027-1038, 2019;Deep learning (DL) systems are widely used in domains including aircraft collision avoidance systems, Alzheimer's disease diagnosis, and autonomous driving cars. Despite the requirement for high reliability, DL systems are difficult to test. Existing DL testing work focuses on testing the DL models, not the implementations (e.g., DL software libraries) of the models. One key challenge of testing DL libraries is the difficulty of knowing the expected output of DL libraries given an input instance. Fortunately, there are multiple implementations of the same DL algorithms in different DL libraries. Thus, we propose CRADLE, a new approach that focuses on finding and localizing bugs in DL software libraries. CRADLE (1) performs cross-implementation inconsistency checking to detect bugs in DL libraries, and (2) leverages anomaly propagation tracking and analysis to localize faulty functions in DL libraries that cause the bugs. We evaluate CRADLE on three libraries (TensorFlow, CNTK, and Theano), 11 datasets (including ImageNet, MNIST, and KGS Go game), and 30 pre-trained models. CRADLE detects 12 bugs and 104 unique inconsistencies, and highlights functions relevant to the causes of inconsistencies for all 104 unique inconsistencies.;https://ieeexplore.ieee.org/abstract/document/8812095/;tJr_9LXLJO0J
Pham, H. V., Qian, S., Wang, J., Lutellier, T., Rosenthal, J., Tan, L., ... & Nagappan, N. (2020, December). Problems and opportunities in training deep learning software systems: An analysis of variance. In Proceedings of the 35th IEEE/ACM international conference on automated software engineering (pp. 771-783).;4_dl_testing_deep_network;2020;Problems and opportunities in training deep learning software systems: An analysis of variance;Hung Viet Pham, Shangshu Qian, Jiannan Wang, Thibaud Lutellier, Jonathan Rosenthal, Lin Tan, Yaoliang Yu, Nachiappan Nagappan;Proceedings of the 35th IEEE/ACM international conference on automated software engineering, 771-783, 2020;Deep learning (DL) training algorithms utilize nondeterminism to improve models' accuracy and training efficiency. Hence, multiple identical training runs (e.g., identical training data, algorithm, and network) produce different models with different accuracies and training times. In addition to these algorithmic factors, DL libraries (e.g., TensorFlow and cuDNN) introduce additional variance (referred to as implementation-level variance) due to parallelism, optimization, and floating-point computation.This work is the first to study the variance of DL systems and the awareness of this variance among researchers and practitioners. Our experiments on three datasets with six popular networks show large overall accuracy differences among identical training runs. Even after excluding weak models, the accuracy difference is 10.8%. In addition, implementation-level factors alone cause the accuracy difference across identical training runs to be up to 2.9%, the per-class accuracy difference to be up to 52.4%, and the training time difference to be up to 145.3%. All core libraries (TensorFlow, CNTK, and Theano) and low-level libraries (e.g., cuDNN) exhibit implementation-level variance across all evaluated versions.Our researcher and practitioner survey shows that 83.8% of the 901 participants are unaware of or unsure about any implementation-level variance. In addition, our literature survey shows that only 19.5Â±3% of papers in recent top software engineering (SE), artificial intelligence (AI), and systems conferences use multiple identical training runs to quantify the variance of their DL approaches. This paper raises awareness of DL variance and directs SE researchers to challenging tasks such as creating deterministic DL implementations to facilitate debugging and improving the reproducibility of DL software and results.;https://dl.acm.org/doi/abs/10.1145/3324884.3416545;hxsePKtnJ20J
Qin, Y., Wang, H., Xu, C., Ma, X., & Lu, J. (2018, July). Syneva: Evaluating ml programs by mirror program synthesis. In 2018 IEEE International Conference on Software Quality, Reliability and Security (QRS) (pp. 171-182). IEEE.;10_testing_test_machine_metamorphic;2018;Syneva: Evaluating ml programs by mirror program synthesis;Yi Qin, Huiyan Wang, Chang Xu, Xiaoxing Ma, Jian Lu;2018 IEEE International Conference on Software Quality, Reliability and Security (QRS), 171-182, 2018;Machine learning (ML) programs are being widely used in various human-related applications. However, their testing always remains to be a challenging problem, and one can hardly decide whether and how the existing knowledge extracted from training scenarios suit new scenarios. Existing approaches typically have restricted usages due to their assumptions on the availability of an oracle, comparable implementation, or manual inspection efforts. We solve this problem by proposing a novel program synthesis based approach, SynEva, that can systematically construct an oracle-alike mirror program for similarity measurement, and automatically compare it with the existing knowledge on new scenarios to decide how the knowledge suits the new scenarios. SynEva is lightweight and fully automated. Our experimental evaluation with real-world data sets validates SynEva's effectiveness by strong correlation and little overhead results. We expect that SynEva can apply to, and help evaluate, more ML programs for new scenarios.;https://ieeexplore.ieee.org/abstract/document/8424969/;uefmhTLM2JwJ
Ren, X., Yu, B., Qi, H., Juefei-Xu, F., Li, Z., Xue, W., ... & Zhao, J. (2020, September). Few-shot guided mix for dnn repairing. In 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME) (pp. 717-721). IEEE.;4_dl_testing_deep_network;2020;Few-shot guided mix for dnn repairing;Xuhong Ren, Bing Yu, Hua Qi, Felix Juefei-Xu, Zhuo Li, Wanli Xue, Lei Ma, Jianjun Zhao;2020 IEEE International Conference on Software Maintenance and Evolution (ICSME), 717-721, 2020;Although deep neural networks (DNNs) achieve rather high performance in many cutting-edge applications (e.g., autonomous driving, medical diagnose), their trustworthiness on real-world scenarios still posts concerns, where some specific failure examples are often encountered during the real-world operational environment. With the limited failure examples collected during the practical operation, how to effectively leverage such failure cases to repair and enhance DNN so as to generalize to more potentially suspicious samples is challenging, but of great importance. In this paper, we formulate the failure-data-driven DNN repairing as a data augmentation problem, and design a novel augmentation-based repairing method, which to the best extent leverages limited failure cases. To realize the DNN repairing effects that generalize to specific failure examples, we originally propose few-shot guided mix (FSGMix) that augments training data with the guidance of failure examples. As a result, our method is able to achieve high generalization to the collected failure examples and other similar suspicious data. The preliminary evaluation on CIFAR-10 dataset demonstrates the potential of our proposed technique, which automatically learns to resolve the potential failure patterns in the DNN operational environment.;https://ieeexplore.ieee.org/abstract/document/9240708/;6za9eLnn3mUJ
Riccio, V., & Tonella, P. (2020, November). Model-based exploration of the frontier of behaviours for deep learning system testing. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 876-888).;4_dl_testing_deep_network;2020;Model-based exploration of the frontier of behaviours for deep learning system testing;Vincenzo Riccio, Paolo Tonella;Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 876-888, 2020;With the increasing adoption of Deep Learning (DL) for critical tasks, such as autonomous driving, the evaluation of the quality of systems that rely on DL has become crucial. Once trained, DL systems produce an output for any arbitrary numeric vector provided as input, regardless of whether it is within or outside the validity domain of the system under test. Hence, the quality of such systems is determined by the intersection between their validity domain and the regions where their outputs exhibit a misbehaviour.In this paper, we introduce the notion of frontier of behaviours, i.e., the inputs at which the DL system starts to misbehave. If the frontier of misbehaviours is outside the validity domain of the system, the quality check is passed. Otherwise, the inputs at the intersection represent quality deficiencies of the system. We developed DeepJanus, a search-based tool that generates frontier inputs for DL systems. The experimental results obtained for the lane keeping component of a self-driving car show that the frontier of a well trained system contains almost exclusively unrealistic roads that violate the best practices of civil engineering, while the frontier of a poorly trained one includes many valid inputs that point to serious deficiencies of the system.;https://dl.acm.org/doi/abs/10.1145/3368089.3409730;ZxuNsqe9zbAJ
Sankaran, A., Aralikatte, R., Mani, S., Khare, S., Panwar, N., & Gantayat, N. (2017, May). DARVIZ: deep abstract representation, visualization, and verification of deep learning models. In 2017 IEEE/ACM 39th International Conference on Software Engineering: New Ideas and Emerging Technologies Results Track (ICSE-NIER) (pp. 47-50). IEEE.;4_dl_testing_deep_network;2017;DARVIZ: deep abstract representation, visualization, and verification of deep learning models;Anush Sankaran, Rahul Aralikatte, Senthil Mani, Shreya Khare, Naveen Panwar, Neelamadhav Gantayat;2017 IEEE/ACM 39th International Conference on Software Engineering: New Ideas and Emerging Technologies Results Track (ICSE-NIER), 47-50, 2017;Traditional software engineering programming paradigms are mostly object or procedure oriented, driven by deterministic algorithms. With the advent of deep learning and cognitive sciences there is an emerging trend for data-driven programming, creating a shift in the programming paradigm among the software engineering communities. Visualizing and interpreting the execution of a current large scale data-driven software development is challenging. Further, for deep learning development there are many libraries in multiple programming languages such as TensorFlow (Python), CAFFE (C++), Theano (Python), Torch (Lua), and Deeplearning4j (Java), driving a huge need for interoperability across libraries. We propose a model driven development based solution framework, that facilitates intuitive designing of deep learning models in a platform agnostic fashion. This framework could potentially generate library specific code, perform program translation across languages, and debug the training process of a deep learning model from a fault localization and repair perspective. Further we identify open research problems in this emerging domain, and discuss some new software tooling requirements to serve this new age data-driven programming paradigm.;https://ieeexplore.ieee.org/abstract/document/7966878/;4VuvYmEe2ncJ
Santos, S. H., da Silveira, B. N. C., Andrade, S. A., Delamaro, M., & Souza, S. R. (2020, October). An experimental study on applying metamorphic testing in machine learning applications. In Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing (pp. 98-106).;10_testing_test_machine_metamorphic;2020;An experimental study on applying metamorphic testing in machine learning applications;Sebastião HN Santos, Beatriz Nogueira Carvalho da Silveira, Stevão A Andrade, Márcio Delamaro, Simone RS Souza;Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing, 98-106, 2020;Machine learning techniques have been successfully employed in various areas and, in particular, for the development of healthcare applications, aiming to support in more effective and faster diagnostics (such as cancer diagnosis). However, machine learning models may present uncertainties and errors. Errors in the training process, classification, and evaluation can generate incorrect results and, consequently, to wrong clinical decisions, reducing the professionals' confidence in the use of such techniques. Similar to other application domains, the quality should be guaranteed to produce more reliable models capable of assisting health professionals in their daily activities. Metamorphic testing can be an interesting option to validate machine learning applications. Using this testing approach is possible to define relationships that define changes to be made in the application's input data to identify faults. This paper presents an experimental study to evaluate the effectiveness of metamorphic testing to validate machine learning applications. A Machine learning application to verify breast cancer diagnostic was developed, using an available dataset composed of 569 samples whose data were taken from breast cancer images, and used as the software under test, in which the metamorphic testing was applied. The results indicate that metamorphic testing can be an alternative to support the validation of machine learning applications.;https://dl.acm.org/doi/abs/10.1145/3425174.3425226;-g7_cJAno8gJ
Sekhon, J., & Fleming, C. (2019, May). Towards improved testing for deep learning. In 2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER) (pp. 85-88). IEEE.;4_dl_testing_deep_network;2019;Towards improved testing for deep learning;Jasmine Sekhon, Cody Fleming;2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER), 85-88, 2019;The growing use of deep neural networks in safety-critical applications makes it necessary to carry out adequate testing to detect and correct any incorrect behavior for corner case inputs before they can be actually used. Deep neural networks lack an explicit control-flow structure, making it impossible to apply to them traditional software testing criteria such as code coverage. In this paper, we examine existing testing methods for deep neural networks, the opportunities for improvement and the need for a fast, scalable, generalizable end-to-end testing method. We also propose a coverage criterion for deep neural networks that tries to capture all possible parts of the deep neural network's logic.;https://ieeexplore.ieee.org/abstract/document/8805668/;uQm4HQ5dLK0J
Sharma, A., & Wehrheim, H. (2019, April). Testing machine learning algorithms for balanced data usage. In 2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST) (pp. 125-135). IEEE.;6_fairness_discrimination_bias_decision;2019;Testing machine learning algorithms for balanced data usage;Arnab Sharma, Heike Wehrheim;2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST), 125-135, 2019;"With the increased application of machine learning (ML) algorithms to decision-making processes, the question of fairness of such algorithms came into the focus. Fairness testing aims at checking whether a classifier as ""learned"" by an ML algorithm on some training data is biased in the sense of discriminating against some of the attributes (e.g. gender or age). Fairness testing thus targets the prediction phase in ML, not the learning phase. In this paper, we investigate fairness for the learning phase. Our definition of fairness is based on the idea that the learner should treat all data in the training set equally, disregarding issues like names or orderings of features or orderings of data instances. We term this property balanced data usage. We consequently develop a (metamorphic) testing approach called TiLe for checking balanced data usage. TiLe is applied on 14 ML classifiers taken from the scikit-learn library using 4 artificial and 9 real-world data sets for training, finding 12 of the classifiers to be unbalanced.";https://ieeexplore.ieee.org/abstract/document/8730187/;g_lnHX9ajrUJ
Sharma, A., & Wehrheim, H. (2020, July). Higher income, larger loan? monotonicity testing of machine learning models. In Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis (pp. 200-210).;10_testing_test_machine_metamorphic;2020;Higher income, larger loan? monotonicity testing of machine learning models;Arnab Sharma, Heike Wehrheim;Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis, 200-210, 2020;Today, machine learning (ML) models are increasingly applied in decision making. This induces an urgent need for quality assurance of ML models with respect to (often domain-dependent) requirements. Monotonicity is one such requirement. It specifies a software as ''learned'' by an ML algorithm to give an increasing prediction with the increase of some attribute values. While there exist multiple ML algorithms for ensuring monotonicity of the generated model, approaches for checking monotonicity, in particular of black-box models are largely lacking.In this work, we propose verification-based testing of monotonicity, i.e., the formal computation of test inputs on a white-box model via verification technology, and the automatic inference of this approximating white-box model from the black-box model under test. On the white-box model, the space of test inputs can be systematically explored by a directed computation of test cases. The empirical evaluation on 90 black-box models shows that verification-based testing can outperform adaptive random testing as well as property-based techniques with respect to effectiveness and efficiency.;https://dl.acm.org/doi/abs/10.1145/3395363.3397352;Bt-yhyJWeX0J
Sharma, A., & Wehrheim, H. (2020). Automatic fairness testing of machine learning models. In Testing Software and Systems: 32nd IFIP WG 6.1 International Conference, ICTSS 2020, Naples, Italy, December 9–11, 2020, Proceedings 32 (pp. 255-271). Springer International Publishing.;6_fairness_discrimination_bias_decision;2020;Automatic fairness testing of machine learning models;Arnab Sharma, Heike Wehrheim;Testing Software and Systems: 32nd IFIP WG 6.1 International Conference, ICTSS 2020, Naples, Italy, December 9–11, 2020, Proceedings 32, 255-271, 2020;In recent years, there has been an increased application of machine learning (ML) to decision making systems. This has prompted an urgent need for validating requirements on ML models. Fairness is one such requirement to be ensured in numerous application domains. It specifies a software as “learned” by an ML algorithm to not be biased in the sense of discriminating against some attributes (like gender or age), giving different decisions upon flipping the values of these attributes.In this work, we apply verification-based testing (VBT) to the fairness checking of ML models. Verification-based testing employs verification technology to generate test cases potentially violating the property under interest. For fairness testing, we additionally provide a specification language for the formalization of different fairness requirements. From the ML model under test and fairness specification VBT automatically generates test inputs specific to the specified fairness requirement. The empirical evaluation on several benchmark ML models shows verification-based testing to perform better than existing fairness testing techniques with respect to effectiveness.;https://link.springer.com/chapter/10.1007/978-3-030-64881-7_16;qKECIvi2l9oJ
Shen, W., Li, Y., Chen, L., Han, Y., Zhou, Y., & Xu, B. (2020, December). Multiple-boundary clustering and prioritization to promote neural network retraining. In Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering (pp. 410-422).;4_dl_testing_deep_network;2020;Multiple-boundary clustering and prioritization to promote neural network retraining;Weijun Shen, Yanhui Li, Lin Chen, Yuanlei Han, Yuming Zhou, Baowen Xu;Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering, 410-422, 2020;"With the increasing application of deep learning (DL) models in many safety-critical scenarios, effective and efficient DL testing techniques are much in demand to improve the quality of DL models. One of the major challenges is the data gap between the training data to construct the models and the testing data to evaluate them. To bridge the gap, testers aim to collect an effective subset of inputs from the testing contexts, with limited labeling effort, for retraining DL models.To assist the subset selection, we propose Multiple-Boundary Clustering and Prioritization (MCP), a technique to cluster test samples into the boundary areas of multiple boundaries for DL models and specify the priority to select samples evenly from all boundary areas, to make sure enough useful samples for each boundary reconstruction. To evaluate MCP, we conduct an extensive empirical study with three popular DL models and 33 simulated testing contexts. The experiment results show that, compared with state-of-the-art baseline methods, on effectiveness, our approach MCP has a significantly better performance by evaluating the improved quality of retrained DL models; on efficiency, MCP also has the advantages in time costs.";https://dl.acm.org/doi/abs/10.1145/3324884.3416621;77lLRqVdLjYJ
Stocco, A., & Tonella, P. (2020, October). Towards anomaly detectors that learn continuously. In 2020 IEEE international symposium on software reliability engineering workshops (ISSREW) (pp. 201-208). IEEE.;4_dl_testing_deep_network;2020;Towards anomaly detectors that learn continuously;Andrea Stocco, Paolo Tonella;2020 IEEE international symposium on software reliability engineering workshops (ISSREW), 201-208, 2020;In this paper, we first discuss the challenges of adapting an already trained DNN-based anomaly detector with knowledge mined during the execution of the main system. Then, we present a framework for the continual learning of anomaly detectors, which records in-field behavioural data to determine what data are appropriate for adaptation. We evaluated our framework to improve an anomaly detector taken from the literature, in the context of misbehavior prediction for self-driving cars. Our results show that our solution can reduce the false positive rate by a large margin and adapt to nominal behaviour changes while maintaining the original anomaly detection capability.;https://ieeexplore.ieee.org/abstract/document/9307667/;V_AbqMyJjhgJ
Stocco, A., Weiss, M., Calzana, M., & Tonella, P. (2020, June). Misbehaviour prediction for autonomous driving systems. In Proceedings of the ACM/IEEE 42nd international conference on software engineering (pp. 359-371).;4_dl_testing_deep_network;2020;Misbehaviour prediction for autonomous driving systems;Andrea Stocco, Michael Weiss, Marco Calzana, Paolo Tonella;Proceedings of the ACM/IEEE 42nd international conference on software engineering, 359-371, 2020;Deep Neural Networks (DNNs) are the core component of modern autonomous driving systems. To date, it is still unrealistic that a DNN will generalize correctly to all driving conditions. Current testing techniques consist of offline solutions that identify adversarial or corner cases for improving the training phase.In this paper, we address the problem of estimating the confidence of DNNs in response to unexpected execution contexts with the purpose of predicting potential safety-critical misbehaviours and enabling online healing of DNN-based vehicles. Our approach SelfOracle is based on a novel concept of self-assessment oracle, which monitors the DNN confidence at runtime, to predict unsupported driving scenarios in advance. SelfOracle uses autoencoder-and time series-based anomaly detection to reconstruct the driving scenarios seen by the car, and to determine the confidence boundary between normal and unsupported conditions.In our empirical assessment, we evaluated the effectiveness of different variants of SelfOracle at predicting injected anomalous driving contexts, using DNN models and simulation environment from Udacity. Results show that, overall, SelfOracle can predict 77% misbehaviours, up to six seconds in advance, outperforming the online input validation approach of DeepRoad.;https://dl.acm.org/doi/abs/10.1145/3377811.3380353;4d9nHZeyMF4J
Sun, Y., Huang, X., Kroening, D., Sharp, J., Hill, M., & Ashmore, R. (2019, May). DeepConcolic: Testing and debugging deep neural networks. In 2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion) (pp. 111-114). IEEE.;4_dl_testing_deep_network;2019;DeepConcolic: Testing and debugging deep neural networks;Youcheng Sun, Xiaowei Huang, Daniel Kroening, James Sharp, Matthew Hill, Rob Ashmore;2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), 111-114, 2019;Deep neural networks (DNNs) have been deployed in a wide range of applications. We introduce a DNN testing and debugging tool, called DeepConcolic, which is able to detect errors with sufficient rigour so as to be applicable to the testing of DNNs in safety-related applications. DeepConcolic is the first tool that implements a concolic testing technique for DNNs, and the first testing tool that provides users with the functionality of investigating particular parts of a DNN. The tool has been made publicly available and a demo video can be found at https://youtu.be/rliynbhoNLM.;https://ieeexplore.ieee.org/abstract/document/8802786/;HkZZF1xrMDIJ
Sun, Y., Huang, X., Kroening, D., Sharp, J., Hill, M., & Ashmore, R. (2019). Structural test coverage criteria for deep neural networks. ACM Transactions on Embedded Computing Systems (TECS), 18(5s), 1-23.;4_dl_testing_deep_network;2019;Structural test coverage criteria for deep neural networks;Youcheng Sun, Xiaowei Huang, Daniel Kroening, James Sharp, Matthew Hill, Rob Ashmore;ACM Transactions on Embedded Computing Systems (TECS) 18 (5s), 1-23, 2019;Deep neural networks (DNNs) have a wide range of applications, and software employing them must be thoroughly tested, especially in safety-critical domains. However, traditional software test coverage metrics cannot be applied directly to DNNs. In this paper, inspired by the MC/DC coverage criterion, we propose a family of four novel test coverage criteria that are tailored to structural features of DNNs and their semantics. We validate the criteria by demonstrating that test inputs that are generated with guidance by our proposed coverage criteria are able to capture undesired behaviours in a DNN. Test cases are generated using a symbolic approach and a gradient-based heuristic search. By comparing them with existing methods, we show that our criteria achieve a balance between their ability to find bugs (proxied using adversarial examples and correlation with functional coverage) and the computational cost of test input generation. Our experiments are conducted on state-of-the-art DNNs obtained using popular open source datasets, including MNIST, CIFAR-10 and ImageNet.;https://dl.acm.org/doi/abs/10.1145/3358233;rj86gd7_2IkJ
Sun, L., & Zhou, Z. Q. (2018, November). Metamorphic testing for machine translations: MT4MT. In 2018 25th Australasian Software Engineering Conference (ASWEC) (pp. 96-100). IEEE.;10_testing_test_machine_metamorphic;2018;Metamorphic testing for machine translations: MT4MT;Liqun Sun, Zhi Quan Zhou;2018 25th Australasian Software Engineering Conference (ASWEC), 96-100, 2018;Automated machine translation software and services have become widely available and increasingly popular. Due to the complexity and flexibility of natural languages, automated testing and quality assessment of this type of software is extremely challenging, especially in the absence of a human oracle or a reference translation. Furthermore, even if a reference translation is available, some major evaluation metrics, such as BLEU, are not reliable on short sentences, the type of sentence now prevailing on the Internet. To alleviate these problems, we have been using a metamorphic testing technique to test machine translation services in a fully automatic way without the involvement of any human assessor or reference translation. This article reports on our progress, and presents some interesting preliminary experimental results that reveal quality issues of English-to-Chinese translations in two mainstream machine translation services: Google Translate and Microsoft Translator. These preliminary results demonstrate the usefulness and potential of metamorphic testing for applications in the natural language processing domain.;https://ieeexplore.ieee.org/abstract/document/8587292/;rW6OISec7zcJ
Tao, C., Gao, J., & Wang, T. (2019). Testing and quality validation for ai software–perspectives, issues, and practices. IEEE Access, 7, 120164-120175.;10_testing_test_machine_metamorphic;2019;Testing and quality validation for ai software–perspectives, issues, and practices;Chuanqi Tao, Jerry Gao, Tiexin Wang;IEEE Access 7, 120164-120175, 2019;With the fast growth of artificial intelligence and big data computing technologies, more and more software service systems have been developed using diverse machine learning models and technologies to make business and intelligent decisions based on their multimedia input to achieve intelligent features, such as image recognition, recommendation, decision making, prediction, etc. Nevertheless, there are increasing quality problems resulting in erroneous testing costs in enterprises and businesses. Existing work seldom discusses how to perform testing and quality validation for AI software. This paper focuses on quality validation for AI software function features. The paper provides our understanding of AI software testing for new features and requirements. In addition, current AI software testing categories are presented and different testing approaches are discussed. Moreover, test quality assessment and criteria analysis are illustrated. Furthermore, a practical study on quality validation for an image recognition system is performed through a metamorphic testing method. Study results show the feasibility and effectiveness of the approach.;https://ieeexplore.ieee.org/abstract/document/8811507/;QTdIxSkoJI0J
Tian, Y., Zeng, Z., Wen, M., Liu, Y., Kuo, T. Y., & Cheung, S. C. (2020, June). EvalDNN: A toolbox for evaluating deep neural network models. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings (pp. 45-48).;4_dl_testing_deep_network;2020;EvalDNN: A toolbox for evaluating deep neural network models;Yongqiang Tian, Zhihua Zeng, Ming Wen, Yepang Liu, Tzu-yang Kuo, Shing-Chi Cheung;Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings, 45-48, 2020;Recent studies have shown that the performance of deep learning models should be evaluated using various important metrics such as robustness and neuron coverage, besides the widely-used prediction accuracy metric. However, major deep learning frameworks currently only provide APIs to evaluate a model's accuracy. In order to comprehensively assess a deep learning model, framework users and researchers often need to implement new metrics by themselves, which is a tedious job. What is worse, due to the large number of hyper-parameters and inadequate documentation, evaluation results of some deep learning models are hard to reproduce, especially when the models and metrics are both new.To ease the model evaluation in deep learning systems, we have developed EvalDNN, a user-friendly and extensible toolbox supporting multiple frameworks and metrics with a set of carefully designed APIs. Using EvalDNN, evaluation of a pre-trained model with respect to different metrics can be done with a few lines of code. We have evaluated EvalDNN on 79 models from TensorFlow, Keras, GluonCV, and PyTorch. As a result of our effort made to reproduce the evaluation results of existing work, we release a performance benchmark of popular models, which can be a useful reference to facilitate future research. The tool and benchmark are available at https://github.com/yqtianust/EvalDNN and https://yqtianust.github.io/EvalDNN-benchmark/, respectively. A demo video of EvalDNN is available at: https://youtu.be/v69bNJN2bJc.;https://dl.acm.org/doi/abs/10.1145/3377812.3382133;krpRr-wiQboJ
Tizpaz-Niari, S., Černý, P., & Trivedi, A. (2020, July). Detecting and understanding real-world differential performance bugs in machine learning libraries. In Proceedings of the 29th ACM SIGSOFT international symposium on software testing and analysis (pp. 189-199).;10_testing_test_machine_metamorphic;2020;Detecting and understanding real-world differential performance bugs in machine learning libraries;Saeid Tizpaz-Niari, Pavol Černý, Ashutosh Trivedi;Proceedings of the 29th ACM SIGSOFT international symposium on software testing and analysis, 189-199, 2020;Programming errors that degrade the performance of systems are widespread, yet there is very little tool support for finding and diagnosing these bugs. We present a method and a tool based on differential performance analysis---we find inputs for which the performance varies widely, despite having the same size. To ensure that the differences in the performance are robust (i.e. hold also for large inputs), we compare the performance of not only single inputs, but of classes of inputs, where each class has similar inputs parameterized by their size. Thus, each class is represented by a performance function from the input size to performance. Importantly, we also provide an explanation for why the performance differs in a form that can be readily used to fix a performance bug. The two main phases in our method are discovery with fuzzing and explanation with decision tree classifiers, each of which is supported by clustering. First, we propose an evolutionary fuzzing algorithm to generate inputs that characterize different performance functions. For this fuzzing task, the unique challenge is that we not only need the input class with the worst performance, but rather a set of classes exhibiting differential performance. We use clustering to merge similar input classes which significantly improves the efficiency of our fuzzer. Second, we explain the differential performance in terms of program inputs and internals (e.g., methods and conditions). We adapt discriminant learning approaches with clustering and decision trees to localize suspicious code regions. We applied our techniques on a set of micro-benchmarks and real-world machine learning libraries. On a set of micro-benchmarks, we show that our approach outperforms state-of-the-art fuzzers in finding inputs to characterize differential performance. On a set of case-studies, we discover and explain multiple performance bugs in popular machine learning frameworks, for instance in implementations of logistic regression in scikit-learn. Four of these bugs, reported first in this paper, have since been fixed by the developers.;https://dl.acm.org/doi/abs/10.1145/3395363.3404540;ivBXIlTjiA8J
Trujillo, M., Linares-Vásquez, M., Escobar-Velásquez, C., Dusparic, I., & Cardozo, N. (2020, June). Does neuron coverage matter for deep reinforcement learning? a preliminary study. In Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops (pp. 215-220).;4_dl_testing_deep_network;2020;Does neuron coverage matter for deep reinforcement learning? a preliminary study;Miller Trujillo, Mario Linares-Vásquez, Camilo Escobar-Velásquez, Ivana Dusparic, Nicolás Cardozo;Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops, 215-220, 2020;Deep Learning (DL) is powerful family of algorithms used for a wide variety of problems and systems, including safety critical systems. As a consequence, analyzing, understanding, and testing DL models is attracting more practitioners and researchers with the purpose of implementing DL systems that are robust, reliable, efficient, and accurate. First software testing approaches for DL systems have focused on black-box testing, white-box testing, and test cases generation, in particular for deep neural networks (CNNs and RNNs). However, Deep Reinforcement Learning (DRL), which is a branch of DL extending reinforcement learning, is still out of the scope of research providing testing techniques for DL systems. In this paper, we present a first step towards testing of DRL systems. In particular, we investigate whether neuron coverage (a widely used metric for white-box testing of DNNs) could be used also for DRL systems, by analyzing coverage evolutionary patterns, and the correlation with RL rewards.;https://dl.acm.org/doi/abs/10.1145/3387940.3391462;cRjc_DQVgakJ
Udeshi, S., Arora, P., & Chattopadhyay, S. (2018, September). Automated directed fairness testing. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering (pp. 98-108).;6_fairness_discrimination_bias_decision;2018;Automated directed fairness testing;Sakshi Udeshi, Pryanshu Arora, Sudipta Chattopadhyay;Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering, 98-108, 2018;Fairness is a critical trait in decision making. As machine-learning models are increasingly being used in sensitive application domains (e.g. education and employment) for decision making, it is crucial that the decisions computed by such models are free of unintended bias. But how can we automatically validate the fairness of arbitrary machine-learning models? For a given machine-learning model and a set of sensitive input parameters, our Aeqitas approach automatically discovers discriminatory inputs that highlight fairness violation. At the core of Aeqitas are three novel strategies to employ probabilistic search over the input space with the objective of uncovering fairness violation. Our Aeqitas approach leverages inherent robustness property in common machine-learning models to design and implement scalable test generation methodologies. An appealing feature of our generated test inputs is that they can be systematically added to the training set of the underlying model and improve its fairness. To this end, we design a fully automated module that guarantees to improve the fairness of the model. We implemented Aeqitas and we have evaluated it on six stateof- the-art classifiers. Our subjects also include a classifier that was designed with fairness in mind. We show that Aeqitas effectively generates inputs to uncover fairness violation in all the subject classifiers and systematically improves the fairness of respective models using the generated test inputs. In our evaluation, Aeqitas generates up to 70% discriminatory inputs (w.r.t. the total number of inputs generated) and leverages these inputs to improve the fairness up to 94%.;https://dl.acm.org/doi/abs/10.1145/3238147.3238165;nGtSnY1b1h4J
Vogelsang, A., & Borg, M. (2019, September). Requirements engineering for machine learning: Perspectives from data scientists. In 2019 IEEE 27th International Requirements Engineering Conference Workshops (REW) (pp. 245-251). IEEE.;1_ml_machine_data_learning;2019;Requirements engineering for machine learning: Perspectives from data scientists;Andreas Vogelsang, Markus Borg;2019 IEEE 27th International Requirements Engineering Conference Workshops (REW), 245-251, 2019;Machine learning (ML) is used increasingly in real-world applications. In this paper, we describe our ongoing endeavor to define characteristics and challenges unique to Requirements Engineering (RE) for ML-based systems. As a first step, we interviewed four data scientists to understand how ML experts approach elicitation, specification, and assurance of requirements and expectations. The results show that changes in the development paradigm, i.e., from coding to training, also demands changes in RE. We conclude that development of ML systems demands requirements engineers to: (1) understand ML performance measures to state good functional requirements, (2) be aware of new quality requirements such as explainability, freedom from discrimination, or specific legal requirements, and (3) integrate ML specifics in the RE process. Our study provides a first contribution towards an RE methodology for ML systems.;https://ieeexplore.ieee.org/abstract/document/8933800/;NIF3idN0r9QJ
Wang, S., & Su, Z. (2020, December). Metamorphic object insertion for testing object detection systems. In Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering (pp. 1053-1065).;4_dl_testing_deep_network;2020;Metamorphic object insertion for testing object detection systems;Shuai Wang, Zhendong Su;Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering, 1053-1065, 2020;Recent advances in deep neural networks (DNNs) have led to object detectors (ODs) that can rapidly process pictures or videos, and recognize the objects that they contain. Despite the promising progress by industrial manufacturers such as Amazon and Google in commercializing deep learning-based ODs as a standard computer vision service, ODs --- similar to traditional software --- may still produce incorrect results. These errors, in turn, can lead to severe negative outcomes for the users. For instance, an autonomous driving system that fails to detect pedestrians can cause accidents or even fatalities. However, despite their importance, principled, systematic methods for testing ODs do not yet exist.To fill this critical gap, we introduce the design and realization of MetaOD, a metamorphic testing system specifically designed for ODs to effectively uncover erroneous detection results. To this end, we (1) synthesize natural-looking images by inserting extra object instances into background images, and (2) design metamorphic conditions asserting the equivalence of OD results between the original and synthetic images after excluding the prediction results on the inserted objects. MetaOD is designed as a streamlined workflow that performs object extraction, selection, and insertion. We develop a set of practical techniques to realize an effective workflow, and generate diverse, natural-looking images for testing. Evaluated on four commercial OD services and four pretrained models provided by the TensorFlow API, MetaOD found tens of thousands of detection failures. To further demonstrate the practical usage of MetaOD, we use the synthetic images that cause erroneous detection results to retrain the model. Our results show that the model performance is significantly increased, from an mAP score of 9.3 to an mAP score of 10.5.;https://dl.acm.org/doi/abs/10.1145/3324884.3416584;uVfpmhgHv2sJ
Wang, H., Xu, J., Xu, C., Ma, X., & Lu, J. (2020, June). Dissector: Input validation for deep learning applications by crossing-layer dissection. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (pp. 727-738).;4_dl_testing_deep_network;2020;Dissector: Input validation for deep learning applications by crossing-layer dissection;Huiyan Wang, Jingwei Xu, Chang Xu, Xiaoxing Ma, Jian Lu;Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, 727-738, 2020;Deep learning (DL) applications are becoming increasingly popular. Their reliabilities largely depend on the performance of DL models integrated in these applications as a central classifying module. Traditional techniques need to retrain the models or rebuild and redeploy the applications for coping with unexpected conditions beyond the models' handling capabilities. In this paper, we take a fault tolerance approach, Dissector, to distinguishing those inputs that represent unexpected conditions (beyond-inputs) from normal inputs that are still within the models' handling capabilities (within-inputs), thus keeping the applications still function with expected reliabilities. The key insight of Dissector is that a DL model should interpret a within-input with increasing confidence, while a beyond-input would probably cause confused guesses in the prediction process. Dissector works in an application-specific way, adaptive to DL models used in applications, and extremely efficiently, scalable to large-size datasets from complex scenarios. The experimental evaluation shows that Dissector outperformed state-of-the-art techniques in the effectiveness (AUC: avg. 0.8935 and up to 0.9894) and efficiency (runtime overhead: only 3.3--5.8 milliseconds). Besides, it also exhibited encouraging usefulness in defensing against adversarial inputs (AUC: avg. 0.9983) and improving a DL model's actual accuracy in use (up to 16% for CIFAR-100 and 20% for ImageNet).;https://dl.acm.org/doi/abs/10.1145/3377811.3380379;LMvyWCSz2EIJ
Wang, Z., Yan, M., Chen, J., Liu, S., & Zhang, D. (2020, November). Deep learning library testing via effective model generation. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 788-799).;4_dl_testing_deep_network;2020;Deep learning library testing via effective model generation;Zan Wang, Ming Yan, Junjie Chen, Shuang Liu, Dongdi Zhang;Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 788-799, 2020;"Deep learning (DL) techniques are rapidly developed and have been widely adopted in practice. However, similar to traditional software systems, DL systems also contain bugs, which could cause serious impacts especially in safety-critical domains. Recently, many research approaches have focused on testing DL models, while little attention has been paid for testing DL libraries, which is the basis of building DL models and directly affects the behavior of DL systems. In this work, we propose a novel approach, LEMON, to testing DL libraries. In particular, we (1) design a series of mutation rules for DL models, with the purpose of exploring different invoking sequences of library code and hard-to-trigger behaviors; and (2) propose a heuristic strategy to guide the model generation process towards the direction of amplifying the inconsistent degrees of the inconsistencies between different DL libraries caused by bugs, so as to mitigate the impact of potential noise introduced by uncertain factors in DL libraries. We conducted an empirical study to evaluate the effectiveness of LEMON with 20 release versions of 4 widely-used DL libraries, i.e., TensorFlow, Theano, CNTK, MXNet. The results demonstrate that LEMON detected 24 new bugs in the latest release versions of these libraries, where 7 bugs have been confirmed and one bug has been fixed by developers. Besides, the results confirm that the heuristic strategy for model generation indeed effectively guides LEMON in amplifying the inconsistent degrees for bugs.";https://dl.acm.org/doi/abs/10.1145/3368089.3409761;1kYif9EqOBwJ
Wu, X., Qin, L., Yu, B., Xie, X., Ma, L., Xue, Y., ... & Zhao, J. (2020, July). How are deep learning models similar? an empirical study on clone analysis of deep learning software. In Proceedings of the 28th International Conference on Program Comprehension (pp. 172-183).;4_dl_testing_deep_network;2020;How are deep learning models similar? an empirical study on clone analysis of deep learning software;Xiongfei Wu, Liangyu Qin, Bing Yu, Xiaofei Xie, Lei Ma, Yinxing Xue, Yang Liu, Jianjun Zhao;Proceedings of the 28th International Conference on Program Comprehension, 172-183, 2020;"Deep learning (DL) has been successfully applied to many cutting-edge applications, e.g., image processing, speech recognition, and natural language processing. As more and more DL software is made open-sourced, publicly available, and organized in model repositories and stores (Model Zoo, ModelDepot), there comes a need to understand the relationships of these DL models regarding their maintenance and evolution tasks. Although clone analysis has been extensively studied for traditional software, up to the present, clone analysis has not been investigated for DL software. Since DL software adopts the data-driven development paradigm, it is still not clear whether and to what extent the clone analysis techniques of traditional software could be adapted to DL software.In this paper, we initiate the first step on the clone analysis of DL software at three different levels, i.e., source code level, model structural level, and input/output (I/0)-semantic level, which would be a key in DL software management, maintenance and evolution. We intend to investigate the similarity between these DL models from clone analysis perspective. Several tools and metrics are selected to conduct clone analysis of DL software at three different levels. Our study on two popular datasets (i.e., MNIST and CIFAR-10) and eight DL models of five architectural families (i.e., LeNet, ResNet, DenseNet, AlexNet, and VGG) shows that: 1). the three levels of similarity analysis are generally adequate to find clones between DL models ranging from structural to semantic; 2). different measures for clone analysis used at each level yield similar results; 3) clone analysis of one single level may not render a complete picture of the similarity of DL models. Our findings open up several research opportunities worth further exploration towards better understanding and more effective clone analysis of DL software.";https://dl.acm.org/doi/abs/10.1145/3387904.3389254;rHY2SzuZlBkJ
Xie, X., Ma, L., Juefei-Xu, F., Xue, M., Chen, H., Liu, Y., ... & See, S. (2019, July). Deephunter: a coverage-guided fuzz testing framework for deep neural networks. In Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis (pp. 146-157).;4_dl_testing_deep_network;2019;Deephunter: a coverage-guided fuzz testing framework for deep neural networks;Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu, Jianjun Zhao, Bo Li, Jianxiong Yin, Simon See;Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis, 146-157, 2019;"The past decade has seen the great potential of applying deep neural network (DNN) based software to safety-critical scenarios, such as autonomous driving. Similar to traditional software, DNNs could exhibit incorrect behaviors, caused by hidden defects, leading to severe accidents and losses. In this paper, we propose DeepHunter, a coverage-guided fuzz testing framework for detecting potential defects of general-purpose DNNs. To this end, we first propose a metamorphic mutation strategy to generate new semantically preserved tests, and leverage multiple extensible coverage criteria as feedback to guide the test generation. We further propose a seed selection strategy that combines both diversity-based and recency-based seed selection. We implement and incorporate 5 existing testing criteria and 4 seed selection strategies in DeepHunter. Large-scale experiments demonstrate that (1) our metamorphic mutation strategy is useful to generate new valid tests with the same semantics as the original seed, by up to a 98% validity ratio; (2) the diversity-based seed selection generally weighs more than recency-based seed selection in boosting the coverage and in detecting defects; (3) DeepHunter outperforms the state of the arts by coverage as well as the quantity and diversity of defects identified; (4) guided by corner-region based criteria, DeepHunter is useful to capture defects during the DNN quantization for platform migration.";https://dl.acm.org/doi/abs/10.1145/3293882.3330579;uBBxhVp39v0J
Yan, S., Tao, G., Liu, X., Zhai, J., Ma, S., Xu, L., & Zhang, X. (2020, November). Correlations between deep neural network model coverage criteria and model quality. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 775-787).;4_dl_testing_deep_network;2020;Correlations between deep neural network model coverage criteria and model quality;Shenao Yan, Guanhong Tao, Xuwei Liu, Juan Zhai, Shiqing Ma, Lei Xu, Xiangyu Zhang;Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 775-787, 2020;Inspired by the great success of using code coverage as guidance in software testing, a lot of neural network coverage criteria have been proposed to guide testing of neural network models (e.g., model accuracy under adversarial attacks). However, while the monotonic relation between code coverage and software quality has been supported by many seminal studies in software engineering, it remains largely unclear whether similar monotonicity exists between neural network model coverage and model quality. This paper sets out to answer this question. Specifically, this paper studies the correlation between DNN model quality and coverage criteria, effects of coverage guided adversarial example generation compared with gradient decent based methods, effectiveness of coverage based retraining compared with existing adversarial training, and the internal relationships among coverage criteria.;https://dl.acm.org/doi/abs/10.1145/3368089.3409671;PrL1nuqBl34J
Zhang, Y., Chen, Y., Cheung, S. C., Xiong, Y., & Zhang, L. (2018, July). An empirical study on TensorFlow program bugs. In Proceedings of the 27th ACM SIGSOFT international symposium on software testing and analysis (pp. 129-140).;4_dl_testing_deep_network;2018;An empirical study on TensorFlow program bugs;Yuhao Zhang, Yifan Chen, Shing-Chi Cheung, Yingfei Xiong, Lu Zhang;Proceedings of the 27th ACM SIGSOFT international symposium on software testing and analysis, 129-140, 2018;Deep learning applications become increasingly popular in important domains such as self-driving systems and facial identity systems. Defective deep learning applications may lead to catastrophic consequences. Although recent research efforts were made on testing and debugging deep learning applications, the characteristics of deep learning defects have never been studied. To fill this gap, we studied deep learning applications built on top of TensorFlow and collected program bugs related to TensorFlow from StackOverflow QA pages and Github projects. We extracted information from QA pages, commit messages, pull request messages, and issue discussions to examine the root causes and symptoms of these bugs. We also studied the strategies deployed by TensorFlow users for bug detection and localization. These findings help researchers and TensorFlow users to gain a better understanding of coding defects in TensorFlow programs and point out a new direction for future research.;https://dl.acm.org/doi/abs/10.1145/3213846.3213866;4hwzBRnT3tIJ
Zhang, R., Xiao, W., Zhang, H., Liu, Y., Lin, H., & Yang, M. (2020, June). An empirical study on program failures of deep learning jobs. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (pp. 1159-1170).;4_dl_testing_deep_network;2020;An empirical study on program failures of deep learning jobs;Ru Zhang, Wencong Xiao, Hongyu Zhang, Yu Liu, Haoxiang Lin, Mao Yang;Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, 1159-1170, 2020;"Deep learning has made significant achievements in many application areas. To train and test models more efficiently, enterprise developers submit and run their deep learning programs on a shared, multi-tenant platform. However, some of the programs fail after a long execution time due to code/script defects, which reduces the development productivity and wastes expensive resources such as GPU, storage, and network I/O.This paper presents the first comprehensive empirical study on program failures of deep learning jobs. 4960 real failures are collected from a deep learning platform in Microsoft. We manually examine their failure messages and classify them into 20 categories. In addition, we identify the common root causes and bug-fix solutions on a sample of 400 failures. To better understand the current testing and debugging practices for deep learning, we also conduct developer interviews. Our major findings include: (1) 48.0% of the failures occur in the interaction with the platform rather than in the execution of code logic, mostly due to the discrepancies between local and platform execution environments; (2) Deep learning specific failures (13.5%) are mainly caused by inappropriate model parameters/structures and framework API misunderstanding; (3) Current debugging practices are not efficient for fault localization in many cases, and developers need more deep learning specific tools. Based on our findings, we further suggest possible research topics and tooling support that could facilitate future deep learning development.";https://dl.acm.org/doi/abs/10.1145/3377811.3380362;tmJJALiYrBwJ
Zhang, X., Xie, X., Ma, L., Du, X., Hu, Q., Liu, Y., ... & Sun, M. (2020, June). Towards characterizing adversarial defects of deep learning software from the lens of uncertainty. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (pp. 739-751).;4_dl_testing_deep_network;2020;Towards characterizing adversarial defects of deep learning software from the lens of uncertainty;Xiyue Zhang, Xiaofei Xie, Lei Ma, Xiaoning Du, Qiang Hu, Yang Liu, Jianjun Zhao, Meng Sun;Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, 739-751, 2020;Over the past decade, deep learning (DL) has been successfully applied to many industrial domain-specific tasks. However, the current state-of-the-art DL software still suffers from quality issues, which raises great concern especially in the context of safety- and security-critical scenarios. Adversarial examples (AEs) represent a typical and important type of defects needed to be urgently addressed, on which a DL software makes incorrect decisions. Such defects occur through either intentional attack or physical-world noise perceived by input sensors, potentially hindering further industry deployment. The intrinsic uncertainty nature of deep learning decisions can be a fundamental reason for its incorrect behavior. Although some testing, adversarial attack and defense techniques have been recently proposed, it still lacks a systematic study to uncover the relationship between AEs and DL uncertainty.In this paper, we conduct a large-scale study towards bridging this gap. We first investigate the capability of multiple uncertainty metrics in differentiating benign examples (BEs) and AEs, which enables to characterize the uncertainty patterns of input data. Then, we identify and categorize the uncertainty patterns of BEs and AEs, and find that while BEs and AEs generated by existing methods do follow common uncertainty patterns, some other uncertainty patterns are largely missed. Based on this, we propose an automated testing technique to generate multiple types of uncommon AEs and BEs that are largely missed by existing techniques. Our further evaluation reveals that the uncommon data generated by our method is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%. Our results call for attention and necessity to generate more diverse data for evaluating quality assurance solutions of DL software.;https://dl.acm.org/doi/abs/10.1145/3377811.3380368;1Vw5c5_hrFYJ
Zhang, X., Yin, Z., Feng, Y., Shi, Q., Liu, J., & Chen, Z. (2019, November). NeuralVis: visualizing and interpreting deep learning models. In 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE) (pp. 1106-1109). IEEE.;4_dl_testing_deep_network;2019;NeuralVis: visualizing and interpreting deep learning models;Xufan Zhang, Ziyue Yin, Yang Feng, Qingkai Shi, Jia Liu, Zhenyu Chen;2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE), 1106-1109, 2019;"Deep Neural Network(DNN) techniques have been prevalent in software engineering. They are employed to facilitate various software engineering tasks and embedded into many software applications. However, because DNNs are built upon a rich data-driven programming paradigm that employs plenty of labeled data to train a set of neurons to construct the internal system logic, analyzing and understanding their behaviors becomes a difficult task for software engineers. In this paper, we present an instance-based visualization tool for DNN, namely NeuralVis, to support software engineers in visualizing and interpreting deep learning models. NeuralVis is designed for: 1). visualizing the structure of DNN models, i.e., neurons, layers, as well as connections; 2). visualizing the data transformation process; 3). integrating existing adversarial attack algorithms for test input generation; 4). comparing intermediate layers' outputs of different inputs. To demonstrate the effectiveness of NeuralVis, we design a task-based user study involving ten participants on two classic DNN models, i.e., LeNet and VGG-12. The result shows NeuralVis can assist engineers in identifying critical features that determine the prediction results. Video: https://youtu.be/solkJri4Z44";https://ieeexplore.ieee.org/abstract/document/8952427/;VyRVTY67FYoJ
Zheng, W., Wang, W., Liu, D., Zhang, C., Zeng, Q., Deng, Y., ... & Xie, T. (2019, May). Testing untestable neural machine translation: An industrial case. In 2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion) (pp. 314-315). IEEE.;10_testing_test_machine_metamorphic;2019;Testing untestable neural machine translation: An industrial case;Wujie Zheng, Wenyu Wang, Dian Liu, Changrong Zhang, Qinsong Zeng, Yuetang Deng, Wei Yang, Pinjia He, Tao Xie;2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), 314-315, 2019;Neural Machine Translation (NMT) has shown great advantages and is becoming increasingly popular. However, in practice, NMT often produces unexpected translation failures in its translations. While reference-based black-box system testing has been a common practice for NMT quality assurance during development, an increasingly critical industrial practice, named in-vivo testing, exposes unseen types or instances of translation failures when real users are using a deployed industrial NMT system. To fill the gap of lacking test oracles for in-vivo testing of NMT systems, we propose a new methodology for automatically identifying translation failures without reference translations. Our evaluation conducted on real-world datasets shows that our methodology effectively detects several targeted types of translation failures. Our experiences on deploying our methodology in both production and development environments of WeChat (a messenger app with over one billion monthly active users) demonstrate high effectiveness of our methodology along with high industry impact.;https://ieeexplore.ieee.org/abstract/document/8802818/;rh1eS03kN3MJ
Zhou, J., Li, F., Dong, J., Zhang, H., & Hao, D. (2020, October). Cost-effective testing of a deep learning model through input reduction. In 2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE) (pp. 289-300). IEEE.;4_dl_testing_deep_network;2020;Cost-effective testing of a deep learning model through input reduction;Jianyi Zhou, Feng Li, Jinhao Dong, Hongyu Zhang, Dan Hao;2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE), 289-300, 2020;With the increasing adoption of Deep Learning (DL) models in various applications, testing DL models is vitally important. However, testing DL models is costly and expensive, e.g., manual labelling is widely-recognized to be costly. To reduce testing cost, we propose to select only a subset of testing data, which is small but representative enough for a quick estimation of the performance of DL models. Our approach, DeepReduce, adopts a two-phase strategy. At first, our approach selects testing data for the purpose of satisfying testing adequacy. Then, it selects more testing data to approximate the distribution between the whole testing data and the selected data by leveraging relative entropy minimization. We evaluate DeepReduce on four widely-used datasets (with 15 models in total). We find that DeepReduce reduces the whole testing data to 7.5% on average and can reliably estimate the performance of DL models.;https://ieeexplore.ieee.org/abstract/document/9251075/;nCdCeVx8s3cJ
Zhou, L., Yu, B., Berend, D., Xie, X., Li, X., Zhao, J., & Liu, X. (2020, December). An empirical study on robustness of DNNs with out-of-distribution awareness. In 2020 27th Asia-Pacific Software Engineering Conference (APSEC) (pp. 266-275). IEEE.;4_dl_testing_deep_network;2020;An empirical study on robustness of DNNs with out-of-distribution awareness;Lingjun Zhou, Bing Yu, David Berend, Xiaofei Xie, Xiaohong Li, Jianjun Zhao, Xusheng Liu;2020 27th Asia-Pacific Software Engineering Conference (APSEC), 266-275, 2020;The state-of-the-art deep neural network (DNN) achieves impressive performance on the input that is similar to training data. However, it fails to make reasonable decisions on the input that is quite different from training data, i.e., out-of-distribution (OOD) examples. Although many techniques have been proposed to detect OOD examples in recent years, it is still a lack of a systematic study about the effectiveness and robustness of different techniques as well as the performance of OOD-aware DNN models. In this paper, we conduct a comprehensive study to unveil the mystery of current OOD detection techniques, and investigate the differences between OOD-unaware/-aware DNNs in model performance, robustness, and uncertainty. We first compare the effectiveness of existing detection techniques and identify the best one. Then, evasion attacks are performed to evaluate the robustness of techniques. Furthermore, we compare the accuracy and robustness between OOD-unaware/-aware DNNs. At last, we study the uncertainty of different models on various kinds of data. Empirical results show OOD-aware detection modules have better performance and are more robust against random noises and evasion attacks. OOD-awareness seldom degrades the accuracy of DNN models in training/test datasets. In contrast, it makes the DNN model more robust against adversarial attacks and noisy inputs. Our study calls for attention to the development of OOD-aware DNN models and the necessity to take data distribution into account when robust and reliable DNN models are desired.;https://ieeexplore.ieee.org/abstract/document/9359272/;2EvrBYzL1WsJ
Nakamichi, K., Ohashi, K., Namba, I., Yamamoto, R., Aoyama, M., Joeckel, L., ... & Heidrich, J. (2020, August). Requirements-driven method to determine quality characteristics and measurements for machine learning software and its evaluation. In 2020 IEEE 28th International Requirements Engineering Conference (RE) (pp. 260-270). IEEE.;1_ml_machine_data_learning;2020;Requirements-driven method to determine quality characteristics and measurements for machine learning software and its evaluation;Koji Nakamichi, Kyoko Ohashi, Isao Namba, Rieko Yamamoto, Mikio Aoyama, Lisa Joeckel, Julien Siebert, Jens Heidrich;2020 IEEE 28th International Requirements Engineering Conference (RE), 260-270, 2020;"As the applications of machine learning algorithms in various fields are widely demanded, the development of machine learning software systems (MLS) is rapidly increasing. The quality of MLS is different from that of conventional software systems, in the sense that it depends on the amount and distribution of training data in a model learning and input data during operation. This is a major challenge in quality assurance of MLS development for the enterprise. In this paper, we propose a requirements-driven method to determine the quality characteristics of the MLS. Major contributions of this paper include: (1) Extending the quality characteristics of ISO 25010, which defines the conventional software quality, to those unique to MLS; this paper also defines its measuring method. (2) A method to identify requirements, i.e., issues to be determined in the requirements definition, in order to derive the quality characteristics and measurement methods for MLS, since the quality characteristics and the measurement method depend on the goals of the system under development. In order to evaluate the proposed method, we carried out an empirical study of the quality characteristics and measurement methods related to functional correctness and the maturity of the MLS for the enterprise. Based on the study, we compare the quality characteristics and measurement methods derived by the proposed method with those suggested by developers, and demonstrate the effectiveness of the proposed method.";https://ieeexplore.ieee.org/abstract/document/9218162/;JvlYxncjApIJ
Menzies, T. (2019). The five laws of SE for AI. IEEE Software, 37(1), 81-85.;1_ml_machine_data_learning;2019;The five laws of SE for AI;Tim Menzies;IEEE Software 37 (1), 81-85, 2019;It is time to talk about software engineering (SE) for artificial intelligence (AI). As shown in Figure 1, industry is becoming increasingly dependent on AI software. Clearly, AI is useful for SE. But what about the other way around? How important is SE for AI? Many thought leaders in the AI industry are asking how to better develop and maintain AI software (see Figure 2).;https://ieeexplore.ieee.org/abstract/document/8938116/;mpx2Gp9Bc5kJ
Pedroza, G., & Adedjouma, M. (2019, July). Safe-by-design development method for artificial intelligent based systems. In SEKE 2019: The 31st International Conference on Software Engineering and Knowledge Engineering (pp. 391-397).;2_safety_system_autonomous_vehicle;2019;Safe-by-design development method for artificial intelligent based systems;Gabriel Pedroza, Morayo Adedjouma;SEKE 2019: The 31st International Conference on Software Engineering and Knowledge Engineering, 391-397, 2019;Albeit Artificial Intelligent (AI) based systems are nowadays deployed in a variety of safety critical domains, current engineering methods and standards are barely applicable for their development and assurance. The lack of common criteria to assess safety levels as well as the dependency of certain development phases w.r.t. the chosen technology (e.g., machine learning modules) are among the identified drawbacks. In addition, the development of such engineering methods has been hampered by the emerging challenges in AI-based systems design mainly regarding autonomy, correctness and prevention of catastrophic risks. In this paper we propose an approach to conduct a safe-by-design development process for AI based systems. The approach relies upon a method which benefits from a reference AI architecture and safety principles. This contribution helps to address safety concerns and to comprehend current AI architectures diversity and particularities.;https://hal-cea.archives-ouvertes.fr/cea-02273349/;9i256WnvV30J
Mattos, D. I., Bosch, J., & Olsson, H. H. (2019, October). Leveraging business transformation with machine learning experiments. In International Conference on Software Business (pp. 183-191). Cham: Springer International Publishing.;1_ml_machine_data_learning;2019;Leveraging business transformation with machine learning experiments;David Issa Mattos, Jan Bosch, Helena HolmstrÃ¶m Olsson;International Conference on Software Business, 183-191, 2019;The deployment of production-quality ML solutions, even for simple applications, requires significant software engineering effort. Often, companies do not fully understand the consequences and the business impact of ML-based systems, prior to the development of these systems. To minimize investment risks while evaluating the potential business impact of an ML system, companies can utilize continuous experimentation techniques. Based on action research, we report on the experience of developing and deploying a business-oriented ML-based dynamic pricing system in collaboration with a home shopping e-commerce company using a continuous experimentation (CE) approach. We identified a set of generic challenges in ML development that we present together with tactics and opportunities.;https://link.springer.com/chapter/10.1007/978-3-030-33742-1_15;RheSylk8_8EJ
Dreossi, T., Fremont, D. J., Ghosh, S., Kim, E., Ravanbakhsh, H., Vazquez-Chanlatte, M., & Seshia, S. A. (2019, July). Verifai: A toolkit for the formal design and analysis of artificial intelligence-based systems. In International Conference on Computer Aided Verification (pp. 432-442). Cham: Springer International Publishing.;2_safety_system_autonomous_vehicle;2019;Verifai: A toolkit for the formal design and analysis of artificial intelligence-based systems;Tommaso Dreossi, Daniel J Fremont, Shromona Ghosh, Edward Kim, Hadi Ravanbakhsh, Marcell Vazquez-Chanlatte, Sanjit A Seshia;International Conference on Computer Aided Verification, 432-442, 2019;We present VerifAI, a software toolkit for the formal design and analysis of systems that include artificial intelligence (AI) and machine learning (ML) components. VerifAI particularly addresses challenges with applying formal methods to ML components such as perception systems based on deep neural networks, as well as systems containing them, and to model and analyze system behavior in the presence of environment uncertainty. We describe the initial version of VerifAI, which centers on simulation-based verification and synthesis, guided by formal models and specifications. We give examples of several use cases, including temporal-logic falsification, model-based systematic fuzz testing, parameter synthesis, counterexample analysis, and data set augmentation.;https://link.springer.com/chapter/10.1007/978-3-030-25540-4_25;ekSFrUHAjC4J
Nakajima, S., & Chen, T. Y. (2019). Generating biased dataset for metamorphic testing of machine learning programs. In Testing Software and Systems: 31st IFIP WG 6.1 International Conference, ICTSS 2019, Paris, France, October 15–17, 2019, Proceedings 31 (pp. 56-64). Springer International Publishing.;10_testing_test_machine_metamorphic;2019;Generating biased dataset for metamorphic testing of machine learning programs;Shin Nakajima, Tsong Yueh Chen;Testing Software and Systems: 31st IFIP WG 6.1 International Conference, ICTSS 2019, Paris, France, October 15â€“17, 2019, Proceedings 31, 56-64, 2019;Although both positive and negative testing are important for assuring quality of programs, generating a variety of test inputs for such testing purposes is difficult for machine learning software. This paper studies why it is difficult, and then proposes a new method of generating datasets that are test inputs to machine learning programs. The proposed idea is demonstrated with a case study of classifying hand-written numbers.;https://link.springer.com/chapter/10.1007/978-3-030-31280-0_4;1zIjvwviTwMJ
Zhao, X., & Gao, X. (2018, July). An ai software test method based on scene deductive approach. In 2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C) (pp. 14-20). IEEE.;2_safety_system_autonomous_vehicle;2018;An ai software test method based on scene deductive approach;Xinghan Zhao, Xiangfei Gao;2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C), 14-20, 2018;Artificial intelligence (AI) software has high algorithm complexity, and the scale and dimension of the input and output parameters are high, and the test oracle isn't explicit. These features make a lot of difficulties for the design of test cases. This paper proposes an AI software testing method based on scene deductive approach. It models the input, output parameters and the environment, uses the random algorithm to generate the inputs of the test cases, then use the algorithm of deductive approach to make the software testing automatically, and use the test assertions to verify the results of the test. After description of the theory, this paper uses intelligent tracking car as an example to illustrate the application of this method and the problems needing attention. In the end, the paper describes the shortcoming of this method and the future research directions.;https://ieeexplore.ieee.org/abstract/document/8431945/;0ARckgLXBa4J
Xie, T. (2018, February). Intelligent software engineering: Synergy between AI and software engineering. In Proceedings of the 11th Innovations in Software Engineering Conference (pp. 1-1).;1_ml_machine_data_learning;2018;Intelligent software engineering: Synergy between AI and software engineering;Tao Xie;Proceedings of the 11th Innovations in Software Engineering Conference, 1-1, 2018;"There has been a long history of applying AI technologies to address software engineering problems especially on tool automation. On the other hand, given the increasing importance and popularity of AI software, recent research efforts have been on exploring software engineering solutions to improve the productivity of developing AI software and the dependability of AI software. The emerging field of intelligent software engineering is to focus on two aspects: (1) instilling intelligence in solutions for software engineering problems; (2) providing software engineering solutions for intelligent software. This extended abstract shares perspectives on these two aspects of intelligent software engineering.";https://dl.acm.org/doi/abs/10.1145/3172871.3172891;nTiCEOvGVOEJ
Osman, M. H., Kugele, S., & Shafaei, S. (2019, December). Run-time safety monitoring framework for AI-based systems: Automated driving cases. In 2019 26th Asia-Pacific Software Engineering Conference (APSEC) (pp. 442-449). IEEE.;2_safety_system_autonomous_vehicle;2019;Run-time safety monitoring framework for AI-based systems: Automated driving cases;Mohd Hafeez Osman, Stefan Kugele, Sina Shafaei;2019 26th Asia-Pacific Software Engineering Conference (APSEC), 442-449, 2019;"Intelligent systems based on artificial intelligence techniques are increasing and are recently being accepted in the automotive domain. In the competition of automobile makers to provide fully automated vehicles, it is perceived that artificial intelligence will profoundly influence the automotive electric and electronic architecture in the future. However, while such systems provide highly advanced functions, safety risk increases as AI-based systems may produce uncertain output and behaviour. In this paper, we devise a run-time safety monitoring framework for AI-based intelligence systems focusing on autonomous driving functions. In detail, this paper describes (i) the characteristics of a safety monitoring framework; (ii) the safety monitoring framework itself, and (iii) we develop a prototype and implement the framework for two critical driving functions: Lane detection and object detection. Through an implementation of the framework to a prototypic control environment, we show the possibility of this framework in the real context. Finally, we discuss the techniques used in developing the safety monitoring framework and describes the encountered challenges.";https://ieeexplore.ieee.org/abstract/document/8945611/;SWXhjNb5o-kJ
Koseler, K., McGraw, K., & Stephan, M. (2019, February). Realization of a Machine Learning Domain Specific Modeling Language: A Baseball Analytics Case Study. In MODELSWARD (pp. 13-24).;1_ml_machine_data_learning;2019;Realization of a Machine Learning Domain Specific Modeling Language: A Baseball Analytics Case Study;Kaan Koseler, Kelsea McGraw, Matthew Stephan;MODELSWARD, 13-24, 2019;"Accompanying the Big Data (BD) paradigm is a resurgence in machine learning (ML). Using ML techniques to work with BD is a complex task, requiring specialized knowledge of the problem space, domain specific concepts, and appropriate ML approaches. However, specialists who possess that knowledge and programming ability are difficult to find and expensive to train. Model-Driven Engineering (MDE) allows developers to implement quality software through modeling using high-level domain specific concepts. In this research, we attempt to fill the gap between MDE and the industrial need for development of ML software by demonstrating the plausibility of applying MDE to BD. Specifically, we apply MDE to the setting of the thriving industry of professional baseball analytics. Our case study involves developing an MDE solution for the binary classification problem of predicting if a baseball pitch will be a fastball. We employ and refine an existing, but untested, ML Domain-Specific Modeling Language (DSML); devise model instances representing prediction features; create a code generation scheme; and evaluate our solution. We show our MDE solution is comparable to the one developed through traditional programming, distribute all our artifacts for public use and extension, and discuss the impact of our work and lessons we learned.";https://www.scitepress.org/Papers/2019/72458/72458.pdf;CwYyBZuJAusJ
Coates, D. L., & Martin, A. (2019). An instrument to evaluate the maturity of bias governance capability in artificial intelligence projects. IBM Journal of Research and Development, 63(4/5), 7-1.;12_ai_ethical_ethic_intelligence;2019;An instrument to evaluate the maturity of bias governance capability in artificial intelligence projects;Daphne L Coates, Andrew Martin;IBM Journal of Research and Development 63 (4/5), 7: 1-7: 15, 2019;Artificial intelligence (AI) promises unprecedented contributions to both business and society, attracting a surge of interest from many organizations. However, there is evidence that bias is already prevalent in AI datasets and algorithms, which, albeit unintended, is considered to be unethical, suboptimal, unsustainable, and challenging to manage. It is believed that the governance of data and algorithmic bias must be deeply embedded in the values, mindsets, and procedures of AI software development teams, but currently there is a paucity of actionable mechanisms to help. In this paper, we describe a maturity framework based on ethical principles and best practices, which can be used to evaluate an organization's capability to govern bias. We also design, construct, validate, and test an original instrument for operationalizing the framework, which considers both technical and organizational aspects. The instrument has been developed and validated through a two-phase study involving field experts and academics. The framework and instrument are presented for ongoing evolution and utilization.;https://ieeexplore.ieee.org/abstract/document/8708913/;4Oim6tnBKhcJ
Patel, K. (2010, October). Lowering the barrier to applying machine learning. In Adjunct proceedings of the 23nd annual ACM symposium on User interface software and technology (pp. 355-358).;1_ml_machine_data_learning;2010;Lowering the barrier to applying machine learning;Kayur Patel;Adjunct proceedings of the 23nd annual ACM symposium on User interface software and technology, 355-358, 2010;Machine learning algorithms are key components in many cutting edge applications of computation. However, the full potential of machine learning has not been realized because using machine learning is hard, even for otherwise tech-savvy developers. This is because developing with machine learning is different than normal programming. My thesis is that developers applying machine learning need new general-purpose tools that provide structure for common processes and common pipelines while remaining flexible to account for variability in problems. In this paper, I describe my efforts to understanding the difficulties that developers face when applying machine learning. I then describe Gestalt, a general-purpose integrated development environment designed the application of machine learning. Finally, I describe work on developing a pattern language for building machine learning systems and creating new techniques that help developers understand the interaction between their data and learning algorithms.;https://dl.acm.org/doi/abs/10.1145/1866218.1866222;_NKjMV920Z0J
Dreossi, T., Donzé, A., & Seshia, S. A. (2019). Compositional falsification of cyber-physical systems with machine learning components. Journal of Automated Reasoning, 63, 1031-1053.;2_safety_system_autonomous_vehicle;2019;Compositional falsification of cyber-physical systems with machine learning components;Tommaso Dreossi, Alexandre DonzÃ©, Sanjit A Seshia;Journal of Automated Reasoning 63, 1031-1053, 2019;Cyber-physical systems (CPS), such as automotive systems, are starting to include sophisticated machine learning (ML) components. Their correctness, therefore, depends on properties of the inner ML modules. While learning algorithms aim to generalize from examples, they are only as good as the examples provided, and recent efforts have shown that they can produce inconsistent output under small adversarial perturbations. This raises the question: can the output from learning components lead to a failure of the entire CPS? In this work, we address this question by formulating it as a problem of falsifying signal temporal logic specifications for CPS with ML components. We propose a compositional falsification framework where a temporal logic falsifier and a machine learning analyzer cooperate with the aim of finding falsifying executions of the considered model. The efficacy of the proposed technique is shown on an automatic emergency braking system model with a perception component based on deep neural networks.;https://link.springer.com/article/10.1007/s10817-018-09509-5;lcDfQxn0T50J
Srisakaokul, S., Wu, Z., Astorga, A., Alebiosu, O., & Xie, T. (2018, February). Multiple-Implementation Testing of Supervised Learning Software. In AAAI Workshops (pp. 384-391).;10_testing_test_machine_metamorphic;2018;Multiple-Implementation Testing of Supervised Learning Software;Siwakorn Srisakaokul, Zhengkai Wu, Angello Astorga, Oreoluwa Alebiosu, Tao Xie;AAAI Workshops, 384-391, 2018;Machine Learning (ML) algorithms are now used in a wide range of application domains in society. Naturally, software implementations of these algorithms have become ubiquitous. Faults in ML software can cause substantial losses in these application domains. Thus, it is very critical to conduct effective testing of ML software to detect and eliminate its faults. However, testing ML software is difficult, partly because producing test oracles used for checking behavior correctness (such as using expected properties or expected test outputs) is challenging. In this paper, we propose an approach of multiple-implementation testing to test supervised learning software, a major type of ML software. In particular, our approach derives a test inputâ€™s proxy oracle from the majority-voted output running the test input of multiple implementations of the same algorithm (based on a pre-defined percentage threshold). Our approach reports likely those test inputs whose outputs (produced by an implementation under test) are different from the majority-voted outputs as failing tests. We evaluate our approach on two highly popular supervised learning algorithms: k-Nearest Neighbor (kNN) and Naive Bayes (NB). Our results show that our approach is highly effective in detecting faults in real-world supervised learning software. In particular, our approach detects 13 real faults and 1 potential fault from 19 kNN implementations and 16 real faults from 7 NB implementations. Our approach can even detect 7 real faults and 1 potential fault among the three popularly used open-source ML projects (Weka, RapidMiner, and KNIME).;http://aastorg2.web.engr.illinois.edu/publications/edsmls18-mitest.pdf;OW9Y5e_pK3AJ
Yang, W., & Xie, T. (2018, June). Telemade: A Testing Framework for Learning-Based Malware Detection Systems. In AAAI Workshops (pp. 400-403).;5_adversarial_attack_example_model;2018;Telemade: A Testing Framework for Learning-Based Malware Detection Systems;Wei Yang, Tao Xie;AAAI Workshops, 400-403, 2018;Learning-based malware detectors may be erroneous due to two inherent limitations. First, there is a lack of differentiability: selected features may not reflect essential differences between malware and benign apps. Second, there is a lack of comprehensiveness: the used machine learning (ML) models are usually based on prior knowledge of existing malware (ie, training dataset) so malware can evolve to evade the detection. There is a strong need for an automated framework to help security analysts to detect errors in learning-based malware detection systems. Existing techniques to generate adversarial samples for learning-based systems (that take images as inputs) employ feature mutations based on feature vectors. Such techniques are infeasible to generate adversarial samples (eg, evasive malware) for malware detection systems because the synthesized mutations may break the inherent constraints posed by code structures of the malware, causing either crashes or malfunctioning of malicious payloads. To address the challenge, we propose Telemade, a testing framework for learning-based malware detectors.;http://taoxie.cs.illinois.edu/publications/edsmls18-telemade.pdf;Z0ddY-E5xhsJ
Islam, M. J., Pan, R., Nguyen, G., & Rajan, H. (2020, June). Repairing deep neural networks: Fix patterns and challenges. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (pp. 1135-1146).;4_dl_testing_deep_network;2020;Repairing deep neural networks: Fix patterns and challenges;Md Johirul Islam, Rangeet Pan, Giang Nguyen, Hridesh Rajan;Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, 1135-1146, 2020;"Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.";https://dl.acm.org/doi/abs/10.1145/3377811.3380378;T_f0Jxke5WUJ
Adedjouma, M., Pedroza, G., & Bannour, B. (2018, May). Representative safety assessment of autonomous vehicle for public transportation. In 2018 IEEE 21st International Symposium on Real-Time Distributed Computing (ISORC) (pp. 124-129). IEEE.;2_safety_system_autonomous_vehicle;2018;Representative safety assessment of autonomous vehicle for public transportation;Morayo Adedjouma, Gabriel Pedroza, Boutheina Bannour;2018 IEEE 21st International Symposium on Real-Time Distributed Computing (ISORC), 124-129, 2018;The implementations and testing in real conditions of Autonomous Vehicles (AV) for private usage show important advances. However, a lack still exists in addressing the particularities of AVs for Public Transportation. Such particularities range from limited safety mechanisms aboard, risky situations associated to particular users and complex self-driving situations up to the limited passengers-vehicle interactions possible. Since, to our knowledge, no comprehensive safety assessment actually exists and the current automotive related standards do not address identified aspects, in this paper, we propose to conduct a minimal but representative safety assessment based upon a local but real autonomous vehicle implementation. To conduct our study, the Hazard Analysis and Risks Assessment introduced in the ISO 26262 standard is taken as a basis. Initial outcomes suggest that critical autonomy aspects, like machine learning of complex operational situations, the metrics for quantitative assessment of autonomy, and potential conflicts between autonomy principles and external safety fences can have critical safety impacts and demand further discussions.;https://ieeexplore.ieee.org/abstract/document/8421156/;cRWkeFNiAbYJ
Molina, C. B. S. T., De Almeida, J. R., Vismari, L. F., Gonzalez, R. I. R., Naufal, J. K., & Camargo, J. (2017, June). Assuring fully autonomous vehicles safety by design: The autonomous vehicle control (avc) module strategy. In 2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W) (pp. 16-21). IEEE.;2_safety_system_autonomous_vehicle;2017;Assuring fully autonomous vehicles safety by design: The autonomous vehicle control (avc) module strategy;Caroline Bianca Santos Tancredi Molina, Jorge Rady De Almeida, Lucio F Vismari, Rodrigo Ignacio R Gonzalez, Jamil K Naufal, JoÃ£o Camargo;2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W), 16-21, 2017;Massive investment in 'intelligent' vehicle technologies is going to turn autonomous vehicles into reality in a few years. The insertion of this intelligence at the road vehicles is expected to cause a reduction in traffic accidents due to the mitigation of human drivers errors and imperfections by computerized autopilots. However, autonomous vehicles shall mitigate the existing hazards at the roadway transportation systems while not creating new hazards. Thus, some critical aspects need to be better considered, such as how to ensure safety in this new vehicle paradigm. There is no specific method to analyze and assure the safety levels of the autonomous vehicle system. Despite the ISO 26262 - a new safety standard that specifies requirements and activities throughout the road vehicles development lifecycle - it cannot be applied to the autonomous road vehicles scope. This paper proposes a design strategy that may be used at the architecture design level of autonomous vehicles that may facilitate the development, analysis and, consequently, safety level assuring. The main idea is to implement an independent module - the Autonomous Vehicle Control (AVC) - that is going to both interact with the vehicle's systems and create a protection layer that is independent of the way the vehicle's system was developed. So, the AVC could be used with any autonomous vehicle system and could be tested individually. This strategy is based on both recommended practices published by Society of Automotive Engineers (SAE) and on approaches used on other transportation system domains. Another important point is that the proposed module will be intended, in principle, for fully autonomous cars (high levels of driving automation). So, it is expected that, in the future, the proposed module can be used to develop a safety software standard or to suit the existing ones to the needs of autonomous road vehicles.;https://ieeexplore.ieee.org/abstract/document/8023692/;MoXjOZq0rCYJ
Nakajima, S., & Bui, H. N. (2016, December). Dataset coverage for testing machine learning computer programs. In 2016 23rd Asia-Pacific Software Engineering Conference (APSEC) (pp. 297-304). IEEE.;10_testing_test_machine_metamorphic;2016;Dataset coverage for testing machine learning computer programs;Shin Nakajima, Hai Ngoc Bui;2016 23rd Asia-Pacific Software Engineering Conference (APSEC), 297-304, 2016;"Machine learning programs are non-testable, and thus testing with pseudo oracles is recommended. Although metamorphic testing is effective for testing with pseudo oracles, identifying metamorphic properties has been mostly ad hoc. This paper proposes a systematic method to derive a set of metamorphic properties for machine learning classifiers, support vector machines. The proposal includes a new notion of test coverage for the machine learning programs; this test coverage provides a clear guideline for conducting a series of metamorphic testing.";https://ieeexplore.ieee.org/abstract/document/7890601/;sEfW9Lo4z4IJ
Nakajima, S. (2018). Generalized oracle for testing machine learning computer programs. In Software Engineering and Formal Methods: SEFM 2017 Collocated Workshops: DataMod, FAACS, MSE, CoSim-CPS, and FOCLASA, Trento, Italy, September 4-5, 2017, Revised Selected Papers 15 (pp. 174-179). Springer International Publishing.;10_testing_test_machine_metamorphic;2018;Generalized oracle for testing machine learning computer programs;Shin Nakajima;Software Engineering and Formal Methods: SEFM 2017 Collocated Workshops: DataMod, FAACS, MSE, CoSim-CPS, and FOCLASA, Trento, Italy, September 4-5, 2017, Revised Selected …, 2018;Computation results of machine learning programs are not possible to be anticipated, because the results are sensitive to distribution of data in input dataset. Additionally, these computer programs sometimes adopt randomized algorithms for finding sub-optimal solutions or improving runtime efficiencies to reach solutions. The computation is probabilistic and the results vary from execution to execution even for a same input. The characteristics imply that no deterministic test oracle exists to check correctness of programs. This paper studies how a notion of oracles is elaborated so that these programs can be tested, and shows a systematic way of deriving testing properties from mathematical formulations of given machine learning problems.;https://link.springer.com/chapter/10.1007/978-3-319-74781-1_13;LK18swkfrSsJ
Nakajima, S. (2018, October). Quality assurance of machine learning software. In 2018 IEEE 7th Global Conference on Consumer Electronics (GCCE) (pp. 601-604). IEEE.;1_ml_machine_data_learning;2018;Quality assurance of machine learning software;Shin Nakajima;2018 IEEE 7th Global Conference on Consumer Electronics (GCCE), 601-604, 2018;"Functionalities of machine learning software are dependent on a set of data input to them; a slight change in a training dataset has much impact on learning parameter values and thus on inference results. ML-based systems bring about a new challenge to quality assurance methods. This paper reviews two traditional views of service and product qualities. Furthermore, it introduces a platform view, in which co-creation of value is a major concern.";https://ieeexplore.ieee.org/abstract/document/8574766/;5yLKEBo2JbQJ
Bosch, J., Olsson, H. H., & Crnkovic, I. (2018, December). It takes three to tango: Requirement, outcome/data, and AI driven development. In SiBW (pp. 177-192).;1_ml_machine_data_learning;2018;It takes three to tango: Requirement, outcome/data, and AI driven development;Jan Bosch, Helena Holmström Olsson, Ivica Crnkovic;SiBW, 177-192, 2018;"Today’s software-intensive organizations are experiencing a paradigm-shift with regards to how to develop software systems. With the increasing availability and access to data and with artificial intelligence (AI) and technologies such as machine learning and deep learning emerging, the traditional requirement driven approach to software development is becoming complemented with other approaches. In addition to having development teams executing on requirements specified by product management, the development of software systems is progressing towards a data driven practice where teams receive an outcome to realize and where design decisions are taken based on continuous collection and analysis of data. On top of this, and due to artificial intelligence components being introduced to more and more software systems, learning algorithms, automatically generated models and data is replacing code and the development process is no longer only a manual effort but instead a combination of human and automated processes. In this paper, and based on multi-case study research in embedded systems and online companies, we see that companies use different approaches to software development but that they often take a requirement driven approach even if they would benefit from one of the other two. Also, we see that picking the wrong approach results in a number of problems such as eg inefficiency and waste of development efforts. To help address these problems, we develop a holistic development framework and we provide guidelines on how to improve effectiveness in development. The contribution of this paper is two-fold. First, we identify that there are three distinct approaches to software development;(1) Requirement driven development,(2) Outcome/data driven development and (3) AI driven development and we outline the typical problems that companies experience when using the wrong approach for the wrong purpose. Second, we provide a holistic framework with guidelines for when to use what approach to software development.";https://publications.hse.ru/pubs/share/direct/271776002.pdf#page=187;S_NYcHfsTnsJ
Hains, G., Jakobsson, A., & Khmelevsky, Y. (2018, April). Towards formal methods and software engineering for deep learning: security, safety and productivity for dl systems development. In 2018 Annual IEEE international systems conference (syscon) (pp. 1-5). IEEE.;5_adversarial_attack_example_model;2018;Towards formal methods and software engineering for deep learning: security, safety and productivity for dl systems development;GaÃ©tan Hains, Arvid Jakobsson, Youry Khmelevsky;2018 Annual IEEE international systems conference (syscon), 1-5, 2018;Deep Learning (DL) techniques are now widespread and being integrated into many important systems. Their classification and recognition abilities ensure their relevance for multiple application domains far beyond pure signal processing. As a machine-learning technique that relies on training instead of explicit algorithm programming they offer a high degree of productivity. But recent research has shown that they can be vulnerable to attacks and the verification of their correctness is only just emerging as a scientific and engineering possibility. Moreover DL tools are not integrated into classical software engineering so software tools to specify, modify and verify them would make them even more mainstream as software-hardware systems. This paper surveys recent work and proposes research directions and methodologies for this purpose.;https://ieeexplore.ieee.org/abstract/document/8369576/;gBnknAC6i38J
Lefortier, D., Truchet, A., & de Rijke, M. (2015). Sources of variability in large-scale machine learning systems. In Machine Learning Systems (NIPS 2015 Workshop).;1_ml_machine_data_learning;2015;Sources of variability in large-scale machine learning systems;Damien Lefortier, Anthony Truchet, Maarten de Rijke;Machine Learning Systems (NIPS 2015 Workshop), 2015;We investigate sources of variability of a state-of-the-art distributed machine learning system for learning click and conversion prediction models for display advertising. We focus on three main sources of variability: asynchronous updates in the learning algorithm, downsampling of the data, and the non-deterministic order of examples received by each learning instance. We observe that some sources of variability can lead to significant differences between the models obtained and cause issues for, eg, regression testing, debugging, and offline evaluation. We present effective solutions to stabilize the system and remove these sources of variability, thus fully solving the issues related to regression testing and to debugging. Moreover, we discuss potential limitations of this stabilization for drawing conclusions, in which case we may want to take the variability produced by the machine learning system into account in confidence intervals.;http://learningsys.org/papers/LearningSys_2015_paper_7.pdf;SGQWvWWY4vcJ
Shafaei, S., Kugele, S., Osman, M. H., & Knoll, A. (2018). Uncertainty in machine learning: A safety perspective on autonomous driving. In Computer Safety, Reliability, and Security: SAFECOMP 2018 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Västerås, Sweden, September 18, 2018, Proceedings 37 (pp. 458-464). Springer International Publishing.;2_safety_system_autonomous_vehicle;2018;Uncertainty in machine learning: A safety perspective on autonomous driving;Sina Shafaei, Stefan Kugele, Mohd Hafeez Osman, Alois Knoll;Computer Safety, Reliability, and Security: SAFECOMP 2018 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Västerås, Sweden, September 18, 2018, Proceedings 37, 458-464, 2018;With recent efforts to make vehicles intelligent, solutions based on machine learning have been accepted to the ecosystem. These systems in the automotive domain are growing fast, speeding up the promising future of highly and fully automated driving, and respectively, raising new challenges regarding safety assurance approaches. Uncertainty in data and the machine learning methods is a key point to investigate one of the main origins of safety-related concerns. In this work, we inspect this issue in the domain of autonomous driving with an emphasis on four safety-related cases, then introduce our proposals to address the challenges and mitigate them. The core of our approach is on introducing monitoring limiters during development time of such intelligent systems.;https://link.springer.com/chapter/10.1007/978-3-319-99229-7_39;0O6y07ngKgwJ
Koopman, P., & Wagner, M. (2016). Challenges in autonomous vehicle testing and validation. SAE International Journal of Transportation Safety, 4(1), 15-24.;2_safety_system_autonomous_vehicle;2016;Challenges in autonomous vehicle testing and validation;Philip Koopman, Michael Wagner;SAE International Journal of Transportation Safety 4 (1), 15-24, 2016;Software testing is all too often simply a bug hunt rather than a well-considered exercise in ensuring quality. A more methodical approach than a simple cycle of system-level test-fail-patch-test will be required to deploy safe autonomous vehicles at scale. The ISO 26262 development V process sets up a framework that ties each type of testing to a corresponding design or requirement document, but presents challenges when adapted to deal with the sorts of novel testing problems that face autonomous vehicles. This paper identifies five major challenge areas in testing according to the V model for autonomous vehicles: driver out of the loop, complex requirements, non-deterministic algorithms, inductive learning algorithms, and fail-operational systems. General solution approaches that seem promising across these different challenge areas include: phased deployment using successively relaxed operational scenarios, use of a monitor/actuator pair architecture to separate the most complex autonomy functions from simpler safety functions, and fault injection as a way to perform more efficient edge case testing. While significant challenges remain in safety-certifying the type of algorithms that provide high-level autonomy themselves, it seems within reach to instead architect the system and its accompanying design process to be able to employ existing software safety approaches.;https://www.jstor.org/stable/26167741;uN7H3u0QlHMJ
Salay, R., Queiroz, R., & Czarnecki, K. (2017). An analysis of ISO 26262: Using machine learning safely in automotive software. arXiv preprint arXiv:1709.02435.;2_safety_system_autonomous_vehicle;2017;An analysis of ISO 26262: Using machine learning safely in automotive software;Rick Salay, Rodrigo Queiroz, Krzysztof Czarnecki;arXiv preprint arXiv:1709.02435, 2017;"Machine learning (ML) plays an ever-increasing role in advanced automotive functionality for driver assistance and autonomous operation; however, its adequacy from the perspective of safety certification remains controversial. In this paper, we analyze the impacts that the use of ML as an implementation approach has on ISO 26262 safety lifecycle and ask what could be done to address them. We then provide a set of recommendations on how to adapt the standard to accommodate ML.";https://arxiv.org/abs/1709.02435;Aoa3yEShrxwJ
Shalev-Shwartz, S., Shammah, S., & Shashua, A. (2017). On a formal model of safe and scalable self-driving cars. arXiv preprint arXiv:1708.06374.;2_safety_system_autonomous_vehicle;2017;On a formal model of safe and scalable self-driving cars;Shai Shalev-Shwartz, Shaked Shammah, Amnon Shashua;arXiv preprint arXiv:1708.06374, 2017;"In recent years, car makers and tech companies have been racing towards self driving cars. It seems that the main parameter in this race is who will have the first car on the road. The goal of this paper is to add to the equation two additional crucial parameters. The first is standardization of safety assurance --- what are the minimal requirements that every self-driving car must satisfy, and how can we verify these requirements. The second parameter is scalability --- engineering solutions that lead to unleashed costs will not scale to millions of cars, which will push interest in this field into a niche academic corner, and drive the entire field into a ""winter of autonomous driving"". In the first part of the paper we propose a white-box, interpretable, mathematical model for safety assurance, which we call Responsibility-Sensitive Safety (RSS). In the second part we describe a design of a system that adheres to our safety assurance requirements and is scalable to millions of cars.";https://arxiv.org/abs/1708.06374;akS5xaIAvzEJ
Kery, M. B., Radensky, M., Arya, M., John, B. E., & Myers, B. A. (2018, April). The story in the notebook: Exploratory data science using a literate programming tool. In Proceedings of the 2018 CHI conference on human factors in computing systems (pp. 1-11).;9_data_science_software_process;2018;The story in the notebook: Exploratory data science using a literate programming tool;Mary Beth Kery, Marissa Radensky, Mahima Arya, Bonnie E John, Brad A Myers;Proceedings of the 2018 CHI conference on human factors in computing systems, 1-11, 2018;Literate programming tools are used by millions of programmers today, and are intended to facilitate presenting data analyses in the form of a narrative. We interviewed 21 data scientists to study coding behaviors in a literate programming environment and how data scientists kept track of variants they explored. For participants who tried to keep a detailed history of their experimentation, both informal and formal versioning attempts led to problems, such as reduced notebook readability. During iteration, participants actively curated their notebooks into narratives, although primarily through cell structure rather than markdown explanations. Next, we surveyed 45 data scientists and asked them to envision how they might use their past history in an future version control system. Based on these results, we give design guidance for future literate programming tools, such as providing history search based on how programmers recall their explorations, through contextual details including images and parameters.;https://dl.acm.org/doi/abs/10.1145/3173574.3173748;2g40EQlfsX8J
Yang, Q. (2017, March). The role of design in creating machine-learning-enhanced user experience. In 2017 AAAI spring symposium series.;1_ml_machine_data_learning;2017;The role of design in creating machine-learning-enhanced user experience;Qian Yang;2017 AAAI spring symposium series, 2017;Machine learning (ML) applications that directly interface with everyday users are now increasingly pervasive and powerful. However, user experience (UX) practitioners are lagging behind in leveraging this increasing common technology. ML is not yet a standard part of UX design practice, in either design patterns, prototyping tools, or education. This paper is a reflection on my experience designing ML-mediated UX. I illustrate the role UX practice can play in making machine intelligence usable and valuable for everyday users: it can help identify 1) how to choose the right ML applications. 2) how to design the ML right. The separation of the two concerns is a first step to untangling the tight interplay between ML and UX. I highlight the unique challenges and the implications for future research directions.;https://cdn.aaai.org/ocs/15363/15363-68257-1-PB.pdf;p67gfv6C6GgJ
Fremont, D. J., Kim, E., Pant, Y. V., Seshia, S. A., Acharya, A., Bruso, X., ... & Mehta, S. (2020, September). Formal scenario-based testing of autonomous vehicles: From simulation to the real world. In 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC) (pp. 1-8). IEEE.;2_safety_system_autonomous_vehicle;2020;Formal scenario-based testing of autonomous vehicles: From simulation to the real world;Daniel J Fremont, Edward Kim, Yash Vardhan Pant, Sanjit A Seshia, Atul Acharya, Xantha Bruso, Paul Wells, Steve Lemke, Qiang Lu, Shalin Mehta;2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC), 1-8, 2020;We present a new approach to automated scenario-based testing of the safety of autonomous vehicles, especially those using advanced artificial intelligence-based components, spanning both simulation-based evaluation as well as testing in the real world. Our approach is based on formal methods, combining formal specification of scenarios and safety properties, algorithmic test case generation using formal simulation, test case selection for track testing, executing test cases on the track, and analyzing the resulting data. Experiments with a real autonomous vehicle at an industrial testing facility support our hypotheses that (i) formal simulation can be effective at identifying test cases to run on the track, and (ii) the gap between simulated and real worlds can be systematically evaluated and bridged.;https://ieeexplore.ieee.org/abstract/document/9294368/;h3JnEha3qv4J
Tran, H. D., Yang, X., Manzanas Lopez, D., Musau, P., Nguyen, L. V., Xiang, W., ... & Johnson, T. T. (2020, July). NNV: the neural network verification tool for deep neural networks and learning-enabled cyber-physical systems. In International Conference on Computer Aided Verification (pp. 3-17). Cham: Springer International Publishing.;2_safety_system_autonomous_vehicle;2020;NNV: the neural network verification tool for deep neural networks and learning-enabled cyber-physical systems;Hoang-Dung Tran, Xiaodong Yang, Diego Manzanas Lopez, Patrick Musau, Luan Viet Nguyen, Weiming Xiang, Stanley Bak, Taylor T Johnson;International Conference on Computer Aided Verification, 3-17, 2020;This paper presents the Neural Network Verification (NNV) software tool, a set-based verification framework for deep neural networks (DNNs) and learning-enabled cyber-physical systems (CPS). The crux of NNV is a collection of reachability algorithms that make use of a variety of set representations, such as polyhedra, star sets, zonotopes, and abstract-domain representations. NNV supports both exact (sound and complete) and over-approximate (sound) reachability algorithms for verifying safety and robustness properties of feed-forward neural networks (FFNNs) with various activation functions. For learning-enabled CPS, such as closed-loop control systems incorporating neural networks, NNV provides exact and over-approximate reachability analysis schemes for linear plant models and FFNN controllers with piecewise-linear activation functions, such as ReLUs. For similar neural network control systems (NNCS) that instead have nonlinear plant models, NNV supports over-approximate analysis by combining the star set analysis used for FFNN controllers with zonotope-based analysis for nonlinear plant dynamics building on CORA. We evaluate NNV using two real-world case studies: the first is safety verification of ACAS Xu networks, and the second deals with the safety verification of a deep learning-based adaptive cruise control system.;https://link.springer.com/chapter/10.1007/978-3-030-53288-8_1;5sytTdqetn8J
Lamrani, I., Banerjee, A., & Gupta, S. K. (2020, January). Toward Operational Safety Verification Via Hybrid Automata Mining Using I/O Traces of AI-Enabled CPS. In SafeAI@ AAAI (pp. 186-194).;2_safety_system_autonomous_vehicle;2020;Toward Operational Safety Verification Via Hybrid Automata Mining Using I/O Traces of AI-Enabled CPS;Imane Lamrani, Ayan Banerjee, Sandeep KS Gupta;SafeAI@ AAAI, 186-194, 2020;AI enabled cyber-physical systems such as artificial pancreas suffer from the” no oracle problem”. The system is subjected to inputs and scenarios which are not observed during training time and hence the expected outputs are not known. Hence, popular model-based verification techniques that characterize behavior of a control system before deployment using predictive models may be inaccurate and may result in incorrect safety analysis results. In this research, we propose an operational safety verification technique through hybrid system mining from input/output traces of deployed AI-enabled cyber-physical systems. The hybrid automaton model enables formal verification of safety despite the” no oracle problem”. We apply our technique to the artificial pancreas control system utilizing data from an outpatient study on an artificial pancreas system. We demonstrate that our technique successfully infers accurate hybrid automata representation of these systems in the field and can be used to perform safety analysis to ascertain safety of the system in presence of inputs and scenarios for which the expected output of the system is unknown. We identify an evaluation scenario under which there exists a clear safety violation.;https://www.researchgate.net/profile/Imane-Lamrani/publication/340850162_Toward_Operational_Safety_Verification_Via_Hybrid_Automata_Mining_Using_IO_Traces_of_AI-Enabled_CPS/links/5ea0bd8492851c87d1acf72e/Toward-Operational-Safety-Verification-Via-Hybrid-Automata-Mining-Using-I-O-Traces-of-AI-Enabled-CPS.pdf;nlPXmwB8YcgJ
Kästner, C., & Kang, E. (2020, June). Teaching software engineering for AI-enabled systems. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering Education and Training (pp. 45-48).;1_ml_machine_data_learning;2020;Teaching software engineering for AI-enabled systems;Christian KÃ¤stner, Eunsuk Kang;Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering Education and Training, 45-48, 2020;Software engineers have significant expertise to offer when building intelligent systems, drawing on decades of experience and methods for building systems that are scalable, responsive and robust, even when built on unreliable components. Systems with artificial-intelligence or machine-learning (ML) components raise new challenges and require careful engineering. We designed a new course to teach software-engineering skills to students with a background in ML. We specifically go beyond traditional ML courses that teach modeling techniques under artificial conditions and focus, in lecture and assignments, on realism with large and changing datasets, robust and evolvable infrastructure, and purposeful requirements engineering that considers ethics and fairness as well. We describe the course and our infrastructure and share experience and all material from teaching the course for the first time.;https://dl.acm.org/doi/abs/10.1145/3377814.3381714;KEHq7m3DploJ
Islam, M. J., Nguyen, H. A., Pan, R., & Rajan, H. (2019). What do developers ask about ml libraries? a large-scale study using stack overflow. arXiv preprint arXiv:1906.11940.;1_ml_machine_data_learning;2019;What do developers ask about ml libraries? a large-scale study using stack overflow;Md Johirul Islam, Hoan Anh Nguyen, Rangeet Pan, Hridesh Rajan;arXiv preprint arXiv:1906.11940, 2019;Modern software systems are increasingly including machine learning (ML) as an integral component. However, we do not yet understand the difficulties faced by software developers when learning about ML libraries and using them within their systems. To that end, this work reports on a detailed (manual) examination of 3,243 highly-rated Q&A posts related to ten ML libraries, namely Tensorflow, Keras, scikit-learn, Weka, Caffe, Theano, MLlib, Torch, Mahout, and H2O, on Stack Overflow, a popular online technical Q&A forum. We classify these questions into seven typical stages of an ML pipeline to understand the correlation between the library and the stage. Then we study the questions and perform statistical analysis to explore the answer to four research objectives (finding the most difficult stage, understanding the nature of problems, nature of libraries and studying whether the difficulties stayed consistent over time). Our findings reveal the urgent need for software engineering (SE) research in this area. Both static and dynamic analyses are mostly absent and badly needed to help developers find errors earlier. While there has been some early research on debugging, much more work is needed. API misuses are prevalent and API design improvements are sorely needed. Last and somewhat surprisingly, a tug of war between providing higher levels of abstractions and the need to understand the behavior of the trained model is prevalent.;https://arxiv.org/abs/1906.11940;2Hh-iwOTjW4J
Seshia, S. A., Sadigh, D., & Sastry, S. S. (2022). Toward verified artificial intelligence. Communications of the ACM, 65(7), 46-55.;2_safety_system_autonomous_vehicle;2022;Toward verified artificial intelligence;Sanjit A Seshia, Dorsa Sadigh, S Shankar Sastry;Communications of the ACM 65 (7), 46-55, 2022;Making AI more trustworthy with a formal methods-based approach to AI system verification and validation.;https://dl.acm.org/doi/abs/10.1145/3503914;IspB9_2xZjIJ
Tramèr, F., Zhang, F., Juels, A., Reiter, M. K., & Ristenpart, T. (2016). Stealing machine learning models via prediction {APIs}. In 25th USENIX security symposium (USENIX Security 16) (pp. 601-618).;5_adversarial_attack_example_model;2016;Stealing machine learning models via prediction {APIs};Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, Thomas Ristenpart;25th USENIX security symposium (USENIX Security 16), 601-618, 2016;Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (“predictive analytics”) systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.;https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/tramer;dE_KQLNyt8cJ
Bunel, R., Turkaslan, I., Torr, P. H., Kohli, P., & Kumar, M. P. (2018). Piecewise linear neural networks verification: A comparative study.;4_dl_testing_deep_network;2018;Piecewise linear neural networks verification: A comparative study.;Rudy Bunel, Ilker Turkaslan, Philip HS Torr, Pushmeet Kohli, M Pawan Kumar;-;The success of Deep Learning and its potential use in many important safety- critical applications has motivated research on formal verification of Neural Net- work (NN) models. Despite the reputation of learned NN models to behave as black boxes and theoretical hardness results of the problem of proving their prop- erties, researchers have been successful in verifying some classes of models by exploiting their piecewise linear structure. Unfortunately, most of these works test their algorithms on their own models and do not offer any comparison with other approaches. As a result, the advantages and downsides of the different al- gorithms are not well understood. Motivated by the need of accelerating progress in this very important area, we investigate the trade-offs of a number of different approaches based on Mixed Integer Programming, Satisfiability Modulo Theory, as well as a novel method based on the Branch-and-Bound framework. We also propose a new data set of benchmarks, in addition to a collection of previously released testcases that can be used to compare existing methods. Our analysis not only allowed a comparison to be made between different strategies, the compar- ision of results from different solvers also revealed implementation bugs in pub- lished methods. We expect that the availability of our benchmark and the analysis of the different approaches will allow researchers to invent and evaluate promising approaches for making progress on this important topic.;https://openreview.net/forum?id=BkPrDFgR-;cku5sNMcugYJ
Santhanam, P., Farchi, E., & Pankratius, V. (2019). Engineering reliable deep learning systems. arXiv preprint arXiv:1910.12582.;1_ml_machine_data_learning;2019;Engineering reliable deep learning systems;P Santhanam, Eitan Farchi, Victor Pankratius;arXiv preprint arXiv:1910.12582, 2019;"Recent progress in artificial intelligence (AI) using deep learning techniques has triggered its wide-scale use across a broad range of applications. These systems can already perform tasks such as natural language processing of voice and text, visual recognition, question-answering, recommendations and decision support. However, at the current level of maturity, the use of an AI component in mission-critical or safety-critical applications can have unexpected consequences. Consequently, serious concerns about reliability, repeatability, trust, and maintainability of AI applications remain. As AI becomes pervasive despite its shortcomings, more systematic ways of approaching AI software development and certification are needed. These fundamental aspects establish the need for a discipline on ""AI Engineering"". This paper presents the current perspective of relevant AI engineering concepts and some key challenges that need to be overcome to make significant progress in this important area.";https://arxiv.org/abs/1910.12582;grALlDt802AJ
Jahangirova, G., & Tonella, P. (2020, October). An empirical evaluation of mutation operators for deep learning systems. In 2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST) (pp. 74-84). IEEE.;4_dl_testing_deep_network;2020;An empirical evaluation of mutation operators for deep learning systems;Gunel Jahangirova, Paolo Tonella;2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST), 74-84, 2020;Deep Learning (DL) is increasingly adopted to solve complex tasks such as image recognition or autonomous driving. Companies are considering the inclusion of DL components in production systems, but one of their main concerns is how to assess the quality of such systems. Mutation testing is a technique to inject artificial faults into a system, under the assumption that the capability to expose (kilt) such artificial faults translates into the capability to expose also real faults. Researchers have proposed approaches and tools (e.g., Deep-Mutation and MuNN) that make mutation testing applicable to deep learning systems. However, existing definitions of mutation killing, based on accuracy drop, do not take into account the stochastic nature of the training process (accuracy may drop even when re-training the un-mutated system). Moreover, the same mutation operator might be effective or might be trivial/impossible to kill, depending on its hyper-parameter configuration. We conducted an empirical evaluation of existing operators, showing that mutation killing requires a stochastic definition and identifying the subset of effective mutation operators together with the associated most effective configurations.;https://ieeexplore.ieee.org/abstract/document/9159089/;knyfr0Ag5vcJ
Kuwajima, H., & Ishikawa, F. (2019, October). Adapting SQuaRE for quality assessment of artificial intelligence systems. In 2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW) (pp. 13-18). IEEE.;1_ml_machine_data_learning;2019;Adapting SQuaRE for quality assessment of artificial intelligence systems;Hiroshi Kuwajima, Fuyuki Ishikawa;2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW), 13-18, 2019;More and more software practitioners are tackling towards industrial applications of artificial intelligence (AI) systems, especially those based on machine learning (ML). However, many of existing principles and approaches to traditional software systems do not work effectively for the system behavior obtained by training not by logical design. In addition, unique kinds of requirements are emerging such as fairness and explainability. To provide clear guidance to understand and tackle these difficulties, we present an analysis on what quality concepts we should evaluate for AI systems. We base our discussion on ISO/IEC 25000 series, known as SQuaRE, and identify how it should be adapted for the unique nature of ML and Ethics guidelines for trustworthy AI from European Commission. We thus provide holistic insights for quality of AI systems by incorporating the ML nature and AI ethics to the traditional software quality concepts.;https://ieeexplore.ieee.org/abstract/document/8990311/;kB1MDlbnfH4J
Nakajima, S. (2019, November). Distortion and faults in machine learning software. In International Workshop on Structured Object-Oriented Formal Language and Method (pp. 29-41). Cham: Springer International Publishing.;4_dl_testing_deep_network;2019;Distortion and faults in machine learning software;Shin Nakajima;International Workshop on Structured Object-Oriented Formal Language and Method, 29-41, 2019;Machine learning software, deep neural networks (DNN) software in particular, discerns valuable information from a large dataset, a set of data, so as to synthesize approximate input-output relations. The outcomes of such DNN programs are dependent on the quality of both learning programs and datasets. However, the quality assurance of DNN software is difficult. The trained machine learning models, defining the functional behavior of the approximate relations, are unknown prior to its development, and the validation is conducted indirectly in terms of the prediction performance. This paper introduces a hypothesis that faults in DNN programs manifest themselves as distortions in trained machine learning models. Relative distortion degrees measured with appropriate observer functions may indicate that the programs have some hidden faults. The proposal is demonstrated with the cases of the MNIST dataset.;https://link.springer.com/chapter/10.1007/978-3-030-41418-4_3;fGnDR_h8E5UJ
Nakajima, S. (2019, November). Quality evaluation assurance levels for deep neural networks software. In 2019 International Conference on Technologies and Applications of Artiﬁcial Intelligence (TAAI) (pp. 1-6). IEEE.;1_ml_machine_data_learning;2019;Quality evaluation assurance levels for deep neural networks software;Shin Nakajima;2019 International Conference on Technologies and Applications of Artiﬁcial Intelligence (TAAI), 1-6, 2019;Quality of machine learning software products or services is dependent on datasets used for training. However, defining quality of datasets is difficult, which might bring about risks in business situations. Independent, third-party testing laboratories would mitigate the risks. This paper proposes quality evaluation assurance levels, which is a basis of a third-party evaluation and certification framework. Moreover, quality of machine learning software is indeed viewed from three perspectives: prediction performance quality, training mechanism quality, and lifecycle support quality enabling continuous operations.;https://ieeexplore.ieee.org/abstract/document/8959916/;TRslWeXY6ewJ
Jentzsch, S. F., & Hochgeschwender, N. (2019, November). Don't forget your roots! using provenance data for transparent and explainable development of machine learning models. In 2019 34th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW) (pp. 37-40). IEEE.;3_explanation_model_machine_learning;2019;Don't forget your roots! using provenance data for transparent and explainable development of machine learning models;Sophie F Jentzsch, Nico Hochgeschwender;2019 34th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW), 37-40, 2019;Explaining reasoning and behaviour of artificial intelligent systems to human users becomes increasingly urgent, especially in the field of machine learning. Many recent contributions approach this issue with post-hoc methods, meaning they consider the final system and its outcomes, while the roots of included artefacts are widely neglected. However, we argue in this position paper that there needs to be a stronger focus on the development process. Without insights into specific design decisions and meta information that accrue during the development an accurate explanation of the resulting model is hardly possible. To remedy this situation we propose to increase process transparency by applying provenance methods, which serves also as a basis for increased explainability.;https://ieeexplore.ieee.org/abstract/document/8967411/;apoUmNKcqeIJ
Bozic, J., & Wotawa, F. (2018, September). Security testing for chatbots. In IFIP International Conference on Testing Software and Systems (pp. 33-38). Cham: Springer International Publishing.;1_ml_machine_data_learning;2018;Security testing for chatbots;Josip Bozic, Franz Wotawa;IFIP International Conference on Testing Software and Systems, 33-38, 2018;Services like chatbots that provide information to customers in real-time are of increasing importance for the online market. Chatbots offer an intuitive interface to answer user requests in an interactive manner. The inquiries are of wide-range and include information about specific goods and services but also financial issues and personal advices. The notable advantages of these programs are the simplicity of use and speed of the search process. In some cases, chatbots have even surpassed classical web, mobile applications, and social networks. Chatbots might have access to huge amount of data or personal information. Therefore, they might be a valuable target for hackers, and known web application vulnerabilities might be a security issue for chatbots as well. In this paper, we discuss the challenges of security testing for chatbots. We provide an overview about an automated testing approach adapted to chatbots, and first experimental results.;https://link.springer.com/chapter/10.1007/978-3-319-99927-2_3;p_B4nfAskl8J
Pulina, L., & Tacchella, A. (2010). An abstraction-refinement approach to verification of artificial neural networks. In Computer Aided Verification: 22nd International Conference, CAV 2010, Edinburgh, UK, July 15-19, 2010. Proceedings 22 (pp. 243-257). Springer Berlin Heidelberg.;2_safety_system_autonomous_vehicle;2010;An abstraction-refinement approach to verification of artificial neural networks;Luca Pulina, Armando Tacchella;Computer Aided Verification: 22nd International Conference, CAV 2010, Edinburgh, UK, July 15-19, 2010. Proceedings 22, 243-257, 2010;A key problem in the adoption of artificial neural networks in safety-related applications is that misbehaviors can be hardly ruled out with traditional analytical or probabilistic techniques. In this paper we focus on specific networks known as Multi-Layer Perceptrons (MLPs), and we propose a solution to verify their safety using abstractions to Boolean combinations of linear arithmetic constraints. We show that our abstractions are consistent, i.e., whenever the abstract MLP is declared to be safe, the same holds for the concrete one. Spurious counterexamples, on the other hand, trigger refinements and can be leveraged to automate the correction of misbehaviors. We describe an implementation of our approach based on the HySAT solver, detailing the abstraction-refinement process and the automated correction strategy. Finally, we present experimental results confirming the feasibility of our approach on a realistic case study.;https://link.springer.com/chapter/10.1007/978-3-642-14295-6_24;iEjMMDdJAkcJ
Dreossi, T., Jha, S., & Seshia, S. A. (2018). Semantic adversarial deep learning. In Computer Aided Verification: 30th International Conference, CAV 2018, Held as Part of the Federated Logic Conference, FloC 2018, Oxford, UK, July 14-17, 2018, Proceedings, Part I 30 (pp. 3-26). Springer International Publishing.;5_adversarial_attack_example_model;2018;Semantic adversarial deep learning;Tommaso Dreossi, Somesh Jha, Sanjit A Seshia;Computer Aided Verification: 30th International Conference, CAV 2018, Held as Part of the Federated Logic Conference, FloC 2018, Oxford, UK, July 14-17, 2018, Proceedings, Part …, 2018;Fueled by massive amounts of data, models produced by machine-learning (ML) algorithms, especially deep neural networks, are being used in diverse domains where trustworthiness is a concern, including automotive systems, finance, health care, natural language processing, and malware detection. Of particular concern is the use of ML algorithms in cyber-physical systems (CPS), such as self-driving cars and aviation, where an adversary can cause serious consequences.However, existing approaches to generating adversarial examples and devising robust ML algorithms mostly ignore the semantics and context of the overall system containing the ML component. For example, in an autonomous vehicle using deep learning for perception, not every adversarial example for the neural network might lead to a harmful consequence. Moreover, one may want to prioritize the search for adversarial examples towards those that significantly modify the desired semantics of the overall system. Along the same lines, existing algorithms for constructing robust ML algorithms ignore the specification of the overall system. In this paper, we argue that the semantics and specification of the overall system has a crucial role to play in this line of research. We present preliminary research results that support this claim.;https://link.springer.com/chapter/10.1007/978-3-319-96145-3_1;TBAT-2w0onYJ
Kulesza, T., Burnett, M., Wong, W. K., & Stumpf, S. (2015, March). Principles of explanatory debugging to personalize interactive machine learning. In Proceedings of the 20th international conference on intelligent user interfaces (pp. 126-137).;3_explanation_model_machine_learning;2015;Principles of explanatory debugging to personalize interactive machine learning;Todd Kulesza, Margaret Burnett, Weng-Keen Wong, Simone Stumpf;Proceedings of the 20th international conference on intelligent user interfaces, 126-137, 2015;How can end users efficiently influence the predictions that machine learning systems make on their behalf? This paper presents Explanatory Debugging, an approach in which the system explains to users how it made each of its predictions, and the user then explains any necessary corrections back to the learning system. We present the principles underlying this approach and a prototype instantiating it. An empirical evaluation shows that Explanatory Debugging increased participants' understanding of the learning system by 52% and allowed participants to correct its mistakes up to twice as efficiently as participants using a traditional learning system.;https://dl.acm.org/doi/abs/10.1145/2678025.2701399;y83BVOzHVUYJ
Sun, X., Zhou, T., Li, G., Hu, J., Yang, H., & Li, B. (2017, December). An empirical study on real bugs for machine learning programs. In 2017 24th Asia-Pacific Software Engineering Conference (APSEC) (pp. 348-357). IEEE.;1_ml_machine_data_learning;2017;An empirical study on real bugs for machine learning programs;Xiaobing Sun, Tianchi Zhou, Gengjie Li, Jiajun Hu, Hui Yang, Bin Li;2017 24th Asia-Pacific Software Engineering Conference (APSEC), 348-357, 2017;"Due to the availability of various open source Machine Learning (ML) tools and libraries, developers nowadays can easily implement their purposes by just invoking machine learning APIs without knowing the details of the algorithm. However, the owners of ML tools and libraries usually pay more attention to the correctness and functionality of their algorithm, while spending much less effort on maintaining their code and keeping their code at a high quality level. Considering the popularity of machine learning in today's world, low quality ML tools and libraries can have a huge impact on the software products that use ML algorithms. So in this paper, we conduct an empirical study on real machine learning bugs to examine their patterns and how they evolve over time. We collect three popular machine learning projects on Github, and manually analyzed 329 closed bugs from the perspectives of their bug category, fix pattern, fix scale, fix duration, and type of software maintenance. The results show that (1) there are seven categories of bugs in machine learning programs; (2) twelve different fix patterns are commonly used to fix the bugs; (3) 63.83% of the patches belong to micro-scale-fix and small-scale-fix, and 68.39% of the bugs are fixed within one month; (4) 47.77% of the bug fixes belong to corrective activity from the view of software maintenance.";https://ieeexplore.ieee.org/abstract/document/8305957/;H0YVoe-4m0YJ
Leofante, F., Pulina, L., & Tacchella, A. (2016). Learning with Safety Requirements: State of the Art and Open Questions. RCRA@ AI* IA, 11-25.;2_safety_system_autonomous_vehicle;2016;Learning with Safety Requirements: State of the Art and Open Questions;Francesco Leofante, Luca Pulina, Armando Tacchella;RCRA@ AI* IA, 11-25, 2016;A standing challenge for the adoption of machine learning (ML) techniques in safety-related applications is that misbehaviors can be hardly ruled out with traditional analytical or probabilistic techniques. In previous contributions of ours, we have shown how to deal with safety requirements considering both Multi-Layer Perceptrons (MLPs) and Support Vector Machines (SVMs), two of the most effective and well-known ML techniques. The key to provide safety guarantees for MLPs and SVMs is to combine Satisfiability Modulo Theory (SMT) solvers with suitable abstraction techniques. The purpose of this paper is to provide an overview of problems and related solution techniques when it comes to guarantee that the input-output function of trained computational models will behave according to specifications. We also summarize experimental results showing what can be effectively assessed in practice using state-of-the-art SMT solvers. Our empirical results are the starting point to introduce some open questions that we believe should be considered in future research about learning with safety requirements.;http://ceur-ws.org/Vol-1745/paper2.pdf;C5PyOWHIdZwJ
Katz, G., Barrett, C., Dill, D. L., Julian, K., & Kochenderfer, M. J. (2017). Reluplex: An efficient SMT solver for verifying deep neural networks. In Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30 (pp. 97-117). Springer International Publishing.;4_dl_testing_deep_network;2017;Reluplex: An efficient SMT solver for verifying deep neural networks;Guy Katz, Clark Barrett, David L Dill, Kyle Julian, Mykel J Kochenderfer;Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30, 97-117, 2017;Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.;https://link.springer.com/chapter/10.1007/978-3-319-63387-9_5;yNoQt6O3guoJ
Gao, J., Tao, C., Jie, D., & Lu, S. (2019, April). What is AI software testing? and why. In 2019 IEEE International Conference on Service-Oriented System Engineering (SOSE) (pp. 27-2709). IEEE.;1_ml_machine_data_learning;2019;What is AI software testing? and why;Jerry Gao, Chuanqi Tao, Dou Jie, Shengqiang Lu;2019 IEEE International Conference on Service-Oriented System Engineering (SOSE), 27-2709, 2019;With the fast advance of artificial intelligence technology and data-driven machine learning techniques, building high-quality AI-based software in different application domains is becoming a very hot research topic in both academic and industry communities. Today, many machine learning models and artificial technologies have been developed to build smart application systems based on multimedia inputs to achieve intelligent functional features, such as recommendation, object detection, classification, and prediction, natural language processing and translation, and so on. This brings strong demand in quality validation and assurance for AI software systems. Current research work seldom discusses AI software testing questions, challenges, and validation approaches with clear quality requirements and criteria. This paper focuses on AI software quality validation, including validation focuses, features, and process, and potential testing approaches. Moreover, it presents a test process and a classification-based test modeling for AI classification function testing. Finally, it discusses the challenges, issues, and needs in AI software testing.;https://ieeexplore.ieee.org/abstract/document/8705808/;EdAUH6gWJ0UJ
Dutta, S., Jha, S., Sanakaranarayanan, S., & Tiwari, A. (2017). Output range analysis for deep neural networks. arXiv preprint arXiv:1709.09130.;4_dl_testing_deep_network;2017;Output range analysis for deep neural networks;Souradeep Dutta, Susmit Jha, Sriram Sanakaranarayanan, Ashish Tiwari;arXiv preprint arXiv:1709.09130, 2017;"Deep neural networks (NN) are extensively used for machine learning tasks such as image classification, perception and control of autonomous systems. Increasingly, these deep NNs are also been deployed in high-assurance applications. Thus, there is a pressing need for developing techniques to verify neural networks to check whether certain user-expected properties are satisfied. In this paper, we study a specific verification problem of computing a guaranteed range for the output of a deep neural network given a set of inputs represented as a convex polyhedron. Range estimation is a key primitive for verifying deep NNs. We present an efficient range estimation algorithm that uses a combination of local search and linear programming problems to efficiently find the maximum and minimum values taken by the outputs of the NN over the given input set. In contrast to recently proposed ""monolithic"" optimization approaches, we use local gradient descent to repeatedly find and eliminate local minima of the function. The final global optimum is certified using a mixed integer programming instance. We implement our approach and compare it with Reluplex, a recently proposed solver for deep neural networks. We demonstrate the effectiveness of the proposed approach for verification of NNs used in automated control as well as those used in classification.";https://arxiv.org/abs/1709.09130;ucuRhBWHa24J
Arnold, M., Bellamy, R. K., Hind, M., Houde, S., Mehta, S., Mojsilović, A., ... & Varshney, K. R. (2019). FactSheets: Increasing trust in AI services through supplier's declarations of conformity. IBM Journal of Research and Development, 63(4/5), 6-1.;1_ml_machine_data_learning;2019;FactSheets: Increasing trust in AI services through supplier's declarations of conformity;Matthew Arnold, Rachel KE Bellamy, Michael Hind, Stephanie Houde, Sameep Mehta, Aleksandra Mojsilović, Ravi Nair, K Natesan Ramamurthy, Alexandra Olteanu, David Piorkowski, Darrell Reimer, John Richards, Jason Tsay, Kush R Varshney;IBM Journal of Research and Development 63 (4/5), 6: 1-6: 13, 2019;Accuracy is an important concern for suppliers of artificial intelligence (AI) services, but considerations beyond accuracy, such as safety (which includes fairness and explainability), security, and provenance, are also critical elements to engender consumers’ trust in a service. Many industries use transparent, standardized, but often not legally required documents called supplier's declarations of conformity (SDoCs) to describe the lineage of a product along with the safety and performance testing it has undergone. SDoCs may be considered multidimensional fact sheets that capture and quantify various aspects of the product and its development to make it worthy of consumers’ trust. In this article, inspired by this practice, we propose FactSheets to help increase trust in AI services. We envision such documents to contain purpose, performance, safety, security, and provenance information to be completed by AI service providers for examination by consumers. We suggest a comprehensive set of declaration items tailored to AI in the Appendix of this article.;https://ieeexplore.ieee.org/abstract/document/8843893/;0ZLoUEm3SxgJ
Fiebrink, R., Cook, P. R., & Trueman, D. (2011, May). Human model evaluation in interactive supervised learning. In Proceedings of the SIGCHI conference on human factors in computing systems (pp. 147-156).;3_explanation_model_machine_learning;2011;Human model evaluation in interactive supervised learning;Rebecca Fiebrink, Perry R Cook, Dan Trueman;Proceedings of the SIGCHI conference on human factors in computing systems, 147-156, 2011;Model evaluation plays a special role in interactive machine learning (IML) systems in which users rely on their assessment of a model's performance in order to determine how to improve it. A better understanding of what model criteria are important to users can therefore inform the design of user interfaces for model evaluation as well as the choice and design of learning algorithms. We present work studying the evaluation practices of end users interactively building supervised learning systems for real-world gesture analysis problems. We examine users' model evaluation criteria, which span conventionally relevant criteria such as accuracy and cost, as well as novel criteria such as unexpectedness. We observed that users employed evaluation techniques---including cross-validation and direct, real-time evaluation---not only to make relevant judgments of algorithms' performance and interactively improve the trained models, but also to learn to provide more effective training data. Furthermore, we observed that evaluation taught users about what types of models were easy or possible to build, and users sometimes used this information to modify the learning problem definition or their plans for using the trained models in practice. We discuss the implications of these findings with regard to the role of generalization accuracy in IML, the design of new algorithms and interfaces, and the scope of potential benefits of incorporating human interaction in the design of supervised learning systems.;https://dl.acm.org/doi/abs/10.1145/1978942.1978965;Uy5slSzk3_cJ
Ji, Y., Zhang, X., Ji, S., Luo, X., & Wang, T. (2018, October). Model-reuse attacks on deep learning systems. In Proceedings of the 2018 ACM SIGSAC conference on computer and communications security (pp. 349-363).;5_adversarial_attack_example_model;2018;Model-reuse attacks on deep learning systems;Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, Ting Wang;Proceedings of the 2018 ACM SIGSAC conference on computer and communications security, 349-363, 2018;Many of today's machine learning (ML) systems are built by reusing an array of, often pre-trained, primitive models, each fulfilling distinct functionality (e.g., feature extraction). The increasing use of primitive models significantly simplifies and expedites the development cycles of ML systems. Yet, because most of such models are contributed and maintained by untrusted sources, their lack of standardization or regulation entails profound security implications, about which little is known thus far. In this paper, we demonstrate that malicious primitive models pose immense threats to the security of ML systems. We present a broad class of model-reuse attacks wherein maliciously crafted models trigger host ML systems to misbehave on targeted inputs in a highly predictable manner. By empirically studying four deep learning systems (including both individual and ensemble systems) used in skin cancer screening, speech recognition, face verification, and autonomous steering, we show that such attacks are (i) effective - the host systems misbehave on the targeted inputs as desired by the adversary with high probability, (ii) evasive - the malicious models function indistinguishably from their benign counterparts on non-targeted inputs, (iii) elastic - the malicious models remain effective regardless of various system design choices and tuning strategies, and (iv) easy - the adversary needs little prior knowledge about the data used for system tuning or inference. We provide analytical justification for the effectiveness of model-reuse attacks, which points to the unprecedented complexity of today's primitive models. This issue thus seems fundamental to many ML systems. We further discuss potential countermeasures and their challenges, which lead to several promising research directions.;https://dl.acm.org/doi/abs/10.1145/3243734.3243757;1EdtLfWYn0QJ
Nushi, B., Kamar, E., & Horvitz, E. (2018, June). Towards accountable ai: Hybrid human-machine analyses for characterizing system failure. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing (Vol. 6, pp. 126-135).;1_ml_machine_data_learning;2018;Towards accountable ai: Hybrid human-machine analyses for characterizing system failure;Besmira Nushi, Ece Kamar, Eric Horvitz;Proceedings of the AAAI Conference on Human Computation and Crowdsourcing 6, 126-135, 2018;As machine learning systems move from computer-science laboratories into the open world, their accountability becomes a high priority problem. Accountability requires deep understanding of system behavior and its failures. Current evaluation methods such as single-score error metrics and confusion matrices provide aggregate views of system performance that hide important shortcomings. Understanding details about failures is important for identifying pathways for refinement, communicating the reliability of systems in different settings, and for specifying appropriate human oversight and engagement. Characterization of failures and shortcomings is particularly complex for systems composed of multiple machine learned components. For such systems, existing evaluation methods have limited expressiveness in describing and explaining the relationship among input content, the internal states of system components, and final output quality. We present Pandora, a set of hybrid human-machine methods and tools for describing and explaining system failures. Pandora leverages both human and system-generated observations to summarize conditions of system malfunction with respect to the input content and system architecture. We share results of a case study with a machine learning pipeline for image captioning that show how detailed performance views can be beneficial for analysis and debugging.;https://ojs.aaai.org/index.php/HCOMP/article/view/13337;5eO2DKvcALMJ
Foidl, H., Felderer, M., & Biffl, S. (2019, August). Technical debt in data-intensive software systems. In 2019 45th Euromicro conference on software engineering and advanced applications (SEAA) (pp. 338-341). IEEE.;9_data_science_software_process;2019;Technical debt in data-intensive software systems;Harald Foidl, Michael Felderer, Stefan Biffl;2019 45th Euromicro conference on software engineering and advanced applications (SEAA), 338-341, 2019;The ever-increasing amount, variety as well as generation and processing speed of today's data pose a variety of new challenges for developing Data-Intensive Software Systems (DISS). As with developing other kinds of software systems, developing DISS is often done under severe pressure and strict schedules. Thus, developers of DISS often have to make technical compromises to meet business concerns. This position paper proposes a conceptual model that outlines where Technical Debt (TD) can emerge and proliferate within such data-centric systems by separating a DISS into three parts (Software Systems, Data Storage Systems and Data). Further, the paper illustrates the proliferation of Database Schema Smells as TD items within a relational database-centric software system based on two examples.;https://ieeexplore.ieee.org/abstract/document/8906747/;qq5cqeDJpYsJ
Deak, R. M., & Morra, J. H. (2018). Aloha: A machine learning framework for engineers. In Proceedings of the SysML Conference.;1_ml_machine_data_learning;2018;Aloha: A machine learning framework for engineers;Ryan M Deak, Jonathan H Morra;Proceedings of the SysML Conference, 2018;The process of deploying machine learnt models to production systems can be difficult, time consuming, and prone to many engineering and data-related risks. Here, we present Aloha 1, a model representation framework that mitigates many of these risks. In Aloha, feature expansion and reporting are subsumed within the model boundary, allowing Aloha to read user-defined, rather than framework-defined data types, which eases integration in preexisting engineering pipelines. Alohaâ€™s reporting functionality has a variety of use cases including data anomaly detection and hierarchical model output logging. The latter benefit enables simple A/B testing and is also useful in classification scenarios when offline analysis on raw model scores is desirable. Feature expansion and reporting are performed non-strictly to avoid unnecessary computation.;https://mlsys.org/Conferences/2019/doc/2018/13.pdf;FpT85H3ub7IJ
Aniculaesei, A., Grieser, J., Rausch, A., Rehfeldt, K., & Warnecke, T. (2019, September). Graceful degradation of decision and control responsibility for autonomous systems based on dependability cages. In 5th International Symposium on Future Active Safety Technology toward Zero, Blacksburg, Virginia, USA.;2_safety_system_autonomous_vehicle;2019;Graceful degradation of decision and control responsibility for autonomous systems based on dependability cages;Adina Aniculaesei, Jorg Grieser, Andreas Rausch, Karina Rehfeldt, Tim Warnecke;5th International Symposium on Future Active Safety Technology toward Zero, Blacksburg, Virginia, USA, 2019;Safe and reliable autonomous vehicles are key to solving many of todayâ€™s mobility and transportation problems. However, this reliability cannot be achieved during development, as it is impossible to specify and test all situations in which a vehicle may find itself during operation. Therefore, this paper presents a novel software monitoring approach that is capable of assessing whether a situation observed by the vehicle was already tested and the vehicle behaves according to its specification. If this is not the case, it gradually transfers decision and control responsibilities from the autonomous vehicle to a remote control center in order to cope with errors or completely unknown situations. The presented concept defines four different levels of possible interventions, differing in the degree of responsibility transferred to a human remote operator.;https://www.researchgate.net/profile/Adina-Aniculaesei/publication/335692219_Graceful_Degradation_of_Decision_and_Control_Responsibility_for_Autonomous_Systems_based_on_Dependability_Cages/links/5e4aaf73458515072da6d563/Graceful-Degradation-of-Decision-and-Control-Responsibility-for-Autonomous-Systems-based-on-Dependability-Cages.pdf;WOXAeunHzjUJ
Tuncali, C. E., Fainekos, G., Prokhorov, D., Ito, H., & Kapinski, J. (2019). Requirements-driven test generation for autonomous vehicles with machine learning components. IEEE Transactions on Intelligent Vehicles, 5(2), 265-280.;2_safety_system_autonomous_vehicle;2019;Requirements-driven test generation for autonomous vehicles with machine learning components;Cumhur Erkan Tuncali, Georgios Fainekos, Danil Prokhorov, Hisahiro Ito, James Kapinski;IEEE Transactions on Intelligent Vehicles 5 (2), 265-280, 2019;Autonomous vehicles are complex systems that are challenging to test and debug. A requirements-driven approach to the development process can decrease the resources required to design and test these systems, while simultaneously increasing the reliability. We present a testing framework that uses signal temporal logic (STL), which is a precise and unambiguous requirements language. Our framework evaluates test cases against the STL formulae and additionally uses the requirements to automatically identify test cases that fail to satisfy the requirements. One of the key features of our tool is the support for machine learning (ML) components in the system design, such as deep neural networks. The framework allows evaluation of the control algorithms, including the ML components, and it also includes models of CCD camera, lidar, and radar sensors, as well as the vehicle environment. We use multiple methods to generate test cases, including covering arrays, which is an efficient method to search discrete variable spaces. The resulting test cases can be used to debug the controller design by identifying controller behaviors that do not satisfy requirements. The test cases can also enhance the testing phase of development by identifying critical corner cases that correspond to the limits of the system's allowed behaviors. We present STL requirements for an autonomous vehicle system, which capture both component-level and system-level behaviors. Additionally, we present three driving scenarios and demonstrate how our requirements-driven testing framework can be used to identify critical system behaviors, which can be used to support the development process.;https://ieeexplore.ieee.org/abstract/document/8911483/;BGszvUCzVg4J
Sun, X., Khedr, H., & Shoukry, Y. (2019, April). Formal verification of neural network controlled autonomous systems. In Proceedings of the 22nd ACM International Conference on Hybrid Systems: Computation and Control (pp. 147-156).;2_safety_system_autonomous_vehicle;2019;Formal verification of neural network controlled autonomous systems;Xiaowu Sun, Haitham Khedr, Yasser Shoukry;Proceedings of the 22nd ACM International Conference on Hybrid Systems: Computation and Control, 147-156, 2019;In this paper, we consider the problem of formally verifying the safety of an autonomous robot equipped with a Neural Network (NN) controller that processes LiDAR images to produce control actions. Given a workspace that is characterized by a set of polytopic obstacles, our objective is to compute the set of safe initial states such that a robot trajectory starting from these initial states is guaranteed to avoid the obstacles. Our approach is to construct a finite state abstraction of the system and use standard reachability analysis over the finite state abstraction to compute the set of safe initial states. To mathematically model the imaging function, that maps the robot position to the LiDAR image, we introduce the notion of imaging-adapted partitions of the workspace in which the imaging function is guaranteed to be affine. Given this workspace partitioning, a discrete-time linear dynamics of the robot, and a pre-trained NN controller with Rectified Linear Unit (ReLU) non-linearity, we utilize a Satisfiability Modulo Convex (SMC) encoding to enumerate all the possible assignments of different ReLUs. To accelerate this process, we develop a pre-processing algorithm that could rapidly prune the space of feasible ReLU assignments. Finally, we demonstrate the efficiency of the proposed algorithms using numerical simulations with the increasing complexity of the neural network controller.;https://dl.acm.org/doi/abs/10.1145/3302504.3311802;pu1blFnBDVIJ
Kuwajima, H., Yasuoka, H., & Nakae, T. (2019). Open problems in engineering machine learning systems and the quality model. arXiv preprint arXiv:1904.00001.;1_ml_machine_data_learning;2019;only citation on google scholar;only citation on google scholar;only citation on google scholar;only citation on google scholar;only citation on google scholar;only citation on google scholar
Banks, A., & Ashmore, R. (2019). Requirements Assurance in Machine Learning. In SafeAI@ AAAI.;2_safety_system_autonomous_vehicle;2019;Requirements Assurance in Machine Learning.;Alec Banks, Rob Ashmore;SafeAI@ AAAI, 2019;Training data is an important aspect of approaches that use Machine Learning techniques. More precisely, we assert that training data captures the requirements that should be satisfied by the trained algorithm. Hence, for safety applications, any argument relating to behavioural correctness has to consider how those requirements are embodied within the training data. To support this, based on approaches for requirements assurance in traditional safety-related software, we develop nine specific areas where confidence is required in training data. These are illustrated using a fictional example.;https://ceur-ws.org/Vol-2301/paper_8.pdf;y1HEt0GjUT0J
Srisakaokul, S., Zhang, Y., Zhong, Z., Yang, W., Xie, T., & Li, B. (2018). Muldef: Multi-model-based defense against adversarial examples for neural networks. arXiv preprint arXiv:1809.00065.;5_adversarial_attack_example_model;2018;Muldef: Multi-model-based defense against adversarial examples for neural networks;Siwakorn Srisakaokul, Yuhao Zhang, Zexuan Zhong, Wei Yang, Tao Xie, Bo Li;arXiv preprint arXiv:1809.00065, 2018;Despite being popularly used in many applications, neural network models have been found to be vulnerable to adversarial examples, i.e., carefully crafted examples aiming to mislead machine learning models. Adversarial examples can pose potential risks on safety and security critical applications. However, existing defense approaches are still vulnerable to attacks, especially in a white-box attack scenario. To address this issue, we propose a new defense approach, named MulDef, based on robustness diversity. Our approach consists of (1) a general defense framework based on multiple models and (2) a technique for generating these multiple models to achieve high defense capability. In particular, given a target model, our framework includes multiple models (constructed from the target model) to form a model family. The model family is designed to achieve robustness diversity (i.e., an adversarial example successfully attacking one model cannot succeed in attacking other models in the family). At runtime, a model is randomly selected from the family to be applied on each input example. Our general framework can inspire rich future research to construct a desirable model family achieving higher robustness diversity. Our evaluation results show that MulDef (with only up to 5 models in the family) can substantially improve the target model's accuracy on adversarial examples by 22-74% in a white-box attack scenario, while maintaining similar accuracy on legitimate examples.;https://arxiv.org/abs/1809.00065;IAmFJQQKNsEJ
Machida, F. (2019, June). N-version machine learning models for safety critical systems. In 2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W) (pp. 48-51). IEEE.;2_safety_system_autonomous_vehicle;2019;N-version machine learning models for safety critical systems;Fumio Machida;2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W), 48-51, 2019;"Quality control of machine learning systems is a fundamental challenge in industries to provide intelligent services or products using machine learning. While recent advances in machine learning algorithms substantially improve the performance of intelligent tasks such as object recognition, their outputs are essentially stochastic and very sensitive to input data. Such an output uncertainty is a big obstacle to ensure the quality of safety critical applications like autonomous vehicle and hence architectural design to mitigate the impact of error output becomes a great importance. In this paper, we propose N-version machine learning architecture that aims to improve system reliability against probabilistic outputs of individual machine learning modules. The key idea of this architecture is exploiting two kinds of diversities; input diversity and model diversity. Our study first formally defines these diversity metrics and analytically shows the improved reliability by N-version machine learning architecture. Since we treat a machine learning module as a black-box, the proposed architecture and the reliability property are generally applicable to any machine learning algorithms and applications.";https://ieeexplore.ieee.org/abstract/document/8806018/;DD2FPstkyToJ
Yaghoubi, S., & Fainekos, G. (2019, April). Gray-box adversarial testing for control systems with machine learning components. In Proceedings of the 22nd ACM International Conference on Hybrid Systems: Computation and Control (pp. 179-184).;2_safety_system_autonomous_vehicle;2019;Gray-box adversarial testing for control systems with machine learning components;Shakiba Yaghoubi, Georgios Fainekos;Proceedings of the 22nd ACM International Conference on Hybrid Systems: Computation and Control, 179-184, 2019;Neural Networks (NN) have been proposed in the past as an effective means for both modeling and control of systems with very complex dynamics. However, despite the extensive research, NN-based controllers have not been adopted by the industry for safety critical systems. The primary reason is that systems with learning based controllers are notoriously hard to test and verify. Even harder is the analysis of such systems against system-level specifications. In this paper, we provide a gradient based method for searching the input space of a closed-loop control system in order to find adversarial samples against some system-level requirements. Our experimental results show that combined with randomized search, our method outperforms Simulated Annealing optimization.;https://dl.acm.org/doi/abs/10.1145/3302504.3311814;NmnVHGWuczIJ
Ishikawa, F., & Matsuno, Y. (2018). Continuous argument engineering: Tackling uncertainty in machine learning based systems. In Computer Safety, Reliability, and Security: SAFECOMP 2018 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Västerås, Sweden, September 18, 2018, Proceedings 37 (pp. 14-21). Springer International Publishing.;2_safety_system_autonomous_vehicle;2018;Continuous argument engineering: Tackling uncertainty in machine learning based systems;Fuyuki Ishikawa, Yutaka Matsuno;Computer Safety, Reliability, and Security: SAFECOMP 2018 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Västerås, Sweden, September 18, 2018, Proceedings 37, 14-21, 2018;Components or systems implemented by using machine learning techniques have intrinsic difficulties caused by uncertainty. Specifically, it is impossible to logically or deductively conclude what they can(not) do or how they behave for untested inputs. In addition, such systems are often applied to the real world, which has uncertain requirements and environments. In this paper, we discuss what becomes difficult or even impossible in the use of arguments or assurance cases for machine learning based systems. We then propose an approach for continuously analyzing, managing, and updating arguments while accepting uncertainty as intrinsic in nature.;https://link.springer.com/chapter/10.1007/978-3-319-99229-7_2;NStFE6nzGTUJ
Bansal, S., & Tomlin, C. J. (2019). Control and safety of autonomous vehicles with learning-enabled components. Safe, Autonomous and Intelligent Vehicles, 57-75.;2_safety_system_autonomous_vehicle;2019;Control and safety of autonomous vehicles with learning-enabled components;Somil Bansal, Claire J Tomlin;Safe, Autonomous and Intelligent Vehicles, 57-75, 2019;"Real-world autonomous systems, such as autonomous vehicles, often operate in uncertain and partially observable environments. In such scenarios, designing a controller that achieves the desired behavior on the system is a challenging problem. The proven efficacy of learning-based control schemes strongly motivates their application to autonomous vehicles. However, guaranteeing correct operation of learning-based schemes during and after the learning process is currently an unresolved issue, which is of vital importance in safety-critical systems such as autonomous vehicles. Hamilton-Jacobi (HJ) reachability analysis is an important formal verification method for guaranteeing performance and safety properties of dynamical systems; it has been applied to many small-scale systems in the past decade. Its advantages include compatibility with general nonlinear system dynamics, formal treatment of bounded disturbances, and the availability of well-developed numerical tools.In this chapter, we provide a brief overview of the challenges associated with system verification when learning is involved in the control loop, some recent attempts to address these challenges based on HJ reachability, and the open questions that remain to be answered.";https://link.springer.com/chapter/10.1007/978-3-319-97301-2_4;l_8hPyHqatUJ
Hartsell, C., Mahadevan, N., Ramakrishna, S., Dubey, A., Bapty, T., Johnson, T., ... & Karsai, G. (2019, April). Model-based design for CPS with learning-enabled components. In Proceedings of the Workshop on Design Automation for CPS and IoT (pp. 1-9).;2_safety_system_autonomous_vehicle;2019;Model-based design for CPS with learning-enabled components;Charles Hartsell, Nagabhushan Mahadevan, Shreyas Ramakrishna, Abhishek Dubey, Theodore Bapty, Taylor Johnson, Xenofon Koutsoukos, Janos Sztipanovits, Gabor Karsai;Proceedings of the Workshop on Design Automation for CPS and IoT, 1-9, 2019;Recent advances in machine learning led to the appearance of Learning-Enabled Components (LECs) in Cyber-Physical Systems. LECs are being evaluated and used for various, complex functions including perception and control. However, very little tool support is available for design automation in such systems. This paper introduces an integrated toolchain that supports the architectural modeling of CPS with LECs, but also has extensive support for the engineering and integration of LECs, including support for training data collection, LEC training, LEC evaluation and verification, and system software deployment. Additionally, the toolsuite supports the modeling and analysis of safety cases - a critical part of the engineering process for mission and safety critical systems.;https://dl.acm.org/doi/abs/10.1145/3313151.3313166;FjHDr6hyEZ0J
Spieker, H., & Gotlieb, A. (2019). Towards testing of deep learning systems with training set reduction. arXiv preprint arXiv:1901.04169.;4_dl_testing_deep_network;2019;Towards testing of deep learning systems with training set reduction;Helge Spieker, Arnaud Gotlieb;arXiv preprint arXiv:1901.04169, 2019;Testing the implementation of deep learning systems and their training routines is crucial to maintain a reliable code base. Modern software development employs processes, such as Continuous Integration, in which changes to the software are frequently integrated and tested. However, testing the training routines requires running them and fully training a deep learning model can be resource-intensive, when using the full data set. Using only a subset of the training data can improve test run time, but can also reduce its effectiveness. We evaluate different ways for training set reduction and their ability to mimic the characteristics of model training with the original full data set. Our results underline the usefulness of training set reduction, especially in resource-constrained environments.;https://arxiv.org/abs/1901.04169;Ew4YrD1nVAEJ
Sarathy, P., Baruah, S., Cook, S., & Wolf, M. (2019, September). Realizing the promise of artificial intelligence for unmanned aircraft systems through behavior bounded assurance. In 2019 IEEE/AIAA 38th Digital Avionics Systems Conference (DASC) (pp. 1-8). IEEE.;2_safety_system_autonomous_vehicle;2019;Realizing the promise of artificial intelligence for unmanned aircraft systems through behavior bounded assurance;Prakash Sarathy, Sanjoy Baruah, Stephen Cook, Marilyn Wolf;2019 IEEE/AIAA 38th Digital Avionics Systems Conference (DASC), 1-8, 2019;"A key value proposition for incorporation of Artificial Intelligence (AI) and Machine Learning (ML) methods into aviation is that they offer means of understanding data in ways that allow hitherto unprecedented insights for decision making, whether by a human or a machine. When these techniques are applied to cyber-physical systems, such as unmanned aircraft systems (UAS), they can result in positive societal impacts (e.g., search and rescue). However, the advantages of such techniques must be balanced against appropriate safety and security requirements so that taken together the system can ensure an acceptable level of confidence and assurance in both civilian and military applications. To this end, there is a need for the capability to suitably characterize such techniques and assess how they can be integrated into a viable assurance framework that can maximize safety and security benefits while bounding the inherent risk of non-determinism arising from such these approaches. This paper focuses on assurance and behavior bounds for decision making systems from a) algorithmic functional performance; b) schedulability analysis and candidate scheduling paradigms; and c) processor architectures (including multi-core) to support minimized interference in general. We will place particular emphasis on machine learning approaches for control, navigation and guidance applications for unmanned systems. This paper will review available and emerging approaches (e.g., formal methods, modeling and simulation, real-time monitors/agents among others) to ensuring behavior assurance for unmanned systems engaged in missions of moderate-to-high complexity. The intent is to examine behavior assurance for advanced autonomous operations within a holistic life-cycle process.";https://ieeexplore.ieee.org/abstract/document/9081649/;O17M8tNlS-EJ
Jenn, E., Albore, A., Mamalet, F., Flandin, G., Gabreau, C., Delseny, H., ... & Pagetti, C. (2020, January). Identifying challenges to the certification of machine learning for safety critical systems. In European congress on embedded real time systems (ERTS 2020).;2_safety_system_autonomous_vehicle;2020;Identifying challenges to the certification of machine learning for safety critical systems;Eric Jenn, Alexandre Albore, Franck Mamalet, Grégory Flandin, Christophe Gabreau, Hervé Delseny, Adrien Gauffriau, Hugues Bonnin, Lucian Alecu, Jérémy Pirard, Baptiste Lefevre, Jean-Marc Gabriel, Cyril Cappi, Laurent Gardès, Sylvaine Picard, Gilles Dulon, Brice Beltran, Jean-Christophe Bianic, Mathieu Damour, Kevin Delmas, Claire Pagetti;European congress on embedded real time systems (ERTS 2020), 2020;Today, Machine Learning (ML) seems to be one of the only technically and economically viable solution to automate some complex tasks usually realized by humans, such as driving vehicles, recognizing voice, etc. However, these techniques come with new potential risks and as so, have only been applied in systems where the benefits of the technique are considered worth this increase of risk. But when dependability is at stake, the risk level must be contained. Giving confidence in the ML-based system to the developer of the system, to the regulation or certification authority that delivers the authorization to commission the system, or to the human operator that will interact with the system, becomes an essential objective. In order to identify the main challenges for placing a justifiable reliance on systems embedding ML and, eventually, certify those systems, the Institut de Recherche Technologique Saint-Exupery de Toulouse (IRT) has created a workgroup involving key players in the automotive, railways, and aeronautical domains.;https://www.erts2020.org/uploads/program/ERTS2020_paper_17.pdf;tw8EEDe2SqcJ
Machida, F. (2019, December). On the diversity of machine learning models for system reliability. In 2019 IEEE 24th Pacific Rim International Symposium on Dependable Computing (PRDC) (pp. 276-27609). IEEE.;2_safety_system_autonomous_vehicle;2019;On the diversity of machine learning models for system reliability;Fumio Machida;2019 IEEE 24th Pacific Rim International Symposium on Dependable Computing (PRDC), 276-27609, 2019;The diversity of system components is one of the important contributing factors of reliable and secure software systems. In a software fault-tolerant system using diverse versions of software components, a component failure caused by defects or malicious attacks can be covered by other versions. Machine learning systems can also benefit from such a multi-version approach to improve the system reliability. Nevertheless, there are few studies addressing this issue. In this paper, we experimentally analyze how outputs of machine learning modules can be diversified by using different versions of machine learning algorithms, neural network architectures and perturbated input data. The experiments are conducted on image classification tasks of MNIST data set and Belgian Traffic Sign data set. Different neural network architectures, support vector machines and random forests are used for constructing diverse machine learning models. The diversity is characterized by the coverage of errors over the test samples. We observe that the different machine learning models have quite different error coverages that can be leveraged for system reliability design. Based on the experimental results, we construct the reliability model for three-version machine learning architecture with a diversity measure defined as the intersection of error spaces in the sample space. From the presented reliability model, we derive a necessary condition under which three-version architecture achieves a higher system reliability than a single machine learning module.;https://ieeexplore.ieee.org/abstract/document/8952144/;SZBvSlRq0fsJ
Simard, P. Y., Amershi, S., Chickering, D. M., Pelton, A. E., Ghorashi, S., Meek, C., ... & Wernsing, J. (2017). Machine teaching: A new paradigm for building machine learning systems. arXiv preprint arXiv:1707.06742.;1_ml_machine_data_learning;2017;Machine teaching: A new paradigm for building machine learning systems;Patrice Y Simard, Saleema Amershi, David M Chickering, Alicia Edelman Pelton, Soroush Ghorashi, Christopher Meek, Gonzalo Ramos, Jina Suh, Johan Verwey, Mo Wang, John Wernsing;arXiv preprint arXiv:1707.06742, 2017;"The current processes for building machine learning systems require practitioners with deep knowledge of machine learning. This significantly limits the number of machine learning systems that can be created and has led to a mismatch between the demand for machine learning systems and the ability for organizations to build them. We believe that in order to meet this growing demand for machine learning systems we must significantly increase the number of individuals that can teach machines. We postulate that we can achieve this goal by making the process of teaching machines easy, fast and above all, universally accessible. While machine learning focuses on creating new algorithms and improving the accuracy of ""learners"", the machine teaching discipline focuses on the efficacy of the ""teachers"". Machine teaching as a discipline is a paradigm shift that follows and extends principles of software engineering and programming languages. We put a strong emphasis on the teacher and the teacher's interaction with data, as well as crucial components such as techniques and design principles of interaction and visualization. In this paper, we present our position regarding the discipline of machine teaching and articulate fundamental machine teaching principles. We also describe how, by decoupling knowledge about machine learning algorithms from the process of teaching, we can accelerate innovation and empower millions of new uses for machine learning models.";https://arxiv.org/abs/1707.06742;Wy3QEuBRFH0J
Sheh, R., & Monteath, I. (2018). Defining explainable ai for requirements analysis. KI-Künstliche Intelligenz, 32, 261-266.;3_explanation_model_machine_learning;2018;Defining explainable ai for requirements analysis;Raymond Sheh, Isaac Monteath;KI-KÃ¼nstliche Intelligenz 32, 261-266, 2018;Explainable artificial intelligence (XAI) has become popular in the last few years. The artificial intelligence (AI) community in general, and the machine learning (ML) community in particular, is coming to the realisation that in many applications, for AI to be trusted, it must not only demonstrate good performance in its decisionmaking, but it also must explain these decisions and convince us that it is making the decisions for the right reasons. However, different applications have different requirements on the information required of the underlying AI system in order to convince us that it is worthy of our trust. How do we define these requirements? In this paper, we present three dimensions for categorising the explanatory requirements of different applications. These are Source, Depth and Scope. We focus on the problem of matching up the explanatory requirements of different applications with the capabilities of underlying ML techniques to provide them. We deliberately avoid including aspects of explanation that are already well-covered by the existing literature and we focus our discussion on ML although the principles apply to AI more broadly.;https://link.springer.com/article/10.1007/s13218-018-0559-3;STLVlIzjpzMJ
Bailis, P., Olukotun, K., Ré, C., & Zaharia, M. (2017). Infrastructure for usable machine learning: The stanford dawn project. arXiv preprint arXiv:1705.07538.;1_ml_machine_data_learning;2017;Infrastructure for usable machine learning: The stanford dawn project;Peter Bailis, Kunle Olukotun, Christopher RÃ©, Matei Zaharia;arXiv preprint arXiv:1705.07538, 2017;Despite incredible recent advances in machine learning, building machine learning applications remains prohibitively time-consuming and expensive for all but the best-trained, best-funded engineering organizations. This expense comes not from a need for new and improved statistical models but instead from a lack of systems and tools for supporting end-to-end machine learning application development, from data preparation and labeling to productionization and monitoring. In this document, we outline opportunities for infrastructure supporting usable, end-to-end machine learning applications in the context of the nascent DAWN (Data Analytics for What's Next) project at Stanford.;https://arxiv.org/abs/1705.07538;fl5WPnKydjMJ
Kumar, A., Braud, T., Tarkoma, S., & Hui, P. (2020, March). Trustworthy AI in the age of pervasive computing and big data. In 2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops) (pp. 1-6). IEEE.;12_ai_ethical_ethic_intelligence;2020;Trustworthy AI in the age of pervasive computing and big data;Abhishek Kumar, Tristan Braud, Sasu Tarkoma, Pan Hui;2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops), 1-6, 2020;The era of pervasive computing has resulted in countless devices that continuously monitor users and their environment, generating an abundance of user behavioural data. Such data may support improving the quality of service, but may also lead to adverse usages such as surveillance and advertisement. In parallel, Artificial Intelligence (AI) systems are being applied to sensitive fields such as healthcare, justice, or human resources, raising multiple concerns on the trustworthiness of such systems. Trust in AI systems is thus intrinsically linked to ethics, including the ethics of algorithms, the ethics of data, or the ethics of practice. In this paper, we formalise the requirements of trustworthy AI systems through an ethics perspective. We specifically focus on the aspects that can be integrated into the design and development of AI systems. After discussing the state of research and the remaining challenges, we show how a concrete use-case in smart cities can benefit from these methods.;https://ieeexplore.ieee.org/abstract/document/9156127/;jBbYs0d8gJYJ
Tuncali, C. E., Fainekos, G., Ito, H., & Kapinski, J. (2018, June). Simulation-based adversarial test generation for autonomous vehicles with machine learning components. In 2018 IEEE Intelligent Vehicles Symposium (IV) (pp. 1555-1562). IEEE.;2_safety_system_autonomous_vehicle;2018;Simulation-based adversarial test generation for autonomous vehicles with machine learning components;Cumhur Erkan Tuncali, Georgios Fainekos, Hisahiro Ito, James Kapinski;2018 IEEE Intelligent Vehicles Symposium (IV), 1555-1562, 2018;Many organizations are developing autonomous driving systems, which are expected to be deployed at a large scale in the near future. Despite this, there is a lack of agreement on appropriate methods to test, debug, and certify the performance of these systems. One of the main challenges is that many autonomous driving systems have machine learning (ML) components, such as deep neural networks, for which formal properties are difficult to characterize. We present a testing framework that is compatible with test case generation and automatic falsification methods, which are used to evaluate cyber-physical systems. We demonstrate how the framework can be used to evaluate closed-loop properties of an autonomous driving system model that includes the ML components, all within a virtual environment. We demonstrate how to use test case generation methods, such as covering arrays, as well as requirement falsification methods to automatically identify problematic test scenarios. The resulting framework can be used to increase the reliability of autonomous driving systems.;https://ieeexplore.ieee.org/abstract/document/8500421/;Dxc99u3B_k8J
Gharib, M., Lollini, P., Botta, M., Amparore, E., Donatelli, S., & Bondavalli, A. (2018, June). On the safety of automotive systems incorporating machine learning based components: a position paper. In 2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W) (pp. 271-274). IEEE.;2_safety_system_autonomous_vehicle;2018;On the safety of automotive systems incorporating machine learning based components: a position paper;Mohamad Gharib, Paolo Lollini, Marco Botta, Elvio Amparore, Susanna Donatelli, Andrea Bondavalli;2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W), 271-274, 2018;Machine learning (ML) components are increasingly adopted in many automated systems. Their ability to learn and work with novel input/incomplete knowledge and their generalization capabilities make them highly desirable solutions for complex problems. This has motivated the inclusion of ML techniques/components in products for many industrial domains including automotive systems. Such systems are safety-critical systems since their failure may cause death or injury to humans. Therefore, their safety must be ensured before they are used in their operational environment. However, existing safety standards and Verification and Validation (V&V) techniques do not properly address the special characteristics of ML-based components such as non-determinism, non-transparency, instability. This position paper presents the authors' view on the safety of automotive systems incorporating ML-based components, and it is intended to motivate and sketch a research agenda for extending a safety standard, namely ISO 26262, to address challenges posed by incorporating ML-based components in automotive systems.;https://ieeexplore.ieee.org/abstract/document/8416259/;86Aqc5GmPogJ
Kuwajima, H., Yasuoka, H., & Nakae, T. (2020). Engineering problems in machine learning systems. Machine Learning, 109(5), 1103-1126.;2_safety_system_autonomous_vehicle;2020;Engineering problems in machine learning systems;Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae;Machine Learning 109 (5), 1103-1126, 2020;Fatal accidents are a major issue hindering the wide acceptance of safety-critical systems that employ machine learning and deep learning models, such as automated driving vehicles. In order to use machine learning in a safety-critical system, it is necessary to demonstrate the safety and security of the system through engineering processes. However, thus far, no such widely accepted engineering concepts or frameworks have been established for these systems. The key to using a machine learning model in a deductively engineered system is decomposing the data-driven training of machine learning models into requirement, design, and verification, particularly for machine learning models used in safety-critical systems. Simultaneously, open problems and relevant technical fields are not organized in a manner that enables researchers to select a theme and work on it. In this study, we identify, classify, and explore the open problems in engineering (safety-critical) machine learning systems—that is, in terms of requirement, design, and verification of machine learning models and systems—as well as discuss related works and research directions, using automated driving vehicles as an example. Our results show that machine learning models are characterized by a lack of requirements specification, lack of design specification, lack of interpretability, and lack of robustness. We also perform a gap analysis on a conventional system quality standard SQuaRE with the characteristics of machine learning models to study quality models for machine learning systems. We find that a lack of requirements specification and lack of robustness have the greatest impact on conventional quality models.;https://link.springer.com/article/10.1007/s10994-020-05872-w?code=2c0c0228-4ff9-45e6-9285-d6dace01edc0;zwecpun6HNoJ
Tuncali, C. E., Kapinski, J., Ito, H., & Deshmukh, J. V. (2018, June). Reasoning about safety of learning-enabled components in autonomous cyber-physical systems. In Proceedings of the 55th Annual Design Automation Conference (pp. 1-6).;2_safety_system_autonomous_vehicle;2018;Reasoning about safety of learning-enabled components in autonomous cyber-physical systems;Cumhur Erkan Tuncali, James Kapinski, Hisahiro Ito, Jyotirmoy V Deshmukh;Proceedings of the 55th Annual Design Automation Conference, 1-6, 2018;We present a simulation-based approach for generating barrier certificate functions for safety verification of cyber-physical systems (CPS) that contain neural network-based controllers. A linear programming solver is utilized to find a candidate generator function from a set of simulation traces obtained by randomly selecting initial states for the CPS model. A level set of the generator function is then selected to act as a barrier certificate for the system, meaning it demonstrates that no unsafe system states are reachable from a given set of initial states. The barrier certificate properties are verified with an SMT solver. This approach is demonstrated on a case study in which a Dubins car model of an autonomous vehicle is controlled by a neural network to follow a given path.;https://dl.acm.org/doi/abs/10.1145/3195970.3199852;5iy7l7tpwHQJ
McDermid, J. A., Jia, Y., & Habli, I. (2019, August). Towards a framework for safety assurance of autonomous systems. In Artificial Intelligence Safety 2019 (pp. 1-7). CEUR Workshop Proceedings.;2_safety_system_autonomous_vehicle;2019;Towards a framework for safety assurance of autonomous systems;John Alexander McDermid, Yan Jia, Ibrahim Habli;Artificial Intelligence Safety 2019, 1-7, 2019;Autonomous systems have the potential to provide great benefit to society. However, they also pose problems for safety assurance, whether fully auton-omous or remotely operated (semi-autonomous). This paper discusses the challenges of safety assur-ance of autonomous systems and proposes a novel framework for safety assurance that, inter alia, uses machine learning to provide evidence for a system safety case and thus enables the safety case to be updated dynamically as system behaviour evolves.;https://eprints.whiterose.ac.uk/150187/;FE770Gg3K8MJ
Zhu, H., Xiong, Z., Magill, S., & Jagannathan, S. (2019, June). An inductive synthesis framework for verifiable reinforcement learning. In Proceedings of the 40th ACM SIGPLAN conference on programming language design and implementation (pp. 686-701).;2_safety_system_autonomous_vehicle;2019;An inductive synthesis framework for verifiable reinforcement learning;He Zhu, Zikang Xiong, Stephen Magill, Suresh Jagannathan;Proceedings of the 40th ACM SIGPLAN conference on programming language design and implementation, 686-701, 2019;Despite the tremendous advances that have been made in the last decade on developing useful machine-learning applications, their wider adoption has been hindered by the lack of strong assurance guarantees that can be made about their behavior. In this paper, we consider how formal verification techniques developed for traditional software systems can be repurposed for verification of reinforcement learning-enabled ones, a particularly important class of machine learning systems. Rather than enforcing safety by examining and altering the structure of a complex neural network implementation, our technique uses blackbox methods to synthesizes deterministic programs, simpler, more interpretable, approximations of the network that can nonetheless guarantee desired safety properties are preserved, even when the network is deployed in unanticipated or previously unobserved environments. Our methodology frames the problem of neural network verification in terms of a counterexample and syntax-guided inductive synthesis procedure over these programs. The synthesis procedure searches for both a deterministic program and an inductive invariant over an infinite state transition system that represents a specification of an application's control logic. Additional specifications defining environment-based constraints can also be provided to further refine the search space. Synthesized programs deployed in conjunction with a neural network implementation dynamically enforce safety conditions by monitoring and preventing potentially unsafe actions proposed by neural policies. Experimental results over a wide range of cyber-physical applications demonstrate that software-inspired formal verification techniques can be used to realize trustworthy reinforcement learning systems with low overhead.;https://dl.acm.org/doi/abs/10.1145/3314221.3314638;zdtYI1K8IQwJ
Chakarov, A., Nori, A., Rajamani, S., Sen, S., & Vijaykeerthy, D. (2016). Debugging machine learning tasks. arXiv preprint arXiv:1603.07292.;10_testing_test_machine_metamorphic;2016;Debugging machine learning tasks;Aleksandar Chakarov, Aditya Nori, Sriram Rajamani, Shayak Sen, Deepak Vijaykeerthy;arXiv preprint arXiv:1603.07292, 2016;Unlike traditional programs (such as operating systems or word processors) which have large amounts of code, machine learning tasks use programs with relatively small amounts of code (written in machine learning libraries), but voluminous amounts of data. Just like developers of traditional programs debug errors in their code, developers of machine learning tasks debug and fix errors in their data. However, algorithms and tools for debugging and fixing errors in data are less common, when compared to their counterparts for detecting and fixing errors in code. In this paper, we consider classification tasks where errors in training data lead to misclassifications in test points, and propose an automated method to find the root causes of such misclassifications. Our root cause analysis is based on Pearl's theory of causation, and uses Pearl's PS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi, encodes the computation of PS as a probabilistic program, and uses recent work on probabilistic programs and transformations on probabilistic programs (along with gray-box models of machine learning algorithms) to efficiently compute PS. Psi is able to identify root causes of data errors in interesting data sets.;https://arxiv.org/abs/1603.07292;LiNryCqKrpIJ
Alahdab, M., & Çalıklı, G. (2019). Empirical analysis of hidden technical debt patterns in machine learning software. In Product-Focused Software Process Improvement: 20th International Conference, PROFES 2019, Barcelona, Spain, November 27–29, 2019, Proceedings 20 (pp. 195-202). Springer International Publishing.;1_ml_machine_data_learning;2019;Empirical analysis of hidden technical debt patterns in machine learning software;Mohannad Alahdab, Gül Çalıklı;Product-Focused Software Process Improvement: 20th International Conference, PROFES 2019, Barcelona, Spain, November 27–29, 2019, Proceedings 20, 195-202, 2019;[Context/Background] Machine Learning (ML) software has special ability for increasing technical debt due to ML-specific issues besides having all the problems of regular code. The term “Hidden Technical Debt” (HTD) was coined by Sculley et al. to address maintainability issues in ML software as an analogy to technical debt in traditional software. [Goal] The aim of this paper is to empirically analyse how HTD patterns emerge during the early development phase of ML software, namely the prototyping phase. [Method] Therefore, we conducted a case study with subject systems as ML models planned to be integrated into the software system owned by Västtrafik, the public transportation agency in the west area of Sweden. [Results] During our case study, we could detect HTD patterns, which have the potential to emerge in ML prototypes, except for “Legacy Features”, “Correlated features”, and “Plain Old Data Type Smell”. [Conclusion] Preliminary results indicate that emergence of significant amount of HTD patterns can occur during prototyping phase. However, generalizability of our results require analyses of further ML systems from various domains.;https://link.springer.com/chapter/10.1007/978-3-030-35333-9_14;Z0R1HJEVnlUJ
Pons, L., & Ozkaya, I. (2019). Priority quality attributes for engineering ai-enabled systems. arXiv preprint arXiv:1911.02912.;1_ml_machine_data_learning;2019;Priority quality attributes for engineering ai-enabled systems;Lena Pons, Ipek Ozkaya;arXiv preprint arXiv:1911.02912, 2019;Deploying successful software-reliant systems that address their mission goals and user needs within cost, resource, and expected quality constraints require design trade-offs. These trade-offs dictate how systems are structured and how they behave and consequently can effectively be evolved and sustained. Software engineering practices address this challenge by centering system design and evolution around delivering key quality attributes, such as security, privacy, data centricity, sustainability, and explainability. These concerns are more urgent requirements for software-reliant systems that also include AI components due to the uncertainty introduced by data elements. Moreover, systems employed by the public sector exhibit unique design time and runtime challenges due to the regulatory nature of the domains. We assert that the quality attributes of security, privacy, data centricity, sustainability, and explainability pose new challenges to AI engineering and will drive the success of AI-enabled systems in the public sector. In this position paper, we enumerate with examples from healthcare domain concerns related to these requirements to mitigate barriers to architecting and fielding AI-enabled systems in the public sector.;https://arxiv.org/abs/1911.02912;n1BC0sB14q4J
Raji, I. D., Smart, A., White, R. N., Mitchell, M., Gebru, T., Hutchinson, B., ... & Barnes, P. (2020, January). Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing. In Proceedings of the 2020 conference on fairness, accountability, and transparency (pp. 33-44).;12_ai_ethical_ethic_intelligence;2020;Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing;Inioluwa Deborah Raji, Andrew Smart, Rebecca N White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, Parker Barnes;Proceedings of the 2020 conference on fairness, accountability, and transparency, 33-44, 2020;Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source.In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.;https://dl.acm.org/doi/abs/10.1145/3351095.3372873;cXiy63KSNhgJ
Studer, S., Bui, T. B., Drescher, C., Hanuschkin, A., Winkler, L., Peters, S., & Müller, K. R. (2021). Towards CRISP-ML (Q): a machine learning process model with quality assurance methodology. Machine learning and knowledge extraction, 3(2), 392-413.;1_ml_machine_data_learning;2021;Towards CRISP-ML (Q): a machine learning process model with quality assurance methodology;Stefan Studer, Thanh Binh Bui, Christian Drescher, Alexander Hanuschkin, Ludwig Winkler, Steven Peters, Klaus-Robert MÃ¼ller;Machine learning and knowledge extraction 3 (2), 392-413, 2021;Machine learning is an established and frequently used technique in industry and academia, but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners face manifold challenges and risks when developing machine learning applications and have a need for guidance to meet business expectations. This paper therefore proposes a process model for the development of machine learning applications, covering six phases from defining the scope to maintaining the deployed machine learning application. Business and data understanding are executed simultaneously in the first phase, as both have considerable impact on the feasibility of the project. The next phases are comprised of data preparation, modeling, evaluation, and deployment. Special focus is applied to the last phase, as a model running in changing real-time environments requires close monitoring and maintenance to reduce the risk of performance degradation over time. With each task of the process, this work proposes quality assurance methodology that is suitable to address challenges in machine learning development that are identified in the form of risks. The methodology is drawn from practical experience and scientific literature, and has proven to be general and stable. The process model expands on CRISP-DM, a data mining process model that enjoys strong industry support, but fails to address machine learning specific tasks. The presented work proposes an industry- and application-neutral process model tailored for machine learning applications with a focus on technical tasks for quality assurance.;https://www.mdpi.com/2504-4990/3/2/20;2y8NTECc2XwJ
Wu, W., Xu, H., Zhong, S., Lyu, M. R., & King, I. (2019, June). Deep validation: Toward detecting real-world corner cases for deep neural networks. In 2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN) (pp. 125-137). IEEE.;4_dl_testing_deep_network;2019;Deep validation: Toward detecting real-world corner cases for deep neural networks;Weibin Wu, Hui Xu, Sanqiang Zhong, Michael R Lyu, Irwin King;2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 125-137, 2019;The exceptional performance of Deep neural networks (DNNs) encourages their deployment in safety-and dependability-critical systems. However, DNNs often demonstrate erroneous behaviors in real-world corner cases. Existing countermeasures center on improving the testing and bug-fixing practice. Unfortunately, building a bug-free DNN-based system is almost impossible currently due to its black-box nature, so anomaly detection is imperative in practice. Motivated by the idea of data validation in a traditional program, we propose and implement Deep Validation, a novel framework for detecting real-world error-inducing corner cases in a DNN-based system during runtime. We model the specifications of DNNs by resorting to their training data and cast checking input validity of DNNs as the problem of discrepancy estimation. Deep Validation achieves excellent detection results against various corner case scenarios across three popular datasets. Consequently, Deep Validation greatly complements existing efforts and is a crucial step toward building safe and dependable DNN-based systems.;https://ieeexplore.ieee.org/abstract/document/8809533/;1xomJhvViTkJ
Salay, R., & Czarnecki, K. (2019). Improving ML safety with partial specifications. In Computer Safety, Reliability, and Security: SAFECOMP 2019 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Turku, Finland, September 10, 2019, Proceedings 38 (pp. 288-300). Springer International Publishing.;2_safety_system_autonomous_vehicle;2019;Improving ML safety with partial specifications;Rick Salay, Krzysztof Czarnecki;Computer Safety, Reliability, and Security: SAFECOMP 2019 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Turku, Finland, September 10, 2019, Proceedings 38, 288-300, 2019;Advanced autonomy features of vehicles are typically difficult or impossible to specify precisely and this has led to the rise of machine learning (ML) from examples as an alternative implementation approach to traditional programming. Developing software without specifications sacrifices the ability to effectively verify the software yet this is a key component of safety assurance. In this paper, we suggest that while complete specifications may not be possible, partial specifications typically are and these could be used with ML to strengthen safety assurance. We review the types of partial specifications that are applicable for these problems and discuss the places in the ML development workflow that they could be used to improve the safety of ML-based components.;https://link.springer.com/chapter/10.1007/978-3-030-26250-1_23;end2-6GWt1oJ
Zhang, Z., & Xie, X. (2019, July). On the investigation of essential diversities for deep learning testing criteria. In 2019 IEEE 19th International Conference on Software Quality, Reliability and Security (QRS) (pp. 394-405). IEEE.;4_dl_testing_deep_network;2019;On the investigation of essential diversities for deep learning testing criteria;Zhiyi Zhang, Xiaoyuan Xie;2019 IEEE 19th International Conference on Software Quality, Reliability and Security (QRS), 394-405, 2019;Recent years, more and more testing criteria for deep learning systems has been proposed to ensure system robustness and reliability. These criteria were defined based on different perspectives of diversity. However, there lacks comprehensive investigation on what are the most essential diversities that should be considered by a testing criteria for deep learning systems. Therefore, in this paper, we conduct an empirical study to investigate the relation between test diversities and erroneous behaviors of deep learning models. We define five metrics to reflect diversities in neuron activities, and leverage metamorphic testing to detect erroneous behaviors. We investigate the correlation between metrics and erroneous behaviors. We also go further step to measure the quality of test suites under the guidance of defined metrics. Our results provided comprehensive insights on the essential diversities for testing criteria to exhibit good fault detection ability.;https://ieeexplore.ieee.org/abstract/document/8854700/;zI4jN4km34YJ
Sato, N., Kuruma, H., Kaneko, M., Nakagawa, Y., Ogawa, H., Hoang, T. S., & Butler, M. (2018). DeepSaucer: Unified Environment for Verifying Deep Neural Networks. arXiv preprint arXiv:1811.03752.;4_dl_testing_deep_network;2018;DeepSaucer: Unified Environment for Verifying Deep Neural Networks;Naoto Sato, Hironobu Kuruma, Masanori Kaneko, Yuichiroh Nakagawa, Hideto Ogawa, Thai Son Hoang, Michael Butler;arXiv preprint arXiv:1811.03752, 2018;In recent years, a number of methods for verifying DNNs have been developed. Because the approaches of the methods differ and have their own limitations, we think that a number of verification methods should be applied to a developed DNN. To apply a number of methods to the DNN, it is necessary to translate either the implementation of the DNN or the verification method so that one runs in the same environment as the other. Since those translations are time-consuming, a utility tool, named DeepSaucer, which helps to retain and reuse implementations of DNNs, verification methods, and their environments, is proposed. In DeepSaucer, code snippets of loading DNNs, running verification methods, and creating their environments are retained and reused as software assets in order to reduce cost of verifying DNNs. The feasibility of DeepSaucer is confirmed by implementing it on the basis of Anaconda, which provides virtual environment for loading a DNN and running a verification method. In addition, the effectiveness of DeepSaucer is demonstrated by usecase examples.;https://arxiv.org/abs/1811.03752;5Qc0aqyTLMsJ
Wu, C., Sun, L., & Zhou, Z. Q. (2019, May). The impact of a dot: Case studies of a noise metamorphic relation pattern. In 2019 IEEE/ACM 4th International Workshop on Metamorphic Testing (MET) (pp. 17-23). IEEE.;10_testing_test_machine_metamorphic;2019;The impact of a dot: Case studies of a noise metamorphic relation pattern;Chaohua Wu, Liqun Sun, Zhi Quan Zhou;2019 IEEE/ACM 4th International Workshop on Metamorphic Testing (MET), 17-23, 2019;"We propose a ""noise"" metamorphic relation pattern (MRP), which is a sub-pattern under the more general MRP ""symmetry."" We conduct case studies with real-life systems in three different application domains (obstacle perception in autonomous systems, machine translation, and named entity recognition) to show the usefulness of the ""noise"" MRP for software verification and validation.";https://ieeexplore.ieee.org/abstract/document/8785517/;1bAwdLGPA7gJ
Sun, Y., Zhou, Y., Maskell, S., Sharp, J., & Huang, X. (2020, May). Reliability validation of learning enabled vehicle tracking. In 2020 IEEE International Conference on Robotics and Automation (ICRA) (pp. 9390-9396). IEEE.;2_safety_system_autonomous_vehicle;2020;Reliability validation of learning enabled vehicle tracking;Youcheng Sun, Yifan Zhou, Simon Maskell, James Sharp, Xiaowei Huang;2020 IEEE International Conference on Robotics and Automation (ICRA), 9390-9396, 2020;This paper studies the reliability of a real-world learning-enabled system, which conducts dynamic vehicle tracking based on a high-resolution wide-area motion imagery input. The system consists of multiple neural network components - to process the imagery inputs - and multiple symbolic (Kalman filter) components - to analyse the processed information for vehicle tracking. It is known that neural networks suffer from adversarial examples, which make them lack robustness. However, it is unclear if and how the adversarial examples over learning components can affect the overall system-level reliability. By integrating a coverage-guided neural network testing tool, DeepConcolic, with the vehicle tracking system, we found that (1) the overall system can be resilient to some adversarial examples thanks to the existence of other components, and (2) the overall system presents an extra level of uncertainty which cannot be determined by analysing the deep learning components only. This research suggests the need for novel verification and validation methods for learning-enabled systems.;https://ieeexplore.ieee.org/abstract/document/9196932/;ZWcPeHrACqoJ
Braiek, H. B., & Khomh, F. (2019, July). TFCheck: A tensorflow library for detecting training issues in neural network programs. In 2019 IEEE 19th international conference on software quality, reliability and security (QRS) (pp. 426-433). IEEE.;2_safety_system_autonomous_vehicle;2019;TFCheck: A tensorflow library for detecting training issues in neural network programs;Houssem Ben Braiek, Foutse Khomh;2019 IEEE 19th international conference on software quality, reliability and security (QRS), 426-433, 2019;The increasing inclusion of Machine Learning (ML) models in safety-critical systems like autonomous cars have led to the development of multiple model-based ML testing techniques. One common denominator of these testing techniques is their assumption that training programs are adequate and bug-free. These techniques only focus on assessing the performance of the constructed model using manually labeled data or automatically generated data. However, their assumptions about the training program are not always true as training programs can contain inconsistencies and bugs. In this paper, we examine training issues in ML programs and propose a catalog of verification routines that can be used to detect the identified issues, automatically. We implemented the routines in a Tensorflow-based library named TFCheck. Using TFCheck, practitioners can detect the aforementioned issues automatically. To assess the effectiveness of TFCheck, we conducted a case study with real-world, mutants, and synthetic training programs. Results show that TFCheck can successfully detect training issues in ML code implementations.;https://ieeexplore.ieee.org/abstract/document/8854684/;thshvjSuYMcJ
Xie, X., Zhang, Z., Chen, T. Y., Liu, Y., Poon, P. L., & Xu, B. (2020). METTLE: a METamorphic testing approach to assessing and validating unsupervised machine LEarning systems. IEEE Transactions on Reliability, 69(4), 1293-1322.;10_testing_test_machine_metamorphic;2020;METTLE: a METamorphic testing approach to assessing and validating unsupervised machine LEarning systems;Xiaoyuan Xie, Zhiyi Zhang, Tsong Yueh Chen, Yang Liu, Pak-Lok Poon, Baowen Xu;IEEE Transactions on Reliability 69 (4), 1293-1322, 2020;Unsupervised machine learning is the training of an artificial intelligence system using information that is neither classified nor labeled, with a view to modeling the underlying structure or distribution in a dataset. Since unsupervised machine learning systems are widely used in many real-world applications, assessing the appropriateness of these systems and validating their implementations with respect to individual users' requirements and specific application scenarios/contexts are indisputably two important tasks. Such assessments and validation tasks, however, are fairly challenging due to the absence of a priori knowledge of the data. In view of this challenge, in this article, we develop a METamorphic Testing approach to assessing and validating unsupervised machine LEarning systems, abbreviated as mettle. Our approach provides a new way to unveil the (possibly latent) characteristics of various machine learning systems, by explicitly considering the specific expectations and requirements of these systems from individual users' perspectives. To support mettle, we have further formulated 11 generic metamorphic relations (MRs), covering users' generally expected characteristics that should be possessed by machine learning systems. We have performed an experiment and a user evaluation study to evaluate the viability and effectiveness of mettle. Our experiment and user evaluation study have shown that, guided by user-defined MR-based adequacy criteria, end users are able to assess, validate, and select appropriate clustering systems in accordance with their own specific needs. Our investigation has also yielded insightful understanding and interpretation of the behavior of the machine learning systems from an end-user software engineering's perspective, rather than a designer's or implementor's perspective, who normally adopts a theoretical approach.;https://ieeexplore.ieee.org/abstract/document/9036058/;4LPAE4LZFAgJ
Wang, S., & Su, Z. (2019). Metamorphic testing for object detection systems. arXiv preprint arXiv:1912.12162.;4_dl_testing_deep_network;2019;Metamorphic testing for object detection systems;Shuai Wang, Zhendong Su;arXiv preprint arXiv:1912.12162, 2019;Recent advances in deep neural networks (DNNs) have led to object detectors that can rapidly process pictures or videos, and recognize the objects that they contain. Despite the promising progress by industrial manufacturers such as Amazon and Google in commercializing deep learning-based object detection as a standard computer vision service, object detection systems - similar to traditional software - may still produce incorrect results. These errors, in turn, can lead to severe negative outcomes for the users of these object detection systems. For instance, an autonomous driving system that fails to detect pedestrians can cause accidents or even fatalities. However, principled, systematic methods for testing object detection systems do not yet exist, despite their importance. To fill this critical gap, we introduce the design and realization of MetaOD, the first metamorphic testing system for object detectors to effectively reveal erroneous detection results by commercial object detectors. To this end, we (1) synthesize natural-looking images by inserting extra object instances into background images, and (2) design metamorphic conditions asserting the equivalence of object detection results between the original and synthetic images after excluding the prediction results on the inserted objects. MetaOD is designed as a streamlined workflow that performs object extraction, selection, and insertion. Evaluated on four commercial object detection services and four pretrained models provided by the TensorFlow API, MetaOD found tens of thousands of detection defects in these object detectors. To further demonstrate the practical usage of MetaOD, we use the synthetic images that cause erroneous detection results to retrain the model. Our results show that the model performance is increased significantly, from an mAP score of 9.3 to an mAP score of 10.5.;https://arxiv.org/abs/1912.12162;K9xJ9tddlREJ
Ghosh, S., Berkenkamp, F., Ranade, G., Qadeer, S., & Kapoor, A. (2018, May). Verifying controllers against adversarial examples with bayesian optimization. In 2018 IEEE International Conference on Robotics and Automation (ICRA) (pp. 7306-7313). IEEE.;2_safety_system_autonomous_vehicle;2018;Verifying controllers against adversarial examples with bayesian optimization;Shromona Ghosh, Felix Berkenkamp, Gireeja Ranade, Shaz Qadeer, Ashish Kapoor;2018 IEEE International Conference on Robotics and Automation (ICRA), 7306-7313, 2018;Recent successes in reinforcement learning have lead to the development of complex controllers for realworld robots. As these robots are deployed in safety-critical applications and interact with humans, it becomes critical to ensure safety in order to avoid causing harm. A first step in this direction is to test the controllers in simulation. To be able to do this, we need to capture what we mean by safety and then efficiently search the space of all behaviors to see if they are safe. In this paper, we present an active-testing framework based on Bayesian Optimization. We specify safety constraints using logic and exploit structure in the problem in order to test the system for adversarial counter examples that violate the safety specifications. These specifications are defined as complex boolean combinations of smooth functions on the trajectories and, unlike reward functions in reinforcement learning, are expressive and impose hard constraints on the system. In our framework, we exploit regularity assumptions on individual functions in form of a Gaussian Process (GP) prior. We combine these into a coherent optimization framework using problem structure. The resulting algorithm is able to provably verify complex safety specifications or alternatively find counter examples. Experimental results show that the proposed method is able to find adversarial examples quickly.;https://ieeexplore.ieee.org/abstract/document/8460635/;Pu1s1uBu3x8J
Salay, R., & Czarnecki, K. (2018). Using machine learning safely in automotive software: An assessment and adaption of software process requirements in ISO 26262. arXiv preprint arXiv:1808.01614.;2_safety_system_autonomous_vehicle;2018;Using machine learning safely in automotive software: An assessment and adaption of software process requirements in ISO 26262;Rick Salay, Krzysztof Czarnecki;arXiv preprint arXiv:1808.01614, 2018;The use of machine learning (ML) is on the rise in many sectors of software development, and automotive software development is no different. In particular, Advanced Driver Assistance Systems (ADAS) and Automated Driving Systems (ADS) are two areas where ML plays a significant role. In automotive development, safety is a critical objective, and the emergence of standards such as ISO 26262 has helped focus industry practices to address safety in a systematic and consistent way. Unfortunately, these standards were not designed to accommodate technologies such as ML or the type of functionality that is provided by an ADS and this has created a conflict between the need to innovate and the need to improve safety. In this report, we take steps to address this conflict by doing a detailed assessment and adaption of ISO 26262 for ML, specifically in the context of supervised learning. First we analyze the key factors that are the source of the conflict. Then we assess each software development process requirement (Part 6 of ISO 26262) for applicability to ML. Where there are gaps, we propose new requirements to address the gaps. Finally we discuss the application of this adapted and extended variant of Part 6 to ML development scenarios.;https://arxiv.org/abs/1808.01614;5y4UKt0p38wJ
Otero, C. E., & Peter, A. (2014). Research directions for engineering big data analytics software. IEEE Intelligent Systems, 30(1), 13-19.;9_data_science_software_process;2014;Research directions for engineering big data analytics software;Carlos E Otero, Adrian Peter;IEEE Intelligent Systems 30 (1), 13-19, 2014;Many software startups and research and development efforts are actively trying to harness the power of big data and create software with the potential to improve almost every aspect of human life. As these efforts continue to increase, full consideration needs to be given to the engineering aspects of big data software. Since these systems exist to make predictions on complex and continuous massive datasets, they pose unique problems during specification, design, and verification of software that needs to be delivered on time and within budget. But, given the nature of big data software, can this be done? Does big data software engineering really work? This article explores the details of big data software, discusses the main problems encountered when engineering big data software, and proposes avenues for future research.;https://ieeexplore.ieee.org/abstract/document/6914469/;8T_zeQUv_w4J
Huang, S., Liu, E. H., Hui, Z. W., Tang, S. Q., & Zhang, S. J. (2018). Challenges of testing machine learning applications. International Journal of Performability Engineering, 14(6), 1275.;10_testing_test_machine_metamorphic;2018;Challenges of testing machine learning applications;Song Huang, Er-Hu Liu, Zhan-Wei Hui, Shi-Qi Tang, Suo-Juan Zhang;International Journal of Performability Engineering 14 (6), 1275, 2018;Machine learning applications have achieved impressive results in many areas and provided effective solution to deal with image recognition, automatic driven, voice processing etc. problems. As these applications are adopted by multiple critical areas, their reliability and robustness becomes more and more important. Software testing is a typical way to ensure the quality of applications. Approaches for testing machine learning applications are needed. This paper analyzes the characteristics of several machine learning algorithms and concludes the main challenges of testing machine learning applications. Then, multiple preliminary techniques are presented according to the challenges. Moreover, the paper demonstrates how these techniques can be used to solve the problems during the testing of machine learning applications.;http://www.ijpe-online.com/EN/abstract/abstract3626.shtml;gTCEP3IC1VIJ
Pasareanu, C. S., Gopinath, D., & Yu, H. (2018). Compositional verification for autonomous systems with deep learning components. arXiv preprint arXiv:1810.08303.;2_safety_system_autonomous_vehicle;2018;Compositional verification for autonomous systems with deep learning components;Corina S Pasareanu, Divya Gopinath, Huafeng Yu;arXiv preprint arXiv:1810.08303, 2018;As autonomy becomes prevalent in many applications, ranging from recommendation systems to fully autonomous vehicles, there is an increased need to provide safety guarantees for such systems. The problem is difficult, as these are large, complex systems which operate in uncertain environments, requiring data-driven machine-learning components. However, learning techniques such as Deep Neural Networks, widely used today, are inherently unpredictable and lack the theoretical foundations to provide strong assurance guarantees. We present a compositional approach for the scalable, formal verification of autonomous systems that contain Deep Neural Network components. The approach uses assume-guarantee reasoning whereby {\em contracts}, encoding the input-output behavior of individual components, allow the designer to model and incorporate the behavior of the learning-enabled components working side-by-side with the other components. We illustrate the approach on an example taken from the autonomous vehicles domain.;https://arxiv.org/abs/1810.08303;zFXeRkc8GY4J
Koschi, M., Pek, C., Maierhofer, S., & Althoff, M. (2019, October). Computationally efficient safety falsification of adaptive cruise control systems. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC) (pp. 2879-2886). IEEE.;2_safety_system_autonomous_vehicle;2019;Computationally efficient safety falsification of adaptive cruise control systems;Markus Koschi, Christian Pek, Sebastian Maierhofer, Matthias Althoff;2019 IEEE Intelligent Transportation Systems Conference (ITSC), 2879-2886, 2019;Falsification aims to disprove the safety of systems by providing counter-examples that lead to a violation of safety properties. In this work, we present two novel falsification methods to reveal safety flaws in adaptive cruise control (ACC) systems of automated vehicles. Our methods use rapidly-exploring random trees to generate motions for a leading vehicle such that the ACC under test causes a rear-end collision. By considering unsafe states and searching backward in time, we are able to drastically improve computation times and falsify even sophisticated ACC systems. The obtained collision scenarios reveal safety flaws of the ACC under test and can be directly used to improve the system's design. We demonstrate the benefits of our methods by successfully falsifying the safety of state-of-the-art ACC systems and comparing the results to that of existing approaches.;https://ieeexplore.ieee.org/abstract/document/8917287/;xoraJ4JCKvwJ
Uesato, J., Kumar, A., Szepesvari, C., Erez, T., Ruderman, A., Anderson, K., ... & Kohli, P. (2018). Rigorous agent evaluation: An adversarial approach to uncover catastrophic failures. arXiv preprint arXiv:1812.01647.;2_safety_system_autonomous_vehicle;2018;Rigorous agent evaluation: An adversarial approach to uncover catastrophic failures;Jonathan Uesato, Ananya Kumar, Csaba Szepesvari, Tom Erez, Avraham Ruderman, Keith Anderson, Nicolas Heess, Pushmeet Kohli;arXiv preprint arXiv:1812.01647, 2018;This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences. We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure. The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents. We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation. To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach. Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities. The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization. To solve this we propose a continuation approach that learns failure modes in related but less robust agents. Our approach also allows reuse of data already collected for training the agent. We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving. Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days.;https://arxiv.org/abs/1812.01647;M0QaOE26T7UJ
Leotta, M., Olianas, D., Ricca, F., & Noceti, N. (2019, April). How do implementation bugs affect the results of machine learning algorithms?. In Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing (pp. 1304-1313).;10_testing_test_machine_metamorphic;2019;How do implementation bugs affect the results of machine learning algorithms?;Maurizio Leotta, Dario Olianas, Filippo Ricca, Nicoletta Noceti;Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing, 1304-1313, 2019;Applications based on Machine learning (ML) are growing in popularity in a multitude of different contexts such as medicine, bioinformatics, and finance. However, there is a lack of established approaches and strategies able to assure the reliability of this category of software. This has a big impact since nowadays our society relies on (potentially) unreliable applications that could cause, in extreme cases, catastrophic events (e.g., loss of life due to a wrong diagnosis of an ML-based cancer classifier).In this paper, as a preliminary step towards providing a solution to this big problem, we used automatic mutations to mimic realistic bugs in the code of two machine learning algorithms, Multilayer Perceptron and Logistic Regression, with the goal of studying the impact of implementation bugs on their behaviours.Unexpectedly, our experiments show that about 2/3 of the injected bugs are silent since they does not influence the results of the algorithms, while the bugs emerge as runtime errors, exceptions, or modified accuracy of the predictions only in the remaining cases. Moreover, we also discovered that about 1% of the bugs are extremely dangerous since they drastically affect the quality of the prediction only in rare cases and with specific datasets increasing the possibility of going unnoticed.;https://dl.acm.org/doi/abs/10.1145/3297280.3297411;DU941lFoaaEJ
He, P., Meister, C., & Su, Z. (2020, June). Structure-invariant testing for machine translation. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (pp. 961-973).;10_testing_test_machine_metamorphic;2020;Structure-invariant testing for machine translation;Pinjia He, Clara Meister, Zhendong Su;Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, 961-973, 2020;"In recent years, machine translation software has increasingly been integrated into our daily lives. People routinely use machine translation for various applications, such as describing symptoms to a foreign doctor and reading political news in a foreign language. However, the complexity and intractability of neural machine translation (NMT) models that power modern machine translation make the robustness of these systems difficult to even assess, much less guarantee. Machine translation systems can return inferior results that lead to misunderstanding, medical misdiagnoses, threats to personal safety, or political conflicts. Despite its apparent importance, validating the robustness of machine translation systems is very difficult and has, therefore, been much under-explored.To tackle this challenge, we introduce structure-invariant testing (SIT), a novel metamorphic testing approach for validating machine translation software. Our key insight is that the translation results of ""similar"" source sentences should typically exhibit similar sentence structures. Specifically, SIT (1) generates similar source sentences by substituting one word in a given sentence with semantically similar, syntactically equivalent words; (2) represents sentence structure by syntax parse trees (obtained via constituency or dependency parsing); (3) reports sentence pairs whose structures differ quantitatively by more than some threshold. To evaluate SIT, we use it to test Google Translate and Bing Microsoft Translator with 200 source sentences as input, which led to 64 and 70 buggy issues with 69.5% and 70% top-1 accuracy, respectively. The translation errors are diverse, including under-translation, over-translation, incorrect modification, word/phrase mistranslation, and unclear logic.";https://dl.acm.org/doi/abs/10.1145/3377811.3380339;d6KuC-D7iQgJ
Wu, T., Dong, Y., Zhang, Y., & Singa, A. (2020). ExtendAIST: Exploring the space of ai-in-the-loop system testing. Applied Sciences, 10(2), 518.;2_safety_system_autonomous_vehicle;2020;ExtendAIST: Exploring the space of ai-in-the-loop system testing;Tingting Wu, Yunwei Dong, Yu Zhang, Aziz Singa;Applied Sciences 10 (2), 518, 2020;"The AI-in-the-loop system (AIS) has been widely used in various autonomous decision and control systems, such as computing vision, autonomous vehicle, and collision avoidance systems. AIS generates and updates control strategies through learning algorithms, which make the control behaviors non-deterministic and bring about the test oracle problem in AIS testing procedure. The traditional system mainly concerns about properties of safety, reliability, and real-time, while AIS concerns more about the correctness, robustness, and stiffness of system. To perform an AIS testing with the existing testing techniques according to the testing requirements, this paper presents an extendable framework of AI-in-the-loop system testing by exploring the key steps involved in the testing procedure, named ExtendAIST, which contributes to define the execution steps of ExtendAIST and design space of testing techniques. Furthermore, the ExtendAIST framework provides three concerns for AIS testing, which include: (a) the extension points; (b) sub-extension points; and (c) existing techniques commonly present in each point. Therefore, testers can obtain the testing strategy using existing techniques directly for corresponding testing requirements or extend more techniques based on these extension points.";https://www.mdpi.com/2076-3417/10/2/518;cjLuh-uNJHwJ
Majumdar, R., Mathur, A., Pirron, M., Stegner, L., & Zufferey, D. (2019). Paracosm: A language and tool for testing autonomous driving systems. arXiv preprint arXiv:1902.01084.;2_safety_system_autonomous_vehicle;2019;Paracosm: A language and tool for testing autonomous driving systems;Rupak Majumdar, Aman Mathur, Marcus Pirron, Laura Stegner, Damien Zufferey;arXiv preprint arXiv:1902.01084, 2019;Systematic testing of autonomous vehicles operating in complex real-world scenarios is a difficult and expensive problem. We present Paracosm, a reactive language for writing test scenarios for autonomous driving systems. Paracosm allows users to programmatically describe complex driving situations with specific visual features, e.g., road layout in an urban environment, as well as reactive temporal behaviors of cars and pedestrians. Paracosm programs are executed on top of a game engine that provides realistic physics simulation and visual rendering. The infrastructure allows systematic exploration of the state space, both for visual features (lighting, shadows, fog) and for reactive interactions with the environment (pedestrians, other traffic). We define a notion of test coverage for Paracosm configurations based on combinatorial testing and low dispersion sequences. Paracosm comes with an automatic test case generator that uses random sampling for discrete parameters and deterministic quasi-Monte Carlo generation for continuous parameters. Through an empirical evaluation, we demonstrate the modeling and testing capabilities of Paracosm on a suite of autonomous driving systems implemented using deep neural networks developed in research and education. We show how Paracosm can expose incorrect behaviors or degraded performance.;https://arxiv.org/abs/1902.01084;ZRussQVMdlEJ
Dreossi, T., Ghosh, S., Sangiovanni-Vincentelli, A., & Seshia, S. A. (2019). A formalization of robustness for deep neural networks. arXiv preprint arXiv:1903.10033.;5_adversarial_attack_example_model;2019;A formalization of robustness for deep neural networks;Tommaso Dreossi, Shromona Ghosh, Alberto Sangiovanni-Vincentelli, Sanjit A Seshia;arXiv preprint arXiv:1903.10033, 2019;Deep neural networks have been shown to lack robustness to small input perturbations. The process of generating the perturbations that expose the lack of robustness of neural networks is known as adversarial input generation. This process depends on the goals and capabilities of the adversary, In this paper, we propose a unifying formalization of the adversarial input generation process from a formal methods perspective. We provide a definition of robustness that is general enough to capture different formulations. The expressiveness of our formalization is shown by modeling and comparing a variety of adversarial attack techniques.;https://arxiv.org/abs/1903.10033;JKyX6NSXWd0J
Li, Z., Cui, Z., Liu, J., Zheng, L., & Liu, X. (2020, January). Testing neural network classifiers based on metamorphic relations. In 2019 6th International Conference on Dependable Systems and Their Applications (DSA) (pp. 389-394). IEEE.;10_testing_test_machine_metamorphic;2020;Testing neural network classifiers based on metamorphic relations;Zheng Li, Zhanqi Cui, Jianbin Liu, Liwei Zheng, Xiulei Liu;2019 6th International Conference on Dependable Systems and Their Applications (DSA), 389-394, 2020;The application of machine learning programs is becoming increasingly more widespread. Neural networks, which are among the most popular machine learning programs, play important roles in people's daily lives, such as by controlling cars in autonomous driving systems. However, neural networks still lack effective testing methods. To address this problem, this paper proposes a testing method for neural network classifiers based on metamorphic relations. Firstly, it designs metamorphic relations to transform the original data set into derivative data sets. Then, it uses the data before and after the transformation to train and test the neural network classifier, respectively. Finally, it checks whether the output conforms to the metamorphic relations. The neural network classifier is defective if conflicts are detected. Experiments are conducted on a neural network classifier from Stanford's cs231n course to verify the effectiveness of the method. The results show that the defect detection capability of the proposed method is accurate, and 87.5% of the mutants are successfully detected.;https://ieeexplore.ieee.org/abstract/document/9045800/;PxgJlWO-hSUJ
Platzer, A. (2019, September). The logical path to autonomous cyber-physical systems. In International Conference on Quantitative Evaluation of Systems (pp. 25-33). Cham: Springer International Publishing.;2_safety_system_autonomous_vehicle;2019;The logical path to autonomous cyber-physical systems;AndrÃ© Platzer;International Conference on Quantitative Evaluation of Systems, 25-33, 2019;Autonomous cyber-physical systems are systems that combine the physics of motion with advanced cyber algorithms to act on their own without close human supervision. The present consensus is that reasonable levels of autonomy, such as for self-driving cars or autonomous drones, can only be reached with the help of artificial intelligence and machine learning algorithms that cope with the uncertainties of the real world. That makes safety assurance even more challenging than it already is in cyber-physical systems (CPSs) with classically programmed control, precisely because AI techniques are lauded for their flexibility in handling unpredictable situations, but are themselves harder to predict. This paper identifies the logical path toward autonomous cyber-physical systems in multiple steps. First, differential dynamic logic ( ) provides a logical foundation for developing cyber-physical system models with the mathematical rigor that their safety-critical nature demands. Then, its ModelPlex technique provides a logically correct way to tame the subtle relationship of CPS models to CPS implementations. Finally, the resulting logical monitor conditions can then be exploited to safeguard the decisions of learning agents, guide the optimization of learning processes, and resolve the nondeterminism frequently found in verification models. Overall, logic leads the way in combining the best of both worlds: the strong predictions that formal verification techniques provide alongside the strong flexibility that the use of AI provides.;https://link.springer.com/chapter/10.1007/978-3-030-30281-8_2;zcGUrd2yOnwJ
Desai, A., Ghosh, S., Seshia, S. A., Shankar, N., & Tiwari, A. (2019, June). SOTER: a runtime assurance framework for programming safe robotics systems. In 2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN) (pp. 138-150). IEEE.;2_safety_system_autonomous_vehicle;2019;SOTER: a runtime assurance framework for programming safe robotics systems;Ankush Desai, Shromona Ghosh, Sanjit A Seshia, Natarajan Shankar, Ashish Tiwari;2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 138-150, 2019;The recent drive towards achieving greater autonomy and intelligence in robotics has led to high levels of complexity. Autonomous robots increasingly depend on third-party off-the-shelf components and complex machine-learning techniques. This trend makes it challenging to provide strong design-time certification of correct operation. To address these challenges, we present SOTER, a robotics programming framework with two key components: (1) a programming language for implementing and testing high-level reactive robotics software, and (2) an integrated runtime assurance (RTA) system that helps enable the use of uncertified components, while still providing safety guarantees. SOTER provides language primitives to declaratively construct a RTA module consisting of an advanced, high-performance controller (uncertified), a safe, lower-performance controller (certified), and the desired safety specification. The framework provides a formal guarantee that a well-formed RTA module always satisfies the safety specification, without completely sacrificing performance by using higher performance uncertified components whenever safe. SOTER allows the complex robotics software stack to be constructed as a composition of RTA modules, where each uncertified component is protected using a RTA module. To demonstrate the efficacy of our framework, we consider a real-world case-study of building a safe drone surveillance system. Our experiments both in simulation and on actual drones show that the SOTER-enabled RTA ensures the safety of the system, including when untrusted third-party components have bugs or deviate from the desired behavior.;https://ieeexplore.ieee.org/abstract/document/8809550/;m-BRvmzpQjMJ
Tran, H. D., Cai, F., Diego, M. L., Musau, P., Johnson, T. T., & Koutsoukos, X. (2019). Safety verification of cyber-physical systems with reinforcement learning control. ACM Transactions on Embedded Computing Systems (TECS), 18(5s), 1-22.;2_safety_system_autonomous_vehicle;2019;Safety verification of cyber-physical systems with reinforcement learning control;Hoang-Dung Tran, Feiyang Cai, Manzanas Lopez Diego, Patrick Musau, Taylor T Johnson, Xenofon Koutsoukos;ACM Transactions on Embedded Computing Systems (TECS) 18 (5s), 1-22, 2019;This paper proposes a new forward reachability analysis approach to verify safety of cyber-physical systems (CPS) with reinforcement learning controllers. The foundation of our approach lies on two efficient, exact and over-approximate reachability algorithms for neural network control systems using star sets, which is an efficient representation of polyhedra. Using these algorithms, we determine the initial conditions for which a safety-critical system with a neural network controller is safe by incrementally searching a critical initial condition where the safety of the system cannot be established. Our approach produces tight over-approximation error and it is computationally efficient, which allows the application to practical CPS with learning enable components (LECs). We implement our approach in NNV, a recent verification tool for neural networks and neural network control systems, and evaluate its advantages and applicability by verifying safety of a practical Advanced Emergency Braking System (AEBS) with a reinforcement learning (RL) controller trained using the deep deterministic policy gradient (DDPG) method. The experimental results show that our new reachability algorithms are much less conservative than existing polyhedra-based approaches. We successfully determine the entire region of the initial conditions of the AEBS with the RL controller such that the safety of the system is guaranteed, while a polyhedra-based approach cannot prove the safety properties of the system.;https://dl.acm.org/doi/abs/10.1145/3358230;JEb97wcnd9YJ
Yang, Z., Zhao, Z., Pei, H., Wang, B., Karlas, B., Liu, J., ... & Zhang, C. (2020). End-to-end robustness for sensing-reasoning machine learning pipelines. arXiv preprint arXiv:2003.00120.;4_dl_testing_deep_network;2020;"End-to-end Robustness for Sensing-Reasoning Machine
Learning Pipelines";"Zhuolin Yang, Zhikuan Zhao, Hengzhi Pei, Boxin Wang,
Bojan Karlas, Ji Liu, Heng Guo, Bo Li, Ce Zhang";Arxiv;As machine learning (ML) being applied to many mission-critical scenarios, certifying ML model robustness becomes increasingly important. Many previous works focuses on the robustness of independent ML and ensemble models, and can only certify a very small magnitude of the adversarial perturbation. In this paper, we take a different viewpoint and improve learning robustness by going beyond independent ML and ensemble models. We aim at promoting the generic Sensing-Reasoning machine learning pipeline which contains both the sensing (eg deep neural networks) and reasoning (eg Markov logic networks (MLN)) components enriched with domain knowledge. Can domain knowledge help improve learning robustness? Can we formally certify the end-to-end robustness of such an ML pipeline? We first theoretically analyze the computational complexity of checking the provable robustness in the reasoning component. We then derive the provable robustness bound for several concrete reasoning components. We show that for reasoning components such as MLN and a specific family of Bayesian networks it is possible to certify the robustness of the whole pipeline even with a large magnitude of perturbation which cannot be certified by existing work. Finally, we conduct extensive real-world experiments on large scale datasets to evaluate the certified robustness for Sensing-Reasoning ML pipelines.;https://www.researchgate.net/profile/Zhikuan-Zhao/publication/339642627_End-to-end_Robustness_for_Sensing-Reasoning_Machine_Learning_Pipelines/links/5e925ea6a6fdcca7890e246a/End-to-end-Robustness-for-Sensing-Reasoning-Machine-Learning-Pipelines.pdf;Todo
Rill, R. A., & Lőrincz, A. (2019). Cognitive modeling approach for dealing with challenges in cyber-physical systems.;2_safety_system_autonomous_vehicle;2019;Cognitive modeling approach for dealing with challenges in cyber-physical systems;Róbert Adrian Rill, András Lőrincz;-;In this paper, inspired by our previous works, we propose an architecture for the design and realization of cyber-physical systems (CPS) that considers the spatio-temporal context of events, promotes anomaly detection, facilitates efficient human-computer interaction and is capable of discovering novel human and/or machine knowledge. We view deep neural networks as smart sensors and sensory data from the environment represents the semantic and episodic input to a consistency seeking component of the cyber-space. Starting from a knowledge base infused with a deterministic world assumption, this module can detect anomalies and correct estimation errors by combining the outputs of multiple sensors. We also exploit an episodic description of ongoing situations by integrating temporal segmentation with kernel and low-dimensional embedding based methods. We demonstrate parts of the architecture through illustrative examples on our self-collected driving dataset. Our framework can be related to cognitive science foundations and may facilitate reliable functioning of CPS through integrating traditional AI and deep learning methods with deterministic models and reasoning tools. We expect that such knowledge base and cognition driven approaches of joining deep neural networks will be adopted in complex CPS. This looks like a scalable, and beneficial match between human knowledge and the exploding deep learning technologies.;https://pdfs.semanticscholar.org/7c0e/94aefb74e2f4347e3f881483eb4179ee0b32.pdf#page=52;RAEIuhwWSKoJ
Ghosh, S., Pant, Y. V., Ravanbakhsh, H., & Seshia, S. A. (2021, May). Counterexample-guided synthesis of perception models and control. In 2021 American Control Conference (ACC) (pp. 3447-3454). IEEE.;2_safety_system_autonomous_vehicle;2021;Counterexample-guided synthesis of perception models and control;Shromona Ghosh, Yash Vardhan Pant, Hadi Ravanbakhsh, Sanjit A Seshia;2021 American Control Conference (ACC), 3447-3454, 2021;Recent advances in learning-based perception systems have led to drastic improvements in the performance of robotic systems like autonomous vehicles and surgical robots. These perception systems, however, are hard to analyze and errors in them can propagate to cause catastrophic failures. In this paper, we consider the problem of synthesizing safe and robust controllers for robotic systems which rely on complex perception modules for feedback. We propose a counterexample-guided synthesis framework that iteratively builds simple surrogate models of the complex perception module and enables us to find safe control policies. The framework uses a falsifier to find counterexamples, or traces of the systems that violate a safety property, to extract information that enables efficient modeling of the perception modules and errors in it. These models are then used to synthesize controllers that are robust to errors in perception. If the resulting policy is not safe, we gather new counterexamples. By repeating the process, we eventually find a controller which can keep the system safe even when there is a perception failure. We demonstrate our framework on two scenarios in simulation, namely lane keeping and automatic braking, and show that it generates controllers that are safe, as well as a simpler model of a deep neural network-based perception system that can provide meaningful insight into operations of the perception system.;https://ieeexplore.ieee.org/abstract/document/9482896/;YkN4dRpr9ygJ
Koopman, P., & Wagner, M. (2018). Toward a framework for highly automated vehicle safety validation (No. 2018-01-1071). SAE Technical Paper.;2_safety_system_autonomous_vehicle;2018;Toward a framework for highly automated vehicle safety validation;Philip Koopman, Michael Wagner;SAE Technical Paper, 2018;Validating the safety of Highly Automated Vehicles (HAVs) is a significant autonomy challenge. HAV safety validation strategies based solely on brute force on-road testing campaigns are unlikely to be viable. While simulations and exercising edge case scenarios can help reduce validation cost, those techniques alone are unlikely to provide a sufficient level of assurance for full-scale deployment without adopting a more nuanced view of validation data collection and safety analysis. Validation approaches can be improved by using higher fidelity testing to explicitly validate the assumptions and simplifications of lower fidelity testing rather than just obtaining sampled replication of lower fidelity results. Disentangling multiple testing goals can help by separating validation processes for requirements, environmental model sufficiency, autonomy correctness, autonomy robustness, and test scenario sufficiency. For autonomy approaches with implicit designs and requirements, such as machine learning training data sets, establishing observability points in the architecture can help ensure that vehicles pass the right tests for the right reason. These principles could improve both efficiency and effectiveness for demonstrating HAV safety as part of a phased validation plan that includes both a “driver test” and lifecycle monitoring as well as explicitly managing validation uncertainty.;https://www.sae.org/publications/technical-papers/content/2018-01-1071/;Y2mlQslSdSwJ
Fan, C., Qi, B., & Mitra, S. (2018). Data-driven formal reasoning and their applications in safety analysis of vehicle autonomy features. IEEE Design & Test, 35(3), 31-38.;2_safety_system_autonomous_vehicle;2018;Data-driven formal reasoning and their applications in safety analysis of vehicle autonomy features;Chuchu Fan, Bolun Qi, Sayan Mitra;IEEE Design & Test 35 (3), 31-38, 2018;Analyzing the safety of automotive systems at the application level is -especially challenging as often very complex or even incomplete models are available. The authors combine formal reasoning with simulation data to effectively -validate safety or estimate worst case situations of automotive control systems. —Hans-Joachim Wunderlich, Universität Stuttgart;https://ieeexplore.ieee.org/abstract/document/8272345/;SxZSsClMMesJ
Gauerhof, L., Munk, P., & Burton, S. (2018). Structuring validation targets of a machine learning function applied to automated driving. In Computer Safety, Reliability, and Security: 37th International Conference, SAFECOMP 2018, Västerås, Sweden, September 19-21, 2018, Proceedings 37 (pp. 45-58). Springer International Publishing.;2_safety_system_autonomous_vehicle;2018;Structuring validation targets of a machine learning function applied to automated driving;Lydia Gauerhof, Peter Munk, Simon Burton;Computer Safety, Reliability, and Security: 37th International Conference, SAFECOMP 2018, Västerås, Sweden, September 19-21, 2018, Proceedings 37, 45-58, 2018;The validation of highly automated driving vehicles is an important challenge to the automotive industry, since even if the system is free from internal faults, its behaviour might still vary from the original intent. Reasons for these deviations from the intended functionality can be found in the unpredictability of environmental conditions as well the intrinsic uncertainties of the Machine Learning (ML) functions used to make sense of this complex input space.In this paper, we propose a safety assurance case for a pedestrian detection function, a safety-relevant baseline functionality for an automated driving system. Our safety assurance case is presented in the graphical structuring notation (GSN) and combines our arguments against the problems of underspecification [9], the semantic gap [3], and the deductive gap [16].;https://link.springer.com/chapter/10.1007/978-3-319-99130-6_4;gjFLt_Zymi0J
Klück, F., Li, Y., Nica, M., Tao, J., & Wotawa, F. (2018, October). Using ontologies for test suites generation for automated and autonomous driving functions. In 2018 IEEE International symposium on software reliability engineering workshops (ISSREW) (pp. 118-123). IEEE.;2_safety_system_autonomous_vehicle;2018;Using ontologies for test suites generation for automated and autonomous driving functions;Florian KlÃ¼ck, Yihao Li, Mihai Nica, Jianbo Tao, Franz Wotawa;2018 IEEE International symposium on software reliability engineering workshops (ISSREW), 118-123, 2018;In this paper, we outline a general automated testing approach to be applied for verification and validation of automated and autonomous driving functions. The approach makes use of ontologies of environment the system under test is interacting with. Ontologies are automatically converted into input models for combinatorial testing, which are used to generate test cases. The obtained abstract test cases are used to generate concrete test scenarios that provide the basis for simulation used to verify the functionality of the system under test. We discuss the general approach including its potential for automation in the automotive domain where there is growing need for sophisticated verification based on simulation in case of automated and autonomous vehicles.;https://ieeexplore.ieee.org/abstract/document/8539174/;P9XotVK6DPsJ
Feth, P., Schneider, D., & Adler, R. (2017). A conceptual safety supervisor definition and evaluation framework for autonomous systems. In Computer Safety, Reliability, and Security: 36th International Conference, SAFECOMP 2017, Trento, Italy, September 13-15, 2017, Proceedings 36 (pp. 135-148). Springer International Publishing.;2_safety_system_autonomous_vehicle;2017;A conceptual safety supervisor definition and evaluation framework for autonomous systems;Patrik Feth, Daniel Schneider, Rasmus Adler;Computer Safety, Reliability, and Security: 36th International Conference, SAFECOMP 2017, Trento, Italy, September 13-15, 2017, Proceedings 36, 135-148, 2017;The verification and validation (V&V) of autonomous systems is a complex and difficult task, especially when artificial intelligence is used to achieve autonomy. However, without proper V&V, sufficient evidence to argue safety is not attainable. We propose in this work the use of a Safety Supervisor (SSV) to circumvent this issue. However, the design of an adequate SSV is a challenge in itself. To assist in this task, we present a conceptual framework and a corresponding metamodel, which are motivated and justified by existing work in the field. The conceptual framework supports the alignment of future research in the field of runtime safety monitoring. Our vision is for the different parts of the framework to be filled with exchangeable solutions so that a concrete SSV can be derived systematically and efficiently, and that new solutions can be embedded in it and get evaluated against existing approaches. To exemplify our vision, we present an SSV that is based on the ISO 22839 standard for forward collision mitigation.;https://link.springer.com/chapter/10.1007/978-3-319-66266-4_9;VKkmLjNUxZUJ
Wolschke, C., Kuhn, T., Rombach, D., & Liggesmeyer, P. (2017, October). Observation based creation of minimal test suites for autonomous vehicles. In 2017 IEEE International symposium on software reliability engineering workshops (ISSREW) (pp. 294-301). IEEE.;2_safety_system_autonomous_vehicle;2017;Observation based creation of minimal test suites for autonomous vehicles;Christian Wolschke, Thomas Kuhn, Dieter Rombach, Peter Liggesmeyer;2017 IEEE International symposium on software reliability engineering workshops (ISSREW), 294-301, 2017;Autonomous vehicles pose new challenges to their testing, which is required for safety certification. While Autonomous vehicles will use training sets as specification for machine learning algorithms, traditional validation depends on the system's requirements and design. The presented approach uses training sets which are observations of traffic situations as system specification. It aims at deriving test-cases which incorporate the continuous behavior of other traffic participants. Hence, relevant scenarios are mined by analyzing and categorizing behaviors. By using abstract descriptions of the behaviors we discuss how test-cases can be compared to each other, so that similar test-cases are avoided in the test-suite. We demonstrate our approach using a combination of an overtake assistant and an adaptive cruise control.;https://ieeexplore.ieee.org/abstract/document/8109298/;z2T-Cm87QbsJ
Juez, G., Amparan, E., Lattarulo, R., Ruíz, A., Pérez, J., & Espinoza, H. (2017). Early safety assessment of automotive systems using sabotage simulation-based fault injection framework. In Computer Safety, Reliability, and Security: 36th International Conference, SAFECOMP 2017, Trento, Italy, September 13-15, 2017, Proceedings 36 (pp. 255-269). Springer International Publishing.;2_safety_system_autonomous_vehicle;2017;Early safety assessment of automotive systems using sabotage simulation-based fault injection framework;Garazi Juez, Estíbaliz Amparan, Ray Lattarulo, Alejandra Ruíz, Joshué Pérez, Huáscar Espinoza;Computer Safety, Reliability, and Security: 36th International Conference, SAFECOMP 2017, Trento, Italy, September 13-15, 2017, Proceedings 36, 255-269, 2017;As road vehicles increase their autonomy and the driver reduces his role in the control loop, novel challenges on dependability assessment arise. Model-based design combined with a simulation-based fault injection technique and a virtual vehicle poses as a promising solution for an early safety assessment of automotive systems. To start with, the design, where no safety was considered, is stimulated with a set of fault injection simulations (fault forecasting). By doing so, safety strategies can be evaluated during early development phases estimating the relationship of an individual failure to the degree of misbehaviour on vehicle level. After having decided the most suitable safety concept, a second set of fault injection experiments is used to perform an early safety validation of the chosen architecture. This double-step process avoids late redesigns, leading to significant cost and time savings. This paper presents a simulation-based fault injection approach aimed at finding acceptable safety properties for model-based design of automotive systems. We focus on instrumenting the use of this technique to obtain fault effects and the maximum response time of a system before a hazardous event occurs. Through these tangible outcomes, safety concepts and mechanisms can be more accurately dimensioned. In this work, a prototype tool called Sabotage has been developed to set up, configure, execute and analyse the simulation results. The feasibility of this method is demonstrated by applying it to a Lateral Control system.;https://link.springer.com/chapter/10.1007/978-3-319-66266-4_17;tr5mWom2uYAJ
Ingrand, F. (2019, February). Recent trends in formal validation and verification of autonomous robots software. In 2019 Third IEEE International Conference on Robotic Computing (IRC) (pp. 321-328). IEEE.;2_safety_system_autonomous_vehicle;2019;Recent trends in formal validation and verification of autonomous robots software;FÃ©lix Ingrand;2019 Third IEEE International Conference on Robotic Computing (IRC), 321-328, 2019;The consequences of autonomous systems software failures can be potentially dramatic. There is no need to darken the picture, but still, it seems unlikely that people, insurance companies and certification agencies will let autonomous systems fly or drive around without requiring their makers and programmers to prove that the most critical parts of the software are robust and reliable. This is already the case for aeronautic, rail transportation, nuclear plants, medical devices, etc. were software must be certified, which possibly involve its formal validation and verification (V&V). Moreover, autonomous systems go further and embed onboard deliberation functions. This is what make them really autonomous, but open new challenges. We propose to consider the overall problem of V&V of autonomous systems software and examine the current situation with respect to the various type of software used. In particular, we point out that the availability of formal models is rather different depending on the type of component considered. We distinguish these different cases and stress the areas where we think we need to focus our efforts as to improve the overall robustness of autonomous systems.;https://ieeexplore.ieee.org/abstract/document/8675610/;kQh2TPG6G2YJ
Mallozzi, P., Pelliccione, P., & Menghi, C. (2018, May). Keeping intelligence under control. In Proceedings of the 1st International Workshop on Software Engineering for Cognitive Services (pp. 37-40).;2_safety_system_autonomous_vehicle;2018;Keeping intelligence under control;Piergiuseppe Mallozzi, Patrizio Pelliccione, Claudio Menghi;Proceedings of the 1st International Workshop on Software Engineering for Cognitive Services, 37-40, 2018;Modern software systems, such as smart systems, are based on a continuous interaction with the dynamic and partially unknown environment in which they are deployed. Classical development techniques, based on a complete description of how the system must behave in different environmental conditions, are no longer effective. On the contrary, modern techniques should be able to produce systems that autonomously learn how to behave in different environmental conditions.Machine learning techniques allow creating systems that learn how to execute a set of actions to achieve a desired goal. When a change occurs, the system can autonomously learn new policies and strategies for actions execution. This flexibility comes at a cost: the developer has no longer full control on the system behaviour. Thus, there is no way to guarantee that the system will not violate important properties, such as safety-critical properties.To overcome this issue, we believe that machine learning techniques should be combined with suitable reasoning mechanisms aimed at assuring that the decisions taken by the machine learning algorithm do not violate safety-critical requirements. This paper proposes an approach that combines machine learning with runtime monitoring to detect violations of system invariants in the actions execution policies.;https://dl.acm.org/doi/abs/10.1145/3195555.3195558;84s5ZUWdUG0J
Gambi, A., Mueller, M., & Fraser, G. (2019, July). Automatically testing self-driving cars with search-based procedural content generation. In Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis (pp. 318-328).;2_safety_system_autonomous_vehicle;2019;Automatically testing self-driving cars with search-based procedural content generation;Alessio Gambi, Marc Mueller, Gordon Fraser;Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis, 318-328, 2019;Self-driving cars rely on software which needs to be thoroughly tested. Testing self-driving car software in real traffic is not only expensive but also dangerous, and has already caused fatalities. Virtual tests, in which self-driving car software is tested in computer simulations, offer a more efficient and safer alternative compared to naturalistic field operational tests. However, creating suitable test scenarios is laborious and difficult. In this paper we combine procedural content generation, a technique commonly employed in modern video games, and search-based testing, a testing technique proven to be effective in many domains, in order to automatically create challenging virtual scenarios for testing self-driving car soft- ware. Our AsFault prototype implements this approach to generate virtual roads for testing lane keeping, one of the defining features of autonomous driving. Evaluation on two different self-driving car software systems demonstrates that AsFault can generate effective virtual road networks that succeed in revealing software failures, which manifest as cars departing their lane. Compared to random testing AsFault was not only more efficient, but also caused up to twice as many lane departures.;https://dl.acm.org/doi/abs/10.1145/3293882.3330566;V-q7jxS9faMJ
Hauer, F., Schmidt, T., Holzmüller, B., & Pretschner, A. (2019, October). Did we test all scenarios for automated and autonomous driving systems?. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC) (pp. 2950-2955). IEEE.;2_safety_system_autonomous_vehicle;2019;Did we test all scenarios for automated and autonomous driving systems?;Florian Hauer, Tabea Schmidt, Bernd HolzmÃ¼ller, Alexander Pretschner;2019 IEEE Intelligent Transportation Systems Conference (ITSC), 2950-2955, 2019;To ensure safety and functional correctness of automated and autonomous driving systems, virtual scenario-based testing is used. Experts derive traffic scenario types and generate instances of these types with the support of test generation tools. Since driving systems operate in a real-world environment, it is always possible to find a new scenario type as well as new instances of scenario types that are different from all other scenario types and instances. Thus, the testing process to find faulty behavior may continue forever. There is a practical need for test ending criteria for both of the following problems: Did we test all scenario types? Did we sufficiently test each type with specific instances? We address the first question and present a suitable test ending criterion and methodology. Whether the system is tested in each scenario type is reduced to the question whether all test scenarios are known. We analyze driving data to provide a statistical guarantee that all scenario types are covered. We model this as a Coupon Collector's Problem. We present experimental results for the application of this model to different driving tasks of automated and autonomous driving systems.;https://ieeexplore.ieee.org/abstract/document/8917326/;X2PCVeT94LwJ
Lan, S., Huang, C., Wang, Z., Liang, H., Su, W., & Zhu, Q. (2018, October). Design automation for intelligent automotive systems. In 2018 IEEE International Test Conference (ITC) (pp. 1-10). IEEE.;2_safety_system_autonomous_vehicle;2018;Design automation for intelligent automotive systems;Shuyue Lan, Chao Huang, Zhilu Wang, Hengyi Liang, Wenhao Su, Qi Zhu;2018 IEEE International Test Conference (ITC), 1-10, 2018;With rapid advancement of advanced driver assistance systems (ADAS) and autonomous driving functions, modern vehicles have become ever more intelligent than before. Sophisticated machine learning techniques have being developed for vehicle perception, planning and control. However, this also brings significant challenges to the design, implementation and validation of automotive systems, stemming from the fast-growing functional complexity, the adoption of advanced architectural components such as multicore CPUs and GPUs, the dynamic and uncertain physical environment, and the stringent requirements on various system metrics such as safety, security, reliability, performance, fault tolerance, extensibility, and cost. To address these challenges, new design methodologies, algorithms and tools are greatly needed. This paper will discuss the challenges in designing next-generation connected and autonomous vehicles, and the need of design automation techniques to tackle them.;https://ieeexplore.ieee.org/abstract/document/8624723/;X0SSwtZwUmIJ
Bolte, J. A., Bar, A., Lipinski, D., & Fingscheidt, T. (2019, June). Towards corner case detection for autonomous driving. In 2019 IEEE Intelligent vehicles symposium (IV) (pp. 438-445). IEEE.;2_safety_system_autonomous_vehicle;2019;Towards corner case detection for autonomous driving;Jan-Aike Bolte, Andreas Bar, Daniel Lipinski, Tim Fingscheidt;2019 IEEE Intelligent vehicles symposium (IV), 438-445, 2019;The progress in autonomous driving is also due to the increased availability of vast amounts of training data for the underlying machine learning approaches. Machine learning systems are generally known to lack robustness, e.g., if the training data did rarely or not at all cover critical situations. The challenging task of corner case detection in video, which is also somehow related to unusual event or anomaly detection, aims at detecting these unusual situations, which could become critical, and to communicate this to the autonomous driving system (online use case). Such a system, however, could be also used in offline mode to screen vast amounts of data and select only the relevant situations for storing and (re)training machine learning algorithms. So far, the approaches for corner case detection have been limited to videos recorded from a fixed camera, mostly for security surveillance. In this paper, we provide a formal definition of a corner case and propose a system framework for both the online and the offline use case that can handle video signals from front cameras of a naturally moving vehicle and can output a corner case score.;https://ieeexplore.ieee.org/abstract/document/8813817/;72OucaugPhgJ
Koren, M., & Kochenderfer, M. J. (2019, October). Efficient autonomy validation in simulation with adaptive stress testing. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC) (pp. 4178-4183). IEEE.;2_safety_system_autonomous_vehicle;2019;Efficient autonomy validation in simulation with adaptive stress testing;Mark Koren, Mykel J Kochenderfer;2019 IEEE Intelligent Transportation Systems Conference (ITSC), 4178-4183, 2019;During the development of autonomous systems such as driverless cars, it is important to characterize the scenarios that are most likely to result in failure. Adaptive Stress Testing (AST) provides a way to search for the most-likely failure scenario as a Markov decision process (MDP). Our previous work used a deep reinforcement learning (DRL) solver to identify likely failure scenarios. However, the solver's use of a feed-forward neural network with a discretized space of possible initial conditions poses two major problems. First, the system is not treated as a black box, in that it requires analyzing the internal state of the system, which leads to considerable implementation complexities. Second, in order to simulate realistic settings, a new instance of the solver needs to be run for each initial condition. Running a new solver for each initial condition not only significantly increases the computational complexity, but also disregards the underlying relationship between similar initial conditions. We provide a solution to both problems by employing a recurrent neural network that takes a set of initial conditions from a continuous space as input. This approach enables robust and efficient detection of failures because the solution generalizes across the entire space of initial conditions. By simulating an instance where an autonomous car drives while a pedestrian is crossing a road, we demonstrate the solver is now capable of finding solutions for problems that would have previously been intractable.;https://ieeexplore.ieee.org/abstract/document/8917403/;LiV8c5spt_QJ
Bozic, J., Tazl, O. A., & Wotawa, F. (2019, April). Chatbot testing using AI planning. In 2019 IEEE International Conference On Artificial Intelligence Testing (AITest) (pp. 37-44). IEEE.;1_ml_machine_data_learning;2019;Chatbot testing using AI planning;Josip Bozic, Oliver A Tazl, Franz Wotawa;2019 IEEE International Conference On Artificial Intelligence Testing (AITest), 37-44, 2019;Chatbots, i.e., systems that can interact with humans in a more appropriate way using natural language, have been of increasing importance. This is due the fact of the availability of computational means for natural language interaction between computers and humans that are becoming closer to the interaction between humans alone. Consequently, there are more and more chatbots available that are intended to support humans organizing tasks or making decisions. In this paper, we focus on how to verify the communication capabilities provided by chatbots. In particular, we introduce an automated approach for generating communication sequences and carrying them out. The approach is based on AI planning where each action can be assumed to be a certain question that is given to the chatbot. The answer of the chatbot should make the action post-condition true, in order to proceed with the plan. In cases of deviations between the actual chatbot behavior and the expected one, re-planning is required. Besides the approach, we discuss its application to the domain of tourism and outline a case study.;https://ieeexplore.ieee.org/abstract/document/8718222/;vemCNu1XiEgJ
Du, X., Xie, X., Li, Y., Ma, L., Liu, Y., & Zhao, J. (2019, November). A quantitative analysis framework for recurrent neural network. In 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE) (pp. 1062-1065). IEEE.;4_dl_testing_deep_network;2019;A quantitative analysis framework for recurrent neural network;Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, Jianjun Zhao;2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE), 1062-1065, 2019;Recurrent neural network (RNN) has achieved great success in processing sequential inputs for applications such as automatic speech recognition, natural language processing and machine translation. However, quality and reliability issues of RNNs make them vulnerable to adversarial attacks and hinder their deployment in real-world applications. In this paper, we propose a quantitative analysis framework - DeepStellar - to pave the way for effective quality and security analysis of software systems powered by RNNs. DeepStellar is generic to handle various RNN architectures, including LSTM and GRU, scalable to work on industrial-grade RNN models, and extensible to develop customized analyzers and tools. We demonstrated that, with DeepStellar, users are able to design efficient test generation tools, and develop effective adversarial sample detectors. We tested the developed applications on three real RNN models, including speech recognition and image classification. DeepStellar outperforms existing approaches three hundred times in generating defect-triggering tests and achieves 97% accuracy in detecting adversarial attacks. A video demonstration which shows the main features of DeepStellar is available at: https://sites.google.com/view/deepstellar/tool-demo.;https://ieeexplore.ieee.org/abstract/document/8952565/;VtfLba48TjgJ
Jia, M., Wang, X., Xu, Y., Cui, Z., & Xie, R. (2020). Testing machine learning classifiers based on compositional metamorphic relations. International Journal of Performability Engineering, 16(1), 67.;10_testing_test_machine_metamorphic;2020;Testing machine learning classifiers based on compositional metamorphic relations;Minghua Jia, Xiaodong Wang, Yue Xu, Zhanqi Cui, Ruilin Xie;International Journal of Performability Engineering 16 (1), 67, 2020;"With the widespread application of intelligent software, more rigorous requirements are placed on the security and reliability of intelligent software programs. The major challenge is that the traditional test approaches cannot be easily adapted to the testing of intelligent software since the test oracle is not available and testing intelligent software needs to focus on training sets. The classifier is a typical intelligent software which has an uncertain output. As a result, the accuracy rate cannot be used to judge whether the classifier is defective. Therefore, this paper proposes an approach for testing classifiers based on compositional metamorphic relations. Initially, it was recommended to construct composite metamorphic relations to generate derivative test cases from the original test cases; followed by training classifier to predict the classification of the test data set; then checks their consistency between the original test cases and the derivative test cases against compositional metamorphic relations, and the detected violations are reported as bugs. The experiment is carried on the ID3 decision tree and compares the mutant detection capability with the on one-dimensional metamorphic testing. From the results received from the experiment conducted in this research shows that the proposed approach improves 16.7% of mutant kill rates.";http://www.ijpe-online.com/EN/abstract/abstract4343.shtml;chbdmbpzcxgJ
Cheng, C. H., Huang, C. H., & Nührenberg, G. (2019, November). nn-dependability-kit: Engineering neural networks for safety-critical autonomous driving systems. In 2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD) (pp. 1-6). IEEE.;2_safety_system_autonomous_vehicle;2019;nn-dependability-kit: Engineering neural networks for safety-critical autonomous driving systems;Chih-Hong Cheng, Chung-Hao Huang, Georg NÃ¼hrenberg;2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), 1-6, 2019;Can engineering neural networks be approached in a disciplined way similar to how engineers build software for civil aircraft? We present nn-dependability-kit, an open-source toolbox to support safety engineering of neural networks for autonomous driving systems. The rationale behind nn-dependability-kit is to consider a structured approach (via Goal Structuring Notation) to argue the quality of neural networks. In particular, the tool realizes recent scientific results including (a) novel dependability metrics for indicating sufficient elimination of uncertainties in the product life cycle, (b) formal reasoning engine for ensuring that the generalization does not lead to undesired behaviors, and (c) runtime monitoring for reasoning whether a decision of a neural network in operation is supported by prior similarities in the training data. A proprietary version of nn-dependability-kit has been used to improve the quality of a level-3 autonomous driving component developed by Audi for highway maneuvers.;https://ieeexplore.ieee.org/abstract/document/8942153/;vOdNdw-XR_MJ
Cheng, C. H., Nührenberg, G., & Ruess, H. (2017). Maximum resilience of artificial neural networks. In Automated Technology for Verification and Analysis: 15th International Symposium, ATVA 2017, Pune, India, October 3–6, 2017, Proceedings 15 (pp. 251-268). Springer International Publishing.;4_dl_testing_deep_network;2017;Maximum resilience of artificial neural networks;Chih-Hong Cheng, Georg Nührenberg, Harald Ruess;Automated Technology for Verification and Analysis: 15th International Symposium, ATVA 2017, Pune, India, October 3–6, 2017, Proceedings 15, 251-268, 2017;The deployment of Artificial Neural Networks (ANNs) in safety-critical applications poses a number of new verification and certification challenges. In particular, for ANN-enabled self-driving vehicles it is important to establish properties about the resilience of ANNs to noisy or even maliciously manipulated sensory input. We are addressing these challenges by defining resilience properties of ANN-based classifiers as the maximum amount of input or sensor perturbation which is still tolerated. This problem of computing maximum perturbation bounds for ANNs is then reduced to solving mixed integer optimization problems (MIP). A number of MIP encoding heuristics are developed for drastically reducing MIP-solver runtimes, and using parallelization of MIP-solvers results in an almost linear speed-up in the number (up to a certain limit) of computing cores in our experiments. We demonstrate the effectiveness and scalability of our approach by means of computing maximum resilience bounds for a number of ANN benchmark sets ranging from typical image recognition scenarios to the autonomous maneuvering of robots.;https://link.springer.com/chapter/10.1007/978-3-319-68167-2_18;4v0b_7c2cMQJ
Saunders, W., Sastry, G., Stuhlmueller, A., & Evans, O. (2017). Trial without error: Towards safe reinforcement learning via human intervention. arXiv preprint arXiv:1707.05173.;2_safety_system_autonomous_vehicle;2017;Trial without error: Towards safe reinforcement learning via human intervention;William Saunders, Girish Sastry, Andreas Stuhlmueller, Owain Evans;arXiv preprint arXiv:1707.05173, 2017;"AI systems are increasingly applied to complex tasks that involve interaction with humans. During training, such systems are potentially dangerous, as they haven't yet learned to avoid actions that could cause serious harm. How can an AI system explore and learn without making a single mistake that harms humans or otherwise causes serious damage? For model-free reinforcement learning, having a human ""in the loop"" and ready to intervene is currently the only way to prevent all catastrophes. We formalize human intervention for RL and show how to reduce the human labor required by training a supervised learner to imitate the human's intervention decisions. We evaluate this scheme on Atari games, with a Deep RL agent being overseen by a human for four hours. When the class of catastrophes is simple, we are able to prevent all catastrophes without affecting the agent's learning (whereas an RL baseline fails due to catastrophic forgetting). However, this scheme is less successful when catastrophes are more complex: it reduces but does not eliminate catastrophes and the supervised learner fails on adversarial examples found by the agent. Extrapolating to more challenging environments, we show that our implementation would not scale (due to the infeasible amount of human labor required). We outline extensions of the scheme that are necessary if we are to train model-free agents without a single catastrophe.";https://arxiv.org/abs/1707.05173;nuO_cfUsJ94J
Arnold, T., & Scheutz, M. (2018). The “big red button” is too late: an alternative model for the ethical evaluation of AI systems. Ethics and Information Technology, 20, 59-69.;2_safety_system_autonomous_vehicle;2018;The “big red button” is too late: an alternative model for the ethical evaluation of AI systems;Thomas Arnold, Matthias Scheutz;Ethics and Information Technology 20, 59-69, 2018;As a way to address both ominous and ordinary threats of artificial intelligence (AI), researchers have started proposing ways to stop an AI system before it has a chance to escape outside control and cause harm. A so-called “big red button” would enable human operators to interrupt or divert a system while preventing the system from learning that such an intervention is a threat. Though an emergency button for AI seems to make intuitive sense, that approach ultimately concentrates on the point when a system has already “gone rogue” and seeks to obstruct interference. A better approach would be to make ongoing self-evaluation and testing an integral part of a system’s operation, diagnose how the system is in error and to prevent chaos and risk before they start. In this paper, we describe the demands that recent big red button proposals have not addressed, and we offer a preliminary model of an approach that could better meet them. We argue for an ethical core (EC) that consists of a scenario-generation mechanism and a simulation environment that are used to test a system’s decisions in simulated worlds, rather than the real world. This EC would be kept opaque to the system itself: through careful design of memory and the character of the scenario, the system’s algorithms would be prevented from learning about its operation and its function, and ultimately its presence. By monitoring and checking for deviant behavior, we conclude, a continual testing approach will be far more effective, responsive, and vigilant toward a system’s learning and action in the world than an emergency button which one might not get to push in time.;https://link.springer.com/article/10.1007/s10676-018-9447-7;W8NuWRcDXRwJ
Henderson, P., Sinha, K., Angelard-Gontier, N., Ke, N. R., Fried, G., Lowe, R., & Pineau, J. (2018, December). Ethical challenges in data-driven dialogue systems. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society (pp. 123-129).;6_fairness_discrimination_bias_decision;2018;Ethical challenges in data-driven dialogue systems;Peter Henderson, Koustuv Sinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, Joelle Pineau;Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, 123-129, 2018;The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.;https://dl.acm.org/doi/abs/10.1145/3278721.3278777?casa_token=Nlqv3uTSGvMAAAAA:TWeZE5wdnkFJNa4CXG7orqH9YvElqWq78FV-obLk4LEax8KRnawGTA6iexdb0URaMAyuh2xXm-G3AQ;WLG6A5PFjjIJ
Wang, S., Pei, K., Whitehouse, J., Yang, J., & Jana, S. (2018). Formal security analysis of neural networks using symbolic intervals. In 27th USENIX Security Symposium (USENIX Security 18) (pp. 1599-1614).;5_adversarial_attack_example_model;2018;Formal security analysis of neural networks using symbolic intervals;Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, Suman Jana;27th USENIX Security Symposium (USENIX Security 18), 1599-1614, 2018;Due to the increasing deployment of Deep Neural Networks (DNNs) in real-world security-critical domains including autonomous vehicles and collision avoidance systems, formally checking security properties of DNNs, especially under different attacker capabilities, is becoming crucial. Most existing security testing techniques for DNNs try to find adversarial examples without providing any formal security guarantees about the non-existence of such adversarial examples. Recently, several projects have used different types of Satisfiability Modulo Theory (SMT) solvers to formally check security properties of DNNs. However, all of these approaches are limited by the high overhead caused by the solver.;https://www.usenix.org/conference/usenixsecurity18/presentation/wang-shiqi;p067HGK2AdEJ
Wang, J., Sun, J., Zhang, P., & Wang, X. (2018). Detecting adversarial samples for deep neural networks through mutation testing. arXiv preprint arXiv:1805.05010.;5_adversarial_attack_example_model;2018;Detecting adversarial samples for deep neural networks through mutation testing;Jingyi Wang, Jun Sun, Peixin Zhang, Xinyu Wang;arXiv preprint arXiv:1805.05010, 2018;Recently, it has been shown that deep neural networks (DNN) are subject to attacks through adversarial samples. Adversarial samples are often crafted through adversarial perturbation, i.e., manipulating the original sample with minor modifications so that the DNN model labels the sample incorrectly. Given that it is almost impossible to train perfect DNN, adversarial samples are shown to be easy to generate. As DNN are increasingly used in safety-critical systems like autonomous cars, it is crucial to develop techniques for defending such attacks. Existing defense mechanisms which aim to make adversarial perturbation challenging have been shown to be ineffective. In this work, we propose an alternative approach. We first observe that adversarial samples are much more sensitive to perturbations than normal samples. That is, if we impose random perturbations on a normal and an adversarial sample respectively, there is a significant difference between the ratio of label change due to the perturbations. Observing this, we design a statistical adversary detection algorithm called nMutant (inspired by mutation testing from software engineering community). Our experiments show that nMutant effectively detects most of the adversarial samples generated by recently proposed attacking methods. Furthermore, we provide an error bound with certain statistical significance along with the detection.;https://arxiv.org/abs/1805.05010;5ldmGRrqnrAJ
Shen, W., Wan, J., & Chen, Z. (2018, July). Munn: Mutation analysis of neural networks. In 2018 IEEE international conference on software quality, reliability and security companion (QRS-C) (pp. 108-115). IEEE.;4_dl_testing_deep_network;2018;Munn: Mutation analysis of neural networks;Weijun Shen, Jun Wan, Zhenyu Chen;2018 IEEE international conference on software quality, reliability and security companion (QRS-C), 108-115, 2018;"Deep neural networks have made amazing progress in many areas over the past few years. After training deep neural networks, a common way is to build a set of test samples to evaluate the networks. However, test adequacy of deep neural networks is overlooked in both research and practice. This paper proposes MuNN, a mutation analysis method for (deep) neural networks, inspired by the success of mutation analysis in conventional software testing. Five mutation operators are designed on the characteristics of neural networks. The well known dataset MNIST has been used in our experiment. We study two research questions: (1) How does mutation affect neural networks; (2) How does neural depth affect mutation analysis. The experimental results show that mutation analysis of neural networks has strong domain characteristics. This indicates that domain mutation operators are needed to enhance mutation analysis. The experimental results also show that the mutation effects are gradually weakened with the deepening of neurons. We need to design new mutation mechanism for deep neural networks. We believe that mutation analysis can not only be used to measure test adequacy, but also be used to understand neural network operation.";https://ieeexplore.ieee.org/abstract/document/8431960/?casa_token=gTlxS-e9AQoAAAAA:sTYGizlPaZ6R3Wn2kGrXFM90mHv9XwdRwGHqIel8XF_b65kqV9-N7B0KSzfDExKmFm3tYZu3ESI;gwhMaqOANJwJ
Zhou, H., Li, W., Kong, Z., Guo, J., Zhang, Y., Yu, B., ... & Liu, C. (2020, June). Deepbillboard: Systematic physical-world testing of autonomous driving systems. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (pp. 347-358).;4_dl_testing_deep_network;2020;Deepbillboard: Systematic physical-world testing of autonomous driving systems;Husheng Zhou, Wei Li, Zelun Kong, Junfeng Guo, Yuqun Zhang, Bei Yu, Lingming Zhang, Cong Liu;Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, 347-358, 2020;"Deep Neural Networks (DNNs) have been widely applied in autonomous systems such as self-driving vehicles. Recently, DNN testing has been intensively studied to automatically generate adversarial examples, which inject small-magnitude perturbations into inputs to test DNNs under extreme situations. While existing testing techniques prove to be effective, particularly for autonomous driving, they mostly focus on generating digital adversarial perturbations, e.g., changing image pixels, which may never happen in the physical world. Thus, there is a critical missing piece in the literature on autonomous driving testing: understanding and exploiting both digital and physical adversarial perturbation generation for impacting steering decisions. In this paper, we propose a systematic physical-world testing approach, namely DeepBillboard, targeting at a quite common and practical driving scenario: drive-by billboards. DeepBillboard is capable of generating a robust and resilient printable adversarial billboard test, which works under dynamic changing driving conditions including viewing angle, distance, and lighting. The objective is to maximize the possibility, degree, and duration of the steering-angle errors of an autonomous vehicle driving by our generated adversarial billboard. We have extensively evaluated the efficacy and robustness of DeepBillboard by conducting both experiments with digital perturbations and physical-world case studies. The digital experimental results show that DeepBillboard is effective for various steering models and scenes. Furthermore, the physical case studies demonstrate that DeepBillboard is sufficiently robust and resilient for generating physical-world adversarial billboard tests for real-world driving under various weather conditions, being able to mislead the average steering angle error up to 26.44 degrees. To the best of our knowledge, this is the first study demonstrating the possibility of generating realistic and continuous physical-world tests for practical autonomous driving systems; moreover, DeepBillboard can be directly generalized to a variety of other physical entities/surfaces along the curbside, e.g., a graffiti painted on a wall.";https://dl.acm.org/doi/abs/10.1145/3377811.3380422;MSVz6cZnt-AJ
Rubaiyat, A. H. M., Qin, Y., & Alemzadeh, H. (2018, December). Experimental resilience assessment of an open-source driving agent. In 2018 IEEE 23rd Pacific rim international symposium on dependable computing (PRDC) (pp. 54-63). IEEE.;2_safety_system_autonomous_vehicle;2018;Experimental resilience assessment of an open-source driving agent;Abu Hasnat Mohammad Rubaiyat, Yongming Qin, Homa Alemzadeh;2018 IEEE 23rd Pacific rim international symposium on dependable computing (PRDC), 54-63, 2018;Autonomous vehicles (AV) depend on the sensors like RADAR and camera for the perception of the environment, path planning, and control. With the increasing autonomy and interactions with the complex environment, there have been growing concerns regarding the safety and reliability of AVs. This paper presents a Systems-Theoretic Process Analysis (STPA) based fault injection framework to assess the resilience of an open-source driving agent, called openpilot, under different environmental conditions and faults affecting sensor data. To increase the coverage of unsafe scenarios during testing, we use a strategic software fault-injection approach where the triggers for injecting the faults are derived from the unsafe scenarios identified during the high-level hazard analysis of the system. The experimental results show that the proposed strategic fault injection approach increases the hazard coverage compared to random fault injection and, thus, can help with more effective simulation of safety-critical faults and testing of AVs. In addition, the paper provides insights on the performance of openpilot safety mechanisms and its ability in timely detection and recovery from faulty inputs.;https://ieeexplore.ieee.org/abstract/document/8639042/?casa_token=pRUr69KoFN8AAAAA:BpSAeol5uoryrizACxHFikSmvzZEB1ZD1qTTcjgUjDCatUS65EBYNWarK0cAC5Dq5pa9chxvAFk;dSg7cAzvYOIJ
Du, X., Xie, X., Li, Y., Ma, L., Zhao, J., & Liu, Y. (2018). Deepcruiser: Automated guided testing for stateful deep learning systems. arXiv preprint arXiv:1812.05339.;4_dl_testing_deep_network;2018;Deepcruiser: Automated guided testing for stateful deep learning systems;Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Jianjun Zhao, Yang Liu;arXiv preprint arXiv:1812.05339, 2018;Deep learning (DL) defines a data-driven programming paradigm that automatically composes the system decision logic from the training data. In company with the data explosion and hardware acceleration during the past decade, DL achieves tremendous success in many cutting-edge applications. However, even the state-of-the-art DL systems still suffer from quality and reliability issues. It was only until recently that some preliminary progress was made in testing feed-forward DL systems. In contrast to feed-forward DL systems, recurrent neural networks (RNN) follow a very different architectural design, implementing temporal behaviors and memory with loops and internal states. Such stateful nature of RNN contributes to its success in handling sequential inputs such as audio, natural languages and video processing, but also poses new challenges for quality assurance. In this paper, we initiate the very first step towards testing RNN-based stateful DL systems. We model RNN as an abstract state transition system, based on which we define a set of test coverage criteria specialized for stateful DL systems. Moreover, we propose an automated testing framework, DeepCruiser, which systematically generates tests in large scale to uncover defects of stateful DL systems with coverage guidance. Our in-depth evaluation on a state-of-the-art speech-to-text DL system demonstrates the effectiveness of our technique in improving quality and reliability of stateful DL systems.;https://arxiv.org/abs/1812.05339;saWYGBBXExYJ
Byun, T., Sharma, V., Vijayakumar, A., Rayadurgam, S., & Cofer, D. (2019, April). Input prioritization for testing neural networks. In 2019 IEEE International Conference On Artificial Intelligence Testing (AITest) (pp. 63-70). IEEE.;4_dl_testing_deep_network;2019;Input prioritization for testing neural networks;Taejoon Byun, Vaibhav Sharma, Abhishek Vijayakumar, Sanjai Rayadurgam, Darren Cofer;2019 IEEE International Conference On Artificial Intelligence Testing (AITest), 63-70, 2019;Deep neural networks (DNNs) are increasingly being adopted for sensing and control functions in a variety of safety and mission-critical systems such as self-driving cars, autonomous air vehicles, medical diagnostics and industrial robotics. Failures of such systems can lead to loss of life or property, which necessitates stringent verification and validation for providing high assurance. Though formal verification approaches are being investigated, testing remains the primary technique for assessing the dependability of such systems. Due to the nature of the tasks handled by DNNs, the cost of obtaining test oracle data-the expected output, a.k.a. label, for a given input-is high, which significantly impacts the amount and quality of testing that can be performed. Thus, prioritizing input data for testing DNNs in meaningful ways to reduce the cost of labeling can go a long way in increasing testing efficacy. This paper proposes using gauges of the DNN's sentiment derived from the computation performed by the model, as a means to identify inputs that are likely to reveal weaknesses. We empirically assessed the efficacy of three such sentiment measures for prioritization-confidence, uncertainty and surprise-and compare their effectiveness in terms of their fault-revealing capability and retraining effectiveness. The results indicate that sentiment measures can effectively flag inputs that expose unacceptable DNN behavior. For MNIST models, the average percentage of inputs correctly flagged ranged from 88% to 94.8%.;https://ieeexplore.ieee.org/abstract/document/8718224/?casa_token=Dkynvw2P08EAAAAA:Q7FhkT1pkIZsnm74HbaRr3CK0wVFu_QTX507ILWi-pHE8AWrSGvu-ItHEx0GGckdsETM3hWHQpo;LpDIk2sNaSkJ
Chen, Z., Li, G., Pattabiraman, K., & DeBardeleben, N. (2019, November). Binfi: An efficient fault injector for safety-critical machine learning systems. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (pp. 1-23).;2_safety_system_autonomous_vehicle;2019;BinFI an efficient fault injector for safety-critical machine learning systems;Zitao Chen, Guanpeng Li, Karthik Pattabiraman, Nathan DeBardeleben;Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 1-23, 2019;As machine learning (ML) becomes pervasive in high performance computing, ML has found its way into safety-critical domains (e.g., autonomous vehicles). Thus the reliability of ML has grown in importance. Specifically, failures of ML systems can have catastrophic consequences, and can occur due to soft errors, which are increasing in frequency due to system scaling. Therefore, we need to evaluate ML systems in the presence of soft errors.In this work, we propose BinFI, an efficient fault injector (FI) for finding the safety-critical bits in ML applications. We find the widely-used ML computations are often monotonic. Thus we can approximate the error propagation behavior of a ML application as a monotonic function. BinFI uses a binary-search like FI technique to pinpoint the safety-critical bits (also measure the overall resilience). BinFI identifies 99.56% of safety-critical bits (with 99.63% precision) in the systems, which significantly outperforms random FI, with much lower costs.;https://dl.acm.org/doi/abs/10.1145/3295500.3356177?casa_token=LZzxhLKh0T8AAAAA:J4l71DwSqg0hQWUmBp16s34gkazfKGQ2BZVN_OBu8QHCNJd_FYsGmAE8GiH7gqYoN_C6ctXuT6APhQ;G7wfBC8188EJ
Yang, Y., & Rinard, M. (2019). Correctness verification of neural networks. arXiv preprint arXiv:1906.01030.;4_dl_testing_deep_network;2019;Correctness verification of neural networks;Yichen Yang, Martin Rinard;arXiv preprint arXiv:1906.01030, 2019;We present a novel framework for specifying and verifying correctness globally for neural networks on perception tasks. Most previous works on neural network verification for perception tasks focus on robustness verification. Unlike robustness verification, which aims to verify that the prediction of a network is stable in some local regions around labelled points, our framework provides a way to specify correctness globally in the whole target input space and verify that the network is correct for all target inputs (or find the regions where the network is not correct). We provide a specification through 1) a state space consisting of all relevant states of the world and 2) an observation process that produces neural network inputs from the states of the world. Tiling the state and input spaces with a finite number of tiles, obtaining ground truth bounds from the state tiles and network output bounds from the input tiles, then comparing the ground truth and network output bounds delivers an upper bound on the network output error for any inputs of interest. The presented framework also enables detecting illegal inputs -- inputs that are not contained in (or close to) the target input space as defined by the state space and observation process (the neural network is not designed to work on them), so that we can flag when we don't have guarantees. Results from two case studies highlight the ability of our technique to verify error bounds over the whole target input space and show how the error bounds vary over the state and input spaces.;https://arxiv.org/abs/1906.01030;d6tDTZ4J79QJ
Vasconcelos, M., Candello, H., Pinhanez, C., & dos Santos, T. (2017, October). Bottester: testing conversational systems with simulated users. In Proceedings of the XVI Brazilian Symposium on Human Factors in Computing Systems (pp. 1-4).;1_ml_machine_data_learning;2017;Bottester: testing conversational systems with simulated users;Marisa Vasconcelos, Heloisa Candello, Claudio Pinhanez, Thiago dos Santos;Proceedings of the XVI Brazilian Symposium on Human Factors in Computing Systems, 1-4, 2017;Recently, conversation agents have attracted the attention of many companies such as IBM, Facebook, Google, and Amazon which have focused on developing tools or API (Application Programming Interfaces) for developers to create their own chat-bots. In this paper, we focus on new approaches to evaluate such systems presenting some recommendations resulted from evaluating a real chatbot use case. Testing conversational agents or chatbots is not a trivial task due to the multitude aspects/tasks (e.g., natural language understanding, dialog management and, response generation) which must be considered separately and as a mixture. Also, the creation of a general testing tool is a challenge since evaluation is very sensitive to the application context. Finally, exhaustive testing can be a tedious task for the project team what creates a need for a tool to perform it automatically. This paper opens a discussion about how conversational systems testing tools are essential to ensure well-functioning of such systems as well as to help interface designers guiding them to develop consistent conversational interfaces.;https://dl.acm.org/doi/abs/10.1145/3160504.3160584?casa_token=n-k682_xeh0AAAAA:HbOp5pUx35h4Jbi2_QT-kLtQMIVvZEX1YVb-3w2Kh5XJnQkCuMAn_ssjv4rYhS2plPUYUL_hz7rJug;Zz-tgAwrOt4J
Zhou, Z. Q., & Sun, L. (2019). Metamorphic testing of driverless cars. Communications of the ACM, 62(3), 61-67.;10_testing_test_machine_metamorphic;2019;Metamorphic testing of driverless cars;Zhi Quan Zhou, Liqun Sun;Communications of the ACM 62 (3), 61-67, 2019;Metamorphic testing can test untestable software, detecting fatal errors in autonomous vehicles' onboard computer systems.;https://dl.acm.org/doi/pdf/10.1145/3241979?casa_token=g4xXhwIIzJAAAAAA:xVg4-9xHP2QSaRw8_23csxuSA0xD_MlC7XhCy1yQBLm5h5mQ5vkQP9gRAE-O_FHS5bEjz9mRh49S-A;CnTpehObChMJ
Haldar, M., Abdool, M., Ramanathan, P., Xu, T., Yang, S., Duan, H., ... & Legrand, T. (2019, July). Applying deep learning to airbnb search. In proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & Data Mining (pp. 1927-1935).;1_ml_machine_data_learning;2019;Applying deep learning to airbnb search;Malay Haldar, Mustafa Abdool, Prashant Ramanathan, Tao Xu, Shulin Yang, Huizhong Duan, Qing Zhang, Nick Barrow-Williams, Bradley C Turnbull, Brendan M Collins, Thomas Legrand;proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & Data Mining, 1927-1935, 2019;The application to search ranking is one of the biggest machine learning success stories at Airbnb. Much of the initial gains were driven by a gradient boosted decision tree model. The gains, however, plateaued over time. This paper discusses the work done in applying neural networks in an attempt to break out of that plateau. We present our perspective not with the intention of pushing the frontier of new modeling techniques. Instead, ours is a story of the elements we found useful in applying neural networks to a real life product. Deep learning was steep learning for us. To other teams embarking on similar journeys, we hope an account of our struggles and triumphs will provide some useful pointers. Bon voyage!;https://dl.acm.org/doi/abs/10.1145/3292500.3330658?casa_token=qAnP5A9d-xYAAAAA:0r2O5Z9t-0xd4YGmhdWmdtbcufOj_mciBEPyAkkXH2hSV-uj-fM2BMkNqw7BiZao-k5R-o4yUV59lw;6RnqEikEoJoJ
Wang, S., Pei, K., Whitehouse, J., Yang, J., & Jana, S. (2018). Efficient formal safety analysis of neural networks. Advances in neural information processing systems, 31.;4_dl_testing_deep_network;2018;Efficient formal safety analysis of neural networks;Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, Suman Jana;Advances in neural information processing systems 31, 2018;Neural networks are increasingly deployed in real-world safety-critical domains such as autonomous driving, aircraft collision avoidance, and malware detection. However, these networks have been shown to often mispredict on inputs with minor adversarial or even accidental perturbations. Consequences of such errors can be disastrous and even potentially fatal as shown by the recent Tesla autopilot crash. Thus, there is an urgent need for formal analysis systems that can rigorously check neural networks for violations of different safety properties such as robustness against adversarial perturbations within a certain L-norm of a given image. An effective safety analysis system for a neural network must be able to either ensure that a safety property is satisfied by the network or find a counterexample, ie, an input for which the network will violate the property. Unfortunately, most existing techniques for performing such analysis struggle to scale beyond very small networks and the ones that can scale to larger networks suffer from high false positives and cannot produce concrete counterexamples in case of a property violation. In this paper, we present a new efficient approach for rigorously checking different safety properties of neural networks that significantly outperforms existing approaches by multiple orders of magnitude. Our approach can check different safety properties and find concrete counterexamples for networks that are 10x larger than the ones supported by existing analysis techniques. We believe that our approach to estimating tight output bounds of a network for a given input range can also help improve the explainability of neural networks and guide the training process of more robust neural networks.;https://proceedings.neurips.cc/paper/7873-efficient-formal-safety-analysis-of-neural-networks;h3ZDlXtbsPkJ
Clifton, D. A., Gibbons, J., Davies, J., & Tarassenko, L. (2012, June). Machine learning and software engineering in health informatics. In 2012 first international workshop on realizing ai synergies in software engineering (raise) (pp. 37-41). IEEE.;1_ml_machine_data_learning;2012;Machine learning and software engineering in health informatics;David A Clifton, Jeremy Gibbons, Jim Davies, Lionel Tarassenko;2012 first international workshop on realizing ai synergies in software engineering (raise), 37-41, 2012;Health informatics is a field in which the disciplines of software engineering and machine learning necessarily co-exist. This discussion paper considers the interaction of software engineering and machine learning, set within the context of health informatics, where the scale of clinical practice requires new engineering approaches from both disciplines. We introduce applications implemented in large on-going research programmes undertaken between the Departments of Engineering Science and Computer Science at Oxford University, the Oxford University Hospitals NHS Trust, and the Guy's and St Thomas' NHS Foundation Trust, London.;https://ieeexplore.ieee.org/abstract/document/6227968/;C5nj1QTMu4AJ
Kostova, B., Gurses, S., & Wegmann, A. (2020, March). On the Interplay between Requirements, Engineering, and Artificial Intelligence. In REFSQ Workshops.;1_ml_machine_data_learning;2020;On the Interplay between Requirements, Engineering, and Artificial Intelligence.;Blagovesta Kostova, Seda Gurses, Alain Wegmann;REFSQ Workshops, 2020;With this paper, we present our reflections on the issues that are faced by the Requirements Engineering academic discipline and practice. The current reality of the Artificial Intelligence and Machine Learning hype penetrating from research into all industry sectors and all phases of system design and development is a transformative shift that influences the way Requirements Engineering is conducted and the nature of the systems that are engineered. We identify two sides of this transformation with regards to the Requirements Engineering discipline:(1) Artificial Intelligence tools are used more and more during the Requirements Engineering process,(2) the Requirements Engineering process for systems that include Artificial Intelligence is different. By identifying and framing these changes, we pose questions about what it means to engineer requirements. Our analysis asks more questions than it answers. We hope to engage the Requirements Engineering academic community in a larger conversation about the role of Requirements Engineering in the changing world and about a possible new vision of engineering becoming secondary to requirements in Requirements Engineering.;https://ceur-ws.org/Vol-2584/RE4AI-paper7.pdf;R8n9hai5l84J
Dutta, S., Legunsen, O., Huang, Z., & Misailovic, S. (2018, October). Testing probabilistic programming systems. In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 574-586).;10_testing_test_machine_metamorphic;2018;Testing probabilistic programming systems;Saikat Dutta, Owolabi Legunsen, Zixin Huang, Sasa Misailovic;Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 574-586, 2018;Probabilistic programming systems (PP systems) allow developers to model stochastic phenomena and perform efficient inference on the models. The number and adoption of probabilistic programming systems is growing significantly. However, there is no prior study of bugs in these systems and no methodology for systematically testing PP systems. Yet, testing PP systems is highly non-trivial, especially when they perform approximate inference. In this paper, we characterize 118 previously reported bugs in three open-source PP systems—Edward, Pyro and Stan—and pro- pose ProbFuzz, an extensible system for testing PP systems. Prob- Fuzz allows a developer to specify templates of probabilistic models, from which it generates concrete probabilistic programs and data for testing. ProbFuzz uses language-specific translators to generate these concrete programs, which use the APIs of each PP system. ProbFuzz finds potential bugs by checking the output from running the generated programs against several oracles, including an accu- racy checker. Using ProbFuzz, we found 67 previously unknown bugs in recent versions of these PP systems. Developers already accepted 51 bug fixes that we submitted to the three PP systems, and their underlying systems, PyTorch and TensorFlow.;https://dl.acm.org/doi/abs/10.1145/3236024.3236057;dEpmri5AJLMJ
Evtimov, I., Eykholt, K., Fernandes, E., Kohno, T., Li, B., Prakash, A., ... & Song, D. (2017). Robust physical-world attacks on machine learning models. arXiv preprint arXiv:1707.08945, 2(3), 4.;5_adversarial_attack_example_model;2017;"Robust Physical-World Attacks
on Machine Learning Models";Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, and Dawn Song;Arxiv;Deep neural network-based classifiers are known to be vulnerable to adversarial examples that can fool them into misclassifying their input through the addition of small-magnitude perturbations. However, recent studies have demonstrated that such adversarial examples are not very effective in the physical world—they either completely fail to cause misclassification or only work in restricted cases where a relatively complex image is perturbed and printed on paper. In this paper we propose a new attack algorithm—Robust Physical Perturbations (RP2)—that generates perturbations by taking images under different conditions into account. Our algorithm can create spatiallyconstrained perturbations that mimic vandalism or art to reduce the likelihood of detection by a casual observer. We show that adversarial examples generated by RP2 achieve high success rates under various conditions for real road sign recognition by using an evaluation methodology that captures physical world conditions. We physically realized and evaluated two attacks, one that causes a Stop sign to be misclassified as a Speed Limit sign in 100% of the testing conditions, and one that causes a Right Turn sign to be misclassified as either a Stop or Added Lane sign in 100% of the testing conditions.;https://s3.observador.pt/wp-content/uploads/2017/08/08133934/1707-08945.pdf;Todo
